ai_index,ai_title,ai_abstract,ai_conference,ai_domain,ai_model,match_found,human_index,human_title,human_abstract,human_venue,human_overall_score,human_accepted,similarity_score,ai_processed_text,human_processed_text,generated_at
0,Hyper-Riemannian Optimization: Learning Intrinsic Gradient Geometries for Faster Convergence,"Standard deep learning optimizers like Adam operate under a fixed, often Euclidean, geometric assumption, which is misaligned with the complex, curved loss landscapes of neural networks. This mismatch can slow convergence and lead to suboptimal solutions. We introduce Hyper-Riemannian Optimization (HRO), a novel method that dynamically learns the local geometry of the loss surface. HRO employs a small hypernetwork that predicts a low-rank Riemannian metric tensor conditioned on the current model parameters. This allows for efficient, on-the-fly computation of the natural gradient, guiding updates along the path of steepest descent on the underlying statistical manifold. Our core contributions are: (1) a framework for learning task-specific, dynamic Riemannian metrics for optimization; (2) a computationally efficient implementation using low-rank approximations; and (3) theoretical analysis connecting our approach to second-order methods. Empirically, HRO demonstrates significantly faster convergence and achieves superior final performance compared to AdamW and other state-of-the-art optimizers on large-scale Transformer language models and vision transformers on ImageNet. Our work opens a new direction in optimization by treating the geometry of the loss landscape as a learnable component of the training process.",ICLR,deep learning,gemini-2.5-pro,True,1005,Optimizing Learning for Robust Hyperbolic Deep Learning in Computer Vision,"Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the  data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on improving stability for curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across direct classification, generation, and hierarchical metric learning tasks while allowing for larger hyperbolic models.",ICLR.cc/2025/Conference,4.4,False,0.8668,standard deep learning optimizers like adam operate under fixed often euclidean geometric assumption which misaligned the complex curved loss landscapes neural networks hyper riemannian optimization hro that dynamically learns the local geometry the loss surface our core contributions are for learning task specific dynamic riemannian metrics for optimization computationally efficient implementation low rank approximations and theoretical analysis connecting our second order methods empirically hro demonstrates faster convergence and achieves superior final compared adamw and other state the art optimizers large scale transformer language models and vision transformers imagenet our opens direction optimization treating the geometry the loss landscape learnable component the training process,hyperbolic deep learning has become growing direction computer vision for the unique properties afforded the alternate embedding space current riemannian optimizers not account for changes the manifold which greatly harms and forces lower learning rates minimize projection errors our focuses improving stability for curvature learning introducing improved schema for popular learning algorithms and providing normalization constrain embeddings within the variable representative radius the manifold additionally formulation for riemannian adamw and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations greatly reducing the computational penalty the hyperbolic embedding space our demonstrates consistent improvements across direct classification generation and hierarchical learning tasks while allowing for larger hyperbolic models,2025-08-26T00:48:30.450789
1,Latent Trajectory Models: Denoising in a Learned Subspace for Accelerated Generative Diffusion,"Denoising diffusion models have achieved state-of-the-art results in generative modeling but suffer from slow sampling speeds due to the necessity of performing thousands of iterative steps in a high-dimensional data space. We propose Latent Trajectory Models (LTMs), a new class of generative models that significantly accelerate sampling by performing the diffusion process in a compressed, structured latent space. An autoencoder first maps data into a low-dimensional manifold. The diffusion process then operates entirely within this space, where a novel denoising network is trained to predict not just the next step, but the entire future trajectory of the denoising process, effectively reducing the required number of sampling steps. Our key contributions are: (1) a framework for performing diffusion in a learned latent space, decoupling generative complexity from data dimensionality; (2) a trajectory-aware denoising architecture that learns the dynamics of the reverse process; and (3) a stable training objective for the joint autoencoder-diffusion system. LTMs achieve sample quality comparable to traditional diffusion models on datasets like CelebA-HQ and LSUN, while requiring 10-20x fewer sampling steps, making high-fidelity generative modeling more practical for real-world applications.",ICLR,deep learning,gemini-2.5-pro,True,11065,Decouple-Then-Merge: Towards Better Training for Diffusion Models,"Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a $\textbf{De}$couple-then-$\textbf{Me}$rge ($\textbf{DeMe}$) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is included in the supplementary material and will be released on Github.",ICLR.cc/2025/Conference,4.2,nan,0.8458,the diffusion process then operates entirely within this space where denoising network trained predict not just the next step but the entire future trajectory the denoising process reducing the required number sampling steps,diffusion models are trained learning sequence models that reverse each step noise corruption however since the denoising tasks differ each timestep the gradients computed different timesteps may conflict potentially degrading the overall image generation several improved techniques during the finetuning stage promote effective knowledge sharing while minimizing training interference across timesteps experimental significant generation quality improvements upon benchmarks including stable diffusion coco30k imagenet1k partiprompts and ddpm lsun church lsun bedroom and cifar10,2025-08-26T00:48:30.450822
2,Beyond Flatness: Generalization is Governed by the Curvature-Dimension Spectrum of the Loss Landscape,"The prevailing belief that ""flat"" minima generalize well offers a useful heuristic but lacks a precise, quantitative foundation that can explain complex phenomena in deep learning. We argue that a single scalar value for flatness is insufficient. This paper introduces the Curvature-Dimension Spectrum (CDS), a new theoretical framework for understanding generalization. We posit that generalization is determined not by the average curvature, but by the entire eigenvalue distribution of the Hessian at the solution, which characterizes the ""effective dimensionality"" of the function space accessible to the model. Using tools from random matrix theory, we analyze the CDS and derive a novel PAC-Bayes generalization bound that explicitly depends on the spectral decay rate. Our contributions include: (1) the formalization of the CDS as a more descriptive alternative to sharpness; (2) a new generalization bound that leverages this spectral information; and (3) extensive empirical validation. We show that the CDS accurately predicts test error across various architectures, optimizers, and regularization schemes, successfully explaining cases where sharpness-based measures fail. This work provides a more nuanced and powerful lens for analyzing the mechanisms of generalization in deep neural networks.",ICLR,deep learning,gemini-2.5-pro,True,189,Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD,"Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD’s generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called ""omniscient trajectory"". When applied to Gradient Descent’s minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds’ $\Omega(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs. Codes are available at [https://github.com/peng-ze/omniscient-bounds](https://github.com/peng-ze/omniscient-bounds).",ICLR.cc/2025/Conference,7.0,True,0.8262,the prevailing belief that flat minima generalize well offers useful heuristic but lacks precise quantitative foundation that can explain complex phenomena deep learning this provides more nuanced and powerful lens for analyzing the mechanisms generalization deep neural networks,information theoretic generalization bounds have been used the generalization learning algorithms experiments deep neural networks our bound not only correctly reflects the better generalization when flatness improved but also numerically much tighter,2025-08-26T00:48:30.450843
3,Causal Policy Regularization for Robust Offline Reinforcement Learning,"A primary challenge in offline reinforcement learning (RL) is mitigating distributional shift, where the learned policy deviates from the behavior distribution of the static dataset, leading to catastrophic out-of-distribution value estimates. Existing methods employ uncertainty-based or policy-constraint regularization, which can be overly conservative and fail to distinguish spurious correlations from causal relationships. We introduce Causal Policy Regularization (CPR), a novel approach that frames offline RL as a causal inference problem. By modeling the underlying structural causal model of the environment, CPR penalizes policies for relying on non-causal, spurious features in the state space while allowing for safe generalization along causally-valid action pathways. This is achieved by learning a disentangled representation that isolates causally salient factors and regularizing the policy's sensitivity to spurious ones. Our contributions are: (1) a new causal framework for offline RL; (2) the CPR algorithm, which promotes learning policies that are robust to confounders; and (3) empirical results on the D4RL benchmark demonstrating that CPR significantly outperforms state-of-the-art methods, particularly in environments with strong spurious correlations, leading to more robust and generalizable policies.",ICLR,deep learning,gemini-2.5-pro,True,1861,RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning,"In offline reinforcement learning (RL), managing the distribution shift between the learned policy and the static offline dataset is a persistent challenge that can result in overestimated values and suboptimal policies. Traditional offline RL methods address this by introducing conservative biases that limit exploration to well-understood regions, but they often overly restrict the agent's generalization capabilities. Recent work has sought to generate trajectories using generative models to augment the offline dataset, yet these methods still struggle with overestimating synthesized data, especially when out-of-distribution samples are produced. To overcome this issue, we propose RTDiff, a novel diffusion-based data augmentation technique that synthesizes trajectories *in reverse*, moving from unknown to known states. Such reverse generation naturally mitigates the risk of overestimation by ensuring that the agent avoids planning through unknown states. Additionally, reverse trajectory synthesis allows us to generate longer, more informative trajectories that take full advantage of diffusion models' generative strengths while ensuring reliability. We further enhance RTDiff by introducing flexible trajectory length control and improving the efficiency of the generation process through noise management. Our empirical results show that RTDiff significantly improves the performance of several state-of-the-art offline RL algorithms across diverse environments, achieving consistent and superior results by effectively overcoming distribution shift.",ICLR.cc/2025/Conference,5.75,True,0.8650,primary challenge offline reinforcement learning mitigating distributional shift where the learned policy deviates from the behavior distribution the static leading catastrophic out distribution value estimates this achieved learning disentangled representation that isolates causally salient factors and regularizing the policy sensitivity spurious ones our contributions are causal for offline the cpr which promotes learning policies that are robust confounders and empirical the d4rl demonstrating that cpr outperforms state the art methods environments strong spurious correlations leading more robust and generalizable policies,offline reinforcement learning managing the distribution shift between the learned policy and the static offline persistent challenge that can overestimated values and suboptimal policies such reverse generation naturally mitigates the risk overestimation ensuring that the agent avoids planning unknown states further enhance rtdiff introducing flexible trajectory length control and improving the efficiency the generation process noise management,2025-08-26T00:48:30.450853
4,Dynamic Sparse Training with Synaptic Scaffolding,"Dynamic sparse training (DST) methods aim to train sparse neural networks from scratch, reducing computational costs. However, existing methods often suffer from ""representational collapse,"" where pruned neurons lose their ability to reintegrate into the network, and from unstable sparsity structures during training. We introduce Synaptic Scaffolding, a novel DST technique that addresses these issues. Synaptic Scaffolding maintains a ""scaffold"" of recently pruned connections in a dormant state. These connections retain a memory of their former weight values and can be rapidly re-activated if their reintroduction is predicted to significantly reduce the training loss, guided by a low-cost proxy gradient estimator. This allows the network to fluidly explore different sparsity masks without losing valuable learned information. Our contributions are: (1) the Synaptic Scaffolding algorithm for stable and effective DST; (2) a mechanism for preserving and efficiently re-activating pruned weights; and (3) a principled criterion for topological adaptation. We demonstrate that our method achieves state-of-the-art performance, enabling training of networks at extreme sparsity levels (e.g., 98%) from initialization, while matching or exceeding the accuracy of dense counterparts on ImageNet and BERT pre-training tasks.",ICLR,deep learning,gemini-2.5-pro,True,1993,Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness,"It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the ""de facto"" approach to train artificial neural networks if one would like to maximize their robustness against image corruption. In this paper, we question this general practice. Consequently, \textit{we claim that}, contrary to what is commonly thought, the Dynamic Sparse Training methods can consistently outperform Dense Training in terms of robustness accuracy, particularly if the efficiency aspect is not considered as a main objective (i.e., sparsity levels between 10\% and up to 50\%), without adding (or even reducing) resource cost. We validate our claim on two types of data, images and videos, using several traditional and modern deep learning architectures for computer vision and three widely studied Dynamic Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of Dynamic Sparse Training and open new possibilities in improving deep learning robustness beyond the current state of the art.",ICLR.cc/2025/Conference,5.75,True,0.8148,dynamic sparse training dst methods aim train sparse neural networks from scratch reducing computational costs this allows the network fluidly different sparsity masks losing valuable learned information our contributions are the synaptic scaffolding for stable and effective dst mechanism for preserving and activating pruned weights and principled criterion for topological adaptation,perceived that dynamic sparse training opens the door era scalability and efficiency for artificial neural networks perhaps some costs for the classification task the same time dense training accepted being the facto train artificial neural networks one would like maximize their robustness against image corruption consequently textit claim that contrary what thought the dynamic sparse training methods can consistently outperform dense training terms robustness the efficiency aspect not considered main objective our claim two types data images and videos several traditional and modern deep learning architectures for computer vision and three studied dynamic sparse training algorithms our reveal yet unknown benefit dynamic sparse training and open possibilities improving deep learning robustness beyond the current state the art,2025-08-26T00:48:30.450860
5,Composable Inductive Biases: Learning to Select and Fuse GNN Architectures,"The design of Graph Neural Network (GNN) architectures involves choosing specific message-passing, aggregation, and update functions, each encoding a different inductive bias (e.g., homophily, heterophily, structural equivalence). This ""no free lunch"" problem means no single GNN architecture is optimal for all graph structures and tasks. We propose Composable Inductive Biases (CIB), a meta-learning framework that learns to dynamically select and fuse different GNN architectures at a per-node level. CIB features a modular base library of diverse GNN layers (e.g., GCN, GAT, GraphSAGE) and a hyper-network that learns to predict a soft combination weighting over these modules for each node, based on its local neighborhood topology and features. This allows the model to apply GCN-like smoothing in homophilous regions while using GAT-like attention in complex, noisy regions of the same graph. Our contributions are: (1) a framework for learning node-adaptive GNN architectures; (2) a scalable hyper-network for predicting architectural compositions; and (3) state-of-the-art results on a suite of challenging graph datasets with heterogeneous structures, demonstrating superior adaptability compared to any single fixed GNN architecture.",ICLR,deep learning,gemini-2.5-pro,True,9000,Graph Neural Network Is A Mean Field Game,"In current graph neural networks (GNNs), it is a common practice to apply a pre-defined message passing heuristics to all graph data, even though the stereotypical relational inductive bias (e.g., graph heat diffusion) might not fit the unseen graph topology. Such gross simplification might be responsible for the lack of an in-depth understanding of graph learning principles, which challenges us to push the boundary from crafting application-specific GNNs to embracing a ""meta-learning"" paradigm. In this work, we ratchet the gear of GNN another notch forward by formulating GNN as a *mean field game*, that is, the best learning outcome occurs at the *Nash*-equilibrium when the learned graph inference rationale allows each graph node to find what is the best feature representations for not only the individual node but also the entire graph. Following this spirit, we formulate the search for novel GNN mechanism into a variational framework of *mean-field control* (MFC) problem, where the optimal relational inductive bias is essentially the critical point of mean-field information dynamics. Specifically, we seek for the best characteristic MFC functions of transportation mobility (controlling information exchange throughout the graph) and reaction mobility (controlling feature representation learning on each node), on the fly, that uncover the most suitable learning mechanism for a GNN instance by solving an MFC variational problem through the lens of *Hamiltonian flows* (formed in partial differential equations). In this context, our variational framework brings together existing GNN models into various mean-field games with distinct equilibrium states, each characterized by a unique MFC functional. Furthermore, we present an agnostic end-to-end deep model, coined *Nash-GNN* (in honor of Nobel laureate Dr. John Nash), to jointly carve the nature of the inductive bias and fine-tune the GNN hyper-parameters on top of the elucidated learning mechanism. *Nash-GNN* has achieved SOTA performance on diverse graph data including popular benchmark datasets and human connectomes. More importantly, the mathematical insight of mean-field games provides a new window to understand the foundational principles of graph learning as an interactive dynamical system, which allows us to reshape the idea of designing next-generation GNN models.",ICLR.cc/2025/Conference,5.333333333333333,nan,0.8680,the graph neural network gnn architectures involves choosing specific message passing aggregation and update functions each encoding different inductive bias this allows the apply gcn like smoothing homophilous regions while gat like attention complex noisy regions the same graph our contributions are for learning node adaptive gnn architectures scalable hyper network for predicting architectural compositions and state the art suite challenging graph datasets heterogeneous structures demonstrating superior adaptability compared any single fixed gnn,current graph neural networks gnns common practice apply pre defined message passing heuristics all graph data even though the stereotypical relational inductive bias such gross simplification might responsible for the lack depth understanding graph learning principles which challenges push the boundary from crafting application specific gnns embracing meta learning paradigm this ratchet the gear gnn another notch forward formulating gnn mean field game that the best learning outcome occurs the nash equilibrium when the learned graph inference rationale allows each graph node find what the best feature representations for not only the individual node but also the entire graph seek for the best characteristic mfc functions transportation mobility controlling information exchange throughout the graph and reaction mobility controlling feature representation learning each node the fly that uncover the most suitable learning mechanism for gnn instance solving mfc variational problem the lens hamiltonian flows formed partial differential equations furthermore agnostic end end deep coined nash gnn honor nobel laureate john nash jointly carve the nature the inductive bias and fine tune the gnn hyper parameters top the elucidated learning mechanism more importantly the mathematical insight mean field games provides window understand the foundational principles graph learning interactive dynamical which allows reshape the idea designing next generation gnn models,2025-08-26T00:48:30.450869
6,Continual Pre-training: A Framework for Language Models that Never Stop Learning,"Large language models (LLMs) are typically pre-trained on a massive, static dataset and then fine-tuned. This paradigm struggles with new information and evolving language, leading to knowledge cutoffs and model staleness. We propose Continual Pre-training (CPT), a framework that enables language models to efficiently and perpetually learn from a continuous stream of new data without catastrophic forgetting. CPT combines a dynamic memory architecture with an elasticity-based regularization scheme. New information is first encoded into a rapidly-updated, non-parametric memory, while an EWC-inspired (Elastic Weight Consolidation) regularizer selectively consolidates core knowledge into the parametric weights of the transformer, preserving foundational language capabilities. We introduce a ""knowledge novelty detector"" to gate what information triggers a consolidation update. Our contributions are: (1) a scalable framework for continual pre-training of LLMs; (2) a hybrid parametric-non-parametric memory system; and (3) a novelty-gated consolidation strategy. We demonstrate that a CPT-trained model can continuously assimilate new information from daily news streams over months, consistently outperforming statically pre-trained models on time-sensitive question-answering tasks while maintaining performance on general language benchmarks.",ICLR,deep learning,gemini-2.5-pro,True,4155,TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models,"Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a world where the latest dump of Common Crawl (CC), the most prominent public source of pre-training data, is used every month to *continually* train an LLM. We design various dynamic evaluations from the CC data, Wikipedia, StackExchange, and code documentations to measure continual learning metrics such as forgetting and forward transfer. Notably, our TiC-CC training data is more than 100 times larger compared with prior continual learning benchmarks for language modeling. We discover that recent DataComp-LM models trained on data before 2023 have already become outdated, incurring up to 45\% larger noun-perplexity on 2024 Wikipedia articles compared to pre-2023 articles. Further, we use our setup to evaluate the effectiveness of several large-scale continual learning methods and find that replaying older data is most effective for combating forgetting: for previously seen CC dumps, it can reduce the regret on held-out loss by 60\% compared to other optimizer and loss-based interventions. However, some domains evolve more quickly than others, favoring different trade-offs between mixing old and new data.",ICLR.cc/2025/Conference,6.25,False,0.9134,large language models llms are pre trained massive static and then fine tuned this paradigm struggles information and evolving language leading knowledge cutoffs and staleness continual pre training cpt that enables language models and perpetually learn from continuous stream data catastrophic forgetting information first encoded into rapidly updated non parametric memory while ewc inspired elastic weight consolidation regularizer selectively consolidates core knowledge into the parametric weights the transformer preserving foundational language capabilities that cpt trained can continuously assimilate information from daily news streams over months consistently outperforming statically pre trained models time sensitive question answering tasks while maintaining general language benchmarks,large language models llms are trained data crawled over many years from the web various dynamic evaluations from the data wikipedia stackexchange and code documentations measure continual learning metrics such forgetting and forward transfer notably our tic training data more than times larger compared prior continual learning benchmarks for language modeling further use our setup the effectiveness several large scale continual learning methods and find that replaying older data most effective for combating forgetting for previously seen dumps can reduce the regret held out loss compared other optimizer and loss based interventions,2025-08-26T00:48:30.450879
7,Flow-based State Space Models for Irregularly Sampled Time Series,"Real-world time series data, particularly from domains like healthcare and finance, are often sampled irregularly and contain missing values. Traditional models like RNNs and Transformers struggle with this irregularity, often requiring ad-hoc imputation or bucketing. We introduce Flow-based State Space Models (FSSM), a new class of deep learning models designed explicitly for irregularly sampled time series. FSSM represents the latent state dynamics as a continuous-time process governed by a neural ordinary differential equation (Neural ODE). Crucially, we model the evolution of the distribution over the latent state using a continuous normalizing flow, allowing us to precisely compute the exact likelihood of observed data points at any point in time, regardless of sampling intervals. Our contributions include: (1) a novel architecture combining Neural ODEs with continuous normalizing flows for time series; (2) a method for exact likelihood computation for irregularly sampled data; and (3) a principled approach to uncertainty quantification. FSSM achieves state-of-the-art results on challenging medical (MIMIC-III) and financial forecasting benchmarks, outperforming both discrete-time models and prior continuous-time approaches.",ICLR,deep learning,gemini-2.5-pro,True,5731,HOPE for a Robust Parameterization of Long-memory State Space Models,"State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. To achieve state-of-the-art performance, an SSM often needs a specifically designed initialization, and the training of state matrices is on a logarithmic scale with a very small learning rate. To understand these choices from a unified perspective, we view SSMs through the lens of Hankel operator theory. Building upon it, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. Our approach helps improve the initialization and training stability, leading to a more robust parameterization. We efficiently implement these innovations by nonuniformly sampling the transfer functions of LTI systems, and they require fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, our new parameterization endows the SSM with non-decaying memory within a fixed time window, which is empirically corroborated by a sequential CIFAR-10 task with padded noise.",ICLR.cc/2025/Conference,6.6,True,0.8214,flow based state space models fssm class deep learning models designed explicitly for irregularly sampled time series fssm represents the latent state dynamics continuous time process governed neural ordinary differential equation neural ode our contributions include combining neural odes continuous normalizing flows for time series for exact likelihood computation for irregularly sampled data and principled uncertainty quantification,state space models ssms that utilize linear time invariant lti systems are known for their effectiveness learning long sequences achieve state the art ssm often needs designed initialization and the training state matrices logarithmic scale very small learning rate these innovations nonuniformly sampling the transfer functions lti systems and they require fewer parameters compared canonical ssms,2025-08-26T00:48:30.450891
8,Geometric Disentanglement via the Lie Group VAE,"Learning disentangled representations, where distinct latent variables correspond to independent factors of data variation, is a central goal of representation learning. Current methods often struggle to disentangle factors that correspond to geometric transformations (e.g., rotation, scaling) and rely on heuristics. We introduce the Lie Group VAE (LG-VAE), a variational autoencoder architecture that leverages the mathematical structure of Lie groups to achieve principled geometric disentanglement. The LG-VAE's latent space is structured as a direct product of a Lie group and a Euclidean space. The decoder is explicitly designed to be equivariant to the action of the learned group on the latent variables, forcing the model to map geometric transformations in the data space to algebraic operations within the learned Lie group representation. Our contributions are: (1) a novel VAE architecture with a structured Lie group latent space; (2) an equivariant decoder that enforces geometric consistency; and (3) a training objective that learns both the group structure and the disentangled representation simultaneously. We demonstrate that the LG-VAE successfully discovers and disentangles transformations like rotations and translations in 2D and 3D datasets without explicit supervision, achieving superior disentanglement scores and enabling controllable data generation.",ICLR,deep learning,gemini-2.5-pro,True,2235,Interaction Asymmetry: A General Principle for Learning Composable Abstractions,"Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: ""Parts of the same concept have more complex interactions than parts of different concepts"". We formalize this via block diagonality conditions on the $(n+1)$th order derivatives of the generator mapping concepts to observed data, where different orders of ""complexity"" correspond to different $n$. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with $n=0$ or $1$. We provide results for up to $n=2$, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger $n$. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.",ICLR.cc/2025/Conference,7.0,True,0.8597,learning disentangled representations where distinct latent variables correspond independent factors data variation central goal representation learning the decoder explicitly designed equivariant the action the learned group the latent variables forcing the map geometric transformations the data space algebraic operations within the learned lie group representation our contributions are vae structured lie group latent space equivariant decoder that enforces geometric consistency and training objective that learns both the group structure and the disentangled representation simultaneously that the vae discovers and disentangles transformations like rotations and translations and datasets explicit supervision achieving superior disentanglement scores and enabling controllable data generation,learning disentangled representations concepts and composing them unseen ways crucial for generalizing out domain situations our unify recent theoretical for learning concepts objects which are recovered special cases implementation these criteria flexible transformer based vae regularizer the attention weights the decoder,2025-08-26T00:48:30.450900
9,Test-Time Training with Self-Supervised Consistency for Domain Adaptation,"Domain shift, the discrepancy between training and testing data distributions, poses a significant challenge for deploying machine learning models. Test-time adaptation methods aim to update a source-trained model using only unlabeled test data. We propose Test-Time Training with Self-Supervised Consistency (T3SC), a novel approach that adapts models by enforcing geometric and semantic consistency in their representation space. During testing, for each incoming batch, T3SC generates augmented views of the data and enforces a self-supervised consistency loss, compelling the model to produce invariant representations for semantically identical inputs under different perturbations. Unlike methods that rely on entropy minimization, which can reinforce incorrect predictions, our consistency-based objective is more robust to noisy pseudo-labels. Our contributions are: (1) a new test-time adaptation objective based on self-supervised consistency; (2) a framework that requires no modification to the original training pipeline; and (3) a lightweight adaptation procedure with minimal computational overhead. T3SC achieves state-of-the-art results on standard domain adaptation benchmarks like ImageNet-C and VisDA-C, significantly improving model robustness to real-world corruptions and shifts.",ICLR,deep learning,gemini-2.5-pro,True,483,Binary-Feedback Active Test-Time Adaptation,"Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To
address this issue, we introduce a Binary-feedback Active Test-Time Adaptation (BATTA) setting, which uses a few binary feedbacks from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BATTA-RL, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BATTA-RL achieves substantial accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort.",ICLR.cc/2025/Conference,6.0,False,0.8501,domain shift the discrepancy between training and testing data distributions poses significant challenge for deploying machine learning models test time adaptation methods aim update source trained only unlabeled data test time training self supervised consistency t3sc that adapts models enforcing geometric and semantic consistency their representation space our contributions are test time adaptation objective self supervised consistency that requires modification the original training pipeline and lightweight adaptation procedure minimal computational overhead t3sc achieves state the art standard domain adaptation benchmarks like imagenet and visda improving robustness real world corruptions and shifts,deep learning models perform poorly when domain shifts exist between training and data test time adaptation tta paradigm mitigate this issue adapting pre trained models only unlabeled samples however existing tta methods can fail under severe domain shifts while recent active tta approaches requiring full class labels are impractical due high labeling costs address this issue binary feedback active test time adaptation batta setting which uses few binary feedbacks from annotators indicate whether predictions are correct thereby reducing the labeling burden annotators under the setting batta dual path optimization that leverages reinforcement learning balance binary feedback guided adaptation uncertain samples agreement based self adaptation confident predictions,2025-08-26T00:48:30.450905
10,Neuro-Symbolic Inductive Logic Programming for Relational Reasoning,"Deep learning models, particularly Graph Neural Networks, have shown promise in relational reasoning but often struggle with tasks requiring multi-hop, explicit logical deduction and fail to produce interpretable reasoning chains. We introduce Neuro-Symbolic Inductive Logic Programming (NS-ILP), a framework that integrates the expressive power of deep relational embeddings with the formal rigor of Inductive Logic Programming (ILP). NS-ILP learns continuous vector representations of predicates and entities, then uses a differentiable ILP engine to search for logical rules that best explain the relationships in the training data. The key innovation is a differentiable proof mechanism that allows end-to-end training, enabling the neural component to learn representations that are amenable to symbolic reasoning, and the symbolic component to find rules grounded in the learned embeddings. Our contributions are: (1) a fully differentiable integration of ILP with neural networks; (2) a model that produces explicit, human-readable logical rules as part of its output; and (3) superior performance on complex relational reasoning benchmarks like CLUTRR and path-finding queries on knowledge graphs, demonstrating improved generalization and interpretability.",ICLR,deep learning,gemini-2.5-pro,True,98,Systematic Relational Reasoning With Epistemic Graph Neural Networks,"Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work has shown that regular GNNs lack the ability to systematically generalize from training examples on test graphs requiring longer inference chains, which fundamentally limits their reasoning abilities. A common solution relies on neuro-symbolic methods that systematically reason by learning rules, but their scalability is often limited and they tend to make unrealistically strong assumptions, e.g.\ that the answer can always be inferred from a single relational path. We propose the Epistemic GNN (EpiGNN), a novel parameter-efficient and scalable GNN architecture with an epistemic inductive bias for systematic reasoning. Node embeddings in EpiGNNs are treated as epistemic states, and message passing  is implemented accordingly. We show that EpiGNNs achieve state-of-the-art results on link prediction tasks that require systematic reasoning. Furthermore, for inductive knowledge graph completion, EpiGNNs rival the performance of state-of-the-art specialized approaches. Finally, we introduce two new benchmarks that go beyond standard relational reasoning by requiring the aggregation of information from multiple paths. Here, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason accurately.  Code and datasets are available at https://github.com/erg0dic/gnn-sg.",ICLR.cc/2025/Conference,6.5,True,0.8721,deep learning models graph neural networks have shown promise relational reasoning but often struggle tasks requiring multi hop explicit logical deduction and fail produce interpretable reasoning chains neuro symbolic inductive logic programming ilp that integrates the expressive power deep relational embeddings the formal rigor inductive logic programming ilp the key innovation differentiable proof mechanism that allows end end training enabling the neural component learn representations that are amenable symbolic reasoning and the symbolic component find rules grounded the learned embeddings our contributions are fully differentiable integration ilp neural networks that produces explicit human readable logical rules part its output and superior complex relational reasoning benchmarks like clutrr and path finding queries knowledge graphs demonstrating improved generalization and interpretability,focus reasoning relational domains where the use graph neural networks gnns seems like natural choice however previous has shown that regular gnns lack the ability systematically generalize from training examples graphs requiring longer inference chains which fundamentally limits their reasoning abilities common solution relies neuro symbolic methods that systematically reason learning rules but their scalability often limited and they tend make unrealistically strong assumptions the epistemic gnn epignn parameter efficient and scalable gnn epistemic inductive bias for systematic reasoning that epignns achieve state the art link prediction tasks that require systematic reasoning furthermore for inductive knowledge graph completion epignns rival the state the art specialized approaches finally two benchmarks that beyond standard relational reasoning requiring the aggregation information from multiple paths,2025-08-26T00:48:30.450910
11,Deconstructing Transformers: A Spectral Analysis of Self-Attention,"While the Transformer architecture has become ubiquitous, our theoretical understanding of its core component, the self-attention mechanism, remains limited. This paper provides a novel spectral analysis of the self-attention operator. We demonstrate that the attention matrix can be interpreted as the adjacency matrix of a dynamically constructed graph, and we analyze its spectral properties (eigenvalues and eigenvectors). We prove that the dot-product attention mechanism implicitly promotes a low-rank structure in the attention matrix, effectively acting as a spectral filter that amplifies dominant patterns in the token-token affinity space. Furthermore, we show how LayerNorm and residual connections interact with this spectral filtering, stabilizing the spectrum and preventing ""attention collapse"" to a rank-1 matrix. Our contributions are: (1) a new theoretical framework for analyzing self-attention through a spectral lens; (2) a formal proof of the low-rank bias of the attention mechanism; and (3) insights into the role of other architectural components in shaping the attention spectrum. This analysis provides a deeper understanding of how Transformers process information and opens new avenues for designing more efficient and principled attention mechanisms.",ICLR,deep learning,gemini-2.5-pro,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8820,while the transformer has become ubiquitous our theoretical understanding its core component the self attention mechanism remains limited that the attention matrix can interpreted the adjacency matrix dynamically constructed graph and its spectral properties eigenvalues and eigenvectors that the dot product attention mechanism implicitly promotes low rank structure the attention matrix acting spectral filter that amplifies dominant patterns the token token affinity space our contributions are theoretical for analyzing self attention spectral lens formal proof the low rank bias the attention mechanism and insights into the role other architectural components shaping the attention spectrum this analysis provides deeper understanding how transformers process information and opens avenues for designing more efficient and principled attention mechanisms,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T00:48:30.450917
12,Privacy-Preserving Representations via Differentiable Oblivious Hashing,"Learning useful representations of data while preserving the privacy of sensitive attributes is a critical challenge. Existing methods often trade off utility and privacy, or require computationally expensive adversarial training. We propose Differentiable Oblivious Hashing (DOH), a new technique for learning privacy-preserving representations. DOH learns a projection of the input data into a binary hash space using a stochastic, differentiable hashing function. The training objective is twofold: a utility term ensures the hashes are useful for a downstream task, while a novel privacy term, based on the collision entropy of hashes for inputs with different sensitive attributes, forces the model to map them to distinct, non-predictable hash buckets. This ""oblivious"" mapping makes it difficult for an adversary to infer the sensitive attribute from the hash code. Our contributions are: (1) the DOH framework for learning privatized binary representations; (2) a differentiable hashing mechanism suitable for end-to-end training; and (3) a collision-based privacy objective. We demonstrate on benchmark datasets that DOH achieves a superior privacy-utility trade-off compared to adversarial methods and differential privacy approaches, offering a practical and efficient solution for creating anonymized yet functional data representations.",ICLR,deep learning,gemini-2.5-pro,False,,Privacy Preserving Generative Feature Transformation,"Data-Centric AI (DCAI) aims to use AI to get better data for better AI. Feature transformation, as one of the essential tasks of DCAI, can augment the data representation and has garnered significant attention. Existing methods have demonstrated state-of-the-art performance on advancing predictive tasks. However, these methods can lead to serious privacy leakage. For example, sensitive features in original data can be inferred by models trained on transformed data, exposing vulnerabilities in the privacy-preserving capabilities of these methods. To address this issue, we introduce a privacy-preserving feature transformation framework that transforms data representation while preserving privacy from a generative modeling perspective. Specifically, our framework includes two phases: 1) privacy-aware knowledge acquisition and 2) privacy-preserving feature space generation. In the knowledge acquisition phase, we develop an information bottlenecks guided reinforcement learning system to explore and collect privacy-aware feature sets as a knowledge base in token sequence form. In the feature space generation phase, we develop a generative model to encode the knowledge base into a privacy-aware latent space, where the best latent representation is identified and decoded into the optimal privacy-preserving feature space. We solve the optimization via projected gradient ascent that maximizes predictive performance and minimizes privacy exposure. Finally, we present extensive experiments on eight real-world datasets to evaluate how our method can navigate both performance and privacy. The code is available at https://anonymous.4open.science/r/anonymous-2B53/.",ICLR.cc/2025/Conference,3.5,nan,0.7990,learning useful representations data while preserving the privacy sensitive attributes critical challenge differentiable oblivious hashing doh for learning privacy preserving representations our contributions are the doh for learning privatized binary representations differentiable hashing mechanism suitable for end end training and collision based privacy objective,feature transformation one the essential tasks dcai can augment the data representation and has garnered significant attention address this issue privacy preserving feature transformation that transforms data representation while preserving privacy from generative modeling perspective our includes two phases privacy aware knowledge acquisition and privacy preserving feature space generation the knowledge acquisition phase information bottlenecks guided reinforcement learning and collect privacy aware feature sets knowledge base token sequence form the feature space generation phase generative encode the knowledge base into privacy aware latent space where the best latent representation identified and decoded into the optimal privacy preserving feature space solve the optimization projected gradient ascent that maximizes predictive and minimizes privacy exposure,2025-08-26T00:48:30.450925
13,Multi-Agent Reinforcement Learning with Action-Semantic Communication,"Effective communication is key to solving complex cooperative multi-agent reinforcement learning (MARL) problems. Existing communication protocols often involve learning arbitrary, uninterpretable message vectors, which can be sample-inefficient and hard to generalize. We introduce Action-Semantic Communication (ASC), a novel MARL communication protocol where messages are structured to represent proposals and commitments about future actions. Instead of exchanging latent vectors, agents broadcast messages from a discrete, learnable vocabulary, where each ""word"" is tied to a specific action policy or sub-goal. A message like ""I will cover region B"" is not just cheap talk; it is a commitment backed by a specific, executable policy from the agent's learned library. This grounds communication in the action space, making it more interpretable and sample-efficient. Our contributions are: (1) a new MARL communication framework based on action-semantic commitments; (2) a method for jointly learning a communication vocabulary and a library of corresponding policies; and (3) state-of-the-art performance on challenging cooperative tasks like StarCraft II and multi-agent particle environments, demonstrating faster coordination and better generalization to unseen agent configurations.",ICLR,deep learning,gemini-2.5-pro,True,10367,Human-like Communication Strategies for Improved Multi-Agent Reinforcement Learning,"Multi-Agent Reinforcement Learning (MARL) has seen significant progress in recent years, enabling multiple agents to coordinate and optimize their actions in complex environments. However, integrating effective communication protocols into MARL frameworks remains a challenge, as it introduces issues such as increased state space dimensionality, lack of stationarity, and the need for interpretability. Inspired by human communication, which relies on prior knowledge, contextual awareness, and efficient information exchange, we propose a novel framework for incorporating human-like communication strategies to enhance the learning process. Motivated by recent advancements in natural language processing (NLP), multi-modal AI and object detection, we use text-to-mask models and human feedback to learn compact and informative communication strategies that facilitate coordination among agents to improve the overall performance. We demonstrate the efficiency of our approach on various multi-agent tasks and provide insights into emergent communication behaviors observed during training.",ICLR.cc/2025/Conference,3.0,nan,0.9239,effective communication key solving complex cooperative multi agent reinforcement learning marl problems existing communication protocols often involve learning arbitrary uninterpretable message vectors which can sample inefficient and hard generalize our contributions are marl communication action semantic commitments for jointly learning communication vocabulary and library corresponding policies and state the art challenging cooperative tasks like starcraft and multi agent particle environments demonstrating faster coordination and better generalization unseen agent configurations,multi agent reinforcement learning marl has seen significant progress recent years enabling multiple agents coordinate and optimize their actions complex environments however integrating effective communication protocols into marl frameworks remains challenge introduces issues such increased state space dimensionality lack stationarity and the need for interpretability inspired human communication which relies prior knowledge contextual awareness and efficient information exchange for incorporating human like communication strategies enhance the learning process motivated recent advancements natural language processing nlp multi modal and object detection use text mask models and human feedback learn compact and informative communication strategies that facilitate coordination among agents improve the overall,2025-08-26T00:48:30.450932
14,Amortized Bayesian Model Comparison for Neural Network Architectures,"Choosing the right neural network architecture is a crucial, yet resource-intensive, part of the machine learning pipeline. Bayesian model comparison offers a principled way to perform this selection by approximating the marginal likelihood (or model evidence), but existing methods like nested sampling are computationally prohibitive for deep models. We introduce the Amortized Model Comparison Network (AMCN), a meta-learning approach that learns to rapidly approximate the log marginal likelihood for a given model architecture and dataset. The AMCN is a graph hypernetwork trained on a vast collection of synthetic datasets and their corresponding architecture-evidence pairs, pre-computed offline. Once trained, it can predict the evidence for a new architecture and dataset in a single forward pass, bypassing the need for expensive sampling. Our contributions are: (1) a framework for amortizing Bayesian model comparison for neural networks; (2) the AMCN architecture that maps architecture graphs to evidence scores; and (3) a demonstration of its effectiveness in neural architecture search (NAS), where it serves as a highly efficient and accurate performance predictor, discovering high-performing architectures orders of magnitude faster than traditional NAS methods.",ICLR,deep learning,gemini-2.5-pro,True,7586,Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks,"Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.",ICLR.cc/2025/Conference,5.75,True,0.8343,choosing the right neural network crucial yet resource intensive part the machine learning pipeline bayesian comparison offers principled way perform this selection approximating the marginal likelihood evidence but existing methods like nested sampling are computationally prohibitive for deep models the amortized comparison network amcn meta learning that learns rapidly approximate the log marginal likelihood for given and our contributions are for amortizing bayesian comparison for neural networks the amcn that maps graphs evidence scores and demonstration its effectiveness neural search nas where serves highly efficient and accurate predictor discovering high performing architectures orders magnitude faster than traditional nas methods,despite recent advances sampling based inference for bayesian neural networks bnns remains significant challenge probabilistic deep learning consequence sampling still requires longer inference times than non bayesian methods even for small neural networks despite recent advances making software implementations more efficient tackle these challenges ensembling that leverages strategies from optimization and recently proposed sampler called microcanonical langevin monte carlo mclmc for efficient robust and predictable sampling,2025-08-26T00:48:30.450940
15,Unsupervised Learning of 3D-Aware Representations from Single Images via Cross-View Cycle Consistency,"Learning 3D representations from 2D images without explicit 3D supervision is a challenging open problem. We propose Cross-View Cycle Consistency (CVCC), a novel self-supervised framework for learning 3D-aware representations from single, unposed images. Our model learns to decompose an image into a 3D-aware latent representation, which can then be re-rendered from a novel viewpoint. The core of our method lies in a cycle consistency objective: we generate a new view, and then require the model to re-render the original view from this generated image. This forces the latent representation to capture the true underlying 3D structure of the scene, as this is the only information that remains invariant across view changes. Unlike GAN-based approaches, CVCC does not require an adversarial discriminator and is trained with a simple reconstruction loss. Our contributions are: (1) a self-supervised learning objective based on cross-view cycle consistency; (2) an architecture that learns to generate novel views without explicit 3D data or camera poses; and (3) results showing that our learned representations capture rich 3D information, enabling tasks like single-image depth estimation and controllable novel view synthesis on datasets like CelebA and Cats.",ICLR,deep learning,gemini-2.5-pro,True,9186,Position-Query-Based Autoencoders for View Decoupled Cross Point Cloud Reconstruction and a Self-Supervised Learning Framework,"Point cloud learning, especially in a self-supervised way without manual labels, has received emerging attention in both vision and learning communities, with its potential utility in wide areas. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it could thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to achieve new state-of-the-art results and surpasses previous single-modal self-reconstruction methods in 3D self-supervised learning by a margin. Specifically, it outperforms self-reconstruction baseline (Point-MAE) 6.5\%, 7.0\%, 6.7\% in three variants of ScanObjectNN with Mlp-Linear evaluation protocol. Source code will be released.",ICLR.cc/2025/Conference,6.2,False,0.8510,learning representations from images explicit supervision challenging open problem cross view cycle consistency cvcc self supervised for learning aware representations from single unposed images this forces the latent representation capture the true underlying structure the scene this the only information that remains invariant across view changes our contributions are self supervised learning objective cross view cycle consistency that learns generate views explicit data camera poses and showing that our learned representations capture rich information enabling tasks like single image depth estimation and controllable view synthesis datasets like celeba and cats,point cloud learning self supervised way manual labels has received emerging attention both vision and learning communities its potential utility wide areas most existing generative approaches for point cloud self supervised learning focus recovering masked points from visible ones within single view inspired this the potential two view learning this domain achieve this goal crop mechanism for point cloud view generation for the first time and further positional encoding represent the relative position between the two decoupled views the cross reconstruction increases the difficulty pre training compared self reconstruction which enables our achieve state the art and surpasses previous single modal self reconstruction methods self supervised learning margin,2025-08-26T00:48:30.450947
16,A Unified Framework for Contrastive and Non-Contrastive Self-Supervised Learning,"Self-supervised learning (SSL) is dominated by two main paradigms: contrastive methods (e.g., SimCLR) which pull positive pairs together and push negative pairs apart, and non-contrastive methods (e.g., BYOL, SimSiam) which only pull positive pairs together, avoiding collapse through architectural tricks like stop-gradients or momentum encoders. The theoretical relationship between these two families is not well understood. We present a unified framework that views both contrastive and non-contrastive learning through the lens of information-theoretic principles. We show that both approaches can be seen as optimizing a lower bound on the mutual information between augmented views, but differ in their estimation of the negative term. Contrastive methods use explicit negative sampling, while non-contrastive methods use an implicit, batch-wide uniform negative distribution enforced by the architecture. Our key contributions are: (1) a single objective function that can instantiate both SimCLR and BYOL/SimSiam as special cases; (2) a theoretical analysis showing how architectural components in non-contrastive methods act as implicit regularizers; and (3) a new hybrid algorithm derived from our framework that outperforms both pure approaches on standard benchmarks. This work bridges the conceptual gap between dominant SSL paradigms.",ICLR,deep learning,gemini-2.5-pro,True,11317,Self-supervised contrastive learning performs non-linear system identification,"Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",ICLR.cc/2025/Conference,6.4,True,0.8420,self supervised learning ssl dominated two main paradigms contrastive methods unified that views both contrastive and non contrastive learning the lens information theoretic principles,self supervised learning ssl approaches have brought tremendous success across many tasks and domains has been argued that these successes can attributed link between ssl and identifiable representation learning temporal structure and auxiliary variables ensure that latent representations are related the true underlying generative factors the data,2025-08-26T00:48:30.450954
17,Learning Modular and Reusable Skills with a Compositional Policy Graph,"Hierarchical reinforcement learning aims to solve complex, long-horizon tasks by decomposing them into simpler sub-tasks or skills. However, current methods often learn monolithic, non-reusable skills. We introduce the Compositional Policy Graph (CPG), a framework for learning a library of modular and reusable skills that can be dynamically composed to solve new tasks. The CPG is a directed graph where nodes represent low-level policies (skills) and edges, managed by a high-level gating policy, represent feasible transitions between them. We train the entire graph end-to-end using an off-policy algorithm with a novel intrinsic reward structure that encourages each node to learn a distinct yet composable behavior. A key innovation is a graph modularity objective that promotes disentanglement, ensuring skills are self-contained and reusable. Our contributions are: (1) the CPG framework for learning libraries of composable skills; (2) a graph modularity objective for skill disentanglement; and (3) demonstrations on complex robotics manipulation and navigation tasks, where CPG learns interpretable and reusable skills that enable rapid zero-shot transfer to novel, unseen tasks composed of familiar sub-goals.",ICLR,deep learning,gemini-2.5-pro,True,521,DSR: Reinforcement Learning with Dynamical Skill Refinement,"Reinforcement learning with skills (RL with skills) is an efficient paradigm for solving sparse-reward tasks by extracting skills from demonstration datasets and learning high-level policy which selects skills. Because each selected skill by high-level policy is executed for multiple consecutive timesteps, the high-level policy is essentially learned in a temporally abstract Markov decision process (TA-MDP) built on the skills, which shortens the task horizon and reduces the exploration cost. However, these skills are usually sub-optimal because of the potential low quality and low coverage of the datasets, which causes the sub-optimal performance in the downstream task. Refining skills is intuitive, but the change of skills will in turn lead to the non-stationarity of the transition dynamics of TA-MDP which we name temporal abstraction shift. To address the dilemma of sub-optimal skills and temporal abstraction shift, we unify the optimization objectives of the entire hierarchical policy consisting of the high-level policy and the low-level policy whose latent space embeds the skills. We theoretically prove that the unified optimization objective guarantees the performance improvement in TA-MDP, and that optimizing the performance in TA-MDP is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in original MDP. Furthermore, in order to overcome the phenomenon of skill space collapse, we propose the dynamical skill refinement (DSR) mechanism which names our method. The experiment results empirically validate the effectiveness of our method, and show the advantages over the state-of-the-art (SOTA) methods.",ICLR.cc/2025/Conference,5.333333333333333,False,0.8367,hierarchical reinforcement learning aims solve complex long horizon tasks decomposing them into simpler sub tasks skills the compositional policy graph cpg for learning library modular and reusable skills that can dynamically composed solve tasks our contributions are the cpg for learning libraries composable skills graph modularity objective for skill disentanglement and demonstrations complex robotics manipulation and navigation tasks where cpg learns interpretable and reusable skills that enable rapid zero shot transfer unseen tasks composed familiar sub goals,reinforcement learning skills skills efficient paradigm for solving sparse reward tasks extracting skills from demonstration datasets and learning high level policy which selects skills address the dilemma sub optimal skills and temporal ion shift unify the optimization objectives the entire hierarchical policy consisting the high level policy and the low level policy whose latent space embeds the skills theoretically that the unified optimization objective guarantees the improvement mdp and that optimizing the mdp equivalent optimizing lower bound the the entire hierarchical policy original mdp,2025-08-26T00:48:30.450962
18,Parameter-Efficient Fine-Tuning with Learned Intrinsic Subspaces,"As pre-trained models grow larger, fine-tuning all parameters for downstream tasks becomes computationally prohibitive. Parameter-efficient fine-tuning (PEFT) methods like LoRA adapt models by training a small number of extra parameters. We propose Learning Intrinsic Subspaces (LISA), a new PEFT method that operates without adding any new parameters. LISA is based on the hypothesis that adaptation can occur within a low-dimensional subspace of the full parameter space. During fine-tuning, LISA freezes the pre-trained weights and instead learns a low-rank projection matrix that defines a task-specific subspace. Only the coordinates of the model's parameters within this small, learned subspace are updated via gradient descent. This is more efficient than LoRA as it does not require modifying the forward pass with extra weights. Our contributions are: (1) a novel, parameter-free PEFT method; (2) a formulation for learning an optimal adaptation subspace for a given task; and (3) extensive experiments on RoBERTa and ViT. LISA matches the performance of LoRA while using fewer trainable parameters and no additional inference-time latency, establishing a new state-of-the-art for parameter-efficient adaptation.",ICLR,deep learning,gemini-2.5-pro,True,9194,The Quest for Winning Tickets in Low-Rank Adapters,"Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method, offers an effective strategy for adapting large pre-trained models to specific tasks with minimal computational overhead. LoRA achieves this by introducing low-rank parameter matrices to the frozen pre-trained models. However, despite their efficiency, LoRA and its variants modify all elements of a parameter block, which is unnecessary as LoRA primarily aims to adjust a small set of subspaces that capture task-specific knowledge. Drawing inspiration from the Lottery Ticket Hypothesis (LTH), which posits that dense neural networks contain sparse subnetworks capable of performing similarly to fully-parameterized models, we investigate whether similar sparse subnetworks exist for low-rank adapters. We demonstrate that such subnetworks, often referred to as ""winning tickets"" in the context of LTH, indeed exist for low-rank adapters. We introduce a method to identify this sparse subset of weights for each layer by relating the top subspaces of the pretrained parameter block to the elements of the corresponding weight matrix. This subset is then fine-tuned using LoRA. We show that this sparse subset is not necessarily unique; as long as sparsity is kept within a certain bound defined by the task, random subnetworks with similar sparsity can act as winning tickets. Building on this discovery, we propose a novel approach called Partial-LoRA, which adds sparse low-rank parameters to pre-trained models. Through extensive experiments on 8 vision and 4 language tasks, we demonstrate that Partial-LoRA can reduce trainable parameters by up to 87% while maintaining or even improving model performance in some cases. Our work thus reduces memory needs and theoretically grounds sparse LoRAs.",ICLR.cc/2025/Conference,5.2,False,0.8452,learning intrinsic subspaces lisa peft that operates adding any parameters lisa the hypothesis that adaptation can occur within low dimensional subspace the full parameter space our contributions are parameter free peft formulation for learning optimal adaptation subspace for given task and extensive experiments roberta and vit lisa matches the lora while fewer trainable parameters and additional inference time latency establishing state the art for parameter efficient adaptation,low rank adaptation lora prominent parameter efficient fine tuning peft offers effective strategy for adapting large pre trained models specific tasks minimal computational overhead however despite their efficiency lora and its variants modify all elements parameter block which unnecessary lora primarily aims adjust small set subspaces that capture task specific knowledge drawing inspiration from the lottery ticket hypothesis lth which posits that dense neural networks contain sparse subnetworks capable performing similarly fully parameterized models whether similar sparse subnetworks exist for low rank adapters extensive experiments vision and language tasks that partial lora can reduce trainable parameters while maintaining even improving some cases,2025-08-26T00:48:30.450965
19,Generalization in Deep Learning via the Information Bottleneck in Activation Space,"A central mystery in deep learning is how overparameterized networks, which can easily memorize the training data, still generalize well to unseen examples. We propose a new explanation through the lens of an Information Bottleneck (IB) principle applied not to the weights, but to the network's activations. We argue that stochastic elements in training (e.g., SGD, dropout) act as a noisy channel on the activations, and the network learns to implicitly compress its intermediate representations to be maximally informative about the label while minimizing sensitivity to this noise. This compression forces the network to discard spurious, instance-specific details in favor of robust, generalizable features. Our contributions are: (1) a new theoretical perspective: the Activation Information Bottleneck (AIB) hypothesis for generalization; (2) we derive a measurable quantity, the ""activation compression ratio,"" and show it is highly correlated with test accuracy across different models and training settings; (3) we demonstrate that explicit regularization encouraging lower-information activations can improve generalization. This work reframes the study of generalization away from the weight space and towards the information content of the learned representations themselves.",ICLR,deep learning,gemini-2.5-pro,True,5896,Decoding Generalization from Memorization in Deep Neural Networks,"Overparameterized Deep Neural Networks that generalize well have been key to the dramatic success of Deep Learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. It has also been known that deep networks possess the ability to memorize training data, as evidenced by perfect or high training accuracies on models trained with corrupted data that have class labels shuffled to varying degrees. Concomitantly, such models are known to generalize poorly, i.e. they suffer from poor test accuracies, due to which it is thought that the act of memorizing substantially degrades the ability to generalize. It has, however, been unclear why the poor generalization that accompanies such memorization, comes about. One possibility is that in the process of training with corrupted data, the layers of the network irretrievably re-organize their representations in a manner that makes generalization difficult. The other possibility is that the network retains significant ability to generalize, but the trained network somehow “chooses” to readout in a manner that is detrimental to generalization. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially improved generalization, even in the face of memorization. Furthermore, such generalization abilities can be easily decoded from the internals of the trained model, and we build a technique to do so from the outputs of specific layers of the network. We demonstrate results on multiple models trained with a number of standard datasets.",ICLR.cc/2025/Conference,3.8,False,0.8615,central mystery deep learning how overparameterized networks which can easily memorize the training data still generalize well unseen examples sgd dropout act noisy channel the activations and the network learns implicitly compress its intermediate representations maximally informative about the label while minimizing sensitivity this noise this compression forces the network discard spurious instance specific details favor robust generalizable features,overparameterized deep neural networks that generalize well have been key the dramatic success deep learning recent years has also been known that deep networks possess the ability memorize training data evidenced perfect high training accuracies models trained corrupted data that have class labels shuffled varying degrees one possibility that the process training corrupted data the layers the network irretrievably organize their representations manner that makes generalization difficult the other possibility that the network retains significant ability generalize but the trained network somehow chooses readout manner that detrimental generalization furthermore such generalization abilities can easily decoded from the internals the trained and build from the outputs specific layers the network,2025-08-26T00:48:30.450971
20,Concept-based Explainability with Differentiable Decision Trees,"Existing explainability methods often produce saliency maps that highlight input features but fail to explain a model's decision in terms of high-level, human-understandable concepts. We introduce Differentiable Concept-based Decision Trees (DCDT), a framework that generates explanations as a sequence of logical decisions based on learned concepts. First, a concept discovery module, trained jointly with the main task model, learns a set of disentangled concept vectors (e.g., ""striped,"" ""wheeled"" for an image classifier). Then, a differentiable decision tree is trained to predict the final output using only these learned concepts as input features. The path through this tree for a given input provides a transparent, logical explanation (e.g., ""IF concept 'wheeled' is present AND concept 'has_engine' is present THEN predict 'car'""). Our contributions are: (1) an end-to-end trainable model that combines deep concept learning with interpretable decision logic; (2) a soft, differentiable decision tree architecture; and (3) qualitative and quantitative evaluations showing DCDT generates more faithful and human-understandable explanations than saliency-based methods, while maintaining competitive performance on the primary task.",ICLR,deep learning,gemini-2.5-pro,True,9726,Decision Rules are in the Pixels: Towards Pixel-level Evaluation of Saliency-based XAI Models,"The intricate and opaque nature of deep neural networks (DNNs) makes it difficult to decipher how they make decisions. Explainable artificial intelligence (XAI) has emerged as a promising remedy to this conundrum. However, verifying the correctness of XAI methods remains challenging, due to the absence of universally accepted ground-truth explanations. In this study, we focus on assessing the correctness of saliency-based XAI models applied to DNN-based image classifiers at the pixel level. The proposed evaluation protocol departs significantly from previous human-centric correctness assessment at the semantically meaningful object part level, which may not correspond to the actual decision rules derived by classifiers. A crucial step in our approach involves introducing a spatially localized shortcut, a form of decision rule that DNN-based classifiers tend to adopt preferentially, without disrupting original image patterns and decision rules therein. After verifying the shortcut as the dominant decision rule, we estimate the Shapley value for each pixel within the shortcut area to generate the ground-truth explanation map, assuming that pixels outside this area have null contributions. We quantitatively evaluate fourteen saliency-based XAI methods for classifiers utilizing convolutional neural networks and vision Transformers, trained on perturbed CIFAR-10, CIFAR-100, and ImageNet datasets, respectively. Comprehensive experimental results show that existing saliency-based XAI models struggle to offer accurate pixel-level attributions, casting doubt on the recent progress in saliency-based XAI.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8063,our contributions are end end trainable that combines deep concept learning interpretable decision logic soft differentiable decision tree and qualitative and quantitative evaluations showing dcdt generates more faithful and human understandable explanations than saliency based methods while maintaining competitive the primary task,the intricate and opaque nature deep neural networks dnns makes difficult decipher how they make decisions explainable artificial intelligence xai has emerged promising remedy this conundrum quantitatively fourteen saliency based xai methods for classifiers utilizing convolutional neural networks and vision transformers trained perturbed cifar cifar and imagenet datasets respectively,2025-08-26T00:48:30.450979
21,Learning Fair Representations by Calibrating the Eigenspectrum of the Covariance Matrix,"Algorithmic fairness aims to ensure that models do not disproportionately harm specific demographic groups. Many methods achieve fairness by learning representations that are invariant to sensitive attributes, often through adversarial training. We propose a novel, non-adversarial approach to learning fair representations by directly shaping the geometry of the representation space. We observe that in unfair models, the principal components (eigenvectors) of the feature covariance matrix often align with the sensitive attribute. Our method, Spectral Fairness Calibration (SFC), adds a regularization term to the training objective that penalizes this alignment. Specifically, SFC encourages the eigenvectors of the representation covariance matrix to be orthogonal to the direction of sensitive attribute variation. This effectively ""re-calibrates"" the representation space to remove information about the sensitive attribute from its main axes of variation, without needing a complex adversarial min-max game. Our contributions are: (1) a new perspective on fairness through the lens of spectral analysis; (2) the SFC regularizer, a simple and efficient non-adversarial fairness intervention; and (3) empirical results showing that SFC achieves state-of-the-art fairness-accuracy trade-offs on benchmark datasets.",ICLR,deep learning,gemini-2.5-pro,True,5494,Adversarial Latent Feature Augmentation for Fairness,"Achieving fairness in machine learning remains a critical challenge, especially due to the opaque effects of data augmentation on input spaces within nonlinear neural networks. Nevertheless, current approaches that emphasize augmenting latent features, rather than input spaces, offer limited insights into their ability to detect and mitigate bias. In response, we introduce the concept of the ""unfair region"" in the latent space, a subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high, leading to unfair prediction results. To address this, we propose Adversarial Latent Feature Augmentation (ALFA), a method that leverages adversarial fairness attacks to perturb latent space features, which are then used as data augmentation for fine-tuning. ALFA intentionally shifts latent features into unfair regions, and the last layer of the network is fine-tuned with these perturbed features, leading to a corrected decision boundary that enhances fairness in classification in a cost-effective manner. We present a theoretical framework demonstrating that our adversarial fairness objective reliably generates biased feature perturbations, and that fine-tuning on samples from these unfair regions ensures fairness improvements. Extensive experiments across diverse datasets, modalities, and backbone networks validate that training with these adversarial features significantly enhances fairness while maintaining predictive accuracy in classification tasks.",ICLR.cc/2025/Conference,6.5,True,0.8589,algorithmic fairness aims ensure that models not disproportionately harm specific demographic groups many methods achieve fairness learning representations that are invariant sensitive attributes often adversarial training non adversarial learning fair representations directly shaping the geometry the representation space observe that unfair models the principal components eigenvectors the feature covariance matrix often align the sensitive attribute our spectral fairness calibration sfc adds regularization term the training objective that penalizes this alignment sfc encourages the eigenvectors the representation covariance matrix orthogonal the direction sensitive attribute variation this calibrates the representation space remove information about the sensitive attribute from its main axes variation needing complex adversarial min max game our contributions are perspective fairness the lens spectral analysis the sfc regularizer simple and efficient non adversarial fairness intervention and empirical showing that sfc achieves state the art fairness accuracy trade offs datasets,achieving fairness machine learning remains critical challenge due the opaque effects data augmentation input spaces within nonlinear neural networks response the concept the unfair region the latent space subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high leading unfair prediction address this adversarial latent feature augmentation alfa that leverages adversarial fairness attacks perturb latent space features which are then used data augmentation for fine tuning alfa intentionally shifts latent features into unfair regions and the last layer the network fine tuned these perturbed features leading corrected decision boundary that enhances fairness classification cost effective manner theoretical demonstrating that our adversarial fairness objective reliably generates biased feature perturbations and that fine tuning samples from these unfair regions ensures fairness improvements extensive experiments across diverse datasets modalities and backbone networks that training these adversarial features enhances fairness while maintaining predictive classification tasks,2025-08-26T00:48:30.450985
22,Training Data Attribution using Functional Gradient Hacking,"Understanding which training examples are most responsible for a model's prediction is crucial for debugging, fairness, and identifying dataset biases. Existing data attribution methods are often computationally expensive or rely on strong simplifying assumptions. We introduce Functional Gradient Hacking (FGH), a fast and accurate method for training data attribution. FGH is based on the insight that the influence of a training point can be approximated by measuring how much that point's functional gradient (the gradient of the loss with respect to the model's output logits) aligns with the direction of a parameter update designed to ""hack"" or change a specific test prediction. By framing attribution as a functional projection problem, we avoid the need to compute expensive inverse Hessian-vector products. Our contributions are: (1) a new, theoretically-grounded framework for data attribution; (2) the FGH algorithm, which is orders of magnitude faster than influence functions; and (3) extensive experiments demonstrating that FGH provides more faithful attributions for complex models like Transformers compared to prior art, successfully identifying mislabeled examples and sources of bias.",ICLR,deep learning,gemini-2.5-pro,True,7033,Dynamic Influence Tracker: Estimating Sample Influence in SGD-Trained Models across Arbitrary Time Windows,"Understanding how training samples affect models improves model interpretability, optimization strategies, and anomaly detection. However, existing methods for estimating sample influence provide only static assessments, rely on restrictive assumptions, and require high computational costs. 
	We propose Dynamic Influence Tracker (DIT), a novel method to estimate time-varying sample influence in models trained with Stochastic Gradient Descent (SGD). DIT enables fine-grained analysis of sample influence within arbitrary time windows during training through a two-phase algorithm. The training phase efficiently captures and stores necessary information about the SGD trajectory, while the inference phase computes the influence of samples on the model within a specified time window. We provide a theoretical error bound for our estimator without assuming convexity, showing its reliability across various learning scenarios. Our experimental results reveal the evolution of sample influence throughout the training process, enhancing understanding of learning dynamics. We show DIT's effectiveness in improving model performance through anomalous sample detection and its potential for advancing curriculum learning.",ICLR.cc/2025/Conference,5.0,False,0.8120,understanding which training examples are most responsible for model prediction crucial for debugging fairness and identifying biases fgh the insight that the influence training point can approximated measuring how much that point functional gradient the gradient the loss respect the model output logits aligns the direction parameter update designed hack change specific prediction,understanding how training samples affect models improves interpretability optimization strategies and anomaly detection provide theoretical error bound for our estimator assuming convexity showing its reliability across various learning scenarios our experimental reveal the evolution sample influence throughout the training process enhancing understanding learning dynamics dit effectiveness improving anomalous sample detection and its potential for advancing curriculum learning,2025-08-26T00:48:30.450994
23,Physics-Informed Neural Radiance Fields for Dynamic Scenes,"Neural Radiance Fields (NeRF) have revolutionized novel view synthesis for static scenes, but modeling dynamic, moving scenes remains a major challenge. We introduce Physics-Informed NeRF (PI-NeRF), a framework that integrates principles of classical mechanics directly into the representation of dynamic scenes. Instead of learning a simple deformation field, PI-NeRF models a scene as a collection of particles, each with a position, radiance, and density, governed by a learnable Neural Potential Energy Function. A Hamiltonian dynamics solver, implemented as a differentiable layer, evolves the state of these particles through time. This physics-based parameterization provides a strong inductive bias for realistic motion, allowing the model to generalize to unseen time steps and produce physically plausible interpolations. Our contributions are: (1) a novel representation for dynamic scenes grounded in Hamiltonian mechanics; (2) the integration of a differentiable physics solver into the NeRF framework; and (3) state-of-the-art results on novel view and novel time synthesis for complex, non-rigidly deforming scenes, demonstrating significantly improved temporal coherence and physical plausibility compared to existing dynamic NeRF models.",ICLR,deep learning,gemini-2.5-pro,True,10977,NeuGen: Amplifying the ‘Neural’ in Neural Radiance Fields for Domain Generalization,"Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures, capable of initiating training from scratch or fine-tuning pre-trained models, which cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows benchmarking performance on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.",ICLR.cc/2025/Conference,4.5,False,0.8827,neural radiance fields nerf have revolutionized view synthesis for static scenes but modeling dynamic moving scenes remains major challenge physics informed nerf nerf that integrates principles classical mechanics directly into the representation dynamic scenes instead learning simple deformation field nerf models scene collection particles each position radiance and density governed learnable neural potential energy function our contributions are representation for dynamic scenes grounded hamiltonian mechanics the integration differentiable physics solver into the nerf and state the art view and time synthesis for complex non rigidly deforming scenes demonstrating improved temporal coherence and physical plausibility compared existing dynamic nerf models,neural radiance fields nerf have advanced the field view synthesis yet their generalization across diverse scenes and conditions remains challenging addressing this the integration brain inspired normalization neural generalization neugen into leading nerf architectures which include mvsnerf and geonerf can seamlessly integrated into nerf architectures capable initiating training from scratch fine tuning pre trained models which cultivates comprehensive feature set that improves and robustness image rendering our exemplifies the potential merging neuroscientific principles deep learning frameworks setting precedent for enhanced generalizability and efficiency view synthesis,2025-08-26T00:48:30.450997
24,Equivariant Subgraph Neural Networks,"Graph Neural Networks (GNNs) typically operate on the node and edge level, struggling to capture higher-order structural motifs or subgraph patterns that are crucial for many graph-based tasks, such as molecular property prediction. We propose Equivariant Subgraph Neural Networks (ESNNs), a new class of GNNs that learn representations of subgraphs while respecting the symmetries of the graph. ESNNs first extract a basis set of computationally feasible subgraphs (e.g., all 3- and 4-node motifs) around each node. A neural network, designed to be equivariant to the permutation of nodes within each subgraph, then learns a representation for each subgraph instance. Finally, a permutation-invariant pooling layer aggregates these subgraph representations to produce the final node or graph embedding. Our contributions are: (1) a principled framework for incorporating subgraph information into GNNs; (2) a novel equivariant architecture for learning from sets of subgraphs; and (3) state-of-the-art results on several graph classification and regression benchmarks where higher-order structure is paramount. ESNNs provide a more powerful tool for learning from graphs by explicitly modeling representations at the subgraph level.",ICLR,deep learning,gemini-2.5-pro,True,6374,Improving Graph Neural Networks with Heterophily-based Filtration and Filtration Learning,"Graph neural networks (GNNs) are a powerful method of learning representations of graph-structured data. While they excel at learning class-discriminative representations of nodes in homophilous graphs, where connecting nodes tend to belong to the same class, many GNNs struggle with heterophilous graphs whose inter-class connections can muddy the message passing.  Inspired by this finding, we propose a topological filtration scheme, treating graphs as 1-dimensional simplicial complexes N  with a filter function based on estimated edge heterophily, and introduce two methodologies that use a backbone GNN to learn from the resulting graph filtration. The first trains a GNN on each graph in the filtration sequence consecutively for a portion of the total training time, using embeddings from previous graphs to initialize node embeddings in subsequent graphs. The second approach uses a novel message passing scheme to pass messages jointly within each and between graph levels in the filtration sequence with common nodes. Both methods enhance the influence of early birth adjacent nodes in homophilous subgraphs, yet allow for the model to learn from the full range of heterophilous and homophilous connections in the graph. We further extend our approach to learn a graph filtration sequence of graphs through a learnable node filter function. Experiments show that our heterophily-filtered GNNs achieve superior node classification accuracy on heterophilous and homophilous networks alike.",ICLR.cc/2025/Conference,2.0,nan,0.8785,graph neural networks gnns operate the node and edge level struggling capture higher order structural motifs subgraph patterns that are crucial for many graph based tasks such molecular property prediction equivariant subgraph neural networks esnns class gnns that learn representations subgraphs while respecting the symmetries the graph neural network designed equivariant the permutation nodes within each subgraph then learns representation for each subgraph instance finally permutation invariant pooling layer aggregates these subgraph representations produce the final node graph embedding our contributions are principled for incorporating subgraph information into gnns equivariant for learning from sets subgraphs and state the art several graph classification and regression benchmarks where higher order structure paramount esnns provide more powerful tool for learning from graphs explicitly modeling representations the subgraph level,graph neural networks gnns are powerful learning representations graph structured data while they excel learning class discriminative representations nodes homophilous graphs where connecting nodes tend belong the same class many gnns struggle heterophilous graphs whose inter class connections can muddy the message passing experiments that our heterophily filtered gnns achieve superior node classification heterophilous and homophilous networks alike,2025-08-26T00:48:30.451004
25,Hyper-Riemannian Optimization: Learning Intrinsic Gradient Geometries for Faster Convergence,"Standard deep learning optimizers like Adam operate under a fixed, often Euclidean, geometric assumption, which is misaligned with the complex, curved loss landscapes of neural networks. This mismatch can slow convergence and lead to suboptimal solutions. We introduce Hyper-Riemannian Optimization (HRO), a novel method that dynamically learns the local geometry of the loss surface. HRO employs a small hypernetwork that predicts a low-rank Riemannian metric tensor conditioned on the current model parameters. This allows for efficient, on-the-fly computation of the natural gradient, guiding updates along the path of steepest descent on the underlying statistical manifold. Our core contributions are: (1) a framework for learning task-specific, dynamic Riemannian metrics for optimization; (2) a computationally efficient implementation using low-rank approximations; and (3) theoretical analysis connecting our approach to second-order methods. Empirically, HRO demonstrates significantly faster convergence and achieves superior final performance compared to AdamW and other state-of-the-art optimizers on large-scale Transformer language models and vision transformers on ImageNet. Our work opens a new direction in optimization by treating the geometry of the loss landscape as a learnable component of the training process.",ICLR,deep learning,gemini-2.5-pro,True,1005,Optimizing Learning for Robust Hyperbolic Deep Learning in Computer Vision,"Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the  data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on improving stability for curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across direct classification, generation, and hierarchical metric learning tasks while allowing for larger hyperbolic models.",ICLR.cc/2025/Conference,4.4,False,0.8668,standard deep learning optimizers like adam operate under fixed often euclidean geometric assumption which misaligned the complex curved loss landscapes neural networks hyper riemannian optimization hro that dynamically learns the local geometry the loss surface our core contributions are for learning task specific dynamic riemannian metrics for optimization computationally efficient implementation low rank approximations and theoretical analysis connecting our second order methods empirically hro demonstrates faster convergence and achieves superior final compared adamw and other state the art optimizers large scale transformer language models and vision transformers imagenet our opens direction optimization treating the geometry the loss landscape learnable component the training process,hyperbolic deep learning has become growing direction computer vision for the unique properties afforded the alternate embedding space current riemannian optimizers not account for changes the manifold which greatly harms and forces lower learning rates minimize projection errors our focuses improving stability for curvature learning introducing improved schema for popular learning algorithms and providing normalization constrain embeddings within the variable representative radius the manifold additionally formulation for riemannian adamw and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations greatly reducing the computational penalty the hyperbolic embedding space our demonstrates consistent improvements across direct classification generation and hierarchical learning tasks while allowing for larger hyperbolic models,2025-08-26T00:48:30.451006
26,Latent Trajectory Models: Denoising in a Learned Subspace for Accelerated Generative Diffusion,"Denoising diffusion models have achieved state-of-the-art results in generative modeling but suffer from slow sampling speeds due to the necessity of performing thousands of iterative steps in a high-dimensional data space. We propose Latent Trajectory Models (LTMs), a new class of generative models that significantly accelerate sampling by performing the diffusion process in a compressed, structured latent space. An autoencoder first maps data into a low-dimensional manifold. The diffusion process then operates entirely within this space, where a novel denoising network is trained to predict not just the next step, but the entire future trajectory of the denoising process, effectively reducing the required number of sampling steps. Our key contributions are: (1) a framework for performing diffusion in a learned latent space, decoupling generative complexity from data dimensionality; (2) a trajectory-aware denoising architecture that learns the dynamics of the reverse process; and (3) a stable training objective for the joint autoencoder-diffusion system. LTMs achieve sample quality comparable to traditional diffusion models on datasets like CelebA-HQ and LSUN, while requiring 10-20x fewer sampling steps, making high-fidelity generative modeling more practical for real-world applications.",ICLR,deep learning,gemini-2.5-pro,True,11065,Decouple-Then-Merge: Towards Better Training for Diffusion Models,"Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a $\textbf{De}$couple-then-$\textbf{Me}$rge ($\textbf{DeMe}$) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is included in the supplementary material and will be released on Github.",ICLR.cc/2025/Conference,4.2,nan,0.8456,the diffusion process then operates entirely within this space where denoising network trained predict not just the next step but the entire future trajectory the denoising process reducing the required number sampling steps,diffusion models are trained learning sequence models that reverse each step noise corruption however since the denoising tasks differ each timestep the gradients computed different timesteps may conflict potentially degrading the overall image generation several improved techniques during the finetuning stage promote effective knowledge sharing while minimizing training interference across timesteps experimental significant generation quality improvements upon benchmarks including stable diffusion coco30k imagenet1k partiprompts and ddpm lsun church lsun bedroom and cifar10,2025-08-26T00:48:30.451007
27,Beyond Flatness: Generalization is Governed by the Curvature-Dimension Spectrum of the Loss Landscape,"The prevailing belief that ""flat"" minima generalize well offers a useful heuristic but lacks a precise, quantitative foundation that can explain complex phenomena in deep learning. We argue that a single scalar value for flatness is insufficient. This paper introduces the Curvature-Dimension Spectrum (CDS), a new theoretical framework for understanding generalization. We posit that generalization is determined not by the average curvature, but by the entire eigenvalue distribution of the Hessian at the solution, which characterizes the ""effective dimensionality"" of the function space accessible to the model. Using tools from random matrix theory, we analyze the CDS and derive a novel PAC-Bayes generalization bound that explicitly depends on the spectral decay rate. Our contributions include: (1) the formalization of the CDS as a more descriptive alternative to sharpness; (2) a new generalization bound that leverages this spectral information; and (3) extensive empirical validation. We show that the CDS accurately predicts test error across various architectures, optimizers, and regularization schemes, successfully explaining cases where sharpness-based measures fail. This work provides a more nuanced and powerful lens for analyzing the mechanisms of generalization in deep neural networks.",ICLR,deep learning,gemini-2.5-pro,True,189,Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD,"Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD’s generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called ""omniscient trajectory"". When applied to Gradient Descent’s minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds’ $\Omega(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs. Codes are available at [https://github.com/peng-ze/omniscient-bounds](https://github.com/peng-ze/omniscient-bounds).",ICLR.cc/2025/Conference,7.0,True,0.8261,the prevailing belief that flat minima generalize well offers useful heuristic but lacks precise quantitative foundation that can explain complex phenomena deep learning this provides more nuanced and powerful lens for analyzing the mechanisms generalization deep neural networks,information theoretic generalization bounds have been used the generalization learning algorithms experiments deep neural networks our bound not only correctly reflects the better generalization when flatness improved but also numerically much tighter,2025-08-26T00:48:30.451009
28,Causal Policy Regularization for Robust Offline Reinforcement Learning,"A primary challenge in offline reinforcement learning (RL) is mitigating distributional shift, where the learned policy deviates from the behavior distribution of the static dataset, leading to catastrophic out-of-distribution value estimates. Existing methods employ uncertainty-based or policy-constraint regularization, which can be overly conservative and fail to distinguish spurious correlations from causal relationships. We introduce Causal Policy Regularization (CPR), a novel approach that frames offline RL as a causal inference problem. By modeling the underlying structural causal model of the environment, CPR penalizes policies for relying on non-causal, spurious features in the state space while allowing for safe generalization along causally-valid action pathways. This is achieved by learning a disentangled representation that isolates causally salient factors and regularizing the policy's sensitivity to spurious ones. Our contributions are: (1) a new causal framework for offline RL; (2) the CPR algorithm, which promotes learning policies that are robust to confounders; and (3) empirical results on the D4RL benchmark demonstrating that CPR significantly outperforms state-of-the-art methods, particularly in environments with strong spurious correlations, leading to more robust and generalizable policies.",ICLR,deep learning,gemini-2.5-pro,True,1861,RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning,"In offline reinforcement learning (RL), managing the distribution shift between the learned policy and the static offline dataset is a persistent challenge that can result in overestimated values and suboptimal policies. Traditional offline RL methods address this by introducing conservative biases that limit exploration to well-understood regions, but they often overly restrict the agent's generalization capabilities. Recent work has sought to generate trajectories using generative models to augment the offline dataset, yet these methods still struggle with overestimating synthesized data, especially when out-of-distribution samples are produced. To overcome this issue, we propose RTDiff, a novel diffusion-based data augmentation technique that synthesizes trajectories *in reverse*, moving from unknown to known states. Such reverse generation naturally mitigates the risk of overestimation by ensuring that the agent avoids planning through unknown states. Additionally, reverse trajectory synthesis allows us to generate longer, more informative trajectories that take full advantage of diffusion models' generative strengths while ensuring reliability. We further enhance RTDiff by introducing flexible trajectory length control and improving the efficiency of the generation process through noise management. Our empirical results show that RTDiff significantly improves the performance of several state-of-the-art offline RL algorithms across diverse environments, achieving consistent and superior results by effectively overcoming distribution shift.",ICLR.cc/2025/Conference,5.75,True,0.8651,primary challenge offline reinforcement learning mitigating distributional shift where the learned policy deviates from the behavior distribution the static leading catastrophic out distribution value estimates this achieved learning disentangled representation that isolates causally salient factors and regularizing the policy sensitivity spurious ones our contributions are causal for offline the cpr which promotes learning policies that are robust confounders and empirical the d4rl demonstrating that cpr outperforms state the art methods environments strong spurious correlations leading more robust and generalizable policies,offline reinforcement learning managing the distribution shift between the learned policy and the static offline persistent challenge that can overestimated values and suboptimal policies such reverse generation naturally mitigates the risk overestimation ensuring that the agent avoids planning unknown states further enhance rtdiff introducing flexible trajectory length control and improving the efficiency the generation process noise management,2025-08-26T00:48:30.451010
29,Dynamic Sparse Training with Synaptic Scaffolding,"Dynamic sparse training (DST) methods aim to train sparse neural networks from scratch, reducing computational costs. However, existing methods often suffer from ""representational collapse,"" where pruned neurons lose their ability to reintegrate into the network, and from unstable sparsity structures during training. We introduce Synaptic Scaffolding, a novel DST technique that addresses these issues. Synaptic Scaffolding maintains a ""scaffold"" of recently pruned connections in a dormant state. These connections retain a memory of their former weight values and can be rapidly re-activated if their reintroduction is predicted to significantly reduce the training loss, guided by a low-cost proxy gradient estimator. This allows the network to fluidly explore different sparsity masks without losing valuable learned information. Our contributions are: (1) the Synaptic Scaffolding algorithm for stable and effective DST; (2) a mechanism for preserving and efficiently re-activating pruned weights; and (3) a principled criterion for topological adaptation. We demonstrate that our method achieves state-of-the-art performance, enabling training of networks at extreme sparsity levels (e.g., 98%) from initialization, while matching or exceeding the accuracy of dense counterparts on ImageNet and BERT pre-training tasks.",ICLR,deep learning,gemini-2.5-pro,True,1993,Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness,"It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the ""de facto"" approach to train artificial neural networks if one would like to maximize their robustness against image corruption. In this paper, we question this general practice. Consequently, \textit{we claim that}, contrary to what is commonly thought, the Dynamic Sparse Training methods can consistently outperform Dense Training in terms of robustness accuracy, particularly if the efficiency aspect is not considered as a main objective (i.e., sparsity levels between 10\% and up to 50\%), without adding (or even reducing) resource cost. We validate our claim on two types of data, images and videos, using several traditional and modern deep learning architectures for computer vision and three widely studied Dynamic Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of Dynamic Sparse Training and open new possibilities in improving deep learning robustness beyond the current state of the art.",ICLR.cc/2025/Conference,5.75,True,0.8147,dynamic sparse training dst methods aim train sparse neural networks from scratch reducing computational costs this allows the network fluidly different sparsity masks losing valuable learned information our contributions are the synaptic scaffolding for stable and effective dst mechanism for preserving and activating pruned weights and principled criterion for topological adaptation,perceived that dynamic sparse training opens the door era scalability and efficiency for artificial neural networks perhaps some costs for the classification task the same time dense training accepted being the facto train artificial neural networks one would like maximize their robustness against image corruption consequently textit claim that contrary what thought the dynamic sparse training methods can consistently outperform dense training terms robustness the efficiency aspect not considered main objective our claim two types data images and videos several traditional and modern deep learning architectures for computer vision and three studied dynamic sparse training algorithms our reveal yet unknown benefit dynamic sparse training and open possibilities improving deep learning robustness beyond the current state the art,2025-08-26T00:48:30.451012
30,Composable Inductive Biases: Learning to Select and Fuse GNN Architectures,"The design of Graph Neural Network (GNN) architectures involves choosing specific message-passing, aggregation, and update functions, each encoding a different inductive bias (e.g., homophily, heterophily, structural equivalence). This ""no free lunch"" problem means no single GNN architecture is optimal for all graph structures and tasks. We propose Composable Inductive Biases (CIB), a meta-learning framework that learns to dynamically select and fuse different GNN architectures at a per-node level. CIB features a modular base library of diverse GNN layers (e.g., GCN, GAT, GraphSAGE) and a hyper-network that learns to predict a soft combination weighting over these modules for each node, based on its local neighborhood topology and features. This allows the model to apply GCN-like smoothing in homophilous regions while using GAT-like attention in complex, noisy regions of the same graph. Our contributions are: (1) a framework for learning node-adaptive GNN architectures; (2) a scalable hyper-network for predicting architectural compositions; and (3) state-of-the-art results on a suite of challenging graph datasets with heterogeneous structures, demonstrating superior adaptability compared to any single fixed GNN architecture.",ICLR,deep learning,gemini-2.5-pro,True,9000,Graph Neural Network Is A Mean Field Game,"In current graph neural networks (GNNs), it is a common practice to apply a pre-defined message passing heuristics to all graph data, even though the stereotypical relational inductive bias (e.g., graph heat diffusion) might not fit the unseen graph topology. Such gross simplification might be responsible for the lack of an in-depth understanding of graph learning principles, which challenges us to push the boundary from crafting application-specific GNNs to embracing a ""meta-learning"" paradigm. In this work, we ratchet the gear of GNN another notch forward by formulating GNN as a *mean field game*, that is, the best learning outcome occurs at the *Nash*-equilibrium when the learned graph inference rationale allows each graph node to find what is the best feature representations for not only the individual node but also the entire graph. Following this spirit, we formulate the search for novel GNN mechanism into a variational framework of *mean-field control* (MFC) problem, where the optimal relational inductive bias is essentially the critical point of mean-field information dynamics. Specifically, we seek for the best characteristic MFC functions of transportation mobility (controlling information exchange throughout the graph) and reaction mobility (controlling feature representation learning on each node), on the fly, that uncover the most suitable learning mechanism for a GNN instance by solving an MFC variational problem through the lens of *Hamiltonian flows* (formed in partial differential equations). In this context, our variational framework brings together existing GNN models into various mean-field games with distinct equilibrium states, each characterized by a unique MFC functional. Furthermore, we present an agnostic end-to-end deep model, coined *Nash-GNN* (in honor of Nobel laureate Dr. John Nash), to jointly carve the nature of the inductive bias and fine-tune the GNN hyper-parameters on top of the elucidated learning mechanism. *Nash-GNN* has achieved SOTA performance on diverse graph data including popular benchmark datasets and human connectomes. More importantly, the mathematical insight of mean-field games provides a new window to understand the foundational principles of graph learning as an interactive dynamical system, which allows us to reshape the idea of designing next-generation GNN models.",ICLR.cc/2025/Conference,5.333333333333333,nan,0.8679,the graph neural network gnn architectures involves choosing specific message passing aggregation and update functions each encoding different inductive bias this allows the apply gcn like smoothing homophilous regions while gat like attention complex noisy regions the same graph our contributions are for learning node adaptive gnn architectures scalable hyper network for predicting architectural compositions and state the art suite challenging graph datasets heterogeneous structures demonstrating superior adaptability compared any single fixed gnn,current graph neural networks gnns common practice apply pre defined message passing heuristics all graph data even though the stereotypical relational inductive bias such gross simplification might responsible for the lack depth understanding graph learning principles which challenges push the boundary from crafting application specific gnns embracing meta learning paradigm this ratchet the gear gnn another notch forward formulating gnn mean field game that the best learning outcome occurs the nash equilibrium when the learned graph inference rationale allows each graph node find what the best feature representations for not only the individual node but also the entire graph seek for the best characteristic mfc functions transportation mobility controlling information exchange throughout the graph and reaction mobility controlling feature representation learning each node the fly that uncover the most suitable learning mechanism for gnn instance solving mfc variational problem the lens hamiltonian flows formed partial differential equations furthermore agnostic end end deep coined nash gnn honor nobel laureate john nash jointly carve the nature the inductive bias and fine tune the gnn hyper parameters top the elucidated learning mechanism more importantly the mathematical insight mean field games provides window understand the foundational principles graph learning interactive dynamical which allows reshape the idea designing next generation gnn models,2025-08-26T00:48:30.451013
31,Continual Pre-training: A Framework for Language Models that Never Stop Learning,"Large language models (LLMs) are typically pre-trained on a massive, static dataset and then fine-tuned. This paradigm struggles with new information and evolving language, leading to knowledge cutoffs and model staleness. We propose Continual Pre-training (CPT), a framework that enables language models to efficiently and perpetually learn from a continuous stream of new data without catastrophic forgetting. CPT combines a dynamic memory architecture with an elasticity-based regularization scheme. New information is first encoded into a rapidly-updated, non-parametric memory, while an EWC-inspired (Elastic Weight Consolidation) regularizer selectively consolidates core knowledge into the parametric weights of the transformer, preserving foundational language capabilities. We introduce a ""knowledge novelty detector"" to gate what information triggers a consolidation update. Our contributions are: (1) a scalable framework for continual pre-training of LLMs; (2) a hybrid parametric-non-parametric memory system; and (3) a novelty-gated consolidation strategy. We demonstrate that a CPT-trained model can continuously assimilate new information from daily news streams over months, consistently outperforming statically pre-trained models on time-sensitive question-answering tasks while maintaining performance on general language benchmarks.",ICLR,deep learning,gemini-2.5-pro,True,4155,TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models,"Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a world where the latest dump of Common Crawl (CC), the most prominent public source of pre-training data, is used every month to *continually* train an LLM. We design various dynamic evaluations from the CC data, Wikipedia, StackExchange, and code documentations to measure continual learning metrics such as forgetting and forward transfer. Notably, our TiC-CC training data is more than 100 times larger compared with prior continual learning benchmarks for language modeling. We discover that recent DataComp-LM models trained on data before 2023 have already become outdated, incurring up to 45\% larger noun-perplexity on 2024 Wikipedia articles compared to pre-2023 articles. Further, we use our setup to evaluate the effectiveness of several large-scale continual learning methods and find that replaying older data is most effective for combating forgetting: for previously seen CC dumps, it can reduce the regret on held-out loss by 60\% compared to other optimizer and loss-based interventions. However, some domains evolve more quickly than others, favoring different trade-offs between mixing old and new data.",ICLR.cc/2025/Conference,6.25,False,0.9134,large language models llms are pre trained massive static and then fine tuned this paradigm struggles information and evolving language leading knowledge cutoffs and staleness continual pre training cpt that enables language models and perpetually learn from continuous stream data catastrophic forgetting information first encoded into rapidly updated non parametric memory while ewc inspired elastic weight consolidation regularizer selectively consolidates core knowledge into the parametric weights the transformer preserving foundational language capabilities that cpt trained can continuously assimilate information from daily news streams over months consistently outperforming statically pre trained models time sensitive question answering tasks while maintaining general language benchmarks,large language models llms are trained data crawled over many years from the web various dynamic evaluations from the data wikipedia stackexchange and code documentations measure continual learning metrics such forgetting and forward transfer notably our tic training data more than times larger compared prior continual learning benchmarks for language modeling further use our setup the effectiveness several large scale continual learning methods and find that replaying older data most effective for combating forgetting for previously seen dumps can reduce the regret held out loss compared other optimizer and loss based interventions,2025-08-26T00:48:30.451015
32,Flow-based State Space Models for Irregularly Sampled Time Series,"Real-world time series data, particularly from domains like healthcare and finance, are often sampled irregularly and contain missing values. Traditional models like RNNs and Transformers struggle with this irregularity, often requiring ad-hoc imputation or bucketing. We introduce Flow-based State Space Models (FSSM), a new class of deep learning models designed explicitly for irregularly sampled time series. FSSM represents the latent state dynamics as a continuous-time process governed by a neural ordinary differential equation (Neural ODE). Crucially, we model the evolution of the distribution over the latent state using a continuous normalizing flow, allowing us to precisely compute the exact likelihood of observed data points at any point in time, regardless of sampling intervals. Our contributions include: (1) a novel architecture combining Neural ODEs with continuous normalizing flows for time series; (2) a method for exact likelihood computation for irregularly sampled data; and (3) a principled approach to uncertainty quantification. FSSM achieves state-of-the-art results on challenging medical (MIMIC-III) and financial forecasting benchmarks, outperforming both discrete-time models and prior continuous-time approaches.",ICLR,deep learning,gemini-2.5-pro,True,5731,HOPE for a Robust Parameterization of Long-memory State Space Models,"State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. To achieve state-of-the-art performance, an SSM often needs a specifically designed initialization, and the training of state matrices is on a logarithmic scale with a very small learning rate. To understand these choices from a unified perspective, we view SSMs through the lens of Hankel operator theory. Building upon it, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. Our approach helps improve the initialization and training stability, leading to a more robust parameterization. We efficiently implement these innovations by nonuniformly sampling the transfer functions of LTI systems, and they require fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, our new parameterization endows the SSM with non-decaying memory within a fixed time window, which is empirically corroborated by a sequential CIFAR-10 task with padded noise.",ICLR.cc/2025/Conference,6.6,True,0.8214,flow based state space models fssm class deep learning models designed explicitly for irregularly sampled time series fssm represents the latent state dynamics continuous time process governed neural ordinary differential equation neural ode our contributions include combining neural odes continuous normalizing flows for time series for exact likelihood computation for irregularly sampled data and principled uncertainty quantification,state space models ssms that utilize linear time invariant lti systems are known for their effectiveness learning long sequences achieve state the art ssm often needs designed initialization and the training state matrices logarithmic scale very small learning rate these innovations nonuniformly sampling the transfer functions lti systems and they require fewer parameters compared canonical ssms,2025-08-26T00:48:30.451016
33,Geometric Disentanglement via the Lie Group VAE,"Learning disentangled representations, where distinct latent variables correspond to independent factors of data variation, is a central goal of representation learning. Current methods often struggle to disentangle factors that correspond to geometric transformations (e.g., rotation, scaling) and rely on heuristics. We introduce the Lie Group VAE (LG-VAE), a variational autoencoder architecture that leverages the mathematical structure of Lie groups to achieve principled geometric disentanglement. The LG-VAE's latent space is structured as a direct product of a Lie group and a Euclidean space. The decoder is explicitly designed to be equivariant to the action of the learned group on the latent variables, forcing the model to map geometric transformations in the data space to algebraic operations within the learned Lie group representation. Our contributions are: (1) a novel VAE architecture with a structured Lie group latent space; (2) an equivariant decoder that enforces geometric consistency; and (3) a training objective that learns both the group structure and the disentangled representation simultaneously. We demonstrate that the LG-VAE successfully discovers and disentangles transformations like rotations and translations in 2D and 3D datasets without explicit supervision, achieving superior disentanglement scores and enabling controllable data generation.",ICLR,deep learning,gemini-2.5-pro,True,2235,Interaction Asymmetry: A General Principle for Learning Composable Abstractions,"Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: ""Parts of the same concept have more complex interactions than parts of different concepts"". We formalize this via block diagonality conditions on the $(n+1)$th order derivatives of the generator mapping concepts to observed data, where different orders of ""complexity"" correspond to different $n$. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with $n=0$ or $1$. We provide results for up to $n=2$, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger $n$. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.",ICLR.cc/2025/Conference,7.0,True,0.8597,learning disentangled representations where distinct latent variables correspond independent factors data variation central goal representation learning the decoder explicitly designed equivariant the action the learned group the latent variables forcing the map geometric transformations the data space algebraic operations within the learned lie group representation our contributions are vae structured lie group latent space equivariant decoder that enforces geometric consistency and training objective that learns both the group structure and the disentangled representation simultaneously that the vae discovers and disentangles transformations like rotations and translations and datasets explicit supervision achieving superior disentanglement scores and enabling controllable data generation,learning disentangled representations concepts and composing them unseen ways crucial for generalizing out domain situations our unify recent theoretical for learning concepts objects which are recovered special cases implementation these criteria flexible transformer based vae regularizer the attention weights the decoder,2025-08-26T00:48:30.451017
34,Test-Time Training with Self-Supervised Consistency for Domain Adaptation,"Domain shift, the discrepancy between training and testing data distributions, poses a significant challenge for deploying machine learning models. Test-time adaptation methods aim to update a source-trained model using only unlabeled test data. We propose Test-Time Training with Self-Supervised Consistency (T3SC), a novel approach that adapts models by enforcing geometric and semantic consistency in their representation space. During testing, for each incoming batch, T3SC generates augmented views of the data and enforces a self-supervised consistency loss, compelling the model to produce invariant representations for semantically identical inputs under different perturbations. Unlike methods that rely on entropy minimization, which can reinforce incorrect predictions, our consistency-based objective is more robust to noisy pseudo-labels. Our contributions are: (1) a new test-time adaptation objective based on self-supervised consistency; (2) a framework that requires no modification to the original training pipeline; and (3) a lightweight adaptation procedure with minimal computational overhead. T3SC achieves state-of-the-art results on standard domain adaptation benchmarks like ImageNet-C and VisDA-C, significantly improving model robustness to real-world corruptions and shifts.",ICLR,deep learning,gemini-2.5-pro,True,483,Binary-Feedback Active Test-Time Adaptation,"Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To
address this issue, we introduce a Binary-feedback Active Test-Time Adaptation (BATTA) setting, which uses a few binary feedbacks from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BATTA-RL, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BATTA-RL achieves substantial accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort.",ICLR.cc/2025/Conference,6.0,False,0.8501,domain shift the discrepancy between training and testing data distributions poses significant challenge for deploying machine learning models test time adaptation methods aim update source trained only unlabeled data test time training self supervised consistency t3sc that adapts models enforcing geometric and semantic consistency their representation space our contributions are test time adaptation objective self supervised consistency that requires modification the original training pipeline and lightweight adaptation procedure minimal computational overhead t3sc achieves state the art standard domain adaptation benchmarks like imagenet and visda improving robustness real world corruptions and shifts,deep learning models perform poorly when domain shifts exist between training and data test time adaptation tta paradigm mitigate this issue adapting pre trained models only unlabeled samples however existing tta methods can fail under severe domain shifts while recent active tta approaches requiring full class labels are impractical due high labeling costs address this issue binary feedback active test time adaptation batta setting which uses few binary feedbacks from annotators indicate whether predictions are correct thereby reducing the labeling burden annotators under the setting batta dual path optimization that leverages reinforcement learning balance binary feedback guided adaptation uncertain samples agreement based self adaptation confident predictions,2025-08-26T00:48:30.451019
35,Neuro-Symbolic Inductive Logic Programming for Relational Reasoning,"Deep learning models, particularly Graph Neural Networks, have shown promise in relational reasoning but often struggle with tasks requiring multi-hop, explicit logical deduction and fail to produce interpretable reasoning chains. We introduce Neuro-Symbolic Inductive Logic Programming (NS-ILP), a framework that integrates the expressive power of deep relational embeddings with the formal rigor of Inductive Logic Programming (ILP). NS-ILP learns continuous vector representations of predicates and entities, then uses a differentiable ILP engine to search for logical rules that best explain the relationships in the training data. The key innovation is a differentiable proof mechanism that allows end-to-end training, enabling the neural component to learn representations that are amenable to symbolic reasoning, and the symbolic component to find rules grounded in the learned embeddings. Our contributions are: (1) a fully differentiable integration of ILP with neural networks; (2) a model that produces explicit, human-readable logical rules as part of its output; and (3) superior performance on complex relational reasoning benchmarks like CLUTRR and path-finding queries on knowledge graphs, demonstrating improved generalization and interpretability.",ICLR,deep learning,gemini-2.5-pro,True,98,Systematic Relational Reasoning With Epistemic Graph Neural Networks,"Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work has shown that regular GNNs lack the ability to systematically generalize from training examples on test graphs requiring longer inference chains, which fundamentally limits their reasoning abilities. A common solution relies on neuro-symbolic methods that systematically reason by learning rules, but their scalability is often limited and they tend to make unrealistically strong assumptions, e.g.\ that the answer can always be inferred from a single relational path. We propose the Epistemic GNN (EpiGNN), a novel parameter-efficient and scalable GNN architecture with an epistemic inductive bias for systematic reasoning. Node embeddings in EpiGNNs are treated as epistemic states, and message passing  is implemented accordingly. We show that EpiGNNs achieve state-of-the-art results on link prediction tasks that require systematic reasoning. Furthermore, for inductive knowledge graph completion, EpiGNNs rival the performance of state-of-the-art specialized approaches. Finally, we introduce two new benchmarks that go beyond standard relational reasoning by requiring the aggregation of information from multiple paths. Here, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason accurately.  Code and datasets are available at https://github.com/erg0dic/gnn-sg.",ICLR.cc/2025/Conference,6.5,True,0.8721,deep learning models graph neural networks have shown promise relational reasoning but often struggle tasks requiring multi hop explicit logical deduction and fail produce interpretable reasoning chains neuro symbolic inductive logic programming ilp that integrates the expressive power deep relational embeddings the formal rigor inductive logic programming ilp the key innovation differentiable proof mechanism that allows end end training enabling the neural component learn representations that are amenable symbolic reasoning and the symbolic component find rules grounded the learned embeddings our contributions are fully differentiable integration ilp neural networks that produces explicit human readable logical rules part its output and superior complex relational reasoning benchmarks like clutrr and path finding queries knowledge graphs demonstrating improved generalization and interpretability,focus reasoning relational domains where the use graph neural networks gnns seems like natural choice however previous has shown that regular gnns lack the ability systematically generalize from training examples graphs requiring longer inference chains which fundamentally limits their reasoning abilities common solution relies neuro symbolic methods that systematically reason learning rules but their scalability often limited and they tend make unrealistically strong assumptions the epistemic gnn epignn parameter efficient and scalable gnn epistemic inductive bias for systematic reasoning that epignns achieve state the art link prediction tasks that require systematic reasoning furthermore for inductive knowledge graph completion epignns rival the state the art specialized approaches finally two benchmarks that beyond standard relational reasoning requiring the aggregation information from multiple paths,2025-08-26T00:48:30.451020
36,Deconstructing Transformers: A Spectral Analysis of Self-Attention,"While the Transformer architecture has become ubiquitous, our theoretical understanding of its core component, the self-attention mechanism, remains limited. This paper provides a novel spectral analysis of the self-attention operator. We demonstrate that the attention matrix can be interpreted as the adjacency matrix of a dynamically constructed graph, and we analyze its spectral properties (eigenvalues and eigenvectors). We prove that the dot-product attention mechanism implicitly promotes a low-rank structure in the attention matrix, effectively acting as a spectral filter that amplifies dominant patterns in the token-token affinity space. Furthermore, we show how LayerNorm and residual connections interact with this spectral filtering, stabilizing the spectrum and preventing ""attention collapse"" to a rank-1 matrix. Our contributions are: (1) a new theoretical framework for analyzing self-attention through a spectral lens; (2) a formal proof of the low-rank bias of the attention mechanism; and (3) insights into the role of other architectural components in shaping the attention spectrum. This analysis provides a deeper understanding of how Transformers process information and opens new avenues for designing more efficient and principled attention mechanisms.",ICLR,deep learning,gemini-2.5-pro,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8820,while the transformer has become ubiquitous our theoretical understanding its core component the self attention mechanism remains limited that the attention matrix can interpreted the adjacency matrix dynamically constructed graph and its spectral properties eigenvalues and eigenvectors that the dot product attention mechanism implicitly promotes low rank structure the attention matrix acting spectral filter that amplifies dominant patterns the token token affinity space our contributions are theoretical for analyzing self attention spectral lens formal proof the low rank bias the attention mechanism and insights into the role other architectural components shaping the attention spectrum this analysis provides deeper understanding how transformers process information and opens avenues for designing more efficient and principled attention mechanisms,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T00:48:30.451022
37,Privacy-Preserving Representations via Differentiable Oblivious Hashing,"Learning useful representations of data while preserving the privacy of sensitive attributes is a critical challenge. Existing methods often trade off utility and privacy, or require computationally expensive adversarial training. We propose Differentiable Oblivious Hashing (DOH), a new technique for learning privacy-preserving representations. DOH learns a projection of the input data into a binary hash space using a stochastic, differentiable hashing function. The training objective is twofold: a utility term ensures the hashes are useful for a downstream task, while a novel privacy term, based on the collision entropy of hashes for inputs with different sensitive attributes, forces the model to map them to distinct, non-predictable hash buckets. This ""oblivious"" mapping makes it difficult for an adversary to infer the sensitive attribute from the hash code. Our contributions are: (1) the DOH framework for learning privatized binary representations; (2) a differentiable hashing mechanism suitable for end-to-end training; and (3) a collision-based privacy objective. We demonstrate on benchmark datasets that DOH achieves a superior privacy-utility trade-off compared to adversarial methods and differential privacy approaches, offering a practical and efficient solution for creating anonymized yet functional data representations.",ICLR,deep learning,gemini-2.5-pro,False,,Privacy Preserving Generative Feature Transformation,"Data-Centric AI (DCAI) aims to use AI to get better data for better AI. Feature transformation, as one of the essential tasks of DCAI, can augment the data representation and has garnered significant attention. Existing methods have demonstrated state-of-the-art performance on advancing predictive tasks. However, these methods can lead to serious privacy leakage. For example, sensitive features in original data can be inferred by models trained on transformed data, exposing vulnerabilities in the privacy-preserving capabilities of these methods. To address this issue, we introduce a privacy-preserving feature transformation framework that transforms data representation while preserving privacy from a generative modeling perspective. Specifically, our framework includes two phases: 1) privacy-aware knowledge acquisition and 2) privacy-preserving feature space generation. In the knowledge acquisition phase, we develop an information bottlenecks guided reinforcement learning system to explore and collect privacy-aware feature sets as a knowledge base in token sequence form. In the feature space generation phase, we develop a generative model to encode the knowledge base into a privacy-aware latent space, where the best latent representation is identified and decoded into the optimal privacy-preserving feature space. We solve the optimization via projected gradient ascent that maximizes predictive performance and minimizes privacy exposure. Finally, we present extensive experiments on eight real-world datasets to evaluate how our method can navigate both performance and privacy. The code is available at https://anonymous.4open.science/r/anonymous-2B53/.",ICLR.cc/2025/Conference,3.5,nan,0.7990,learning useful representations data while preserving the privacy sensitive attributes critical challenge differentiable oblivious hashing doh for learning privacy preserving representations our contributions are the doh for learning privatized binary representations differentiable hashing mechanism suitable for end end training and collision based privacy objective,feature transformation one the essential tasks dcai can augment the data representation and has garnered significant attention address this issue privacy preserving feature transformation that transforms data representation while preserving privacy from generative modeling perspective our includes two phases privacy aware knowledge acquisition and privacy preserving feature space generation the knowledge acquisition phase information bottlenecks guided reinforcement learning and collect privacy aware feature sets knowledge base token sequence form the feature space generation phase generative encode the knowledge base into privacy aware latent space where the best latent representation identified and decoded into the optimal privacy preserving feature space solve the optimization projected gradient ascent that maximizes predictive and minimizes privacy exposure,2025-08-26T00:48:30.451023
38,Multi-Agent Reinforcement Learning with Action-Semantic Communication,"Effective communication is key to solving complex cooperative multi-agent reinforcement learning (MARL) problems. Existing communication protocols often involve learning arbitrary, uninterpretable message vectors, which can be sample-inefficient and hard to generalize. We introduce Action-Semantic Communication (ASC), a novel MARL communication protocol where messages are structured to represent proposals and commitments about future actions. Instead of exchanging latent vectors, agents broadcast messages from a discrete, learnable vocabulary, where each ""word"" is tied to a specific action policy or sub-goal. A message like ""I will cover region B"" is not just cheap talk; it is a commitment backed by a specific, executable policy from the agent's learned library. This grounds communication in the action space, making it more interpretable and sample-efficient. Our contributions are: (1) a new MARL communication framework based on action-semantic commitments; (2) a method for jointly learning a communication vocabulary and a library of corresponding policies; and (3) state-of-the-art performance on challenging cooperative tasks like StarCraft II and multi-agent particle environments, demonstrating faster coordination and better generalization to unseen agent configurations.",ICLR,deep learning,gemini-2.5-pro,True,10367,Human-like Communication Strategies for Improved Multi-Agent Reinforcement Learning,"Multi-Agent Reinforcement Learning (MARL) has seen significant progress in recent years, enabling multiple agents to coordinate and optimize their actions in complex environments. However, integrating effective communication protocols into MARL frameworks remains a challenge, as it introduces issues such as increased state space dimensionality, lack of stationarity, and the need for interpretability. Inspired by human communication, which relies on prior knowledge, contextual awareness, and efficient information exchange, we propose a novel framework for incorporating human-like communication strategies to enhance the learning process. Motivated by recent advancements in natural language processing (NLP), multi-modal AI and object detection, we use text-to-mask models and human feedback to learn compact and informative communication strategies that facilitate coordination among agents to improve the overall performance. We demonstrate the efficiency of our approach on various multi-agent tasks and provide insights into emergent communication behaviors observed during training.",ICLR.cc/2025/Conference,3.0,nan,0.9238,effective communication key solving complex cooperative multi agent reinforcement learning marl problems existing communication protocols often involve learning arbitrary uninterpretable message vectors which can sample inefficient and hard generalize our contributions are marl communication action semantic commitments for jointly learning communication vocabulary and library corresponding policies and state the art challenging cooperative tasks like starcraft and multi agent particle environments demonstrating faster coordination and better generalization unseen agent configurations,multi agent reinforcement learning marl has seen significant progress recent years enabling multiple agents coordinate and optimize their actions complex environments however integrating effective communication protocols into marl frameworks remains challenge introduces issues such increased state space dimensionality lack stationarity and the need for interpretability inspired human communication which relies prior knowledge contextual awareness and efficient information exchange for incorporating human like communication strategies enhance the learning process motivated recent advancements natural language processing nlp multi modal and object detection use text mask models and human feedback learn compact and informative communication strategies that facilitate coordination among agents improve the overall,2025-08-26T00:48:30.451025
39,Amortized Bayesian Model Comparison for Neural Network Architectures,"Choosing the right neural network architecture is a crucial, yet resource-intensive, part of the machine learning pipeline. Bayesian model comparison offers a principled way to perform this selection by approximating the marginal likelihood (or model evidence), but existing methods like nested sampling are computationally prohibitive for deep models. We introduce the Amortized Model Comparison Network (AMCN), a meta-learning approach that learns to rapidly approximate the log marginal likelihood for a given model architecture and dataset. The AMCN is a graph hypernetwork trained on a vast collection of synthetic datasets and their corresponding architecture-evidence pairs, pre-computed offline. Once trained, it can predict the evidence for a new architecture and dataset in a single forward pass, bypassing the need for expensive sampling. Our contributions are: (1) a framework for amortizing Bayesian model comparison for neural networks; (2) the AMCN architecture that maps architecture graphs to evidence scores; and (3) a demonstration of its effectiveness in neural architecture search (NAS), where it serves as a highly efficient and accurate performance predictor, discovering high-performing architectures orders of magnitude faster than traditional NAS methods.",ICLR,deep learning,gemini-2.5-pro,True,7586,Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks,"Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.",ICLR.cc/2025/Conference,5.75,True,0.8343,choosing the right neural network crucial yet resource intensive part the machine learning pipeline bayesian comparison offers principled way perform this selection approximating the marginal likelihood evidence but existing methods like nested sampling are computationally prohibitive for deep models the amortized comparison network amcn meta learning that learns rapidly approximate the log marginal likelihood for given and our contributions are for amortizing bayesian comparison for neural networks the amcn that maps graphs evidence scores and demonstration its effectiveness neural search nas where serves highly efficient and accurate predictor discovering high performing architectures orders magnitude faster than traditional nas methods,despite recent advances sampling based inference for bayesian neural networks bnns remains significant challenge probabilistic deep learning consequence sampling still requires longer inference times than non bayesian methods even for small neural networks despite recent advances making software implementations more efficient tackle these challenges ensembling that leverages strategies from optimization and recently proposed sampler called microcanonical langevin monte carlo mclmc for efficient robust and predictable sampling,2025-08-26T00:48:30.451026
40,Unsupervised Learning of 3D-Aware Representations from Single Images via Cross-View Cycle Consistency,"Learning 3D representations from 2D images without explicit 3D supervision is a challenging open problem. We propose Cross-View Cycle Consistency (CVCC), a novel self-supervised framework for learning 3D-aware representations from single, unposed images. Our model learns to decompose an image into a 3D-aware latent representation, which can then be re-rendered from a novel viewpoint. The core of our method lies in a cycle consistency objective: we generate a new view, and then require the model to re-render the original view from this generated image. This forces the latent representation to capture the true underlying 3D structure of the scene, as this is the only information that remains invariant across view changes. Unlike GAN-based approaches, CVCC does not require an adversarial discriminator and is trained with a simple reconstruction loss. Our contributions are: (1) a self-supervised learning objective based on cross-view cycle consistency; (2) an architecture that learns to generate novel views without explicit 3D data or camera poses; and (3) results showing that our learned representations capture rich 3D information, enabling tasks like single-image depth estimation and controllable novel view synthesis on datasets like CelebA and Cats.",ICLR,deep learning,gemini-2.5-pro,True,9186,Position-Query-Based Autoencoders for View Decoupled Cross Point Cloud Reconstruction and a Self-Supervised Learning Framework,"Point cloud learning, especially in a self-supervised way without manual labels, has received emerging attention in both vision and learning communities, with its potential utility in wide areas. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it could thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to achieve new state-of-the-art results and surpasses previous single-modal self-reconstruction methods in 3D self-supervised learning by a margin. Specifically, it outperforms self-reconstruction baseline (Point-MAE) 6.5\%, 7.0\%, 6.7\% in three variants of ScanObjectNN with Mlp-Linear evaluation protocol. Source code will be released.",ICLR.cc/2025/Conference,6.2,False,0.8509,learning representations from images explicit supervision challenging open problem cross view cycle consistency cvcc self supervised for learning aware representations from single unposed images this forces the latent representation capture the true underlying structure the scene this the only information that remains invariant across view changes our contributions are self supervised learning objective cross view cycle consistency that learns generate views explicit data camera poses and showing that our learned representations capture rich information enabling tasks like single image depth estimation and controllable view synthesis datasets like celeba and cats,point cloud learning self supervised way manual labels has received emerging attention both vision and learning communities its potential utility wide areas most existing generative approaches for point cloud self supervised learning focus recovering masked points from visible ones within single view inspired this the potential two view learning this domain achieve this goal crop mechanism for point cloud view generation for the first time and further positional encoding represent the relative position between the two decoupled views the cross reconstruction increases the difficulty pre training compared self reconstruction which enables our achieve state the art and surpasses previous single modal self reconstruction methods self supervised learning margin,2025-08-26T00:48:30.451027
41,A Unified Framework for Contrastive and Non-Contrastive Self-Supervised Learning,"Self-supervised learning (SSL) is dominated by two main paradigms: contrastive methods (e.g., SimCLR) which pull positive pairs together and push negative pairs apart, and non-contrastive methods (e.g., BYOL, SimSiam) which only pull positive pairs together, avoiding collapse through architectural tricks like stop-gradients or momentum encoders. The theoretical relationship between these two families is not well understood. We present a unified framework that views both contrastive and non-contrastive learning through the lens of information-theoretic principles. We show that both approaches can be seen as optimizing a lower bound on the mutual information between augmented views, but differ in their estimation of the negative term. Contrastive methods use explicit negative sampling, while non-contrastive methods use an implicit, batch-wide uniform negative distribution enforced by the architecture. Our key contributions are: (1) a single objective function that can instantiate both SimCLR and BYOL/SimSiam as special cases; (2) a theoretical analysis showing how architectural components in non-contrastive methods act as implicit regularizers; and (3) a new hybrid algorithm derived from our framework that outperforms both pure approaches on standard benchmarks. This work bridges the conceptual gap between dominant SSL paradigms.",ICLR,deep learning,gemini-2.5-pro,True,11317,Self-supervised contrastive learning performs non-linear system identification,"Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",ICLR.cc/2025/Conference,6.4,True,0.8419,self supervised learning ssl dominated two main paradigms contrastive methods unified that views both contrastive and non contrastive learning the lens information theoretic principles,self supervised learning ssl approaches have brought tremendous success across many tasks and domains has been argued that these successes can attributed link between ssl and identifiable representation learning temporal structure and auxiliary variables ensure that latent representations are related the true underlying generative factors the data,2025-08-26T00:48:30.451029
42,Learning Modular and Reusable Skills with a Compositional Policy Graph,"Hierarchical reinforcement learning aims to solve complex, long-horizon tasks by decomposing them into simpler sub-tasks or skills. However, current methods often learn monolithic, non-reusable skills. We introduce the Compositional Policy Graph (CPG), a framework for learning a library of modular and reusable skills that can be dynamically composed to solve new tasks. The CPG is a directed graph where nodes represent low-level policies (skills) and edges, managed by a high-level gating policy, represent feasible transitions between them. We train the entire graph end-to-end using an off-policy algorithm with a novel intrinsic reward structure that encourages each node to learn a distinct yet composable behavior. A key innovation is a graph modularity objective that promotes disentanglement, ensuring skills are self-contained and reusable. Our contributions are: (1) the CPG framework for learning libraries of composable skills; (2) a graph modularity objective for skill disentanglement; and (3) demonstrations on complex robotics manipulation and navigation tasks, where CPG learns interpretable and reusable skills that enable rapid zero-shot transfer to novel, unseen tasks composed of familiar sub-goals.",ICLR,deep learning,gemini-2.5-pro,True,521,DSR: Reinforcement Learning with Dynamical Skill Refinement,"Reinforcement learning with skills (RL with skills) is an efficient paradigm for solving sparse-reward tasks by extracting skills from demonstration datasets and learning high-level policy which selects skills. Because each selected skill by high-level policy is executed for multiple consecutive timesteps, the high-level policy is essentially learned in a temporally abstract Markov decision process (TA-MDP) built on the skills, which shortens the task horizon and reduces the exploration cost. However, these skills are usually sub-optimal because of the potential low quality and low coverage of the datasets, which causes the sub-optimal performance in the downstream task. Refining skills is intuitive, but the change of skills will in turn lead to the non-stationarity of the transition dynamics of TA-MDP which we name temporal abstraction shift. To address the dilemma of sub-optimal skills and temporal abstraction shift, we unify the optimization objectives of the entire hierarchical policy consisting of the high-level policy and the low-level policy whose latent space embeds the skills. We theoretically prove that the unified optimization objective guarantees the performance improvement in TA-MDP, and that optimizing the performance in TA-MDP is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in original MDP. Furthermore, in order to overcome the phenomenon of skill space collapse, we propose the dynamical skill refinement (DSR) mechanism which names our method. The experiment results empirically validate the effectiveness of our method, and show the advantages over the state-of-the-art (SOTA) methods.",ICLR.cc/2025/Conference,5.333333333333333,False,0.8367,hierarchical reinforcement learning aims solve complex long horizon tasks decomposing them into simpler sub tasks skills the compositional policy graph cpg for learning library modular and reusable skills that can dynamically composed solve tasks our contributions are the cpg for learning libraries composable skills graph modularity objective for skill disentanglement and demonstrations complex robotics manipulation and navigation tasks where cpg learns interpretable and reusable skills that enable rapid zero shot transfer unseen tasks composed familiar sub goals,reinforcement learning skills skills efficient paradigm for solving sparse reward tasks extracting skills from demonstration datasets and learning high level policy which selects skills address the dilemma sub optimal skills and temporal ion shift unify the optimization objectives the entire hierarchical policy consisting the high level policy and the low level policy whose latent space embeds the skills theoretically that the unified optimization objective guarantees the improvement mdp and that optimizing the mdp equivalent optimizing lower bound the the entire hierarchical policy original mdp,2025-08-26T00:48:30.451030
43,Parameter-Efficient Fine-Tuning with Learned Intrinsic Subspaces,"As pre-trained models grow larger, fine-tuning all parameters for downstream tasks becomes computationally prohibitive. Parameter-efficient fine-tuning (PEFT) methods like LoRA adapt models by training a small number of extra parameters. We propose Learning Intrinsic Subspaces (LISA), a new PEFT method that operates without adding any new parameters. LISA is based on the hypothesis that adaptation can occur within a low-dimensional subspace of the full parameter space. During fine-tuning, LISA freezes the pre-trained weights and instead learns a low-rank projection matrix that defines a task-specific subspace. Only the coordinates of the model's parameters within this small, learned subspace are updated via gradient descent. This is more efficient than LoRA as it does not require modifying the forward pass with extra weights. Our contributions are: (1) a novel, parameter-free PEFT method; (2) a formulation for learning an optimal adaptation subspace for a given task; and (3) extensive experiments on RoBERTa and ViT. LISA matches the performance of LoRA while using fewer trainable parameters and no additional inference-time latency, establishing a new state-of-the-art for parameter-efficient adaptation.",ICLR,deep learning,gemini-2.5-pro,True,9194,The Quest for Winning Tickets in Low-Rank Adapters,"Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method, offers an effective strategy for adapting large pre-trained models to specific tasks with minimal computational overhead. LoRA achieves this by introducing low-rank parameter matrices to the frozen pre-trained models. However, despite their efficiency, LoRA and its variants modify all elements of a parameter block, which is unnecessary as LoRA primarily aims to adjust a small set of subspaces that capture task-specific knowledge. Drawing inspiration from the Lottery Ticket Hypothesis (LTH), which posits that dense neural networks contain sparse subnetworks capable of performing similarly to fully-parameterized models, we investigate whether similar sparse subnetworks exist for low-rank adapters. We demonstrate that such subnetworks, often referred to as ""winning tickets"" in the context of LTH, indeed exist for low-rank adapters. We introduce a method to identify this sparse subset of weights for each layer by relating the top subspaces of the pretrained parameter block to the elements of the corresponding weight matrix. This subset is then fine-tuned using LoRA. We show that this sparse subset is not necessarily unique; as long as sparsity is kept within a certain bound defined by the task, random subnetworks with similar sparsity can act as winning tickets. Building on this discovery, we propose a novel approach called Partial-LoRA, which adds sparse low-rank parameters to pre-trained models. Through extensive experiments on 8 vision and 4 language tasks, we demonstrate that Partial-LoRA can reduce trainable parameters by up to 87% while maintaining or even improving model performance in some cases. Our work thus reduces memory needs and theoretically grounds sparse LoRAs.",ICLR.cc/2025/Conference,5.2,False,0.8452,learning intrinsic subspaces lisa peft that operates adding any parameters lisa the hypothesis that adaptation can occur within low dimensional subspace the full parameter space our contributions are parameter free peft formulation for learning optimal adaptation subspace for given task and extensive experiments roberta and vit lisa matches the lora while fewer trainable parameters and additional inference time latency establishing state the art for parameter efficient adaptation,low rank adaptation lora prominent parameter efficient fine tuning peft offers effective strategy for adapting large pre trained models specific tasks minimal computational overhead however despite their efficiency lora and its variants modify all elements parameter block which unnecessary lora primarily aims adjust small set subspaces that capture task specific knowledge drawing inspiration from the lottery ticket hypothesis lth which posits that dense neural networks contain sparse subnetworks capable performing similarly fully parameterized models whether similar sparse subnetworks exist for low rank adapters extensive experiments vision and language tasks that partial lora can reduce trainable parameters while maintaining even improving some cases,2025-08-26T00:48:30.451032
44,Generalization in Deep Learning via the Information Bottleneck in Activation Space,"A central mystery in deep learning is how overparameterized networks, which can easily memorize the training data, still generalize well to unseen examples. We propose a new explanation through the lens of an Information Bottleneck (IB) principle applied not to the weights, but to the network's activations. We argue that stochastic elements in training (e.g., SGD, dropout) act as a noisy channel on the activations, and the network learns to implicitly compress its intermediate representations to be maximally informative about the label while minimizing sensitivity to this noise. This compression forces the network to discard spurious, instance-specific details in favor of robust, generalizable features. Our contributions are: (1) a new theoretical perspective: the Activation Information Bottleneck (AIB) hypothesis for generalization; (2) we derive a measurable quantity, the ""activation compression ratio,"" and show it is highly correlated with test accuracy across different models and training settings; (3) we demonstrate that explicit regularization encouraging lower-information activations can improve generalization. This work reframes the study of generalization away from the weight space and towards the information content of the learned representations themselves.",ICLR,deep learning,gemini-2.5-pro,True,5896,Decoding Generalization from Memorization in Deep Neural Networks,"Overparameterized Deep Neural Networks that generalize well have been key to the dramatic success of Deep Learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. It has also been known that deep networks possess the ability to memorize training data, as evidenced by perfect or high training accuracies on models trained with corrupted data that have class labels shuffled to varying degrees. Concomitantly, such models are known to generalize poorly, i.e. they suffer from poor test accuracies, due to which it is thought that the act of memorizing substantially degrades the ability to generalize. It has, however, been unclear why the poor generalization that accompanies such memorization, comes about. One possibility is that in the process of training with corrupted data, the layers of the network irretrievably re-organize their representations in a manner that makes generalization difficult. The other possibility is that the network retains significant ability to generalize, but the trained network somehow “chooses” to readout in a manner that is detrimental to generalization. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially improved generalization, even in the face of memorization. Furthermore, such generalization abilities can be easily decoded from the internals of the trained model, and we build a technique to do so from the outputs of specific layers of the network. We demonstrate results on multiple models trained with a number of standard datasets.",ICLR.cc/2025/Conference,3.8,False,0.8614,central mystery deep learning how overparameterized networks which can easily memorize the training data still generalize well unseen examples sgd dropout act noisy channel the activations and the network learns implicitly compress its intermediate representations maximally informative about the label while minimizing sensitivity this noise this compression forces the network discard spurious instance specific details favor robust generalizable features,overparameterized deep neural networks that generalize well have been key the dramatic success deep learning recent years has also been known that deep networks possess the ability memorize training data evidenced perfect high training accuracies models trained corrupted data that have class labels shuffled varying degrees one possibility that the process training corrupted data the layers the network irretrievably organize their representations manner that makes generalization difficult the other possibility that the network retains significant ability generalize but the trained network somehow chooses readout manner that detrimental generalization furthermore such generalization abilities can easily decoded from the internals the trained and build from the outputs specific layers the network,2025-08-26T00:48:30.451033
45,Concept-based Explainability with Differentiable Decision Trees,"Existing explainability methods often produce saliency maps that highlight input features but fail to explain a model's decision in terms of high-level, human-understandable concepts. We introduce Differentiable Concept-based Decision Trees (DCDT), a framework that generates explanations as a sequence of logical decisions based on learned concepts. First, a concept discovery module, trained jointly with the main task model, learns a set of disentangled concept vectors (e.g., ""striped,"" ""wheeled"" for an image classifier). Then, a differentiable decision tree is trained to predict the final output using only these learned concepts as input features. The path through this tree for a given input provides a transparent, logical explanation (e.g., ""IF concept 'wheeled' is present AND concept 'has_engine' is present THEN predict 'car'""). Our contributions are: (1) an end-to-end trainable model that combines deep concept learning with interpretable decision logic; (2) a soft, differentiable decision tree architecture; and (3) qualitative and quantitative evaluations showing DCDT generates more faithful and human-understandable explanations than saliency-based methods, while maintaining competitive performance on the primary task.",ICLR,deep learning,gemini-2.5-pro,True,9726,Decision Rules are in the Pixels: Towards Pixel-level Evaluation of Saliency-based XAI Models,"The intricate and opaque nature of deep neural networks (DNNs) makes it difficult to decipher how they make decisions. Explainable artificial intelligence (XAI) has emerged as a promising remedy to this conundrum. However, verifying the correctness of XAI methods remains challenging, due to the absence of universally accepted ground-truth explanations. In this study, we focus on assessing the correctness of saliency-based XAI models applied to DNN-based image classifiers at the pixel level. The proposed evaluation protocol departs significantly from previous human-centric correctness assessment at the semantically meaningful object part level, which may not correspond to the actual decision rules derived by classifiers. A crucial step in our approach involves introducing a spatially localized shortcut, a form of decision rule that DNN-based classifiers tend to adopt preferentially, without disrupting original image patterns and decision rules therein. After verifying the shortcut as the dominant decision rule, we estimate the Shapley value for each pixel within the shortcut area to generate the ground-truth explanation map, assuming that pixels outside this area have null contributions. We quantitatively evaluate fourteen saliency-based XAI methods for classifiers utilizing convolutional neural networks and vision Transformers, trained on perturbed CIFAR-10, CIFAR-100, and ImageNet datasets, respectively. Comprehensive experimental results show that existing saliency-based XAI models struggle to offer accurate pixel-level attributions, casting doubt on the recent progress in saliency-based XAI.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8063,our contributions are end end trainable that combines deep concept learning interpretable decision logic soft differentiable decision tree and qualitative and quantitative evaluations showing dcdt generates more faithful and human understandable explanations than saliency based methods while maintaining competitive the primary task,the intricate and opaque nature deep neural networks dnns makes difficult decipher how they make decisions explainable artificial intelligence xai has emerged promising remedy this conundrum quantitatively fourteen saliency based xai methods for classifiers utilizing convolutional neural networks and vision transformers trained perturbed cifar cifar and imagenet datasets respectively,2025-08-26T00:48:30.451035
46,Learning Fair Representations by Calibrating the Eigenspectrum of the Covariance Matrix,"Algorithmic fairness aims to ensure that models do not disproportionately harm specific demographic groups. Many methods achieve fairness by learning representations that are invariant to sensitive attributes, often through adversarial training. We propose a novel, non-adversarial approach to learning fair representations by directly shaping the geometry of the representation space. We observe that in unfair models, the principal components (eigenvectors) of the feature covariance matrix often align with the sensitive attribute. Our method, Spectral Fairness Calibration (SFC), adds a regularization term to the training objective that penalizes this alignment. Specifically, SFC encourages the eigenvectors of the representation covariance matrix to be orthogonal to the direction of sensitive attribute variation. This effectively ""re-calibrates"" the representation space to remove information about the sensitive attribute from its main axes of variation, without needing a complex adversarial min-max game. Our contributions are: (1) a new perspective on fairness through the lens of spectral analysis; (2) the SFC regularizer, a simple and efficient non-adversarial fairness intervention; and (3) empirical results showing that SFC achieves state-of-the-art fairness-accuracy trade-offs on benchmark datasets.",ICLR,deep learning,gemini-2.5-pro,True,5494,Adversarial Latent Feature Augmentation for Fairness,"Achieving fairness in machine learning remains a critical challenge, especially due to the opaque effects of data augmentation on input spaces within nonlinear neural networks. Nevertheless, current approaches that emphasize augmenting latent features, rather than input spaces, offer limited insights into their ability to detect and mitigate bias. In response, we introduce the concept of the ""unfair region"" in the latent space, a subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high, leading to unfair prediction results. To address this, we propose Adversarial Latent Feature Augmentation (ALFA), a method that leverages adversarial fairness attacks to perturb latent space features, which are then used as data augmentation for fine-tuning. ALFA intentionally shifts latent features into unfair regions, and the last layer of the network is fine-tuned with these perturbed features, leading to a corrected decision boundary that enhances fairness in classification in a cost-effective manner. We present a theoretical framework demonstrating that our adversarial fairness objective reliably generates biased feature perturbations, and that fine-tuning on samples from these unfair regions ensures fairness improvements. Extensive experiments across diverse datasets, modalities, and backbone networks validate that training with these adversarial features significantly enhances fairness while maintaining predictive accuracy in classification tasks.",ICLR.cc/2025/Conference,6.5,True,0.8589,algorithmic fairness aims ensure that models not disproportionately harm specific demographic groups many methods achieve fairness learning representations that are invariant sensitive attributes often adversarial training non adversarial learning fair representations directly shaping the geometry the representation space observe that unfair models the principal components eigenvectors the feature covariance matrix often align the sensitive attribute our spectral fairness calibration sfc adds regularization term the training objective that penalizes this alignment sfc encourages the eigenvectors the representation covariance matrix orthogonal the direction sensitive attribute variation this calibrates the representation space remove information about the sensitive attribute from its main axes variation needing complex adversarial min max game our contributions are perspective fairness the lens spectral analysis the sfc regularizer simple and efficient non adversarial fairness intervention and empirical showing that sfc achieves state the art fairness accuracy trade offs datasets,achieving fairness machine learning remains critical challenge due the opaque effects data augmentation input spaces within nonlinear neural networks response the concept the unfair region the latent space subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high leading unfair prediction address this adversarial latent feature augmentation alfa that leverages adversarial fairness attacks perturb latent space features which are then used data augmentation for fine tuning alfa intentionally shifts latent features into unfair regions and the last layer the network fine tuned these perturbed features leading corrected decision boundary that enhances fairness classification cost effective manner theoretical demonstrating that our adversarial fairness objective reliably generates biased feature perturbations and that fine tuning samples from these unfair regions ensures fairness improvements extensive experiments across diverse datasets modalities and backbone networks that training these adversarial features enhances fairness while maintaining predictive classification tasks,2025-08-26T00:48:30.451036
47,Training Data Attribution using Functional Gradient Hacking,"Understanding which training examples are most responsible for a model's prediction is crucial for debugging, fairness, and identifying dataset biases. Existing data attribution methods are often computationally expensive or rely on strong simplifying assumptions. We introduce Functional Gradient Hacking (FGH), a fast and accurate method for training data attribution. FGH is based on the insight that the influence of a training point can be approximated by measuring how much that point's functional gradient (the gradient of the loss with respect to the model's output logits) aligns with the direction of a parameter update designed to ""hack"" or change a specific test prediction. By framing attribution as a functional projection problem, we avoid the need to compute expensive inverse Hessian-vector products. Our contributions are: (1) a new, theoretically-grounded framework for data attribution; (2) the FGH algorithm, which is orders of magnitude faster than influence functions; and (3) extensive experiments demonstrating that FGH provides more faithful attributions for complex models like Transformers compared to prior art, successfully identifying mislabeled examples and sources of bias.",ICLR,deep learning,gemini-2.5-pro,True,7033,Dynamic Influence Tracker: Estimating Sample Influence in SGD-Trained Models across Arbitrary Time Windows,"Understanding how training samples affect models improves model interpretability, optimization strategies, and anomaly detection. However, existing methods for estimating sample influence provide only static assessments, rely on restrictive assumptions, and require high computational costs. 
	We propose Dynamic Influence Tracker (DIT), a novel method to estimate time-varying sample influence in models trained with Stochastic Gradient Descent (SGD). DIT enables fine-grained analysis of sample influence within arbitrary time windows during training through a two-phase algorithm. The training phase efficiently captures and stores necessary information about the SGD trajectory, while the inference phase computes the influence of samples on the model within a specified time window. We provide a theoretical error bound for our estimator without assuming convexity, showing its reliability across various learning scenarios. Our experimental results reveal the evolution of sample influence throughout the training process, enhancing understanding of learning dynamics. We show DIT's effectiveness in improving model performance through anomalous sample detection and its potential for advancing curriculum learning.",ICLR.cc/2025/Conference,5.0,False,0.8120,understanding which training examples are most responsible for model prediction crucial for debugging fairness and identifying biases fgh the insight that the influence training point can approximated measuring how much that point functional gradient the gradient the loss respect the model output logits aligns the direction parameter update designed hack change specific prediction,understanding how training samples affect models improves interpretability optimization strategies and anomaly detection provide theoretical error bound for our estimator assuming convexity showing its reliability across various learning scenarios our experimental reveal the evolution sample influence throughout the training process enhancing understanding learning dynamics dit effectiveness improving anomalous sample detection and its potential for advancing curriculum learning,2025-08-26T00:48:30.451038
48,Physics-Informed Neural Radiance Fields for Dynamic Scenes,"Neural Radiance Fields (NeRF) have revolutionized novel view synthesis for static scenes, but modeling dynamic, moving scenes remains a major challenge. We introduce Physics-Informed NeRF (PI-NeRF), a framework that integrates principles of classical mechanics directly into the representation of dynamic scenes. Instead of learning a simple deformation field, PI-NeRF models a scene as a collection of particles, each with a position, radiance, and density, governed by a learnable Neural Potential Energy Function. A Hamiltonian dynamics solver, implemented as a differentiable layer, evolves the state of these particles through time. This physics-based parameterization provides a strong inductive bias for realistic motion, allowing the model to generalize to unseen time steps and produce physically plausible interpolations. Our contributions are: (1) a novel representation for dynamic scenes grounded in Hamiltonian mechanics; (2) the integration of a differentiable physics solver into the NeRF framework; and (3) state-of-the-art results on novel view and novel time synthesis for complex, non-rigidly deforming scenes, demonstrating significantly improved temporal coherence and physical plausibility compared to existing dynamic NeRF models.",ICLR,deep learning,gemini-2.5-pro,True,10977,NeuGen: Amplifying the ‘Neural’ in Neural Radiance Fields for Domain Generalization,"Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures, capable of initiating training from scratch or fine-tuning pre-trained models, which cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows benchmarking performance on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.",ICLR.cc/2025/Conference,4.5,False,0.8827,neural radiance fields nerf have revolutionized view synthesis for static scenes but modeling dynamic moving scenes remains major challenge physics informed nerf nerf that integrates principles classical mechanics directly into the representation dynamic scenes instead learning simple deformation field nerf models scene collection particles each position radiance and density governed learnable neural potential energy function our contributions are representation for dynamic scenes grounded hamiltonian mechanics the integration differentiable physics solver into the nerf and state the art view and time synthesis for complex non rigidly deforming scenes demonstrating improved temporal coherence and physical plausibility compared existing dynamic nerf models,neural radiance fields nerf have advanced the field view synthesis yet their generalization across diverse scenes and conditions remains challenging addressing this the integration brain inspired normalization neural generalization neugen into leading nerf architectures which include mvsnerf and geonerf can seamlessly integrated into nerf architectures capable initiating training from scratch fine tuning pre trained models which cultivates comprehensive feature set that improves and robustness image rendering our exemplifies the potential merging neuroscientific principles deep learning frameworks setting precedent for enhanced generalizability and efficiency view synthesis,2025-08-26T00:48:30.451039
49,Equivariant Subgraph Neural Networks,"Graph Neural Networks (GNNs) typically operate on the node and edge level, struggling to capture higher-order structural motifs or subgraph patterns that are crucial for many graph-based tasks, such as molecular property prediction. We propose Equivariant Subgraph Neural Networks (ESNNs), a new class of GNNs that learn representations of subgraphs while respecting the symmetries of the graph. ESNNs first extract a basis set of computationally feasible subgraphs (e.g., all 3- and 4-node motifs) around each node. A neural network, designed to be equivariant to the permutation of nodes within each subgraph, then learns a representation for each subgraph instance. Finally, a permutation-invariant pooling layer aggregates these subgraph representations to produce the final node or graph embedding. Our contributions are: (1) a principled framework for incorporating subgraph information into GNNs; (2) a novel equivariant architecture for learning from sets of subgraphs; and (3) state-of-the-art results on several graph classification and regression benchmarks where higher-order structure is paramount. ESNNs provide a more powerful tool for learning from graphs by explicitly modeling representations at the subgraph level.",ICLR,deep learning,gemini-2.5-pro,True,6374,Improving Graph Neural Networks with Heterophily-based Filtration and Filtration Learning,"Graph neural networks (GNNs) are a powerful method of learning representations of graph-structured data. While they excel at learning class-discriminative representations of nodes in homophilous graphs, where connecting nodes tend to belong to the same class, many GNNs struggle with heterophilous graphs whose inter-class connections can muddy the message passing.  Inspired by this finding, we propose a topological filtration scheme, treating graphs as 1-dimensional simplicial complexes N  with a filter function based on estimated edge heterophily, and introduce two methodologies that use a backbone GNN to learn from the resulting graph filtration. The first trains a GNN on each graph in the filtration sequence consecutively for a portion of the total training time, using embeddings from previous graphs to initialize node embeddings in subsequent graphs. The second approach uses a novel message passing scheme to pass messages jointly within each and between graph levels in the filtration sequence with common nodes. Both methods enhance the influence of early birth adjacent nodes in homophilous subgraphs, yet allow for the model to learn from the full range of heterophilous and homophilous connections in the graph. We further extend our approach to learn a graph filtration sequence of graphs through a learnable node filter function. Experiments show that our heterophily-filtered GNNs achieve superior node classification accuracy on heterophilous and homophilous networks alike.",ICLR.cc/2025/Conference,2.0,nan,0.8785,graph neural networks gnns operate the node and edge level struggling capture higher order structural motifs subgraph patterns that are crucial for many graph based tasks such molecular property prediction equivariant subgraph neural networks esnns class gnns that learn representations subgraphs while respecting the symmetries the graph neural network designed equivariant the permutation nodes within each subgraph then learns representation for each subgraph instance finally permutation invariant pooling layer aggregates these subgraph representations produce the final node graph embedding our contributions are principled for incorporating subgraph information into gnns equivariant for learning from sets subgraphs and state the art several graph classification and regression benchmarks where higher order structure paramount esnns provide more powerful tool for learning from graphs explicitly modeling representations the subgraph level,graph neural networks gnns are powerful learning representations graph structured data while they excel learning class discriminative representations nodes homophilous graphs where connecting nodes tend belong the same class many gnns struggle heterophilous graphs whose inter class connections can muddy the message passing experiments that our heterophily filtered gnns achieve superior node classification heterophilous and homophilous networks alike,2025-08-26T00:48:30.451040
50,Adaptive Layer-wise Preconditioning for Deep Learning Optimization,"State-of-the-art optimizers like Adam apply the same adaptive learning rate logic to all parameters, ignoring the vast functional differences between layers in a deep neural network. For instance, the geometry of the loss surface with respect to early convolutional layers is fundamentally different from that of final linear layers. We introduce AdaPrecon, an optimizer that learns a layer-specific preconditioning matrix to rescale gradients according to their unique geometry. AdaPrecon maintains a low-rank, diagonal-plus-rank-one approximation of the inverse Hessian for each layer, updated efficiently via an online Kalman filtering-like process. This allows it to capture layer-wise curvature information with minimal memory and computational overhead. Our key contributions are: (1) a formal argument for the necessity of layer-wise preconditioning; (2) the AdaPrecon algorithm, which efficiently learns and applies these preconditions; and (3) a theoretical analysis of its convergence properties. On a wide range of tasks, including training large-scale Vision Transformers and GANs, AdaPrecon converges significantly faster and to better final solutions than AdamW and other second-order methods, demonstrating the power of heterogeneous, layer-aware optimization.",ICLR,deep learning,gemini-2.5-pro,True,1076,Continuous-Time Analysis of Adaptive Optimization and Normalization,"Adaptive optimization algorithms, particularly Adam and its variant AdamW, are fundamental to modern deep learning, however, their training dynamics lack comprehensive theoretical understanding, with limited insight into why common practices—such as specific hyperparameter choices and normalization layers—contribute to successful generalization. This work presents a continuous-time formulation of Adam and AdamW, facilitating a tractable analysis of training dynamics that can shed light on such practical questions. We theoretically derive a stable region for Adam's hyperparameters $(\beta, \gamma)$ that ensures bounded updates, empirically verifying these predictions by observing unstable exponential growth of parameter updates outside this region. Furthermore, we theoretically justify the success of normalization layers by uncovering an implicit meta-adaptive effect of scale-invariant architectural components. This insight leads to an explicit optimizer, $2$-Adam, which we generalize to $k$-Adam—an optimizer that applies an adaptive normalization procedure $k$ times, encompassing Adam (corresponding to $k=1$) and Adam with a normalization layer (corresponding to $k=2$). Overall, our continuous-time formulation of Adam facilitates a principled analysis, offering deeper understanding of optimal hyperparameter choices and architectural decisions in modern deep learning.",ICLR.cc/2025/Conference,4.25,False,0.8736,state the art optimizers like adam apply the same adaptive learning rate logic all parameters ignoring the vast functional differences between layers deep neural network wide range tasks including training large scale vision transformers and gans adaprecon converges faster and better final solutions than adamw and other second order methods demonstrating the power heterogeneous layer aware optimization,adaptive optimization algorithms adam and its variant adamw are fundamental modern deep learning however their training dynamics lack comprehensive theoretical understanding limited insight into why common practices such specific hyperparameter choices and normalization layers contribute successful generalization overall our continuous time formulation adam facilitates principled analysis offering deeper understanding optimal hyperparameter choices and architectural decisions modern deep learning,2025-08-26T00:48:30.451044
51,Wavelet Diffusion: A Multi-Resolution Approach for High-Fidelity Audio Generation,"Denoising diffusion models have excelled at image synthesis but struggle with raw audio generation due to the extremely long sequence lengths and the importance of phase information across multiple time scales. We propose Wavelet Diffusion, a generative model that performs the diffusion process in the wavelet domain. By applying a discrete wavelet transform, we decompose the raw audio signal into multiple frequency sub-bands, each representing different temporal resolutions. The diffusion and denoising processes are then applied in this structured, multi-resolution space. This approach allows the model to capture both coarse, low-frequency structure and fine-grained, high-frequency details more efficiently. Our contributions are: (1) a novel framework for audio generation via diffusion in the wavelet domain; (2) a specialized U-Net architecture that operates on wavelet coefficients and respects their multi-resolution structure; and (3) a significant reduction in computational complexity compared to time-domain models. Wavelet Diffusion achieves state-of-the-art results in unconditional audio generation on the SC09 and VCTK datasets, producing samples with superior fidelity and coherence.",ICLR,deep learning,gemini-2.5-pro,True,11065,Decouple-Then-Merge: Towards Better Training for Diffusion Models,"Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a $\textbf{De}$couple-then-$\textbf{Me}$rge ($\textbf{DeMe}$) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is included in the supplementary material and will be released on Github.",ICLR.cc/2025/Conference,4.2,nan,0.8500,denoising diffusion models have excelled image synthesis but struggle raw audio generation due the extremely long sequence lengths and the importance phase information across multiple time scales wavelet diffusion generative that performs the diffusion process the wavelet domain our contributions are for audio generation diffusion the wavelet domain specialized net that operates wavelet coefficients and respects their multi resolution structure and significant reduction computational complexity compared time domain models wavelet diffusion achieves state the art unconditional audio generation the sc09 and vctk datasets producing samples superior fidelity and coherence,diffusion models are trained learning sequence models that reverse each step noise corruption however since the denoising tasks differ each timestep the gradients computed different timesteps may conflict potentially degrading the overall image generation several improved techniques during the finetuning stage promote effective knowledge sharing while minimizing training interference across timesteps experimental significant generation quality improvements upon benchmarks including stable diffusion coco30k imagenet1k partiprompts and ddpm lsun church lsun bedroom and cifar10,2025-08-26T00:48:30.451046
52,Generalization and the Fractal Dimension of the Loss Landscape,"The relationship between the geometry of the loss landscape and generalization in deep neural networks remains a central open question. While measures like sharpness have provided some insights, they fail to capture the multi-scale, intricate nature of these high-dimensional surfaces. We propose a new perspective: generalization is controlled by the *fractal dimension* of the solution manifold at the bottom of a loss basin. We hypothesize that wider, more robust minima correspond to regions with a lower fractal dimension, indicating a smoother, more regular function space. We introduce a practical algorithm, the Box-Counting Manifold Explorer (BCME), to estimate this dimension by analyzing the scaling of the loss function under random perturbations in parameter space. Our contributions are: (1) a novel theoretical framework linking generalization to the fractal dimension of the solution space; (2) the BCME algorithm for its empirical estimation; and (3) extensive experiments showing that the measured fractal dimension is a more consistent and accurate predictor of the generalization gap than sharpness across various architectures, datasets, and regularization techniques.",ICLR,deep learning,gemini-2.5-pro,True,5253,Stagewise Development in Transformers and the Geometry of the Loss Landscape,"Deep learning involves navigating a high-dimensional parameter space guided by the loss landscape. In the process, complex computational structures form and re-form inside the neural network, leading to shifts in input--output behavior. It is a priority for the science of deep learning to uncover principles governing the development of neural network structure and behavior. Drawing from the framework of singular learning theory, we propose that model development is governed by the local geometry of the loss landscape. We investigate this link by monitoring the geometry of the loss landscape throughout training for transformers trained as language models or for a synthetic in-context regression task. We divide training into ``developmental stages'' marking discrete shifts in loss landscape geometry. We then confirm that these stages coincide with significant changes in the internal computational structure and the input--output behavior of our models. Our findings provide new insights into transformer development and underscore the potential of a geometric perspective for understanding modern deep learning.",ICLR.cc/2025/Conference,5.5,False,0.8463,the relationship between the geometry the loss landscape and generalization deep neural networks remains central open question,deep learning involves navigating high dimensional parameter space guided the loss landscape the process complex computational structures form and form inside the neural network leading shifts input output behavior priority for the science deep learning uncover principles governing the development neural network structure and behavior drawing from the singular learning theory that development governed the local geometry the loss landscape this link monitoring the geometry the loss landscape throughout training for transformers trained language models for synthetic context regression task our provide insights into transformer development and underscore the potential geometric perspective for understanding modern deep learning,2025-08-26T00:48:30.451054
53,Intrinsically Motivated Exploration by Learning the Dynamics of Novelty,"Effective exploration remains a major hurdle in reinforcement learning, especially in sparse-reward environments. Many intrinsic motivation methods encourage visiting novel states but fail to distinguish between stochastic noise and genuinely new, learnable parts of the environment. We introduce the Novelty Dynamics Learner (NDL), an exploration strategy where the agent is intrinsically motivated to improve its forward-dynamics model of *how novelty changes*. The agent builds a world model that predicts not the next state, but the novelty of the next state, given the current state and action. The intrinsic reward is the agent's ability to accurately predict this ""novelty landscape."" This encourages the agent to systematically seek out frontiers between known and unknown territories, as these are the regions where its novelty-dynamics model can be maximally improved. Our contributions include: (1) a new intrinsic motivation based on learning the dynamics of novelty itself; (2) a practical algorithm combining a novelty-dynamics world model with a standard policy learner; and (3) state-of-the-art performance on notoriously hard exploration benchmarks like Montezuma's Revenge and procedurally generated mazes.",ICLR,deep learning,gemini-2.5-pro,True,7753,The Critic as an Explorer: Lightweight and Provably Efficient Exploration for Deep Reinforcement Learning,"Exploration remains a critical challenge in reinforcement learning (RL), with many existing methods either lacking theoretical guarantees or being computationally impractical for real-world applications. We introduce Litee, a lightweight algorithm that repurposes the value network in standard deep RL algorithms to effectively drive exploration without introducing additional parameters. Litee utilizes linear multi-armed bandit (MAB) techniques, enabling efficient exploration with provable sub-linear regret bounds while preserving the core structure of existing RL algorithms. Litee is simple to implement, requiring only around 10 lines of code. It also substantially reduces computational overhead compared to previous theoretically grounded methods, lowering the complexity from O(n^3) to O(d^3), where n is the number of network parameters and d is the size of the embedding in the value network. Furthermore, we propose Litee+, an extension that adds a small auxiliary network to better handle sparse reward environments, with only a minor increase in parameter count (less than 1%) and additional 10 lines of code. Experiments on the MiniHack suite and MuJoCo demonstrate that Litee and Litee+ empirically outperform state-of-the-art baselines, effectively bridging the gap between theoretical rigor and practical efficiency in RL exploration.",ICLR.cc/2025/Conference,3.75,False,0.8729,effective exploration remains major hurdle reinforcement learning sparse reward environments our contributions include intrinsic motivation learning the dynamics novelty itself practical combining novelty dynamics world standard policy learner and state the art notoriously hard exploration benchmarks like montezuma revenge and procedurally generated mazes,exploration remains critical challenge reinforcement learning many existing methods either lacking theoretical guarantees being computationally impractical for real world applications litee lightweight that repurposes the value network standard deep algorithms drive exploration introducing additional parameters also reduces computational overhead compared previous theoretically grounded methods lowering the complexity from where the number network parameters and the size the embedding the value network furthermore litee extension that adds small auxiliary network better handle sparse reward environments only minor increase parameter count less than and additional lines code,2025-08-26T00:48:30.451059
54,Orthogonal Projection for Rehearsal-Free Continual Learning,"Catastrophic forgetting is a major obstacle for continual learning, where models must learn a sequence of tasks. Rehearsal-based methods mitigate this by storing past data, but this violates privacy constraints and scales poorly. We introduce Orthogonal Projection Continual Learning (OPCL), a rehearsal-free method that prevents forgetting by constraining parameter updates. For each new task, OPCL identifies the subspace of the network's parameters crucial for previous tasks. It then constrains the gradient updates for the new task to be strictly orthogonal to this ""old task"" subspace. This ensures that learning the new task does not interfere with the knowledge required to perform previous ones. The core of our method is an efficient, online singular value decomposition technique to track the principal components of the Fisher Information Matrix for each task. Our contributions are: (1) a novel rehearsal-free framework based on orthogonal gradient projection; (2) a scalable method to identify and preserve task-specific subspaces; and (3) results on Split-CIFAR100 and a sequence of five reinforcement learning tasks, demonstrating that OPCL drastically reduces forgetting and outperforms other rehearsal-free methods.",ICLR,deep learning,gemini-2.5-pro,True,7463,Plasticity from Structured Sparsity: Mastering Continual Reinforcement Learning through Fine-grained Network Allocation and Dormant Neuron Exploration,"Continual reinforcement learning faces a central challenge in striking a balance between plasticity and stability to mitigate catastrophic forgetting. In this paper, we introduce SSDE, a novel structure-based method that aims to improve plasticity through a fine-grained allocation strategy with Structured Sparsity and Dormant-guided Exploration. Specifically, SSDE decomposes the parameter space for each task into forward-transfer (frozen) parameters and task-specific (trainable) parameters. Crucially, these parameters are allocated by an efficient co-allocation scheme under sparse coding, ensuring sufficient trainable capacity for new tasks while promoting efficient forward transfer through frozen parameters. Furthermore, structure-based methods often suffer from rigidity due to the accumulation of non-trainable parameters, hindering exploration. To overcome this, we propose a novel exploration technique based on sensitivity-guided dormant neurons, which systematically identifies and resets insensitive parameters. Our comprehensive experiments demonstrate that SSDE outperforms current state-of-the-art methods and achieves a superior success rate of $95\%$% on CW10 Continual World benchmark.",ICLR.cc/2025/Conference,5.75,False,0.8211,orthogonal projection continual learning opcl rehearsal free that prevents forgetting constraining parameter updates this ensures that learning the task does not interfere the knowledge required perform previous ones our contributions are rehearsal free orthogonal gradient projection scalable identify and preserve task specific subspaces and split cifar100 and sequence five reinforcement learning tasks demonstrating that opcl drastically reduces forgetting and outperforms other rehearsal free methods,continual reinforcement learning faces central challenge striking balance between plasticity and stability mitigate catastrophic forgetting crucially these parameters are allocated efficient allocation scheme under sparse coding ensuring sufficient trainable capacity for tasks while promoting efficient forward transfer frozen parameters,2025-08-26T00:48:30.451064
55,HyperGNN: Learning on Hypergraphs with Variable-Order Relational Pooling,"Traditional Graph Neural Networks are limited to pairwise interactions, failing to capture the higher-order relationships prevalent in complex systems like social networks, co-authorship graphs, and biological pathways. Hypergraphs provide a natural abstraction for these systems, but existing HyperGNNs often treat hyperedges as uniform cliques, losing information about their internal structure. We propose HyperGNN, a novel architecture that explicitly models relations of variable order. HyperGNN introduces a two-stage message passing scheme: first within each hyperedge using a permutation-invariant set network, and second between nodes using the aggregated hyperedge information. The key innovation is Variable-Order Relational Pooling (VORP), a learnable pooling mechanism that weights the influence of hyperedges based on their size (order) and feature composition. Our contributions are: (1) a dual message-passing framework for hypergraphs; (2) the VORP module for adaptive higher-order information aggregation; and (3) new state-of-the-art results on several hypergraph node classification benchmarks, demonstrating HyperGNN's ability to effectively leverage complex, multi-way dependencies.",ICLR,deep learning,gemini-2.5-pro,True,9583,Hierarchical Self-Supervised Graph Contrastive Learning: Capturing Multi-Scale Structural Information,"Graph Neural Networks (GNNs) have emerged as powerful tools for learning rep-resentations from graph-structured data Kipf & Welling (2017); Veliˇckovic´ et al.(2018), but often rely heavily on labeled data for training. This paper introduces a novel hierarchical self-supervised graph contrastive learning framework that ef-fectively leverages unlabeled data to enhance node representations. Our method captures rich structural information at multiple scales by incorporating contrastive objectives at the node, subgraph, and graph levels, extending previous work on self-supervised learning for graphs Veliˇckovic´ et al. (2019); You et al. (2020). We employ an adaptive graph augmentation strategy to generate meaningful views of the graph while preserving essential properties. Through extensive experiments on benchmark datasets, including Cora, Citeseer, PubMed Sen & Dhillon (2008), and Reddit Hamilton et al. (2017), we demonstrate that our approach consistently outperforms both supervised and self-supervised baseline models in node clas-sification tasks. Our method shows particular strength in low-label regimes and exhibits strong generalization capabilities in both transductive and inductive set-tings. Ablation studies confirm the importance of each hierarchical component, while qualitative analyses illustrate the discriminative power of the learned em-beddings. This work opens new avenues for self-supervised learning on graphs and has broad implications for applications where labeled data is scarce or ex-pensive to obtain, such as in social networks Perozzi et al. (2014) and biological networks Zitnik et al. (2017).",ICLR.cc/2025/Conference,2.6,False,0.8461,traditional graph neural networks are limited pairwise interactions failing capture the higher order relationships prevalent complex systems like social networks authorship graphs and biological pathways the key innovation variable order relational pooling vorp learnable pooling mechanism that weights the influence hyperedges their size order and feature composition our contributions are dual message passing for hypergraphs the vorp module for adaptive higher order information aggregation and state the art several hypergraph node classification benchmarks demonstrating hypergnn ability leverage complex multi way dependencies,graph neural networks gnns have emerged powerful tools for learning rep resentations from graph structured data kipf welling veliˇckovic this introduces hierarchical self supervised graph contrastive learning that fectively leverages unlabeled data enhance node representations our captures rich structural information multiple scales incorporating contrastive objectives the node subgraph and graph levels extending previous self supervised learning for graphs veliˇckovic that our consistently outperforms both supervised and self supervised models node clas sification tasks this opens avenues for self supervised learning graphs and has broad implications for applications where labeled data scarce pensive obtain such social networks perozzi,2025-08-26T00:48:30.451071
56,Certified Adversarial Robustness via Randomized Smoothing in Function Space,"Randomized smoothing is a powerful technique for providing certified robustness against adversarial attacks, but it is typically applied in the input space, leading to certificates that can be loose and difficult to scale to complex transformations. We introduce Functional Randomized Smoothing (FRS), a new framework that extends this technique to the function space of the model itself. Instead of smoothing the input, FRS trains an ensemble of models and performs smoothing over the aggregated output logits. By adding Gaussian noise to the weights of the final layer during inference and analyzing the resulting distribution of outputs, we can derive a tight, certified radius in the model's function space within which the prediction is constant. This function-space certificate can then be mapped back to derive input-space robustness guarantees for various threat models. Our contributions are: (1) the FRS framework for certified robustness in function space; (2) a derivation of the corresponding certificates and their connection to input-space robustness; and (3) empirical results showing that FRS provides significantly larger certified radii on CIFAR-10 and ImageNet compared to state-of-the-art input-space smoothing techniques.",ICLR,deep learning,gemini-2.5-pro,True,8070,Robust Representation Consistency Model via Contrastive Denoising,"Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3\% on average, with up to 11.6\% at larger radii, while reducing inference costs by 85x on average.",ICLR.cc/2025/Conference,6.75,True,0.8494,randomized smoothing powerful for providing certified robustness against adversarial attacks but applied the input space leading certificates that can loose and difficult scale complex transformations adding gaussian noise the weights the final layer during inference and analyzing the resulting distribution outputs can derive tight certified radius the model function space within which the prediction constant this function space certificate can then mapped back derive input space robustness guarantees for various threat models our contributions are the frs for certified robustness function space derivation the corresponding certificates and their connection input space robustness and empirical showing that frs provides larger certified radii cifar and imagenet compared state the art input space smoothing techniques,robustness essential for deep neural networks security sensitive applications this end randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations,2025-08-26T00:48:30.451078
57,Specformer: A Spectral Decomposition Architecture for Long-Term Time Series Forecasting,"Transformers have shown promise for time series forecasting but struggle with very long-term predictions due to the quadratic complexity of self-attention and their tendency to overfit noisy temporal patterns. We propose Specformer, a novel architecture that operates in the frequency domain to achieve more robust and efficient long-term forecasting. Specformer first uses the Fast Fourier Transform (FFT) to project the input time series into the spectral domain. It then applies a series of specialized attention blocks designed to operate on spectral representations, allowing it to efficiently identify and extrapolate key periodicities and trends. A crucial component is a frequency-gated attention mechanism that learns to focus on the most informative frequency bands for the prediction task. Our contributions are: (1) a new deep learning architecture for time series forecasting based on spectral decomposition; (2) a frequency-gated attention mechanism; and (3) state-of-the-art results on long-term forecasting benchmarks, outperforming both Transformer-based and traditional statistical methods while being linearly scalable with sequence length.",ICLR,deep learning,gemini-2.5-pro,True,5531,FlexTSF: A universal forecasting model for time series with variable regularities,"Developing a foundation model for time series forecasting across diverse domains has attracted significant attention in recent years. Existing works typically assume regularly sampled, well-structured data, limiting their applicability to more generalized scenarios where time series often contain missing values, unequal sequence lengths, and irregular time intervals between measurements. To cover diverse domains and handle variable regularities, we propose FlexTSF, a universal time series forecasting model that possesses better generalization and natively support both regular and irregular time series. FlexTSF produces forecasts in an autoregressive manner and incorporates three novel designs: VT-Norm, a normalization strategy to ablate data domain barriers, IVP Patcher, a patching module to learn representations from flexibly structured time series, and LED attention, an attention mechanism seamlessly integrating these two and propagate forecasts with awareness of domain and time information, enabling effective time series forecasting across varying regularities. Experiments on $12$ datasets show that FlexTSF outperforms state-of-the-art forecasting models respectively designed for regular and irregular time series. Furthermore, after self-supervised pre-training, FlexTSF shows exceptional performance in both zero-shot and few-show settings for time series forecasting.",ICLR.cc/2025/Conference,4.75,False,0.8100,specformer that operates the frequency domain achieve more robust and efficient long term forecasting specformer first uses the fast fourier transform fft project the input time series into the spectral domain then applies series specialized attention blocks designed operate spectral representations allowing identify and extrapolate key periodicities and trends crucial component frequency gated attention mechanism that learns focus the most informative frequency bands for the prediction task our contributions are deep learning for time series forecasting spectral decomposition frequency gated attention mechanism and state the art long term forecasting benchmarks outperforming both transformer based and traditional statistical methods while being linearly scalable sequence length,developing foundation for time series forecasting across diverse domains has attracted significant attention recent years flextsf produces forecasts autoregressive manner and incorporates three designs norm normalization strategy ablate data domain barriers ivp patcher patching module learn representations from flexibly structured time series and led attention attention mechanism seamlessly integrating these two and propagate forecasts awareness domain and time information enabling effective time series forecasting across varying regularities furthermore after self supervised pre training flextsf shows exceptional both zero shot and few show settings for time series forecasting,2025-08-26T00:48:30.451082
58,Learning on Implicitly-Defined Manifolds with Neural Metric Fields,"Geometric deep learning has demonstrated the power of incorporating known manifold structures into network architectures. However, in many real-world problems, data lies on a low-dimensional manifold whose geometry is unknown. We introduce Neural Metric Fields (NMFs), a method for performing deep learning on such implicitly-defined manifolds. NMFs learn a continuous function, parameterized by a neural network, that maps any point in the ambient space to the local Riemannian metric tensor of the underlying manifold. This learned metric allows us to compute intrinsic geometric quantities like geodesic distances and the Laplace-Beltrami operator, enabling us to define geometrically-aware network operations like manifold convolutions. Our contributions are: (1) a framework for learning the Riemannian geometry of data manifolds without explicit parameterization; (2) a method for performing manifold-based deep learning using the learned metric; and (3) demonstrations on synthetic and real-world datasets, showing NMFs can discover non-trivial manifold structures and improve downstream task performance.",ICLR,deep learning,gemini-2.5-pro,True,10326,Reset Method based on the Theory of Manifold Optimization on Real Manifolds,"Manifold optimization is prominent in the fields of applied mathematics, statistics, machine learning, and in particular, deep learning. By leveraging the intrinsic geometric properties of manifolds, constrained optimization problems can be transformed into unconstrained optimization problems on certain manifolds.  An innovative method, Reset Method, is introduced that combines manifold optimization and standard methods (SGD, Adam and AdamW), aiming to enhance the improvement of precision. The efficacy of our proposed method is corroborated by extensive deep learning experiments, providing visible higher precision.",ICLR.cc/2025/Conference,3.0,False,0.8439,geometric deep learning has demonstrated the power incorporating known manifold structures into network architectures neural fields nmfs for performing deep learning such implicitly defined manifolds nmfs learn continuous function parameterized neural network that maps any point the ambient space the local riemannian tensor the underlying manifold this learned allows compute intrinsic geometric quantities like geodesic distances and the laplace beltrami operator enabling define geometrically aware network operations like manifold convolutions our contributions are for learning the riemannian geometry data manifolds explicit parameterization for performing manifold based deep learning the learned and demonstrations synthetic and real world datasets showing nmfs can discover non trivial manifold structures and improve downstream task,manifold optimization prominent the fields applied mathematics statistics machine learning and particular deep learning leveraging the intrinsic geometric properties manifolds constrained optimization problems can transformed into unconstrained optimization problems certain manifolds innovative reset introduced that combines manifold optimization and standard methods sgd adam and adamw aiming enhance the improvement the efficacy our proposed corroborated extensive deep learning experiments providing visible higher,2025-08-26T00:48:30.451087
59,Causal Explanations via Counterfactual Input Optimization,"Saliency-based explainability methods highlight which input features are correlated with a model's prediction, but they do not explain *why* the prediction was made. We propose Causal Counterfactual Explanations (CCE), a method for generating explanations by finding minimal, causally-plausible interventions on the input that change the model's output. Given an input, CCE solves an optimization problem to find a ""counterfactual"" input that is as close as possible to the original but results in a different classification. Crucially, this optimization is constrained by a learned causal model of the data, ensuring that the generated counterfactuals are realistic and do not violate the underlying data-generating process (e.g., changing a ""cat"" into a ""dog"" without changing its background). The difference between the original input and the counterfactual serves as a direct, causal explanation. Our contributions are: (1) a formal framework for generating explanations as minimal causal interventions; (2) an algorithm for finding such explanations via constrained optimization; and (3) qualitative and user studies demonstrating that CCEs are more interpretable and useful than saliency maps for model debugging.",ICLR,deep learning,gemini-2.5-pro,True,1925,Refining Counterfactual Explanations With Joint-Distribution-Informed Shapley Towards Actionable Minimality,"Counterfactual explanations (CE) identify data points that closely resemble the observed data but produce different machine learning (ML) model outputs, offering critical insights into model decisions. Despite the diverse scenarios, goals and tasks to which they are tailored, existing CE methods often lack actionable efficiency because of unnecessary feature changes included within the explanations that are presented to users and stakeholders. We address this problem by proposing a method that minimizes the required feature changes while maintaining the validity of CE, without imposing restrictions on models or CE algorithms, whether instance- or group-based. The key innovation lies in computing a joint distribution between observed and counterfactual data and leveraging it to inform Shapley values for feature attributions (FA). We demonstrate that optimal transport (OT) effectively derives this distribution, especially when the alignment between observed and counterfactual data is unclear in used CE methods. Additionally, a counterintuitive finding is uncovered: it may be misleading to rely on an exact alignment defined by the CE generation mechanism in conducting FA. Our proposed method is validated on extensive experiments across multiple datasets, showcasing its effectiveness in refining CE towards greater actionable efficiency.",ICLR.cc/2025/Conference,7.0,False,0.8456,saliency based explainability methods highlight which input features are correlated model prediction but they not explain why the prediction was made given input cce solves optimization problem find counterfactual input that close possible the original but different classification crucially this optimization constrained learned causal the data ensuring that the generated counterfactuals are realistic and not violate the underlying data generating process,counterfactual explanations identify data points that closely resemble the observed data but produce different machine learning outputs offering critical insights into decisions despite the diverse scenarios goals and tasks which they are tailored existing methods often lack actionable efficiency because unnecessary feature changes included within the explanations that are presented users and stakeholders address this problem proposing that minimizes the required feature changes while maintaining the validity imposing restrictions models algorithms whether instance group based the key innovation lies computing joint distribution between observed and counterfactual data and leveraging inform shapley values for feature attributions additionally counterintuitive uncovered may misleading rely exact alignment defined the generation mechanism conducting,2025-08-26T00:48:30.451092
60,Phase Transitions in Information Propagation Through Deep Transformers,"Despite their empirical success, our understanding of the internal dynamics of deep Transformers is limited. We present a theoretical and empirical study of information propagation in Transformers, framed through the lens of statistical physics. We analyze the evolution of token representations layer-by-layer and discover evidence of a phase transition. In the initial layers, the model is in a ""disordered"" phase, where token representations are highly context-dependent and sensitive to small perturbations. After a critical depth, the model transitions to an ""ordered"" phase, where representations become stable, context-mixing ceases, and token identities become fixed. We show that this transition point is correlated with model performance and can be controlled by architectural parameters like the initialization scale and LayerNorm placement. Our contributions are: (1) a novel analysis framework for Transformers based on phase transitions; (2) empirical evidence of this phenomenon in models like BERT and GPT; and (3) a theoretical model using mean-field theory that qualitatively reproduces these results, providing a deeper understanding of how deep Transformers refine information.",ICLR,deep learning,gemini-2.5-pro,True,3581,Attention-Only Transformers via Unrolled Subspace Denoising,"Despite the great success of transformers in practice, their architectures have been empirically designed, hence lack of mathematical justification and interpretability. Moreover, many empirical studies have indicated that some components of the transformer architectures may be redundant and can be removed or replaced without compromising overall performance. Hence to derive a compact and interpretable transformer architecture, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. Based on the existing literature, the associated denoising operation naturally takes the form of a multi-subspace self-attention (MSSA). By unrolling such iterative denoising operations as a deep network, we arrive at a highly compact architecture that consists of only an MSSA operator with skip connections at each layer, without MLP. We rigorously prove that each layer of the proposed transformer performs so highly efficient denoising that it improves the signal-to-noise ratio of token representations {\em at a linear rate} with respect to the number of layers. Despite its simplicity, extensive experiments on language and vision tasks demonstrate that such a minimalistic attention-only transformer can achieve performance close to conventional transformers, such as GPT-2 and CRATE.",ICLR.cc/2025/Conference,5.0,False,0.8234,despite their empirical success our understanding the internal dynamics deep transformers limited our contributions are analysis for transformers phase transitions empirical evidence this phenomenon models like bert and gpt and theoretical mean field theory that qualitatively reproduces these providing deeper understanding how deep transformers refine information,despite the great success transformers practice their architectures have been empirically designed hence lack mathematical justification and interpretability moreover many empirical studies have indicated that some components the transformer architectures may redundant and can removed replaced compromising overall hence derive compact and interpretable transformer contend that the goal representation learning compress set noisy initial token representations towards mixture low dimensional subspaces unrolling such iterative denoising operations deep network arrive highly compact that consists only mssa operator skip connections each layer mlp rigorously that each layer the proposed transformer performs highly efficient denoising that improves the signal noise ratio token representations linear rate respect the number layers despite its simplicity extensive experiments language and vision tasks that such minimalistic attention only transformer can achieve close conventional transformers such gpt and crate,2025-08-26T00:48:30.451100
61,Federated Learning with Provably Private Personalized Models,"A key challenge in federated learning (FL) is the tension between learning a global model and accommodating statistical heterogeneity across clients. Personalization methods address this but can leak information about a user's local data distribution. We introduce PriPer, a framework for Private Personalized Federated Learning that offers strong privacy guarantees. PriPer learns a global model alongside personalized ""residual"" models for each client. The key innovation is that clients only transmit cryptographically secure projections of their residual model updates to the server. The server can aggregate these projections to update the global model without ever reconstructing any individual client's personalized component. This provides a formal guarantee under the secure multi-party computation model that no information about a client's personalization is revealed to the server or other clients. Our contributions are: (1) a novel FL framework combining personalization with provable privacy; (2) a communication-efficient protocol using secure aggregation; and (3) experiments on standard FL benchmarks showing PriPer matches the accuracy of non-private personalization methods while providing strong formal privacy.",ICLR,deep learning,gemini-2.5-pro,True,1668,pMixFed: Mixing up model coefficients for Efficient Personalized Federated Learning,"Federated Learning  enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy  and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses   data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges,  we propose ``pMixFed'', a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model,  gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate   its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of  heterogeneity when using pMixFed as compared with the state-of-the-art PFL models.",ICLR.cc/2025/Conference,2.0,nan,0.8904,key challenge federated learning the tension between learning global and accommodating statistical heterogeneity across clients priper for private personalized federated learning that offers strong privacy guarantees,federated learning enables decentralized collaborative learning machine learning models which presents challenges such data privacy and client drift for heterogeneous data personalized federated learning pfl addresses data heterogeneity tackling these issues balancing generalization and personalization level adaptive partitioning between shared and personalized layers the gradual transition personalization allow seamless adaptation local clients improved generalization across clients and mitigation catastrophic forgetting,2025-08-26T00:48:30.451108
62,Optimal Transport for Credit Assignment in Multi-Agent Reinforcement Learning,"Assigning credit for a team's success to individual agents is a fundamental problem in cooperative multi-agent reinforcement learning (MARL). Existing methods often rely on heuristics or complex value-decomposition networks. We reframe multi-agent credit assignment as an optimal transport problem. We model the set of agent actions and the set of contributions to the global reward as two discrete distributions. We then compute the optimal transport plan between these distributions, where the transport cost is a function of the agents' states and actions. This plan provides a dense, principled assignment of credit from reward components to the agents responsible. Our method, Optimal Transport for Credit Assignment (OTCA), is implemented as a differentiable layer that can be integrated into any multi-agent actor-critic algorithm. Our contributions are: (1) a novel formulation of credit assignment using optimal transport; (2) the differentiable OTCA layer; and (3) superior performance and sample efficiency in challenging cooperative MARL environments like the StarCraft Multi-Agent Challenge, particularly in scenarios requiring complex coordination.",ICLR,deep learning,gemini-2.5-pro,True,6612,Addressing Extrapolation Error in Multi-Agent Reinforcement Learning,"Cooperative Multi-Agent Reinforcement Learning (MARL) has become a critical tool for addressing complex real-world problems. 
However, scalability remains a significant challenge due to the exponentially growing joint action space. 
In our analysis, we highlight a critical but often overlooked issue: **extrapolation error**, which arises when unseen state-action pairs are inaccurately assigned unrealistic values, severely affecting performance. 
We demonstrate that the success of value factorization methods can be largely attributed to their ability to mitigate this error. 
Building on this insight, we introduce multi-step bootstrapping and ensemble techniques to further reduce extrapolation errors, showing that straightforward modifications can lead to substantial performance improvements. Our findings underscore the importance of recognizing extrapolation error in MARL and highlight the potential of exploring simpler methods to advance the field.",ICLR.cc/2025/Conference,4.0,False,0.8688,assigning credit for team success individual agents fundamental problem cooperative multi agent reinforcement learning marl,cooperative multi agent reinforcement learning marl has become critical tool for addressing complex real world problems,2025-08-26T00:48:30.451115
63,Deformable 3D Models from Video via Unsupervised Neural Mesh Rendering,"Learning 3D representations of deformable objects from monocular video is a key challenge for computer vision. We introduce Neural Deformable Mesh Rendering (NDMR), an unsupervised method that learns a template 3D mesh and its non-rigid deformations directly from a raw video sequence. Our model represents an object as a canonical mesh, a skinning field parameterized by an MLP that predicts vertex deformations over time, and a neural texture field. The entire model is trained end-to-end by rendering the deforming mesh back into 2D and minimizing a photometric reconstruction loss against the input video frames. To avoid degenerate solutions, we introduce a novel rigidity regularization loss that encourages the learned deformations to be ""as rigid as possible."" Our contributions are: (1) an end-to-end unsupervised framework for learning deformable 3D models from video; (2) a differentiable mesh renderer with a learnable skinning field; and (3) a rigidity regularizer to promote plausible deformations. We demonstrate high-quality reconstruction of complex non-rigid motions, such as human faces and moving animals, from real-world videos.",ICLR,deep learning,gemini-2.5-pro,True,103,Locality Sensitive Avatars From Video,"We present locality-sensitive avatar, a neural radiance field (NeRF) based network to learn human motions from monocular videos. To this end, we estimate a canonical representation between different frames of a video with a non-linear mapping from observation to canonical space, which we decompose into a skeletal rigid motion and a non-rigid counterpart. Our key contribution is to retain fine-grained details by modeling the non-rigid part with a graph neural network (GNN) that keeps the pose information local to neighboring body parts. Compared to former canonical representation based methods which solely operate on the coordinate space of a whole shape, our locality-sensitive motion modeling can reproduce both realistic shape contours and vivid fine-grained details. We evaluate on ZJU-MoCap, SynWild, ActorsHQ, MVHumanNet and various outdoor videos. The experiments reveal that with the locality sensitive deformation to canonical feature space, we are the first to achieve state-of-the-art results across novel view synthesis, novel pose animation and 3D shape reconstruction simultaneously. Our code is available at https://github.com/ChunjinSong/lsavatar.",ICLR.cc/2025/Conference,6.0,True,0.8228,learning representations deformable objects from monocular video key challenge for computer vision neural deformable mesh rendering ndmr unsupervised that learns template mesh and its non rigid deformations directly from raw video sequence our represents object canonical mesh skinning field parameterized mlp that predicts vertex deformations over time and neural texture field our contributions are end end unsupervised for learning deformable models from video differentiable mesh renderer learnable skinning field and rigidity regularizer promote plausible deformations,locality sensitive avatar neural radiance field nerf network learn human motions from monocular videos this end estimate canonical representation between different frames video non linear mapping from observation canonical space which decompose into skeletal rigid motion and non rigid counterpart our key retain fine grained details modeling the non rigid part graph neural network gnn that keeps the pose information local neighboring body parts compared former canonical representation methods which solely operate the coordinate space whole shape our locality sensitive motion modeling can reproduce both realistic shape contours and vivid fine grained details the experiments reveal that the locality sensitive deformation canonical feature space are the first achieve state the art across view synthesis pose animation and shape reconstruction simultaneously,2025-08-26T00:48:30.451118
64,Unsupervised Video Representation Learning by Predicting Future Optical Flow,"Self-supervised learning for video has largely focused on instance discrimination or masked prediction tasks, which may not effectively capture motion dynamics. We propose Flow-CLIP, a novel pretext task for learning video representations based on predicting future optical flow. Given a sequence of past frames, our model is trained to predict the optical flow field between the last observed frame and a future frame. This task forces the model to learn not just object appearance but also their complex motion patterns and interactions. The model architecture consists of a 3D CNN encoder that computes a spatio-temporal representation, followed by a lightweight decoder that outputs the predicted flow field. The learned encoder serves as a powerful feature extractor for downstream tasks. Our contributions are: (1) a new self-supervised pretext task for video based on future flow prediction; (2) the Flow-CLIP architecture; and (3) state-of-the-art performance on downstream action recognition benchmarks like UCF101 and HMDB51 when using the pre-trained encoder. This demonstrates that learning motion dynamics is a powerful signal for video understanding.",ICLR,deep learning,gemini-2.5-pro,True,4392,Learning Counterfactual Interventions for Self-Supervised Motion Estimation,"A major challenge in self-supervised learning from visual inputs is extracting information from the learned representations to an explicit and usable form. This is most commonly done by learning readout layers with supervision or using highly specialized heuristics. This is challenging primarily because the self-supervised pretext tasks and the downstream tasks that extract information are not tightly connected in a principled manner---improving the former does not guarantee improvements in the latter. The recently proposed counterfactual world modeling paradigm aims to address this challenge through a masked next frame predictor base model which enables simple counterfactual extraction procedures for extracting optical flow, segments and depth. In this work, we take the next step and parameterize and optimize the counterfactual extraction of optical flow by solving the same simple next frame prediction task as the base model. Our approach achieves state of the art performance for estimation motion on real-world videos while requiring no labeled data. This work sets the foundation for future methods on improving the extraction of more complex visual structures like segments and depth with high accuracy.",ICLR.cc/2025/Conference,5.0,nan,0.8450,self supervised learning for video has largely focused instance discrimination masked prediction tasks which may not capture motion dynamics flow clip pretext task for learning video representations predicting future optical flow the learned encoder serves powerful feature extractor for downstream tasks our contributions are self supervised pretext task for video future flow prediction the flow clip and state the art downstream action recognition benchmarks like ucf101 and hmdb51 when the pre trained encoder this demonstrates that learning motion dynamics powerful signal for video understanding,major challenge self supervised learning from visual inputs extracting information from the learned representations explicit and usable form this most done learning readout layers supervision highly specialized heuristics this take the next step and parameterize and optimize the counterfactual extraction optical flow solving the same simple next frame prediction task the base,2025-08-26T00:48:30.451125
65,Activation Tuning: Parameter-Free Fine-Tuning by Re-Routing and Scaling Activations,"As pre-trained models exceed billions of parameters, full fine-tuning becomes infeasible. Parameter-Efficient Fine-Tuning (PEFT) methods reduce the cost by training a small set of auxiliary parameters. We introduce Activation Tuning (Act-T), a new PEFT paradigm that requires *no additional parameters*. Act-T freezes the entire pre-trained model and instead learns to modulate the intermediate activations for each downstream task. It inserts tiny, lightweight ""gating"" layers between the frozen blocks of the pre-trained model. These gates learn a task-specific routing and scaling policy for the activation channels, effectively re-purposing the existing network pathways without changing the underlying weights. Our contributions are: (1) Act-T, a novel, parameter-free approach to fine-tuning; (2) a theoretical justification showing Act-T is equivalent to learning a block-diagonal weight modification; and (3) empirical results on the GLUE benchmark and VTAB-1k, where Act-T matches the performance of methods like LoRA while adding zero trainable parameters and negligible inference latency.",ICLR,deep learning,gemini-2.5-pro,False,,I-Lora: Iterative Merging of Routing-Tuned Low-Rank Adapters for Multi-task Learning,"The advancement of vision-language models has significantly boosted the performance of embodied and game AI, endowing them with more robust general visual understanding capabilities and logical abilities for action planning. However, the substantial computational cost of model training and the performance degradation during fine-tuning limit the models' ability to learn emerging new tasks continually. Creating a versatile and dynamically updatable vision-language model is an essential area of research. To this end, we propose a Low-Rank Adapter-based fine-tuning approach called I-LoRA, which enables iterative and independent learning of new tasks while preserving the logical capabilities of the previously trained model. Specifically, we first design the routing-tuning method to minimize the impact of original capabilities from the new task by minimizing activation values of LoRA matrices as low as possible in the general task. Secondly, we propose a novel approach to iteratively merge new adapters, allowing for continuous integration of adapters trained on new tasks without being influenced by task order, thereby reducing interference between them. Finally, we conducted extensive experiments on public datasets with significant behavioral and logical differences between tasks. The results demonstrate that our approach achieves excellent single-task performance, strong multi-task compatibility, and flexible scalability without increasing the number of model parameters.",ICLR.cc/2025/Conference,4.0,nan,0.7900,these gates learn task specific routing and scaling policy for the activation channels purposing the existing network pathways changing the underlying weights our contributions are act parameter free fine tuning theoretical justification showing act equivalent learning block diagonal weight modification and empirical the glue and vtab where act matches the methods like lora while adding zero trainable parameters and negligible inference latency,this end low rank adapter based fine tuning called lora which enables iterative and independent learning tasks while preserving the logical capabilities the previously trained the that our achieves excellent single task strong multi task compatibility and flexible scalability increasing the number parameters,2025-08-26T00:48:30.451133
66,A Generalization Bound from the Kolmogorov Complexity of Learned Representations,"Classic generalization bounds from statistical learning theory often fail to explain the performance of overparameterized deep neural networks. We propose a new type of generalization bound derived from algorithmic information theory. Our key insight is that a model generalizes well if the representations it learns for the training data are algorithmically simple, i.e., they have low Kolmogorov complexity. We formalize this by deriving a PAC-Bayes bound where the complexity penalty term is related to the compressibility of the learned representations. While Kolmogorov complexity is uncomputable, we show it can be effectively approximated using standard data compression algorithms (like Lempel-Ziv) applied to the quantized activation values of the network's final hidden layer. Our contributions are: (1) a novel generalization bound based on the algorithmic complexity of representations; (2) a practical method for estimating this bound; and (3) empirical validation demonstrating that our compressibility measure is a strong predictor of the test error and is more tightly correlated with generalization than traditional complexity measures like norm-based capacity controls.",ICLR,deep learning,gemini-2.5-pro,True,4880,Which Algorithms Have Tight Generalization Bounds?,"We study which machine learning algorithms have tight generalization bounds in the overparameterized setting. Our results build on and extend the recent work of Gastpar et al. (2024).

First, we present conditions that preclude the existence of tight generalization bounds. Specifically, we show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds. Next, we show that algorithms that are sufficiently stable do have tight generalization bounds.  We conclude with a simple characterization that relates the existence of tight generalization bounds to the conditional variance of the algorithm's loss.",ICLR.cc/2025/Conference,5.0,False,0.8523,classic generalization bounds from statistical learning theory often fail explain the overparameterized deep neural networks,which machine learning algorithms have tight generalization bounds the overparameterized setting,2025-08-26T00:48:30.451136
67,Post-Hoc Fairness Calibration without Retraining,"Ensuring algorithmic fairness is critical, but retraining large-scale models with fairness constraints is often computationally prohibitive. We introduce Post-hoc Fairness Calibration (PFC), a method that adjusts a pre-trained, biased model to satisfy fairness criteria without any backpropagation or weight updates. PFC works by learning a simple, group-specific calibration function in the model's logit space. For each demographic group, it learns an affine transformation that shifts and rescales the logits to equalize a chosen fairness metric, such as demographic parity or equality of opportunity, across all groups. This transformation is learned on a small, held-out calibration set and can be applied at inference time with minimal overhead. Our contributions are: (1) PFC, a lightweight, retraining-free fairness algorithm; (2) theoretical analysis showing that PFC can provably satisfy common fairness metrics; and (3) experiments on benchmark fairness datasets demonstrating that PFC achieves a better fairness-accuracy trade-off than many in-processing methods, offering a practical solution for deploying fair models when retraining is not an option.",ICLR,deep learning,gemini-2.5-pro,True,6785,Fair Class-Incremental Learning using Sample Weighting,"Model fairness is becoming important in class-incremental learning for Trustworthy AI. While accuracy has been a central focus in class-incremental learning, fairness has been relatively understudied. However, naively using all the samples of the current task for training results in unfair catastrophic forgetting for certain sensitive groups including classes. We theoretically analyze that forgetting occurs if the average gradient vector of the current task data is in an ""opposite direction"" compared to the average gradient vector of a sensitive group, which means their inner products are negative. We then propose a fair class-incremental learning framework that adjusts the training weights of current task samples to change the direction of the average gradient vector and thus reduce the forgetting of underperforming groups and achieve fairness. For various group fairness measures, we formulate optimization problems to minimize the overall losses of sensitive groups while minimizing the disparities among them. We also show the problems can be solved with linear programming and propose an efficient Fairness-aware Sample Weighting (FSW) algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff results than state-of-the-art approaches on real datasets.",ICLR.cc/2025/Conference,4.5,False,0.8380,ensuring algorithmic fairness critical but retraining large scale models fairness constraints often computationally prohibitive post hoc fairness calibration pfc that adjusts pre trained biased satisfy fairness criteria any backpropagation weight updates pfc works learning simple group specific calibration function the model logit space for each demographic group learns affine transformation that shifts and rescales the logits equalize chosen fairness such demographic parity equality opportunity across all groups our contributions are pfc lightweight retraining free fairness theoretical analysis showing that pfc can provably satisfy common fairness metrics and experiments fairness datasets demonstrating that pfc achieves better fairness accuracy trade off than many processing methods offering practical solution for deploying fair models when retraining not option,fairness becoming important class incremental learning for trustworthy while has been central focus class incremental learning fairness has been relatively understudied then fair class incremental learning that adjusts the training weights current task samples change the direction the average gradient vector and thus reduce the forgetting underperforming groups and achieve fairness for various group fairness measures formulate optimization problems minimize the overall losses sensitive groups while minimizing the disparities among them,2025-08-26T00:48:30.451140
68,Attributing Predictions to Synergistic Groups of Training Examples,"Data attribution methods aim to identify which training examples influenced a specific prediction. Most methods focus on the influence of individual examples, ignoring the combinatorial effect of *groups* of examples. We propose Synergistic Group Influence (SGI), a method for discovering small sets of training data points whose combined presence in the training set has a disproportionately large effect on a test prediction. SGI uses a cooperative game theory framework, treating each training example as a player and defining the value of a coalition as the change in the test loss when that group is removed. We develop a scalable search algorithm based on Shapley values to efficiently find the most synergistic groups. Our contributions are: (1) a formal framework for synergistic data attribution; (2) a scalable algorithm for its computation; and (3) case studies on real-world datasets where SGI uncovers complex dataset biases, identifies sources of spurious correlations, and explains challenging model errors that are inexplicable by individual influence methods.",ICLR,deep learning,gemini-2.5-pro,True,5177,Data Attribution for Multitask Learning,"Data attribution quantifies the influence of individual training data points on machine learning models, aiding in their interpretation and improvement. While prior work has primarily focused on single-task learning (STL), this work extends data attribution to multitask learning (MTL). Data attribution in MTL presents new opportunities for interpreting and improving MTL models while also introducing unique technical challenges. On the opportunity side, data attribution in MTL offers a natural way to efficiently measure task relatedness, a key factor that impacts the effectiveness of MTL. However, the shared and task-specific parameters in MTL models present challenges that require specialized data attribution methods. In this paper, we propose the **MultiTask Influence Function** (**MTIF**), a data attribution framework tailored for MTL. MTIF leverages the parameter structure of MTL models to derive influence functions that distinguish between within-task and cross-task influences. Our derivation also sheds light on the applicability of popular approximation techniques for influence function computation, such as EK-FAC and LiSSA, in the MTL setting. Compared to conventional task relatedness measurements, MTIF provides not only task-level relatedness but also data-level influence analysis. The latter enables fine-grained interpretations of task relatedness and facilitates a data selection strategy to effectively mitigate negative transfer in MTL. Extensive experiments on both linear and neural network models show that MTIF effectively approximates leave-one-out and leave-one-task-out effects while offering interpretable insights into task relatedness. Moreover, the data selection strategy enabled by MTIF consistently improves model performance in MTL. Our work establishes a novel connection between data attribution and MTL, offering an efficient and scalable solution for measuring task relatedness and enhancing MTL models.",ICLR.cc/2025/Conference,5.5,False,0.8248,data attribution methods aim identify which training examples influenced specific prediction synergistic group influence sgi for discovering small sets training data points whose combined presence the training set has disproportionately large effect prediction,data attribution quantifies the influence individual training data points machine learning models aiding their interpretation and improvement while prior has primarily focused single task learning stl this extends data attribution multitask learning mtl the latter enables fine grained interpretations task relatedness and facilitates data selection strategy mitigate negative transfer mtl extensive experiments both linear and neural network models that mtif approximates leave one out and leave one task out effects while offering interpretable insights into task relatedness,2025-08-26T00:48:30.451145
69,Neural Particle Flow: A GNN-based Surrogate for Accelerating Fluid Dynamics,"Simulating complex physical systems like fluid dynamics is essential for science and engineering but is often computationally intractable. We introduce Neural Particle Flow (NPF), a graph neural network-based surrogate model that learns to simulate the dynamics of particle-based fluid systems. NPF represents the fluid as a set of interacting particles, modeled as a graph where nodes are particles and edges represent local influence. The GNN learns to predict the forces on each particle by passing messages between neighbors, effectively learning the pressure and viscosity fields. A differentiable integrator then updates the particle positions and velocities. By learning the simulation dynamics, NPF can predict future states orders of magnitude faster than traditional numerical solvers. Our contributions are: (1) a GNN-based surrogate model for particle-based fluid simulation; (2) a stable and energy-conserving training paradigm; and (3) demonstrations on several challenging 2D and 3D fluid dynamics benchmarks, showing that NPF accurately reproduces complex phenomena like vortex shedding and turbulent flow at a fraction of the computational cost.",ICLR,deep learning,gemini-2.5-pro,True,10368,Rigid Body Dynamics Simulation Based on GNNs with Constraints,"In recent years, the utilization of Graph Neural Network (GNN)-based methods for simulating complex physical systems has opened new avenues for the fields of computational science and engineering. Despite their success, current GNN-based methods for rigid body dynamic simulation are constrained to relatively simple scenarios, hindering their practical use in industrial settings where complex mechanical structures and interconnected components prevail. These methods face challenges in handling intricate force relationships within rigid bodies, primarily due to the difficulty in obtaining force-related data for objects in industrial environments. To address this, we propose a novel constraint-guided method that incorporates force analysis into GNN-based simulations. The model incorporates computations related to both contact and non-contact forces into the prediction process. Additionally, it imposes physical constraints on the prediction process based on Kane's equations. We have rigorously demonstrated the model's rationality and effectiveness with thorough theoretical demonstration and empirical analysis. \textit{Codes and anonymous links to the datasets are available in the supplementary materials.",ICLR.cc/2025/Conference,4.75,nan,0.8212,neural particle flow npf graph neural network based surrogate that learns simulate the dynamics particle based fluid systems the gnn learns predict the forces each particle passing messages between neighbors learning the pressure and viscosity fields learning the simulation dynamics npf can predict future states orders magnitude faster than traditional numerical solvers,recent years the utilization graph neural network gnn based methods for simulating complex physical systems has opened avenues for the fields computational science and engineering the incorporates computations related both contact and non contact forces into the prediction process additionally imposes physical constraints the prediction process kane equations,2025-08-26T00:48:30.451149
70,Iterative Refinement with Energy-Guided Decoding for Non-Autoregressive Text Generation,"Non-autoregressive (NAR) models offer significant speed-ups for text generation but often suffer from lower quality due to the assumption of conditional independence between output tokens. We propose Energy-Guided Iterative Refinement (EAGLE), a NAR framework that bridges this quality gap. EAGLE starts by generating a full sequence in parallel. It then iteratively refines this sequence in a fixed number of steps. The key innovation is that each refinement step is guided by a pre-trained Energy-Based Model (EBM) that scores the global coherence of the sequence. The refinement model, a masked language model, is trained to propose edits that maximally decrease this energy score, thereby improving the overall quality. Our contributions are: (1) a new NAR framework combining parallel decoding with EBM-guided refinement; (2) a method for training the refiner to explicitly optimize for sequence-level coherence; and (3) results on WMT14 translation and Gigaword summarization tasks showing EAGLE achieves quality comparable to autoregressive models while being an order of magnitude faster at inference.",ICLR,deep learning,gemini-2.5-pro,True,5272,Integrating Geodesic Interpolation and Flow Matching for Non-Autoregressive Text Generation in Logit Space,"Non-autoregressive language models are emerging as effective alternatives to autoregressive models in the field of natural language processing, facilitating simultaneous token generation. This study introduces a novel flow matching approach that employs Kullback-Leibler (KL) divergence geodesics to interpolate between initial and target distributions for discrete sequences. We formulate a loss function designed to maximize the conditional likelihood of discrete tokens and demonstrate that its maximizer corresponds to the flow matching velocity during logit interpolation. Although preliminary experiments conducted on the TinyStories dataset yielded suboptimal results, we propose an empirical sampling scheme based on a pretrained denoiser that significantly enhances performance. Additionally, we present a more general hybrid approach that achieves strong performance on more complex datasets, such as Fine Web and Lamini Instruction.",ICLR.cc/2025/Conference,3.0,False,0.8619,non autoregressive nar models offer significant speed ups for text generation but often suffer from lower quality due the assumption conditional independence between output tokens the refinement masked language trained edits that maximally decrease this energy thereby improving the overall quality,non autoregressive language models are emerging effective alternatives autoregressive models the field natural language processing facilitating simultaneous token generation,2025-08-26T00:48:30.451154
71,Latent State-Space Discovery for Model-Based Reinforcement Learning,"Model-based reinforcement learning (MBRL) promises greater sample efficiency by learning a dynamics model of the environment. However, learning this model in high-dimensional observation spaces like images is a major challenge. We propose Latent State-Space Discovery (LSSD), a new approach to MBRL that focuses on learning a compact, controllable latent state representation. LSSD jointly trains an autoencoder and a latent dynamics model. The crucial component is a novel ""controllability"" objective that forces the latent space to be structured such that actions correspond to predictable, linear displacements in the embedding space. This inductive bias makes learning the dynamics model significantly easier and more robust. Our contributions are: (1) a new objective for learning controllable latent representations for MBRL; (2) the LSSD algorithm for joint discovery of the representation and dynamics; and (3) results on the DeepMind Control Suite, where LSSD learns more accurate and stable dynamics models from pixels, leading to significantly better final policy performance compared to state-of-the-art visual MBRL methods.",ICLR,deep learning,gemini-2.5-pro,True,9775,Objects matter: object-centric world models improve reinforcement learning in visually complex environments,"Deep reinforcement learning has achieved remarkable success in learning control policies from pixels across a wide range of tasks, yet its application remains hindered by low sample efficiency, requiring significantly more environment interactions than humans to reach comparable performance.
Model-based reinforcement learning (MBRL) offers a solution by leveraging learnt world models to generate simulated experience, thereby improving sample efficiency.
However, in visually complex environments, small or dynamic elements can be critical for decision-making.
Yet, traditional MBRL methods in pixel-based environments typically rely on auto-encoding with an $L_2$ loss, which is dominated by large areas and often fails to capture decision-relevant details.
To address these limitations, we propose an **object-centric MBRL pipeline**, which integrates recent advances in computer vision to allow agents to focus on key decision-related elements.
Our approach consists of four main steps: (1) annotating key objects related to rewards and goals with segmentation masks, (2) extracting object features using a pre-trained, frozen foundation vision model, (3) incorporating these object features with the raw observations to predict environmental dynamics, and (4) training the policy using imagined trajectories generated by this object-centric world model.
Building on the efficient MBRL algorithm STORM, we call this pipeline **OC-STORM**.
We demonstrate OC-STORM's practical value in overcoming the limitations of conventional MBRL approaches on both Atari games and the visually complex game Hollow Knight.
Code and videos are available in the supplementary materials.",ICLR.cc/2025/Conference,6.0,False,0.8872,model based reinforcement learning mbrl promises greater sample efficiency learning dynamics the environment however learning this high dimensional observation spaces like images major challenge latent state space discovery lssd mbrl that focuses learning compact controllable latent state representation the crucial component controllability objective that forces the latent space structured such that actions correspond predictable linear displacements the embedding space this inductive bias makes learning the dynamics easier and more robust our contributions are objective for learning controllable latent representations for mbrl the lssd for joint discovery the representation and dynamics and the deepmind control suite where lssd learns more accurate and stable dynamics models from pixels leading better final policy compared state the art visual mbrl methods,deep reinforcement learning has achieved remarkable success learning control policies from pixels across wide range tasks yet its application remains hindered low sample efficiency requiring more environment interactions than humans reach comparable model based reinforcement learning mbrl offers solution leveraging learnt world models generate simulated experience thereby improving sample efficiency address these limitations object centric mbrl pipeline which integrates recent advances computer vision allow agents focus key decision related elements our consists four main steps annotating key objects related rewards and goals segmentation masks extracting object features pre trained frozen foundation vision incorporating these object features the raw observations predict environmental dynamics and training the policy imagined trajectories generated this object centric world,2025-08-26T00:48:30.451157
72,Rethinking Self-Attention: A Kernel-Trick Perspective for Linear Complexity,"The quadratic complexity of the self-attention mechanism in Transformers is a major bottleneck for processing long sequences. We present a new theoretical perspective that recasts dot-product attention as a kernelized operation. We show that the standard attention mechanism is equivalent to computing a specific kernel function between queries and keys. This insight allows us to leverage the kernel trick to approximate the attention matrix without its explicit instantiation. By using a fast, low-rank approximation of the kernel feature map (e.g., via random Fourier features), we can compute the attention output with a complexity that is linear in the sequence length. Our contributions are: (1) a novel theoretical framing of self-attention as a kernel method; (2) a practical algorithm, Kernelized Attention (Kerplunk), that reduces attention complexity to linear time and constant memory with respect to sequence length; and (3) empirical results on the Long Range Arena benchmark, where Kerplunk matches the performance of full attention while being significantly faster and more memory-efficient.",ICLR,deep learning,gemini-2.5-pro,True,7418,DAPE V2: Process Attention Score as Feature Map for Length Extrapolation,"The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. 
In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, **the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem.** 
The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution.  Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.",ICLR.cc/2025/Conference,5.666666666666667,False,0.8607,the quadratic complexity the self attention mechanism transformers major bottleneck for processing long sequences theoretical perspective that recasts dot product attention kernelized operation that the standard attention mechanism equivalent computing specific kernel function between queries and keys this insight allows leverage the kernel trick approximate the attention matrix its explicit instantiation fast low rank approximation the kernel feature map random fourier features can compute the attention output complexity that linear the sequence length our contributions are theoretical framing self attention kernel practical kernelized attention kerplunk that reduces attention complexity linear time and constant memory respect sequence length and empirical the long range arena where kerplunk matches the full attention while being faster and more memory efficient,the attention mechanism fundamental component the transformer contributing interactions among distinct tokens general the attention scores are determined simply the key query products however this work occasional trial combining dape and nope including additional mlps attention scores position encoding indicates that the classical key query multiplication may limit the transformers this conceptualize attention feature map and apply the convolution operator for neighboring attention scores across different heads mimic the processing methods computer vision the main this identifying and interpreting the transformer length extrapolation problem the limited expressiveness the naive query and key dot product and translate the length extrapolation issue into well understood feature map processing problem the insight which can adapted various attention related models reveals that the current transformer has the potential for further evolution extensive experiments that treating attention feature map and applying convolution processing enhances transformer,2025-08-26T00:48:30.451160
73,Training Language Models with Human Feedback via Differentiable Preference Optimization,"Reinforcement Learning from Human Feedback (RLHF) has been highly effective for aligning large language models, but its implementation is complex, involving training a separate reward model and using reinforcement learning algorithms like PPO. We introduce Differentiable Preference Optimization (DPO), a simpler and more stable method for training language models on preference data. DPO reframes the constrained reward maximization problem as a simple binary classification task. We show that the optimal policy for the standard RLHF objective can be extracted directly by training a model to classify which of two responses is preferred by humans. The method involves only a single stage of fine-tuning on a static preference dataset, eliminating the need for reward model fitting, sampling, and complex RL machinery. Our contributions are: (1) a novel, simplified objective for preference-based alignment; (2) a proof of its equivalence to traditional RLHF under certain conditions; and (3) experiments demonstrating that DPO is as effective or more effective than PPO-based RLHF at controlling style and improving response quality, while being substantially easier to implement and train.",ICLR,deep learning,gemini-2.5-pro,True,11645,Multi-Step Preference Optimization via Two-Player Markov Games,"Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Multi-step Preference Optimization (MPO) is built upon the natural actor-critic framework. We further develop OMPO based on the optimistic online gradient descent algorithm. Theoretically, we provide a rigorous analysis for both algorithms on convergence and show that OMPO requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of our method through experiments on the multi-turn conversations dataset in MT-bench-101.",ICLR.cc/2025/Conference,5.25,False,0.9465,reinforcement learning from human feedback rlhf has been highly effective for aligning large language models but its implementation complex involving training separate reward and reinforcement learning algorithms like ppo differentiable preference optimization dpo simpler and more stable for training language models preference data dpo reframes the constrained reward maximization problem simple binary classification task,reinforcement learning from human feedback rlhf has been highly successful aligning large language models human preferences while prevalent methods like dpo have demonstrated strong they frame interactions the language bandit problem which limits their applicability real world scenarios where multi turn conversations are common our multi step preference optimization mpo built upon the natural actor critic,2025-08-26T00:48:30.451164
74,Bayesian Neural Operators for Uncertainty Quantification in Scientific Machine Learning,"Neural Operators, which learn mappings between infinite-dimensional function spaces, are a powerful tool for learning surrogate models for physical systems governed by partial differential equations (PDEs). However, existing methods provide only point estimates, lacking the crucial uncertainty quantification needed for scientific and engineering applications. We introduce Bayesian Neural Operators (BNOs), a probabilistic extension of the Fourier Neural Operator that captures both aleatoric and epistemic uncertainty. We place priors over the spectral weights of the operator and perform approximate inference using variational methods. This allows the BNO to produce a full predictive posterior distribution over the output function space. A key contribution is our development of a scalable, function-space variational inference algorithm that remains efficient for large-scale problems. Our contributions are: (1) the BNO framework for probabilistic PDE solving; (2) a scalable function-space variational inference scheme; and (3) demonstrations on challenging benchmarks like turbulent flow and subsurface modeling, where BNOs provide well-calibrated uncertainty estimates while matching the accuracy of their deterministic counterparts.",ICLR,deep learning,gemini-2.5-pro,True,4413,Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators,"In recent years, neural operators have emerged as a prominent approach for learning mappings between function spaces, such as the solution operators of parametric PDEs. A notable example is the Fourier Neural Operator (FNO), which models the integral kernel as a convolution operator and uses the Convolution Theorem to learn the kernel directly in the frequency domain. The parameters are decoupled from the resolution of the data, allowing the FNO to take inputs of different resolutions.
However, training at a lower resolution and inferring at a finer resolution does not guarantee consistent performance, nor can fine details, present only in fine-scale data, be learned solely from coarse data. In this work, we address this misconception by defining and examining the discretization mismatch error: the discrepancy between the outputs of the neural operator when using different discretizations of the input data. We demonstrate that neural operators may suffer from discretization mismatch errors that hinder their effectiveness when inferred on data with resolutions different from that of the training data or when trained on data with varying resolutions. As neural operators underpin many critical cross-resolution scientific tasks, such as climate modeling and fluid dynamics, understanding discretization mismatch errors is essential. Based on our findings, we propose a Cross-Resolution Operator-learning Pipeline that is free of aliasing and discretization mismatch errors, enabling efficient cross-resolution and multi-spatial-scale learning, and resulting in superior performance.",ICLR.cc/2025/Conference,6.5,True,0.8675,neural operators which learn mappings between infinite dimensional function spaces are powerful tool for learning surrogate models for physical systems governed partial differential equations pdes bayesian neural operators bnos probabilistic extension the fourier neural operator that captures both aleatoric and epistemic uncertainty,recent years neural operators have emerged prominent for learning mappings between function spaces such the solution operators parametric pdes notable example the fourier neural operator fno which models the integral kernel convolution operator and uses the convolution theorem learn the kernel directly the frequency domain this address this misconception defining and examining the discretization mismatch error the discrepancy between the outputs the neural operator when different discretizations the input data that neural operators may suffer from discretization mismatch errors that hinder their effectiveness when inferred data resolutions different from that the training data when trained data varying resolutions neural operators underpin many critical cross resolution scientific tasks such climate modeling and fluid dynamics understanding discretization mismatch errors essential,2025-08-26T00:48:30.451168
75,Hyper-Riemannian Optimization: Learning Intrinsic Gradient Geometries for Faster Convergence,"Standard deep learning optimizers like Adam operate under a fixed, often Euclidean, geometric assumption, which is misaligned with the complex, curved loss landscapes of neural networks. This mismatch can slow convergence and lead to suboptimal solutions. We introduce Hyper-Riemannian Optimization (HRO), a novel method that dynamically learns the local geometry of the loss surface. HRO employs a small hypernetwork that predicts a low-rank Riemannian metric tensor conditioned on the current model parameters. This allows for efficient, on-the-fly computation of the natural gradient, guiding updates along the path of steepest descent on the underlying statistical manifold. Our core contributions are: (1) a framework for learning task-specific, dynamic Riemannian metrics for optimization; (2) a computationally efficient implementation using low-rank approximations; and (3) theoretical analysis connecting our approach to second-order methods. Empirically, HRO demonstrates significantly faster convergence and achieves superior final performance compared to AdamW and other state-of-the-art optimizers on large-scale Transformer language models and vision transformers on ImageNet. Our work opens a new direction in optimization by treating the geometry of the loss landscape as a learnable component of the training process.",ICLR,deep learning,gemini-2.5-pro,True,1005,Optimizing Learning for Robust Hyperbolic Deep Learning in Computer Vision,"Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the  data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on improving stability for curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across direct classification, generation, and hierarchical metric learning tasks while allowing for larger hyperbolic models.",ICLR.cc/2025/Conference,4.4,False,0.8668,standard deep learning optimizers like adam operate under fixed often euclidean geometric assumption which misaligned the complex curved loss landscapes neural networks hyper riemannian optimization hro that dynamically learns the local geometry the loss surface our core contributions are for learning task specific dynamic riemannian metrics for optimization computationally efficient implementation low rank approximations and theoretical analysis connecting our second order methods empirically hro demonstrates faster convergence and achieves superior final compared adamw and other state the art optimizers large scale transformer language models and vision transformers imagenet our opens direction optimization treating the geometry the loss landscape learnable component the training process,hyperbolic deep learning has become growing direction computer vision for the unique properties afforded the alternate embedding space current riemannian optimizers not account for changes the manifold which greatly harms and forces lower learning rates minimize projection errors our focuses improving stability for curvature learning introducing improved schema for popular learning algorithms and providing normalization constrain embeddings within the variable representative radius the manifold additionally formulation for riemannian adamw and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations greatly reducing the computational penalty the hyperbolic embedding space our demonstrates consistent improvements across direct classification generation and hierarchical learning tasks while allowing for larger hyperbolic models,2025-08-26T00:48:30.451169
76,Latent Trajectory Models: Denoising in a Learned Subspace for Accelerated Generative Diffusion,"Denoising diffusion models have achieved state-of-the-art results in generative modeling but suffer from slow sampling speeds due to the necessity of performing thousands of iterative steps in a high-dimensional data space. We propose Latent Trajectory Models (LTMs), a new class of generative models that significantly accelerate sampling by performing the diffusion process in a compressed, structured latent space. An autoencoder first maps data into a low-dimensional manifold. The diffusion process then operates entirely within this space, where a novel denoising network is trained to predict not just the next step, but the entire future trajectory of the denoising process, effectively reducing the required number of sampling steps. Our key contributions are: (1) a framework for performing diffusion in a learned latent space, decoupling generative complexity from data dimensionality; (2) a trajectory-aware denoising architecture that learns the dynamics of the reverse process; and (3) a stable training objective for the joint autoencoder-diffusion system. LTMs achieve sample quality comparable to traditional diffusion models on datasets like CelebA-HQ and LSUN, while requiring 10-20x fewer sampling steps, making high-fidelity generative modeling more practical for real-world applications.",ICLR,deep learning,gemini-2.5-pro,True,11065,Decouple-Then-Merge: Towards Better Training for Diffusion Models,"Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a $\textbf{De}$couple-then-$\textbf{Me}$rge ($\textbf{DeMe}$) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is included in the supplementary material and will be released on Github.",ICLR.cc/2025/Conference,4.2,nan,0.8458,the diffusion process then operates entirely within this space where denoising network trained predict not just the next step but the entire future trajectory the denoising process reducing the required number sampling steps,diffusion models are trained learning sequence models that reverse each step noise corruption however since the denoising tasks differ each timestep the gradients computed different timesteps may conflict potentially degrading the overall image generation several improved techniques during the finetuning stage promote effective knowledge sharing while minimizing training interference across timesteps experimental significant generation quality improvements upon benchmarks including stable diffusion coco30k imagenet1k partiprompts and ddpm lsun church lsun bedroom and cifar10,2025-08-26T00:48:30.451171
77,Beyond Flatness: Generalization is Governed by the Curvature-Dimension Spectrum of the Loss Landscape,"The prevailing belief that ""flat"" minima generalize well offers a useful heuristic but lacks a precise, quantitative foundation that can explain complex phenomena in deep learning. We argue that a single scalar value for flatness is insufficient. This paper introduces the Curvature-Dimension Spectrum (CDS), a new theoretical framework for understanding generalization. We posit that generalization is determined not by the average curvature, but by the entire eigenvalue distribution of the Hessian at the solution, which characterizes the ""effective dimensionality"" of the function space accessible to the model. Using tools from random matrix theory, we analyze the CDS and derive a novel PAC-Bayes generalization bound that explicitly depends on the spectral decay rate. Our contributions include: (1) the formalization of the CDS as a more descriptive alternative to sharpness; (2) a new generalization bound that leverages this spectral information; and (3) extensive empirical validation. We show that the CDS accurately predicts test error across various architectures, optimizers, and regularization schemes, successfully explaining cases where sharpness-based measures fail. This work provides a more nuanced and powerful lens for analyzing the mechanisms of generalization in deep neural networks.",ICLR,deep learning,gemini-2.5-pro,True,189,Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD,"Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD’s generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called ""omniscient trajectory"". When applied to Gradient Descent’s minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds’ $\Omega(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs. Codes are available at [https://github.com/peng-ze/omniscient-bounds](https://github.com/peng-ze/omniscient-bounds).",ICLR.cc/2025/Conference,7.0,True,0.8262,the prevailing belief that flat minima generalize well offers useful heuristic but lacks precise quantitative foundation that can explain complex phenomena deep learning this provides more nuanced and powerful lens for analyzing the mechanisms generalization deep neural networks,information theoretic generalization bounds have been used the generalization learning algorithms experiments deep neural networks our bound not only correctly reflects the better generalization when flatness improved but also numerically much tighter,2025-08-26T00:48:30.451172
78,Causal Policy Regularization for Robust Offline Reinforcement Learning,"A primary challenge in offline reinforcement learning (RL) is mitigating distributional shift, where the learned policy deviates from the behavior distribution of the static dataset, leading to catastrophic out-of-distribution value estimates. Existing methods employ uncertainty-based or policy-constraint regularization, which can be overly conservative and fail to distinguish spurious correlations from causal relationships. We introduce Causal Policy Regularization (CPR), a novel approach that frames offline RL as a causal inference problem. By modeling the underlying structural causal model of the environment, CPR penalizes policies for relying on non-causal, spurious features in the state space while allowing for safe generalization along causally-valid action pathways. This is achieved by learning a disentangled representation that isolates causally salient factors and regularizing the policy's sensitivity to spurious ones. Our contributions are: (1) a new causal framework for offline RL; (2) the CPR algorithm, which promotes learning policies that are robust to confounders; and (3) empirical results on the D4RL benchmark demonstrating that CPR significantly outperforms state-of-the-art methods, particularly in environments with strong spurious correlations, leading to more robust and generalizable policies.",ICLR,deep learning,gemini-2.5-pro,True,1861,RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning,"In offline reinforcement learning (RL), managing the distribution shift between the learned policy and the static offline dataset is a persistent challenge that can result in overestimated values and suboptimal policies. Traditional offline RL methods address this by introducing conservative biases that limit exploration to well-understood regions, but they often overly restrict the agent's generalization capabilities. Recent work has sought to generate trajectories using generative models to augment the offline dataset, yet these methods still struggle with overestimating synthesized data, especially when out-of-distribution samples are produced. To overcome this issue, we propose RTDiff, a novel diffusion-based data augmentation technique that synthesizes trajectories *in reverse*, moving from unknown to known states. Such reverse generation naturally mitigates the risk of overestimation by ensuring that the agent avoids planning through unknown states. Additionally, reverse trajectory synthesis allows us to generate longer, more informative trajectories that take full advantage of diffusion models' generative strengths while ensuring reliability. We further enhance RTDiff by introducing flexible trajectory length control and improving the efficiency of the generation process through noise management. Our empirical results show that RTDiff significantly improves the performance of several state-of-the-art offline RL algorithms across diverse environments, achieving consistent and superior results by effectively overcoming distribution shift.",ICLR.cc/2025/Conference,5.75,True,0.8651,primary challenge offline reinforcement learning mitigating distributional shift where the learned policy deviates from the behavior distribution the static leading catastrophic out distribution value estimates this achieved learning disentangled representation that isolates causally salient factors and regularizing the policy sensitivity spurious ones our contributions are causal for offline the cpr which promotes learning policies that are robust confounders and empirical the d4rl demonstrating that cpr outperforms state the art methods environments strong spurious correlations leading more robust and generalizable policies,offline reinforcement learning managing the distribution shift between the learned policy and the static offline persistent challenge that can overestimated values and suboptimal policies such reverse generation naturally mitigates the risk overestimation ensuring that the agent avoids planning unknown states further enhance rtdiff introducing flexible trajectory length control and improving the efficiency the generation process noise management,2025-08-26T00:48:30.451174
79,Dynamic Sparse Training with Synaptic Scaffolding,"Dynamic sparse training (DST) methods aim to train sparse neural networks from scratch, reducing computational costs. However, existing methods often suffer from ""representational collapse,"" where pruned neurons lose their ability to reintegrate into the network, and from unstable sparsity structures during training. We introduce Synaptic Scaffolding, a novel DST technique that addresses these issues. Synaptic Scaffolding maintains a ""scaffold"" of recently pruned connections in a dormant state. These connections retain a memory of their former weight values and can be rapidly re-activated if their reintroduction is predicted to significantly reduce the training loss, guided by a low-cost proxy gradient estimator. This allows the network to fluidly explore different sparsity masks without losing valuable learned information. Our contributions are: (1) the Synaptic Scaffolding algorithm for stable and effective DST; (2) a mechanism for preserving and efficiently re-activating pruned weights; and (3) a principled criterion for topological adaptation. We demonstrate that our method achieves state-of-the-art performance, enabling training of networks at extreme sparsity levels (e.g., 98%) from initialization, while matching or exceeding the accuracy of dense counterparts on ImageNet and BERT pre-training tasks.",ICLR,deep learning,gemini-2.5-pro,True,1993,Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness,"It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the ""de facto"" approach to train artificial neural networks if one would like to maximize their robustness against image corruption. In this paper, we question this general practice. Consequently, \textit{we claim that}, contrary to what is commonly thought, the Dynamic Sparse Training methods can consistently outperform Dense Training in terms of robustness accuracy, particularly if the efficiency aspect is not considered as a main objective (i.e., sparsity levels between 10\% and up to 50\%), without adding (or even reducing) resource cost. We validate our claim on two types of data, images and videos, using several traditional and modern deep learning architectures for computer vision and three widely studied Dynamic Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of Dynamic Sparse Training and open new possibilities in improving deep learning robustness beyond the current state of the art.",ICLR.cc/2025/Conference,5.75,True,0.8147,dynamic sparse training dst methods aim train sparse neural networks from scratch reducing computational costs this allows the network fluidly different sparsity masks losing valuable learned information our contributions are the synaptic scaffolding for stable and effective dst mechanism for preserving and activating pruned weights and principled criterion for topological adaptation,perceived that dynamic sparse training opens the door era scalability and efficiency for artificial neural networks perhaps some costs for the classification task the same time dense training accepted being the facto train artificial neural networks one would like maximize their robustness against image corruption consequently textit claim that contrary what thought the dynamic sparse training methods can consistently outperform dense training terms robustness the efficiency aspect not considered main objective our claim two types data images and videos several traditional and modern deep learning architectures for computer vision and three studied dynamic sparse training algorithms our reveal yet unknown benefit dynamic sparse training and open possibilities improving deep learning robustness beyond the current state the art,2025-08-26T00:48:30.451176
80,Composable Inductive Biases: Learning to Select and Fuse GNN Architectures,"The design of Graph Neural Network (GNN) architectures involves choosing specific message-passing, aggregation, and update functions, each encoding a different inductive bias (e.g., homophily, heterophily, structural equivalence). This ""no free lunch"" problem means no single GNN architecture is optimal for all graph structures and tasks. We propose Composable Inductive Biases (CIB), a meta-learning framework that learns to dynamically select and fuse different GNN architectures at a per-node level. CIB features a modular base library of diverse GNN layers (e.g., GCN, GAT, GraphSAGE) and a hyper-network that learns to predict a soft combination weighting over these modules for each node, based on its local neighborhood topology and features. This allows the model to apply GCN-like smoothing in homophilous regions while using GAT-like attention in complex, noisy regions of the same graph. Our contributions are: (1) a framework for learning node-adaptive GNN architectures; (2) a scalable hyper-network for predicting architectural compositions; and (3) state-of-the-art results on a suite of challenging graph datasets with heterogeneous structures, demonstrating superior adaptability compared to any single fixed GNN architecture.",ICLR,deep learning,gemini-2.5-pro,True,9000,Graph Neural Network Is A Mean Field Game,"In current graph neural networks (GNNs), it is a common practice to apply a pre-defined message passing heuristics to all graph data, even though the stereotypical relational inductive bias (e.g., graph heat diffusion) might not fit the unseen graph topology. Such gross simplification might be responsible for the lack of an in-depth understanding of graph learning principles, which challenges us to push the boundary from crafting application-specific GNNs to embracing a ""meta-learning"" paradigm. In this work, we ratchet the gear of GNN another notch forward by formulating GNN as a *mean field game*, that is, the best learning outcome occurs at the *Nash*-equilibrium when the learned graph inference rationale allows each graph node to find what is the best feature representations for not only the individual node but also the entire graph. Following this spirit, we formulate the search for novel GNN mechanism into a variational framework of *mean-field control* (MFC) problem, where the optimal relational inductive bias is essentially the critical point of mean-field information dynamics. Specifically, we seek for the best characteristic MFC functions of transportation mobility (controlling information exchange throughout the graph) and reaction mobility (controlling feature representation learning on each node), on the fly, that uncover the most suitable learning mechanism for a GNN instance by solving an MFC variational problem through the lens of *Hamiltonian flows* (formed in partial differential equations). In this context, our variational framework brings together existing GNN models into various mean-field games with distinct equilibrium states, each characterized by a unique MFC functional. Furthermore, we present an agnostic end-to-end deep model, coined *Nash-GNN* (in honor of Nobel laureate Dr. John Nash), to jointly carve the nature of the inductive bias and fine-tune the GNN hyper-parameters on top of the elucidated learning mechanism. *Nash-GNN* has achieved SOTA performance on diverse graph data including popular benchmark datasets and human connectomes. More importantly, the mathematical insight of mean-field games provides a new window to understand the foundational principles of graph learning as an interactive dynamical system, which allows us to reshape the idea of designing next-generation GNN models.",ICLR.cc/2025/Conference,5.333333333333333,nan,0.8679,the graph neural network gnn architectures involves choosing specific message passing aggregation and update functions each encoding different inductive bias this allows the apply gcn like smoothing homophilous regions while gat like attention complex noisy regions the same graph our contributions are for learning node adaptive gnn architectures scalable hyper network for predicting architectural compositions and state the art suite challenging graph datasets heterogeneous structures demonstrating superior adaptability compared any single fixed gnn,current graph neural networks gnns common practice apply pre defined message passing heuristics all graph data even though the stereotypical relational inductive bias such gross simplification might responsible for the lack depth understanding graph learning principles which challenges push the boundary from crafting application specific gnns embracing meta learning paradigm this ratchet the gear gnn another notch forward formulating gnn mean field game that the best learning outcome occurs the nash equilibrium when the learned graph inference rationale allows each graph node find what the best feature representations for not only the individual node but also the entire graph seek for the best characteristic mfc functions transportation mobility controlling information exchange throughout the graph and reaction mobility controlling feature representation learning each node the fly that uncover the most suitable learning mechanism for gnn instance solving mfc variational problem the lens hamiltonian flows formed partial differential equations furthermore agnostic end end deep coined nash gnn honor nobel laureate john nash jointly carve the nature the inductive bias and fine tune the gnn hyper parameters top the elucidated learning mechanism more importantly the mathematical insight mean field games provides window understand the foundational principles graph learning interactive dynamical which allows reshape the idea designing next generation gnn models,2025-08-26T00:48:30.451177
81,Continual Pre-training: A Framework for Language Models that Never Stop Learning,"Large language models (LLMs) are typically pre-trained on a massive, static dataset and then fine-tuned. This paradigm struggles with new information and evolving language, leading to knowledge cutoffs and model staleness. We propose Continual Pre-training (CPT), a framework that enables language models to efficiently and perpetually learn from a continuous stream of new data without catastrophic forgetting. CPT combines a dynamic memory architecture with an elasticity-based regularization scheme. New information is first encoded into a rapidly-updated, non-parametric memory, while an EWC-inspired (Elastic Weight Consolidation) regularizer selectively consolidates core knowledge into the parametric weights of the transformer, preserving foundational language capabilities. We introduce a ""knowledge novelty detector"" to gate what information triggers a consolidation update. Our contributions are: (1) a scalable framework for continual pre-training of LLMs; (2) a hybrid parametric-non-parametric memory system; and (3) a novelty-gated consolidation strategy. We demonstrate that a CPT-trained model can continuously assimilate new information from daily news streams over months, consistently outperforming statically pre-trained models on time-sensitive question-answering tasks while maintaining performance on general language benchmarks.",ICLR,deep learning,gemini-2.5-pro,True,4155,TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models,"Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a world where the latest dump of Common Crawl (CC), the most prominent public source of pre-training data, is used every month to *continually* train an LLM. We design various dynamic evaluations from the CC data, Wikipedia, StackExchange, and code documentations to measure continual learning metrics such as forgetting and forward transfer. Notably, our TiC-CC training data is more than 100 times larger compared with prior continual learning benchmarks for language modeling. We discover that recent DataComp-LM models trained on data before 2023 have already become outdated, incurring up to 45\% larger noun-perplexity on 2024 Wikipedia articles compared to pre-2023 articles. Further, we use our setup to evaluate the effectiveness of several large-scale continual learning methods and find that replaying older data is most effective for combating forgetting: for previously seen CC dumps, it can reduce the regret on held-out loss by 60\% compared to other optimizer and loss-based interventions. However, some domains evolve more quickly than others, favoring different trade-offs between mixing old and new data.",ICLR.cc/2025/Conference,6.25,False,0.9134,large language models llms are pre trained massive static and then fine tuned this paradigm struggles information and evolving language leading knowledge cutoffs and staleness continual pre training cpt that enables language models and perpetually learn from continuous stream data catastrophic forgetting information first encoded into rapidly updated non parametric memory while ewc inspired elastic weight consolidation regularizer selectively consolidates core knowledge into the parametric weights the transformer preserving foundational language capabilities that cpt trained can continuously assimilate information from daily news streams over months consistently outperforming statically pre trained models time sensitive question answering tasks while maintaining general language benchmarks,large language models llms are trained data crawled over many years from the web various dynamic evaluations from the data wikipedia stackexchange and code documentations measure continual learning metrics such forgetting and forward transfer notably our tic training data more than times larger compared prior continual learning benchmarks for language modeling further use our setup the effectiveness several large scale continual learning methods and find that replaying older data most effective for combating forgetting for previously seen dumps can reduce the regret held out loss compared other optimizer and loss based interventions,2025-08-26T00:48:30.451178
82,Flow-based State Space Models for Irregularly Sampled Time Series,"Real-world time series data, particularly from domains like healthcare and finance, are often sampled irregularly and contain missing values. Traditional models like RNNs and Transformers struggle with this irregularity, often requiring ad-hoc imputation or bucketing. We introduce Flow-based State Space Models (FSSM), a new class of deep learning models designed explicitly for irregularly sampled time series. FSSM represents the latent state dynamics as a continuous-time process governed by a neural ordinary differential equation (Neural ODE). Crucially, we model the evolution of the distribution over the latent state using a continuous normalizing flow, allowing us to precisely compute the exact likelihood of observed data points at any point in time, regardless of sampling intervals. Our contributions include: (1) a novel architecture combining Neural ODEs with continuous normalizing flows for time series; (2) a method for exact likelihood computation for irregularly sampled data; and (3) a principled approach to uncertainty quantification. FSSM achieves state-of-the-art results on challenging medical (MIMIC-III) and financial forecasting benchmarks, outperforming both discrete-time models and prior continuous-time approaches.",ICLR,deep learning,gemini-2.5-pro,True,5731,HOPE for a Robust Parameterization of Long-memory State Space Models,"State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. To achieve state-of-the-art performance, an SSM often needs a specifically designed initialization, and the training of state matrices is on a logarithmic scale with a very small learning rate. To understand these choices from a unified perspective, we view SSMs through the lens of Hankel operator theory. Building upon it, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. Our approach helps improve the initialization and training stability, leading to a more robust parameterization. We efficiently implement these innovations by nonuniformly sampling the transfer functions of LTI systems, and they require fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, our new parameterization endows the SSM with non-decaying memory within a fixed time window, which is empirically corroborated by a sequential CIFAR-10 task with padded noise.",ICLR.cc/2025/Conference,6.6,True,0.8214,flow based state space models fssm class deep learning models designed explicitly for irregularly sampled time series fssm represents the latent state dynamics continuous time process governed neural ordinary differential equation neural ode our contributions include combining neural odes continuous normalizing flows for time series for exact likelihood computation for irregularly sampled data and principled uncertainty quantification,state space models ssms that utilize linear time invariant lti systems are known for their effectiveness learning long sequences achieve state the art ssm often needs designed initialization and the training state matrices logarithmic scale very small learning rate these innovations nonuniformly sampling the transfer functions lti systems and they require fewer parameters compared canonical ssms,2025-08-26T00:48:30.451180
83,Geometric Disentanglement via the Lie Group VAE,"Learning disentangled representations, where distinct latent variables correspond to independent factors of data variation, is a central goal of representation learning. Current methods often struggle to disentangle factors that correspond to geometric transformations (e.g., rotation, scaling) and rely on heuristics. We introduce the Lie Group VAE (LG-VAE), a variational autoencoder architecture that leverages the mathematical structure of Lie groups to achieve principled geometric disentanglement. The LG-VAE's latent space is structured as a direct product of a Lie group and a Euclidean space. The decoder is explicitly designed to be equivariant to the action of the learned group on the latent variables, forcing the model to map geometric transformations in the data space to algebraic operations within the learned Lie group representation. Our contributions are: (1) a novel VAE architecture with a structured Lie group latent space; (2) an equivariant decoder that enforces geometric consistency; and (3) a training objective that learns both the group structure and the disentangled representation simultaneously. We demonstrate that the LG-VAE successfully discovers and disentangles transformations like rotations and translations in 2D and 3D datasets without explicit supervision, achieving superior disentanglement scores and enabling controllable data generation.",ICLR,deep learning,gemini-2.5-pro,True,2235,Interaction Asymmetry: A General Principle for Learning Composable Abstractions,"Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: ""Parts of the same concept have more complex interactions than parts of different concepts"". We formalize this via block diagonality conditions on the $(n+1)$th order derivatives of the generator mapping concepts to observed data, where different orders of ""complexity"" correspond to different $n$. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with $n=0$ or $1$. We provide results for up to $n=2$, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger $n$. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.",ICLR.cc/2025/Conference,7.0,True,0.8597,learning disentangled representations where distinct latent variables correspond independent factors data variation central goal representation learning the decoder explicitly designed equivariant the action the learned group the latent variables forcing the map geometric transformations the data space algebraic operations within the learned lie group representation our contributions are vae structured lie group latent space equivariant decoder that enforces geometric consistency and training objective that learns both the group structure and the disentangled representation simultaneously that the vae discovers and disentangles transformations like rotations and translations and datasets explicit supervision achieving superior disentanglement scores and enabling controllable data generation,learning disentangled representations concepts and composing them unseen ways crucial for generalizing out domain situations our unify recent theoretical for learning concepts objects which are recovered special cases implementation these criteria flexible transformer based vae regularizer the attention weights the decoder,2025-08-26T00:48:30.451181
84,Test-Time Training with Self-Supervised Consistency for Domain Adaptation,"Domain shift, the discrepancy between training and testing data distributions, poses a significant challenge for deploying machine learning models. Test-time adaptation methods aim to update a source-trained model using only unlabeled test data. We propose Test-Time Training with Self-Supervised Consistency (T3SC), a novel approach that adapts models by enforcing geometric and semantic consistency in their representation space. During testing, for each incoming batch, T3SC generates augmented views of the data and enforces a self-supervised consistency loss, compelling the model to produce invariant representations for semantically identical inputs under different perturbations. Unlike methods that rely on entropy minimization, which can reinforce incorrect predictions, our consistency-based objective is more robust to noisy pseudo-labels. Our contributions are: (1) a new test-time adaptation objective based on self-supervised consistency; (2) a framework that requires no modification to the original training pipeline; and (3) a lightweight adaptation procedure with minimal computational overhead. T3SC achieves state-of-the-art results on standard domain adaptation benchmarks like ImageNet-C and VisDA-C, significantly improving model robustness to real-world corruptions and shifts.",ICLR,deep learning,gemini-2.5-pro,True,483,Binary-Feedback Active Test-Time Adaptation,"Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To
address this issue, we introduce a Binary-feedback Active Test-Time Adaptation (BATTA) setting, which uses a few binary feedbacks from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BATTA-RL, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BATTA-RL achieves substantial accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort.",ICLR.cc/2025/Conference,6.0,False,0.8501,domain shift the discrepancy between training and testing data distributions poses significant challenge for deploying machine learning models test time adaptation methods aim update source trained only unlabeled data test time training self supervised consistency t3sc that adapts models enforcing geometric and semantic consistency their representation space our contributions are test time adaptation objective self supervised consistency that requires modification the original training pipeline and lightweight adaptation procedure minimal computational overhead t3sc achieves state the art standard domain adaptation benchmarks like imagenet and visda improving robustness real world corruptions and shifts,deep learning models perform poorly when domain shifts exist between training and data test time adaptation tta paradigm mitigate this issue adapting pre trained models only unlabeled samples however existing tta methods can fail under severe domain shifts while recent active tta approaches requiring full class labels are impractical due high labeling costs address this issue binary feedback active test time adaptation batta setting which uses few binary feedbacks from annotators indicate whether predictions are correct thereby reducing the labeling burden annotators under the setting batta dual path optimization that leverages reinforcement learning balance binary feedback guided adaptation uncertain samples agreement based self adaptation confident predictions,2025-08-26T00:48:30.451183
85,Neuro-Symbolic Inductive Logic Programming for Relational Reasoning,"Deep learning models, particularly Graph Neural Networks, have shown promise in relational reasoning but often struggle with tasks requiring multi-hop, explicit logical deduction and fail to produce interpretable reasoning chains. We introduce Neuro-Symbolic Inductive Logic Programming (NS-ILP), a framework that integrates the expressive power of deep relational embeddings with the formal rigor of Inductive Logic Programming (ILP). NS-ILP learns continuous vector representations of predicates and entities, then uses a differentiable ILP engine to search for logical rules that best explain the relationships in the training data. The key innovation is a differentiable proof mechanism that allows end-to-end training, enabling the neural component to learn representations that are amenable to symbolic reasoning, and the symbolic component to find rules grounded in the learned embeddings. Our contributions are: (1) a fully differentiable integration of ILP with neural networks; (2) a model that produces explicit, human-readable logical rules as part of its output; and (3) superior performance on complex relational reasoning benchmarks like CLUTRR and path-finding queries on knowledge graphs, demonstrating improved generalization and interpretability.",ICLR,deep learning,gemini-2.5-pro,True,98,Systematic Relational Reasoning With Epistemic Graph Neural Networks,"Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work has shown that regular GNNs lack the ability to systematically generalize from training examples on test graphs requiring longer inference chains, which fundamentally limits their reasoning abilities. A common solution relies on neuro-symbolic methods that systematically reason by learning rules, but their scalability is often limited and they tend to make unrealistically strong assumptions, e.g.\ that the answer can always be inferred from a single relational path. We propose the Epistemic GNN (EpiGNN), a novel parameter-efficient and scalable GNN architecture with an epistemic inductive bias for systematic reasoning. Node embeddings in EpiGNNs are treated as epistemic states, and message passing  is implemented accordingly. We show that EpiGNNs achieve state-of-the-art results on link prediction tasks that require systematic reasoning. Furthermore, for inductive knowledge graph completion, EpiGNNs rival the performance of state-of-the-art specialized approaches. Finally, we introduce two new benchmarks that go beyond standard relational reasoning by requiring the aggregation of information from multiple paths. Here, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason accurately.  Code and datasets are available at https://github.com/erg0dic/gnn-sg.",ICLR.cc/2025/Conference,6.5,True,0.8721,deep learning models graph neural networks have shown promise relational reasoning but often struggle tasks requiring multi hop explicit logical deduction and fail produce interpretable reasoning chains neuro symbolic inductive logic programming ilp that integrates the expressive power deep relational embeddings the formal rigor inductive logic programming ilp the key innovation differentiable proof mechanism that allows end end training enabling the neural component learn representations that are amenable symbolic reasoning and the symbolic component find rules grounded the learned embeddings our contributions are fully differentiable integration ilp neural networks that produces explicit human readable logical rules part its output and superior complex relational reasoning benchmarks like clutrr and path finding queries knowledge graphs demonstrating improved generalization and interpretability,focus reasoning relational domains where the use graph neural networks gnns seems like natural choice however previous has shown that regular gnns lack the ability systematically generalize from training examples graphs requiring longer inference chains which fundamentally limits their reasoning abilities common solution relies neuro symbolic methods that systematically reason learning rules but their scalability often limited and they tend make unrealistically strong assumptions the epistemic gnn epignn parameter efficient and scalable gnn epistemic inductive bias for systematic reasoning that epignns achieve state the art link prediction tasks that require systematic reasoning furthermore for inductive knowledge graph completion epignns rival the state the art specialized approaches finally two benchmarks that beyond standard relational reasoning requiring the aggregation information from multiple paths,2025-08-26T00:48:30.451184
86,Deconstructing Transformers: A Spectral Analysis of Self-Attention,"While the Transformer architecture has become ubiquitous, our theoretical understanding of its core component, the self-attention mechanism, remains limited. This paper provides a novel spectral analysis of the self-attention operator. We demonstrate that the attention matrix can be interpreted as the adjacency matrix of a dynamically constructed graph, and we analyze its spectral properties (eigenvalues and eigenvectors). We prove that the dot-product attention mechanism implicitly promotes a low-rank structure in the attention matrix, effectively acting as a spectral filter that amplifies dominant patterns in the token-token affinity space. Furthermore, we show how LayerNorm and residual connections interact with this spectral filtering, stabilizing the spectrum and preventing ""attention collapse"" to a rank-1 matrix. Our contributions are: (1) a new theoretical framework for analyzing self-attention through a spectral lens; (2) a formal proof of the low-rank bias of the attention mechanism; and (3) insights into the role of other architectural components in shaping the attention spectrum. This analysis provides a deeper understanding of how Transformers process information and opens new avenues for designing more efficient and principled attention mechanisms.",ICLR,deep learning,gemini-2.5-pro,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8823,while the transformer has become ubiquitous our theoretical understanding its core component the self attention mechanism remains limited that the attention matrix can interpreted the adjacency matrix dynamically constructed graph and its spectral properties eigenvalues and eigenvectors that the dot product attention mechanism implicitly promotes low rank structure the attention matrix acting spectral filter that amplifies dominant patterns the token token affinity space our contributions are theoretical for analyzing self attention spectral lens formal proof the low rank bias the attention mechanism and insights into the role other architectural components shaping the attention spectrum this analysis provides deeper understanding how transformers process information and opens avenues for designing more efficient and principled attention mechanisms,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T00:48:30.451185
87,Privacy-Preserving Representations via Differentiable Oblivious Hashing,"Learning useful representations of data while preserving the privacy of sensitive attributes is a critical challenge. Existing methods often trade off utility and privacy, or require computationally expensive adversarial training. We propose Differentiable Oblivious Hashing (DOH), a new technique for learning privacy-preserving representations. DOH learns a projection of the input data into a binary hash space using a stochastic, differentiable hashing function. The training objective is twofold: a utility term ensures the hashes are useful for a downstream task, while a novel privacy term, based on the collision entropy of hashes for inputs with different sensitive attributes, forces the model to map them to distinct, non-predictable hash buckets. This ""oblivious"" mapping makes it difficult for an adversary to infer the sensitive attribute from the hash code. Our contributions are: (1) the DOH framework for learning privatized binary representations; (2) a differentiable hashing mechanism suitable for end-to-end training; and (3) a collision-based privacy objective. We demonstrate on benchmark datasets that DOH achieves a superior privacy-utility trade-off compared to adversarial methods and differential privacy approaches, offering a practical and efficient solution for creating anonymized yet functional data representations.",ICLR,deep learning,gemini-2.5-pro,False,,Privacy Preserving Generative Feature Transformation,"Data-Centric AI (DCAI) aims to use AI to get better data for better AI. Feature transformation, as one of the essential tasks of DCAI, can augment the data representation and has garnered significant attention. Existing methods have demonstrated state-of-the-art performance on advancing predictive tasks. However, these methods can lead to serious privacy leakage. For example, sensitive features in original data can be inferred by models trained on transformed data, exposing vulnerabilities in the privacy-preserving capabilities of these methods. To address this issue, we introduce a privacy-preserving feature transformation framework that transforms data representation while preserving privacy from a generative modeling perspective. Specifically, our framework includes two phases: 1) privacy-aware knowledge acquisition and 2) privacy-preserving feature space generation. In the knowledge acquisition phase, we develop an information bottlenecks guided reinforcement learning system to explore and collect privacy-aware feature sets as a knowledge base in token sequence form. In the feature space generation phase, we develop a generative model to encode the knowledge base into a privacy-aware latent space, where the best latent representation is identified and decoded into the optimal privacy-preserving feature space. We solve the optimization via projected gradient ascent that maximizes predictive performance and minimizes privacy exposure. Finally, we present extensive experiments on eight real-world datasets to evaluate how our method can navigate both performance and privacy. The code is available at https://anonymous.4open.science/r/anonymous-2B53/.",ICLR.cc/2025/Conference,3.5,nan,0.7990,learning useful representations data while preserving the privacy sensitive attributes critical challenge differentiable oblivious hashing doh for learning privacy preserving representations our contributions are the doh for learning privatized binary representations differentiable hashing mechanism suitable for end end training and collision based privacy objective,feature transformation one the essential tasks dcai can augment the data representation and has garnered significant attention address this issue privacy preserving feature transformation that transforms data representation while preserving privacy from generative modeling perspective our includes two phases privacy aware knowledge acquisition and privacy preserving feature space generation the knowledge acquisition phase information bottlenecks guided reinforcement learning and collect privacy aware feature sets knowledge base token sequence form the feature space generation phase generative encode the knowledge base into privacy aware latent space where the best latent representation identified and decoded into the optimal privacy preserving feature space solve the optimization projected gradient ascent that maximizes predictive and minimizes privacy exposure,2025-08-26T00:48:30.451187
88,Multi-Agent Reinforcement Learning with Action-Semantic Communication,"Effective communication is key to solving complex cooperative multi-agent reinforcement learning (MARL) problems. Existing communication protocols often involve learning arbitrary, uninterpretable message vectors, which can be sample-inefficient and hard to generalize. We introduce Action-Semantic Communication (ASC), a novel MARL communication protocol where messages are structured to represent proposals and commitments about future actions. Instead of exchanging latent vectors, agents broadcast messages from a discrete, learnable vocabulary, where each ""word"" is tied to a specific action policy or sub-goal. A message like ""I will cover region B"" is not just cheap talk; it is a commitment backed by a specific, executable policy from the agent's learned library. This grounds communication in the action space, making it more interpretable and sample-efficient. Our contributions are: (1) a new MARL communication framework based on action-semantic commitments; (2) a method for jointly learning a communication vocabulary and a library of corresponding policies; and (3) state-of-the-art performance on challenging cooperative tasks like StarCraft II and multi-agent particle environments, demonstrating faster coordination and better generalization to unseen agent configurations.",ICLR,deep learning,gemini-2.5-pro,True,10367,Human-like Communication Strategies for Improved Multi-Agent Reinforcement Learning,"Multi-Agent Reinforcement Learning (MARL) has seen significant progress in recent years, enabling multiple agents to coordinate and optimize their actions in complex environments. However, integrating effective communication protocols into MARL frameworks remains a challenge, as it introduces issues such as increased state space dimensionality, lack of stationarity, and the need for interpretability. Inspired by human communication, which relies on prior knowledge, contextual awareness, and efficient information exchange, we propose a novel framework for incorporating human-like communication strategies to enhance the learning process. Motivated by recent advancements in natural language processing (NLP), multi-modal AI and object detection, we use text-to-mask models and human feedback to learn compact and informative communication strategies that facilitate coordination among agents to improve the overall performance. We demonstrate the efficiency of our approach on various multi-agent tasks and provide insights into emergent communication behaviors observed during training.",ICLR.cc/2025/Conference,3.0,nan,0.9238,effective communication key solving complex cooperative multi agent reinforcement learning marl problems existing communication protocols often involve learning arbitrary uninterpretable message vectors which can sample inefficient and hard generalize our contributions are marl communication action semantic commitments for jointly learning communication vocabulary and library corresponding policies and state the art challenging cooperative tasks like starcraft and multi agent particle environments demonstrating faster coordination and better generalization unseen agent configurations,multi agent reinforcement learning marl has seen significant progress recent years enabling multiple agents coordinate and optimize their actions complex environments however integrating effective communication protocols into marl frameworks remains challenge introduces issues such increased state space dimensionality lack stationarity and the need for interpretability inspired human communication which relies prior knowledge contextual awareness and efficient information exchange for incorporating human like communication strategies enhance the learning process motivated recent advancements natural language processing nlp multi modal and object detection use text mask models and human feedback learn compact and informative communication strategies that facilitate coordination among agents improve the overall,2025-08-26T00:48:30.451188
89,Amortized Bayesian Model Comparison for Neural Network Architectures,"Choosing the right neural network architecture is a crucial, yet resource-intensive, part of the machine learning pipeline. Bayesian model comparison offers a principled way to perform this selection by approximating the marginal likelihood (or model evidence), but existing methods like nested sampling are computationally prohibitive for deep models. We introduce the Amortized Model Comparison Network (AMCN), a meta-learning approach that learns to rapidly approximate the log marginal likelihood for a given model architecture and dataset. The AMCN is a graph hypernetwork trained on a vast collection of synthetic datasets and their corresponding architecture-evidence pairs, pre-computed offline. Once trained, it can predict the evidence for a new architecture and dataset in a single forward pass, bypassing the need for expensive sampling. Our contributions are: (1) a framework for amortizing Bayesian model comparison for neural networks; (2) the AMCN architecture that maps architecture graphs to evidence scores; and (3) a demonstration of its effectiveness in neural architecture search (NAS), where it serves as a highly efficient and accurate performance predictor, discovering high-performing architectures orders of magnitude faster than traditional NAS methods.",ICLR,deep learning,gemini-2.5-pro,True,7586,Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks,"Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.",ICLR.cc/2025/Conference,5.75,True,0.8343,choosing the right neural network crucial yet resource intensive part the machine learning pipeline bayesian comparison offers principled way perform this selection approximating the marginal likelihood evidence but existing methods like nested sampling are computationally prohibitive for deep models the amortized comparison network amcn meta learning that learns rapidly approximate the log marginal likelihood for given and our contributions are for amortizing bayesian comparison for neural networks the amcn that maps graphs evidence scores and demonstration its effectiveness neural search nas where serves highly efficient and accurate predictor discovering high performing architectures orders magnitude faster than traditional nas methods,despite recent advances sampling based inference for bayesian neural networks bnns remains significant challenge probabilistic deep learning consequence sampling still requires longer inference times than non bayesian methods even for small neural networks despite recent advances making software implementations more efficient tackle these challenges ensembling that leverages strategies from optimization and recently proposed sampler called microcanonical langevin monte carlo mclmc for efficient robust and predictable sampling,2025-08-26T00:48:30.451189
90,Unsupervised Learning of 3D-Aware Representations from Single Images via Cross-View Cycle Consistency,"Learning 3D representations from 2D images without explicit 3D supervision is a challenging open problem. We propose Cross-View Cycle Consistency (CVCC), a novel self-supervised framework for learning 3D-aware representations from single, unposed images. Our model learns to decompose an image into a 3D-aware latent representation, which can then be re-rendered from a novel viewpoint. The core of our method lies in a cycle consistency objective: we generate a new view, and then require the model to re-render the original view from this generated image. This forces the latent representation to capture the true underlying 3D structure of the scene, as this is the only information that remains invariant across view changes. Unlike GAN-based approaches, CVCC does not require an adversarial discriminator and is trained with a simple reconstruction loss. Our contributions are: (1) a self-supervised learning objective based on cross-view cycle consistency; (2) an architecture that learns to generate novel views without explicit 3D data or camera poses; and (3) results showing that our learned representations capture rich 3D information, enabling tasks like single-image depth estimation and controllable novel view synthesis on datasets like CelebA and Cats.",ICLR,deep learning,gemini-2.5-pro,True,9186,Position-Query-Based Autoencoders for View Decoupled Cross Point Cloud Reconstruction and a Self-Supervised Learning Framework,"Point cloud learning, especially in a self-supervised way without manual labels, has received emerging attention in both vision and learning communities, with its potential utility in wide areas. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it could thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to achieve new state-of-the-art results and surpasses previous single-modal self-reconstruction methods in 3D self-supervised learning by a margin. Specifically, it outperforms self-reconstruction baseline (Point-MAE) 6.5\%, 7.0\%, 6.7\% in three variants of ScanObjectNN with Mlp-Linear evaluation protocol. Source code will be released.",ICLR.cc/2025/Conference,6.2,False,0.8509,learning representations from images explicit supervision challenging open problem cross view cycle consistency cvcc self supervised for learning aware representations from single unposed images this forces the latent representation capture the true underlying structure the scene this the only information that remains invariant across view changes our contributions are self supervised learning objective cross view cycle consistency that learns generate views explicit data camera poses and showing that our learned representations capture rich information enabling tasks like single image depth estimation and controllable view synthesis datasets like celeba and cats,point cloud learning self supervised way manual labels has received emerging attention both vision and learning communities its potential utility wide areas most existing generative approaches for point cloud self supervised learning focus recovering masked points from visible ones within single view inspired this the potential two view learning this domain achieve this goal crop mechanism for point cloud view generation for the first time and further positional encoding represent the relative position between the two decoupled views the cross reconstruction increases the difficulty pre training compared self reconstruction which enables our achieve state the art and surpasses previous single modal self reconstruction methods self supervised learning margin,2025-08-26T00:48:30.451191
91,A Unified Framework for Contrastive and Non-Contrastive Self-Supervised Learning,"Self-supervised learning (SSL) is dominated by two main paradigms: contrastive methods (e.g., SimCLR) which pull positive pairs together and push negative pairs apart, and non-contrastive methods (e.g., BYOL, SimSiam) which only pull positive pairs together, avoiding collapse through architectural tricks like stop-gradients or momentum encoders. The theoretical relationship between these two families is not well understood. We present a unified framework that views both contrastive and non-contrastive learning through the lens of information-theoretic principles. We show that both approaches can be seen as optimizing a lower bound on the mutual information between augmented views, but differ in their estimation of the negative term. Contrastive methods use explicit negative sampling, while non-contrastive methods use an implicit, batch-wide uniform negative distribution enforced by the architecture. Our key contributions are: (1) a single objective function that can instantiate both SimCLR and BYOL/SimSiam as special cases; (2) a theoretical analysis showing how architectural components in non-contrastive methods act as implicit regularizers; and (3) a new hybrid algorithm derived from our framework that outperforms both pure approaches on standard benchmarks. This work bridges the conceptual gap between dominant SSL paradigms.",ICLR,deep learning,gemini-2.5-pro,True,11317,Self-supervised contrastive learning performs non-linear system identification,"Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",ICLR.cc/2025/Conference,6.4,True,0.8420,self supervised learning ssl dominated two main paradigms contrastive methods unified that views both contrastive and non contrastive learning the lens information theoretic principles,self supervised learning ssl approaches have brought tremendous success across many tasks and domains has been argued that these successes can attributed link between ssl and identifiable representation learning temporal structure and auxiliary variables ensure that latent representations are related the true underlying generative factors the data,2025-08-26T00:48:30.451192
92,Learning Modular and Reusable Skills with a Compositional Policy Graph,"Hierarchical reinforcement learning aims to solve complex, long-horizon tasks by decomposing them into simpler sub-tasks or skills. However, current methods often learn monolithic, non-reusable skills. We introduce the Compositional Policy Graph (CPG), a framework for learning a library of modular and reusable skills that can be dynamically composed to solve new tasks. The CPG is a directed graph where nodes represent low-level policies (skills) and edges, managed by a high-level gating policy, represent feasible transitions between them. We train the entire graph end-to-end using an off-policy algorithm with a novel intrinsic reward structure that encourages each node to learn a distinct yet composable behavior. A key innovation is a graph modularity objective that promotes disentanglement, ensuring skills are self-contained and reusable. Our contributions are: (1) the CPG framework for learning libraries of composable skills; (2) a graph modularity objective for skill disentanglement; and (3) demonstrations on complex robotics manipulation and navigation tasks, where CPG learns interpretable and reusable skills that enable rapid zero-shot transfer to novel, unseen tasks composed of familiar sub-goals.",ICLR,deep learning,gemini-2.5-pro,True,521,DSR: Reinforcement Learning with Dynamical Skill Refinement,"Reinforcement learning with skills (RL with skills) is an efficient paradigm for solving sparse-reward tasks by extracting skills from demonstration datasets and learning high-level policy which selects skills. Because each selected skill by high-level policy is executed for multiple consecutive timesteps, the high-level policy is essentially learned in a temporally abstract Markov decision process (TA-MDP) built on the skills, which shortens the task horizon and reduces the exploration cost. However, these skills are usually sub-optimal because of the potential low quality and low coverage of the datasets, which causes the sub-optimal performance in the downstream task. Refining skills is intuitive, but the change of skills will in turn lead to the non-stationarity of the transition dynamics of TA-MDP which we name temporal abstraction shift. To address the dilemma of sub-optimal skills and temporal abstraction shift, we unify the optimization objectives of the entire hierarchical policy consisting of the high-level policy and the low-level policy whose latent space embeds the skills. We theoretically prove that the unified optimization objective guarantees the performance improvement in TA-MDP, and that optimizing the performance in TA-MDP is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in original MDP. Furthermore, in order to overcome the phenomenon of skill space collapse, we propose the dynamical skill refinement (DSR) mechanism which names our method. The experiment results empirically validate the effectiveness of our method, and show the advantages over the state-of-the-art (SOTA) methods.",ICLR.cc/2025/Conference,5.333333333333333,False,0.8368,hierarchical reinforcement learning aims solve complex long horizon tasks decomposing them into simpler sub tasks skills the compositional policy graph cpg for learning library modular and reusable skills that can dynamically composed solve tasks our contributions are the cpg for learning libraries composable skills graph modularity objective for skill disentanglement and demonstrations complex robotics manipulation and navigation tasks where cpg learns interpretable and reusable skills that enable rapid zero shot transfer unseen tasks composed familiar sub goals,reinforcement learning skills skills efficient paradigm for solving sparse reward tasks extracting skills from demonstration datasets and learning high level policy which selects skills address the dilemma sub optimal skills and temporal ion shift unify the optimization objectives the entire hierarchical policy consisting the high level policy and the low level policy whose latent space embeds the skills theoretically that the unified optimization objective guarantees the improvement mdp and that optimizing the mdp equivalent optimizing lower bound the the entire hierarchical policy original mdp,2025-08-26T00:48:30.451194
93,Parameter-Efficient Fine-Tuning with Learned Intrinsic Subspaces,"As pre-trained models grow larger, fine-tuning all parameters for downstream tasks becomes computationally prohibitive. Parameter-efficient fine-tuning (PEFT) methods like LoRA adapt models by training a small number of extra parameters. We propose Learning Intrinsic Subspaces (LISA), a new PEFT method that operates without adding any new parameters. LISA is based on the hypothesis that adaptation can occur within a low-dimensional subspace of the full parameter space. During fine-tuning, LISA freezes the pre-trained weights and instead learns a low-rank projection matrix that defines a task-specific subspace. Only the coordinates of the model's parameters within this small, learned subspace are updated via gradient descent. This is more efficient than LoRA as it does not require modifying the forward pass with extra weights. Our contributions are: (1) a novel, parameter-free PEFT method; (2) a formulation for learning an optimal adaptation subspace for a given task; and (3) extensive experiments on RoBERTa and ViT. LISA matches the performance of LoRA while using fewer trainable parameters and no additional inference-time latency, establishing a new state-of-the-art for parameter-efficient adaptation.",ICLR,deep learning,gemini-2.5-pro,True,9194,The Quest for Winning Tickets in Low-Rank Adapters,"Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method, offers an effective strategy for adapting large pre-trained models to specific tasks with minimal computational overhead. LoRA achieves this by introducing low-rank parameter matrices to the frozen pre-trained models. However, despite their efficiency, LoRA and its variants modify all elements of a parameter block, which is unnecessary as LoRA primarily aims to adjust a small set of subspaces that capture task-specific knowledge. Drawing inspiration from the Lottery Ticket Hypothesis (LTH), which posits that dense neural networks contain sparse subnetworks capable of performing similarly to fully-parameterized models, we investigate whether similar sparse subnetworks exist for low-rank adapters. We demonstrate that such subnetworks, often referred to as ""winning tickets"" in the context of LTH, indeed exist for low-rank adapters. We introduce a method to identify this sparse subset of weights for each layer by relating the top subspaces of the pretrained parameter block to the elements of the corresponding weight matrix. This subset is then fine-tuned using LoRA. We show that this sparse subset is not necessarily unique; as long as sparsity is kept within a certain bound defined by the task, random subnetworks with similar sparsity can act as winning tickets. Building on this discovery, we propose a novel approach called Partial-LoRA, which adds sparse low-rank parameters to pre-trained models. Through extensive experiments on 8 vision and 4 language tasks, we demonstrate that Partial-LoRA can reduce trainable parameters by up to 87% while maintaining or even improving model performance in some cases. Our work thus reduces memory needs and theoretically grounds sparse LoRAs.",ICLR.cc/2025/Conference,5.2,False,0.8452,learning intrinsic subspaces lisa peft that operates adding any parameters lisa the hypothesis that adaptation can occur within low dimensional subspace the full parameter space our contributions are parameter free peft formulation for learning optimal adaptation subspace for given task and extensive experiments roberta and vit lisa matches the lora while fewer trainable parameters and additional inference time latency establishing state the art for parameter efficient adaptation,low rank adaptation lora prominent parameter efficient fine tuning peft offers effective strategy for adapting large pre trained models specific tasks minimal computational overhead however despite their efficiency lora and its variants modify all elements parameter block which unnecessary lora primarily aims adjust small set subspaces that capture task specific knowledge drawing inspiration from the lottery ticket hypothesis lth which posits that dense neural networks contain sparse subnetworks capable performing similarly fully parameterized models whether similar sparse subnetworks exist for low rank adapters extensive experiments vision and language tasks that partial lora can reduce trainable parameters while maintaining even improving some cases,2025-08-26T00:48:30.451195
94,Generalization in Deep Learning via the Information Bottleneck in Activation Space,"A central mystery in deep learning is how overparameterized networks, which can easily memorize the training data, still generalize well to unseen examples. We propose a new explanation through the lens of an Information Bottleneck (IB) principle applied not to the weights, but to the network's activations. We argue that stochastic elements in training (e.g., SGD, dropout) act as a noisy channel on the activations, and the network learns to implicitly compress its intermediate representations to be maximally informative about the label while minimizing sensitivity to this noise. This compression forces the network to discard spurious, instance-specific details in favor of robust, generalizable features. Our contributions are: (1) a new theoretical perspective: the Activation Information Bottleneck (AIB) hypothesis for generalization; (2) we derive a measurable quantity, the ""activation compression ratio,"" and show it is highly correlated with test accuracy across different models and training settings; (3) we demonstrate that explicit regularization encouraging lower-information activations can improve generalization. This work reframes the study of generalization away from the weight space and towards the information content of the learned representations themselves.",ICLR,deep learning,gemini-2.5-pro,True,5896,Decoding Generalization from Memorization in Deep Neural Networks,"Overparameterized Deep Neural Networks that generalize well have been key to the dramatic success of Deep Learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. It has also been known that deep networks possess the ability to memorize training data, as evidenced by perfect or high training accuracies on models trained with corrupted data that have class labels shuffled to varying degrees. Concomitantly, such models are known to generalize poorly, i.e. they suffer from poor test accuracies, due to which it is thought that the act of memorizing substantially degrades the ability to generalize. It has, however, been unclear why the poor generalization that accompanies such memorization, comes about. One possibility is that in the process of training with corrupted data, the layers of the network irretrievably re-organize their representations in a manner that makes generalization difficult. The other possibility is that the network retains significant ability to generalize, but the trained network somehow “chooses” to readout in a manner that is detrimental to generalization. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially improved generalization, even in the face of memorization. Furthermore, such generalization abilities can be easily decoded from the internals of the trained model, and we build a technique to do so from the outputs of specific layers of the network. We demonstrate results on multiple models trained with a number of standard datasets.",ICLR.cc/2025/Conference,3.8,False,0.8614,central mystery deep learning how overparameterized networks which can easily memorize the training data still generalize well unseen examples sgd dropout act noisy channel the activations and the network learns implicitly compress its intermediate representations maximally informative about the label while minimizing sensitivity this noise this compression forces the network discard spurious instance specific details favor robust generalizable features,overparameterized deep neural networks that generalize well have been key the dramatic success deep learning recent years has also been known that deep networks possess the ability memorize training data evidenced perfect high training accuracies models trained corrupted data that have class labels shuffled varying degrees one possibility that the process training corrupted data the layers the network irretrievably organize their representations manner that makes generalization difficult the other possibility that the network retains significant ability generalize but the trained network somehow chooses readout manner that detrimental generalization furthermore such generalization abilities can easily decoded from the internals the trained and build from the outputs specific layers the network,2025-08-26T00:48:30.451197
95,Concept-based Explainability with Differentiable Decision Trees,"Existing explainability methods often produce saliency maps that highlight input features but fail to explain a model's decision in terms of high-level, human-understandable concepts. We introduce Differentiable Concept-based Decision Trees (DCDT), a framework that generates explanations as a sequence of logical decisions based on learned concepts. First, a concept discovery module, trained jointly with the main task model, learns a set of disentangled concept vectors (e.g., ""striped,"" ""wheeled"" for an image classifier). Then, a differentiable decision tree is trained to predict the final output using only these learned concepts as input features. The path through this tree for a given input provides a transparent, logical explanation (e.g., ""IF concept 'wheeled' is present AND concept 'has_engine' is present THEN predict 'car'""). Our contributions are: (1) an end-to-end trainable model that combines deep concept learning with interpretable decision logic; (2) a soft, differentiable decision tree architecture; and (3) qualitative and quantitative evaluations showing DCDT generates more faithful and human-understandable explanations than saliency-based methods, while maintaining competitive performance on the primary task.",ICLR,deep learning,gemini-2.5-pro,True,9726,Decision Rules are in the Pixels: Towards Pixel-level Evaluation of Saliency-based XAI Models,"The intricate and opaque nature of deep neural networks (DNNs) makes it difficult to decipher how they make decisions. Explainable artificial intelligence (XAI) has emerged as a promising remedy to this conundrum. However, verifying the correctness of XAI methods remains challenging, due to the absence of universally accepted ground-truth explanations. In this study, we focus on assessing the correctness of saliency-based XAI models applied to DNN-based image classifiers at the pixel level. The proposed evaluation protocol departs significantly from previous human-centric correctness assessment at the semantically meaningful object part level, which may not correspond to the actual decision rules derived by classifiers. A crucial step in our approach involves introducing a spatially localized shortcut, a form of decision rule that DNN-based classifiers tend to adopt preferentially, without disrupting original image patterns and decision rules therein. After verifying the shortcut as the dominant decision rule, we estimate the Shapley value for each pixel within the shortcut area to generate the ground-truth explanation map, assuming that pixels outside this area have null contributions. We quantitatively evaluate fourteen saliency-based XAI methods for classifiers utilizing convolutional neural networks and vision Transformers, trained on perturbed CIFAR-10, CIFAR-100, and ImageNet datasets, respectively. Comprehensive experimental results show that existing saliency-based XAI models struggle to offer accurate pixel-level attributions, casting doubt on the recent progress in saliency-based XAI.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8063,our contributions are end end trainable that combines deep concept learning interpretable decision logic soft differentiable decision tree and qualitative and quantitative evaluations showing dcdt generates more faithful and human understandable explanations than saliency based methods while maintaining competitive the primary task,the intricate and opaque nature deep neural networks dnns makes difficult decipher how they make decisions explainable artificial intelligence xai has emerged promising remedy this conundrum quantitatively fourteen saliency based xai methods for classifiers utilizing convolutional neural networks and vision transformers trained perturbed cifar cifar and imagenet datasets respectively,2025-08-26T00:48:30.451198
96,Learning Fair Representations by Calibrating the Eigenspectrum of the Covariance Matrix,"Algorithmic fairness aims to ensure that models do not disproportionately harm specific demographic groups. Many methods achieve fairness by learning representations that are invariant to sensitive attributes, often through adversarial training. We propose a novel, non-adversarial approach to learning fair representations by directly shaping the geometry of the representation space. We observe that in unfair models, the principal components (eigenvectors) of the feature covariance matrix often align with the sensitive attribute. Our method, Spectral Fairness Calibration (SFC), adds a regularization term to the training objective that penalizes this alignment. Specifically, SFC encourages the eigenvectors of the representation covariance matrix to be orthogonal to the direction of sensitive attribute variation. This effectively ""re-calibrates"" the representation space to remove information about the sensitive attribute from its main axes of variation, without needing a complex adversarial min-max game. Our contributions are: (1) a new perspective on fairness through the lens of spectral analysis; (2) the SFC regularizer, a simple and efficient non-adversarial fairness intervention; and (3) empirical results showing that SFC achieves state-of-the-art fairness-accuracy trade-offs on benchmark datasets.",ICLR,deep learning,gemini-2.5-pro,True,5494,Adversarial Latent Feature Augmentation for Fairness,"Achieving fairness in machine learning remains a critical challenge, especially due to the opaque effects of data augmentation on input spaces within nonlinear neural networks. Nevertheless, current approaches that emphasize augmenting latent features, rather than input spaces, offer limited insights into their ability to detect and mitigate bias. In response, we introduce the concept of the ""unfair region"" in the latent space, a subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high, leading to unfair prediction results. To address this, we propose Adversarial Latent Feature Augmentation (ALFA), a method that leverages adversarial fairness attacks to perturb latent space features, which are then used as data augmentation for fine-tuning. ALFA intentionally shifts latent features into unfair regions, and the last layer of the network is fine-tuned with these perturbed features, leading to a corrected decision boundary that enhances fairness in classification in a cost-effective manner. We present a theoretical framework demonstrating that our adversarial fairness objective reliably generates biased feature perturbations, and that fine-tuning on samples from these unfair regions ensures fairness improvements. Extensive experiments across diverse datasets, modalities, and backbone networks validate that training with these adversarial features significantly enhances fairness while maintaining predictive accuracy in classification tasks.",ICLR.cc/2025/Conference,6.5,True,0.8589,algorithmic fairness aims ensure that models not disproportionately harm specific demographic groups many methods achieve fairness learning representations that are invariant sensitive attributes often adversarial training non adversarial learning fair representations directly shaping the geometry the representation space observe that unfair models the principal components eigenvectors the feature covariance matrix often align the sensitive attribute our spectral fairness calibration sfc adds regularization term the training objective that penalizes this alignment sfc encourages the eigenvectors the representation covariance matrix orthogonal the direction sensitive attribute variation this calibrates the representation space remove information about the sensitive attribute from its main axes variation needing complex adversarial min max game our contributions are perspective fairness the lens spectral analysis the sfc regularizer simple and efficient non adversarial fairness intervention and empirical showing that sfc achieves state the art fairness accuracy trade offs datasets,achieving fairness machine learning remains critical challenge due the opaque effects data augmentation input spaces within nonlinear neural networks response the concept the unfair region the latent space subspace that highlights areas where misclassification rates for certain demographic groups are disproportionately high leading unfair prediction address this adversarial latent feature augmentation alfa that leverages adversarial fairness attacks perturb latent space features which are then used data augmentation for fine tuning alfa intentionally shifts latent features into unfair regions and the last layer the network fine tuned these perturbed features leading corrected decision boundary that enhances fairness classification cost effective manner theoretical demonstrating that our adversarial fairness objective reliably generates biased feature perturbations and that fine tuning samples from these unfair regions ensures fairness improvements extensive experiments across diverse datasets modalities and backbone networks that training these adversarial features enhances fairness while maintaining predictive classification tasks,2025-08-26T00:48:30.451200
97,Training Data Attribution using Functional Gradient Hacking,"Understanding which training examples are most responsible for a model's prediction is crucial for debugging, fairness, and identifying dataset biases. Existing data attribution methods are often computationally expensive or rely on strong simplifying assumptions. We introduce Functional Gradient Hacking (FGH), a fast and accurate method for training data attribution. FGH is based on the insight that the influence of a training point can be approximated by measuring how much that point's functional gradient (the gradient of the loss with respect to the model's output logits) aligns with the direction of a parameter update designed to ""hack"" or change a specific test prediction. By framing attribution as a functional projection problem, we avoid the need to compute expensive inverse Hessian-vector products. Our contributions are: (1) a new, theoretically-grounded framework for data attribution; (2) the FGH algorithm, which is orders of magnitude faster than influence functions; and (3) extensive experiments demonstrating that FGH provides more faithful attributions for complex models like Transformers compared to prior art, successfully identifying mislabeled examples and sources of bias.",ICLR,deep learning,gemini-2.5-pro,True,7033,Dynamic Influence Tracker: Estimating Sample Influence in SGD-Trained Models across Arbitrary Time Windows,"Understanding how training samples affect models improves model interpretability, optimization strategies, and anomaly detection. However, existing methods for estimating sample influence provide only static assessments, rely on restrictive assumptions, and require high computational costs. 
	We propose Dynamic Influence Tracker (DIT), a novel method to estimate time-varying sample influence in models trained with Stochastic Gradient Descent (SGD). DIT enables fine-grained analysis of sample influence within arbitrary time windows during training through a two-phase algorithm. The training phase efficiently captures and stores necessary information about the SGD trajectory, while the inference phase computes the influence of samples on the model within a specified time window. We provide a theoretical error bound for our estimator without assuming convexity, showing its reliability across various learning scenarios. Our experimental results reveal the evolution of sample influence throughout the training process, enhancing understanding of learning dynamics. We show DIT's effectiveness in improving model performance through anomalous sample detection and its potential for advancing curriculum learning.",ICLR.cc/2025/Conference,5.0,False,0.8120,understanding which training examples are most responsible for model prediction crucial for debugging fairness and identifying biases fgh the insight that the influence training point can approximated measuring how much that point functional gradient the gradient the loss respect the model output logits aligns the direction parameter update designed hack change specific prediction,understanding how training samples affect models improves interpretability optimization strategies and anomaly detection provide theoretical error bound for our estimator assuming convexity showing its reliability across various learning scenarios our experimental reveal the evolution sample influence throughout the training process enhancing understanding learning dynamics dit effectiveness improving anomalous sample detection and its potential for advancing curriculum learning,2025-08-26T00:48:30.451201
98,Physics-Informed Neural Radiance Fields for Dynamic Scenes,"Neural Radiance Fields (NeRF) have revolutionized novel view synthesis for static scenes, but modeling dynamic, moving scenes remains a major challenge. We introduce Physics-Informed NeRF (PI-NeRF), a framework that integrates principles of classical mechanics directly into the representation of dynamic scenes. Instead of learning a simple deformation field, PI-NeRF models a scene as a collection of particles, each with a position, radiance, and density, governed by a learnable Neural Potential Energy Function. A Hamiltonian dynamics solver, implemented as a differentiable layer, evolves the state of these particles through time. This physics-based parameterization provides a strong inductive bias for realistic motion, allowing the model to generalize to unseen time steps and produce physically plausible interpolations. Our contributions are: (1) a novel representation for dynamic scenes grounded in Hamiltonian mechanics; (2) the integration of a differentiable physics solver into the NeRF framework; and (3) state-of-the-art results on novel view and novel time synthesis for complex, non-rigidly deforming scenes, demonstrating significantly improved temporal coherence and physical plausibility compared to existing dynamic NeRF models.",ICLR,deep learning,gemini-2.5-pro,True,10977,NeuGen: Amplifying the ‘Neural’ in Neural Radiance Fields for Domain Generalization,"Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures, capable of initiating training from scratch or fine-tuning pre-trained models, which cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows benchmarking performance on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.",ICLR.cc/2025/Conference,4.5,False,0.8827,neural radiance fields nerf have revolutionized view synthesis for static scenes but modeling dynamic moving scenes remains major challenge physics informed nerf nerf that integrates principles classical mechanics directly into the representation dynamic scenes instead learning simple deformation field nerf models scene collection particles each position radiance and density governed learnable neural potential energy function our contributions are representation for dynamic scenes grounded hamiltonian mechanics the integration differentiable physics solver into the nerf and state the art view and time synthesis for complex non rigidly deforming scenes demonstrating improved temporal coherence and physical plausibility compared existing dynamic nerf models,neural radiance fields nerf have advanced the field view synthesis yet their generalization across diverse scenes and conditions remains challenging addressing this the integration brain inspired normalization neural generalization neugen into leading nerf architectures which include mvsnerf and geonerf can seamlessly integrated into nerf architectures capable initiating training from scratch fine tuning pre trained models which cultivates comprehensive feature set that improves and robustness image rendering our exemplifies the potential merging neuroscientific principles deep learning frameworks setting precedent for enhanced generalizability and efficiency view synthesis,2025-08-26T00:48:30.451202
99,Equivariant Subgraph Neural Networks,"Graph Neural Networks (GNNs) typically operate on the node and edge level, struggling to capture higher-order structural motifs or subgraph patterns that are crucial for many graph-based tasks, such as molecular property prediction. We propose Equivariant Subgraph Neural Networks (ESNNs), a new class of GNNs that learn representations of subgraphs while respecting the symmetries of the graph. ESNNs first extract a basis set of computationally feasible subgraphs (e.g., all 3- and 4-node motifs) around each node. A neural network, designed to be equivariant to the permutation of nodes within each subgraph, then learns a representation for each subgraph instance. Finally, a permutation-invariant pooling layer aggregates these subgraph representations to produce the final node or graph embedding. Our contributions are: (1) a principled framework for incorporating subgraph information into GNNs; (2) a novel equivariant architecture for learning from sets of subgraphs; and (3) state-of-the-art results on several graph classification and regression benchmarks where higher-order structure is paramount. ESNNs provide a more powerful tool for learning from graphs by explicitly modeling representations at the subgraph level.",ICLR,deep learning,gemini-2.5-pro,True,6374,Improving Graph Neural Networks with Heterophily-based Filtration and Filtration Learning,"Graph neural networks (GNNs) are a powerful method of learning representations of graph-structured data. While they excel at learning class-discriminative representations of nodes in homophilous graphs, where connecting nodes tend to belong to the same class, many GNNs struggle with heterophilous graphs whose inter-class connections can muddy the message passing.  Inspired by this finding, we propose a topological filtration scheme, treating graphs as 1-dimensional simplicial complexes N  with a filter function based on estimated edge heterophily, and introduce two methodologies that use a backbone GNN to learn from the resulting graph filtration. The first trains a GNN on each graph in the filtration sequence consecutively for a portion of the total training time, using embeddings from previous graphs to initialize node embeddings in subsequent graphs. The second approach uses a novel message passing scheme to pass messages jointly within each and between graph levels in the filtration sequence with common nodes. Both methods enhance the influence of early birth adjacent nodes in homophilous subgraphs, yet allow for the model to learn from the full range of heterophilous and homophilous connections in the graph. We further extend our approach to learn a graph filtration sequence of graphs through a learnable node filter function. Experiments show that our heterophily-filtered GNNs achieve superior node classification accuracy on heterophilous and homophilous networks alike.",ICLR.cc/2025/Conference,2.0,nan,0.8785,graph neural networks gnns operate the node and edge level struggling capture higher order structural motifs subgraph patterns that are crucial for many graph based tasks such molecular property prediction equivariant subgraph neural networks esnns class gnns that learn representations subgraphs while respecting the symmetries the graph neural network designed equivariant the permutation nodes within each subgraph then learns representation for each subgraph instance finally permutation invariant pooling layer aggregates these subgraph representations produce the final node graph embedding our contributions are principled for incorporating subgraph information into gnns equivariant for learning from sets subgraphs and state the art several graph classification and regression benchmarks where higher order structure paramount esnns provide more powerful tool for learning from graphs explicitly modeling representations the subgraph level,graph neural networks gnns are powerful learning representations graph structured data while they excel learning class discriminative representations nodes homophilous graphs where connecting nodes tend belong the same class many gnns struggle heterophilous graphs whose inter class connections can muddy the message passing experiments that our heterophily filtered gnns achieve superior node classification heterophilous and homophilous networks alike,2025-08-26T00:48:30.451204
