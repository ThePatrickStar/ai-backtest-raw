ai_index,ai_title,ai_abstract,ai_conference,ai_domain,ai_model,match_found,human_index,human_title,human_abstract,human_venue,human_overall_score,human_accepted,similarity_score,ai_processed_text,human_processed_text,generated_at
0,Variance-Reduced Adaptive Gradient Descent with Optimal Nonconvex Rates,"Motivation: Adaptive gradient methods (Adam/Amsgrad) are widely used in deep learning for fast convergence but lack tight nonconvex complexity guarantees and can suffer from high stochastic variance. Bridging practical adaptivity with provable variance reduction remains an open challenge. Approach: We introduce VR-Adapt, a family of optimizers that combines per-coordinate adaptive learning rates with a variance-reduction control variate built on periodic snapshotting (SVRG-style) and a bias-corrected second-moment estimator. The algorithm interleaves inner adaptive updates with outer full-gradient or large-batch snapshots, and we design a novel Lyapunov potential that captures both adaptivity and variance. Contributions: (1) A unified algorithmic framework that preserves practical per-parameter scaling while decoupling variance via control variates. (2) Theoretical analysis establishing an O(ε^-3) stochastic gradient complexity to find an ε-stationary point for nonconvex smooth objectives, matching optimal variance-reduced SGD rates while retaining adaptivity. (3) Practical modifications for memory-efficient snapshotting and stable bias correction. Results: Empirically, VR-Adapt outperforms Adam, AdamW, and SVRG variants on transformer pretraining, ResNet image classification, and deep autoencoders under matched wall-clock budgets; it achieves faster loss decay and improved generalization with comparable hyperparameter sensitivity. Impact: VR-Adapt provides a principled path to deploy adaptive learning in settings where variance dominates, offering both rigorous guarantees and practical gains for large-scale nonconvex optimization in ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,6242,SGDF: A Method for Reducing Variance in Stochastic Gradient Descent via Filter Estimation,"In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used for optimization, but they typically suffer from slow convergence. Conversely, existing adaptive learning rate optimizers speed up convergence but often compromise generalization. To resolve this issue, we propose a novel optimization method designed to accelerate SGD's convergence without sacrificing generalization. Our approach reduces the variance of the historical gradient, improves first-order moment estimation of SGD by applying Wiener filter theory, and introduces a time-varying adaptive gain. Empirical results demonstrate that SGDF (SGD with Filter) effectively balances convergence and generalization compared to state-of-the-art optimizers.",ICLR.cc/2025/Conference,4.2,nan,0.8671,motivation adaptive gradient methods adam amsgrad are used deep learning for fast convergence but lack tight nonconvex complexity guarantees and can suffer from high stochastic variance adapt family optimizers that combines per coordinate adaptive learning rates variance reduction control variate built periodic snapshotting svrg style and bias corrected second moment estimator empirically adapt outperforms adam adamw and svrg variants transformer pretraining resnet image classification and deep autoencoders under matched wall clock budgets achieves faster loss decay and improved generalization comparable hyperparameter sensitivity impact adapt provides principled path deploy adaptive learning settings where variance dominates offering both rigorous guarantees and practical gains for large scale nonconvex optimization iclr scale models,deep learning stochastic gradient descent sgd and its momentum based variants are used for optimization but they suffer from slow convergence conversely existing adaptive learning rate optimizers speed convergence but often compromise generalization resolve this issue optimization designed accelerate sgd convergence sacrificing generalization,2025-08-26T00:35:16.354184
1,Differentiable Augmented Lagrangian Networks for Constrained Deep Learning,"Motivation: Modern deep learning applications increasingly require enforcing complex constraints (fairness, resource budgets, safety) during training. Existing constrained optimization approaches either sacrifice end-to-end differentiability or lack convergence guarantees when applied to nonconvex networks. Approach: We propose DAL-Net, a differentiable augmented Lagrangian framework that integrates a smooth penalty and multiplier update scheme into minibatch stochastic optimization while enabling implicit differentiation through multiplier trajectories. Our method leverages a proximal augmented Lagrangian step that admits a closed-form minibatch update and an implicit function theorem-based estimator to propagate gradients through the inner constrained solve. Contributions: (1) A stochastic augmented Lagrangian algorithm tailored for deep networks with provable subsequence convergence to KKT points under mild regularity and bounded variance assumptions. (2) A differentiable implicit-gradient estimator for multiplier and primal coupling that is memory-efficient and unbiased up to controllable approximation error. (3) Practical heuristics for constraint relaxation, adaptive penalty scheduling, and warm-starting that stabilize large-scale training. Results: DAL-Net reliably enforces equality and inequality constraints across tasks: fairness-aware classification, energy-constrained CNNs, and constrained reinforcement learning, outperforming penalty-only and projection baselines in constraint satisfaction and held-out accuracy. Impact: By marrying rigorous augmented Lagrangian theory with differentiable inner solves, DAL-Net enables principled constrained deep learning at scale, broadening the class of deployable safe and resource-aware models.",ICLR,optimization,gpt-5-mini,True,3724,Solving hidden monotone variational inequalities with surrogate losses,"Deep learning has proven to be effective in a wide variety of loss minimization problems.
However, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational inequality (VI) problem.
This difference in setting has caused many practical challenges as naive gradient-based approaches from supervised learning tend to diverge and cycle in the VI case.
In this work, we propose a principled surrogate-based approach compatible with deep learning to solve VIs.
We show that our surrogate-based approach has three main benefits: (1) under assumptions that are realistic in practice (when hidden monotone structure is present, interpolation, and sufficient optimization of the surrogates), it guarantees convergence, (2) it provides a unifying perspective of existing methods, and (3) is amenable to existing deep learning optimizers like ADAM.
Experimentally, we demonstrate our surrogate-based approach is effective in min-max optimization and minimizing projected Bellman error. Furthermore, in the deep reinforcement learning case, we propose a novel variant of TD(0) which is more compute and sample efficient.",ICLR.cc/2025/Conference,7.0,True,0.8258,motivation modern deep learning applications increasingly require enforcing complex constraints fairness resource budgets safety during training existing constrained optimization approaches either sacrifice end end differentiability lack convergence guarantees when applied nonconvex networks dal net differentiable augmented lagrangian that integrates smooth penalty and multiplier update scheme into minibatch stochastic optimization while enabling implicit differentiation multiplier trajectories contributions stochastic augmented lagrangian tailored for deep networks provable subsequence convergence kkt points under mild regularity and bounded variance assumptions dal net reliably enforces equality and inequality constraints across tasks fairness aware classification energy constrained cnns and constrained reinforcement learning outperforming penalty only and projection baselines constraint satisfaction and held out impact marrying rigorous augmented lagrangian theory differentiable inner solves dal net enables principled constrained deep learning scale broadening the class deployable safe and resource aware models,deep learning has proven effective wide variety loss minimization problems this difference setting has caused many practical challenges naive gradient based approaches from supervised learning tend diverge and cycle the case this principled surrogate based compatible deep learning solve vis that our surrogate based has three main benefits under assumptions that are realistic practice when hidden monotone structure interpolation and sufficient optimization the surrogates guarantees convergence provides unifying perspective existing methods and amenable existing deep learning optimizers like adam experimentally our surrogate based effective min max optimization and minimizing projected bellman error furthermore the deep reinforcement learning case variant which more compute and sample efficient,2025-08-26T00:35:16.354214
2,HeteroFed: Drift-Corrected Federated Optimization with Partial Participation Guarantees,"Motivation: Federated learning in realistic deployments faces both client heterogeneity and intermittent participation. Existing methods either rely on strong IID assumptions or degrade under client drift and partial participation, limiting convergence and personalization. Approach: We introduce HeteroFed, an optimization framework that explicitly models and corrects client drift via local drift estimators and a global correction term maintained at the server. The algorithm integrates proximal local objectives with a variance-reduced aggregation scheme and an adaptive participation-aware learning rate that compensates for sampling bias. We analyze dynamics using a two-timescale stochastic approximation that decouples drift correction and global model updates. Contributions: (1) A practical federated optimizer that combines per-client drift estimation, proximal personalization, and participation-aware aggregation. (2) Theoretical convergence to stationary points under heterogeneous objectives and arbitrary client sampling, with explicit rates showing improved dependence on client heterogeneity and participation frequency. (3) A personalization variant that provably interpolates between global convergence and per-client optimality. Results: HeteroFed achieves faster convergence and superior personalization versus FedAvg, SCAFFOLD, and FedProx on heterogeneous language modeling, medical imaging, and recommendation datasets with realistic client availability. Impact: HeteroFed provides robust, theoretically grounded federated optimization suitable for deployment in highly heterogeneous ecosystems, improving both global model utility and client-specific performance.",ICLR,optimization,gpt-5-mini,True,6341,Debiasing Federated Learning with Correlated Client Participation,"In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice.  Existing analyses of FedAvg usually assume the participating clients are independently sampled in each round from a uniform distribution, which does not reflect real-world scenarios. This paper introduces a theoretical framework that models client participation in FL as a Markov chain to study optimization convergence when clients have non-uniform and correlated participation across rounds. 
We apply this framework to analyze a more practical pattern: every client must wait a minimum number of $R$ rounds (minimum separation) before re-participating. We theoretically prove and empirically observe that increasing minimum separation reduces the bias induced by intrinsic non-uniformity of client availability in cross-device FL systems. 
Furthermore, we develop an effective debiasing algorithm for FedAvg that provably converges to the unbiased optimal solution under arbitrary minimum separation and unknown client availability distribution.",ICLR.cc/2025/Conference,6.75,True,0.8763,motivation federated learning realistic deployments faces both client heterogeneity and intermittent participation heterofed optimization that explicitly models and corrects client drift local drift estimators and global correction term maintained the server the integrates proximal local objectives variance reduced aggregation scheme and adaptive participation aware learning rate that compensates for sampling bias heterofed achieves faster convergence and superior personalization versus fedavg scaffold and fedprox heterogeneous language modeling medical imaging and recommendation datasets realistic client availability impact heterofed provides robust theoretically grounded federated optimization suitable for deployment highly heterogeneous ecosystems improving both global utility and client specific,cross device federated learning millions mobile clients only small subset clients participate training every communication round and federated averaging fedavg the most popular practice this introduces theoretical that models client participation markov chain optimization convergence when clients have non uniform and correlated participation across rounds,2025-08-26T00:35:16.354233
3,Online Lyapunov Tuning: A Principled Self-Tuning Momentum for Accelerated SGD,"Motivation: Momentum accelerates stochastic optimization but selecting its schedule is task-dependent; static choices often lead to instability or suboptimal speed. Automated momentum adaptation with theoretical guarantees is still lacking for nonconvex stochastic settings. Approach: We propose Online Lyapunov Tuning (OLT), an online controller that adjusts the momentum parameter by minimizing a surrogate Lyapunov decrease estimated from recent minibatch gradients. OLT formulates momentum selection as a small optimization problem per step with closed-form solutions under a quadratic local model, and we prove stability by constructing a composite Lyapunov function that couples momentum and learning rate dynamics. Contributions: (1) A self-tuning momentum algorithm that requires no manual scheduling and is compatible with common optimizers. (2) Rigorous analysis demonstrating non-asymptotic convergence to stationary points and an acceleration regime under locally quasi-convex curvature, with bounds on allowable estimator noise. (3) Implementation strategies that control estimator variance and computational overhead for deep networks. Results: Across convolutional nets, transformers, and GAN training, OLT consistently improves convergence speed and robustness to learning rate misspecification compared to fixed-momentum baselines and adaptive optimizers; in many cases, it achieves higher final accuracy with reduced tuning. Impact: OLT operationalizes Lyapunov-guided control for optimizer hyperparameters, reducing human tuning and increasing reliability of accelerated SGD in large-scale ICLR-style experiments.",ICLR,optimization,gpt-5-mini,True,2739,"HOME-3: HIGH-ORDER MOMENTUM ESTIMATOR USING THIRD-POWER GRADIENT FOR CONVEX, SMOOTH NONCONVEX, AND NONSMOOTH NONCONVEX OPTIMIZATION","Momentum-based gradients are critical for optimizing advanced machine learning models, as they not only accelerate convergence but also help gradient-based optimizers overcome stationary points. While most state-of-the-art momentum techniques rely on lower-power gradients, such as the squared first-order gradient, there has been limited exploration into the potential of higher-power gradients—those raised to powers greater than two, such as the third-power first-order gradient. In this work, we introduce the concept of high-order momentum, where
momentum is constructed using higher-power gradients, with a specific focus on the third-power first-order gradient as a representative example. Our research offers both theoretical and empirical evidence of the benefits of this novel approach. From a theoretical standpoint, we demonstrate that incorporating third-power gradients into momentum can improve the convergence bounds of gradient-based optimizers
for both convex and smooth nonconvex problems. To validate these findings, we conducted extensive empirical experiments across convex, smooth nonconvex, and nonsmooth nonconvex optimization tasks. The results consistently showcase that high-order momentum outperforms traditional momentum-based optimizers, providing superior performance and more efficient optimization.",ICLR.cc/2025/Conference,3.5,nan,0.8487,motivation momentum accelerates stochastic optimization but selecting its schedule task dependent static choices often lead instability suboptimal speed automated momentum adaptation theoretical guarantees still lacking for nonconvex stochastic settings olt formulates momentum selection small optimization problem per step closed form solutions under quadratic local and stability constructing composite lyapunov function that couples momentum and learning rate dynamics implementation strategies that control estimator variance and computational overhead for deep networks across convolutional nets transformers and gan training olt consistently improves convergence speed and robustness learning rate misspecification compared fixed momentum baselines and adaptive optimizers many cases achieves higher final reduced tuning,momentum based gradients are critical for optimizing advanced machine learning models they not only accelerate convergence but also help gradient based optimizers overcome stationary points these conducted extensive empirical experiments across convex smooth nonconvex and nonsmooth nonconvex optimization tasks the consistently showcase that high order momentum outperforms traditional momentum based optimizers providing superior and more efficient optimization,2025-08-26T00:35:16.354241
4,K-Fy: Kronecker-Factored Quasi-Newton Optimization with Global Convergence for Deep Networks,"Motivation: Second-order methods can dramatically speed optimization but are typically impractical for deep networks due to cost and lack of robust global convergence guarantees. Current Kronecker-factored approaches (K-FAC) accelerate training but rely on heuristics without quasi-Newton theoretical grounding. Approach: We present K-Fy, a Kronecker-factored quasi-Newton algorithm that constructs low-rank curvature approximations via layerwise Kronecker factors and enforces a trust-region style globalization employing a novel damping schedule derived from curvature trace statistics. The method estimates a quasi-Newton update using efficient Kronecker algebra and applies a provable backtracking acceptance test to ensure descent. Contributions: (1) A computationally efficient quasi-Newton methodology leveraging Kronecker structure with per-layer updates amenable to minibatch training. (2) A globalization mechanism and adaptive damping that yield provable convergence to stationary points for smooth nonconvex objectives. (3) Complexity bounds for per-iteration cost and overall gradient evaluations under mild stochastic noise. Results: K-Fy attains significant speedups over first-order optimizers and classical K-FAC in both convergence rate and wall-clock time on ImageNet-resized models, transformer finetuning, and large autoencoders, with increased stability across batch sizes. Impact: By fusing principled quasi-Newton theory with Kronecker efficiency, K-Fy makes scalable second-order optimization a practical alternative for large deep models at ICLR scale.",ICLR,optimization,gpt-5-mini,True,8080,AdaFisher: Adaptive Second Order Optimization via Fisher Information,"First-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD. However, their practicality in training DNNs is still limited due to increased per-iteration computations compared to the first-order methods. We present *AdaFisher*--an adaptive second-order optimizer that leverages a *diagonal block-Kronecker* approximation of the Fisher information matrix for adaptive gradient preconditioning. AdaFisher aims to bridge the gap between enhanced *convergence/generalization* capabilities and computational efficiency in second-order optimization framework for training DNNs. Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modeling and stands out for its stability and robustness in hyper-parameter tuning. We demonstrate that AdaFisher **outperforms the SOTA optimizers** in terms of both accuracy and convergence speed. Code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.",ICLR.cc/2025/Conference,6.25,True,0.8662,motivation second order methods can dramatically speed optimization but are impractical for deep networks due cost and lack robust global convergence guarantees attains significant speedups over first order optimizers and classical fac both convergence rate and wall clock time imagenet resized models transformer finetuning and large autoencoders increased stability across batch sizes impact fusing principled quasi newton theory kronecker efficiency makes scalable second order optimization practical alternative for large deep models iclr scale,first order optimization methods are currently the mainstream training deep neural networks dnns despite their widespread second order optimization algorithms exhibit superior convergence properties compared their first order counterparts adafisher aims bridge the gap between enhanced convergence generalization capabilities and computational efficiency second order optimization for training dnns despite the slow pace second order optimizers showcase that adafisher can reliably adopted for image classification language modeling and stands out for its stability and robustness hyper parameter tuning,2025-08-26T00:35:16.354252
5,Loss-Shape DRO: Tractable Distributionally Robust Optimization for Label-Noise Resilient Training,"Motivation: Label noise degrades model performance and generalization; robust training often uses heuristics or ad hoc regularizers lacking principled risk guarantees. Distributionally robust optimization (DRO) is promising but typically intractable for deep networks or yields overly conservative models. Approach: We introduce Loss-Shape DRO, a tractable DRO formulation that constrains perturbations in the empirical loss space via a geometry-aware Wasserstein ball, enabling closed-form duals that reduce to per-sample reweighting and a smooth loss-shaping transformation. We derive stochastic mirror-descent-style updates that efficiently implement the dual dynamics via minibatch reweighting and a smooth surrogate that mitigates variance. Contributions: (1) A novel DRO formulation tailored to classification/regression losses that balances robustness and tractability. (2) Dualization results yielding scalable training algorithms with provable robustness certificates against bounded label corruptions. (3) Generalization bounds showing tighter excess risk under label noise compared to standard ERM and previous DRO proxies. Results: On CIFAR with synthetic symmetric and asymmetric label noise, WebVision, and noisy medical labels, Loss-Shape DRO improves test accuracy and calibration versus robust loss baselines, label-cleaning methods, and classical DRO, while preserving training efficiency. Impact: Loss-Shape DRO provides a principled, scalable approach to noise-robust training that can be adopted in noisy-label settings commonly encountered in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,7746,Universal generalization guarantees for Wasserstein distributionally robust models,"Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.",ICLR.cc/2025/Conference,7.333333333333333,True,0.8754,distributionally robust optimization dro promising but intractable for deep networks yields overly conservative models contributions dro formulation tailored classification regression losses that balances robustness and tractability dualization yielding scalable training algorithms provable robustness certificates against bounded label corruptions,distributionally robust optimization has emerged attractive way train robust machine learning models capturing data uncertainty and distribution shifts contrast establish exact generalization guarantees that cover wide range cases arbitrary transport costs and parametric loss functions including deep learning objectives nonsmooth activations,2025-08-26T00:35:16.354259
6,Curriculum Bandits: Provable Curriculum Scheduling via Stochastic Bilevel Optimization,"Motivation: Curriculum learning—ordering training samples—can accelerate convergence and improve generalization, but designing curricula is typically heuristic and lacks theoretical support. Framing curriculum design as an optimization problem remains under-explored, especially with stochastic training dynamics. Approach: We cast curriculum scheduling as a stochastic bilevel optimization where an outer bandit-like scheduler chooses sampling distributions to minimize validation loss, and an inner learner optimizes model parameters on the selected curriculum. We develop Curriculum Bandits (CurBand), which combines Thompson-sampling-inspired exploration for the scheduler with a truncated implicit differentiation solver for the inner problem. Our analysis uses tools from stochastic bilevel optimization and bandit regret to quantify the trade-off between exploration of curricula and exploitation of promising schedules. Contributions: (1) A formal bilevel curriculum framework that captures dynamic, data-dependent sampling policies. (2) CurBand, an algorithm with provable sublinear regret in scheduler decisions and convergence of inner parameters to stationary points. (3) New theoretical bounds linking scheduler regret to final validation performance. Results: CurBand yields faster convergence and better final validation accuracy than random sampling, loss-based sampling, and learned-scorer baselines on image classification, curriculum for RL exploration, and data-augmentation scheduling tasks. Impact: CurBand establishes a theoretically grounded and practical pathway to automate curriculum design, offering reliable speedups and improved generalization for complex ICLR workloads.",ICLR,optimization,gpt-5-mini,True,7958,Optimal Strong Regret and Violation in Constrained MDPs via Policy Optimization,"We study online learning in constrained MDPs (CMDPs), focusing on the goal of attaining sublinear strong regret and strong cumulative constraint violation. Differently from their standard (weak) counterparts, these metrics do not allow negative terms to compensate positive ones, raising considerable additional challenges. Efroni et al. (2020) were the first to propose an algorithm with sublinear strong regret and strong violation, by exploiting linear programming. Thus, their algorithm is highly inefficient, leaving as an open problem achieving sublinear bounds by means of policy optimization methods, which are much more efficient in practice. Very recently, Muller et al. (2024) have partially addressed this problem by proposing a policy optimization method that allows to attain $\widetilde{\mathcal{O}}(T^{0.93})$ strong regret/violation. This still leaves open the question of whether optimal bounds are achievable by using an approach of this kind. We answer such a question affirmatively, by providing an efficient policy optimization algorithm with $\widetilde{\mathcal{O}}(\sqrt{T})$ strong regret/violation. Our algorithm implements a primal-dual scheme that employs a state-of-the-art policy optimization approach for adversarial (unconstrained) MDPs as primal algorithm, and a UCB-like update for dual variables.",ICLR.cc/2025/Conference,6.0,True,0.8038,framing curriculum optimization problem remains under explored stochastic training dynamics cast curriculum scheduling stochastic bilevel optimization where outer bandit like scheduler chooses sampling distributions minimize validation loss and inner learner optimizes parameters the selected curriculum our analysis uses tools from stochastic bilevel optimization and bandit regret quantify the trade off between exploration curricula and exploitation promising schedules,online learning constrained mdps cmdps focusing the goal attaining sublinear strong regret and strong cumulative constraint violation thus their highly inefficient leaving open problem achieving sublinear bounds means policy optimization methods which are much more efficient practice have partially addressed this problem proposing policy optimization that allows attain widetilde mathcal strong regret violation answer such question affirmatively providing efficient policy optimization widetilde mathcal sqrt strong regret violation our implements primal dual scheme that employs state the art policy optimization for adversarial unconstrained mdps primal and ucb like update for dual variables,2025-08-26T00:35:16.354266
7,ADAPT-Comp: Adaptive Quantization with Error Compensation for Communication-Efficient SGD,"Motivation: Communication bottlenecks limit distributed training; quantization with error compensation reduces costs but commonly uses fixed precision and lacks adaptive control that matches dynamics of optimization, leading to suboptimal trade-offs. Approach: We propose ADAPT-Comp, an adaptive quantization framework that allocates bits per-coordinate using online variance estimates and couples this with error-feedback compensation proven to eliminate bias. ADAPT-Comp dynamically adjusts quantization granularity to gradient statistics, minimizing communication subject to a target optimization error. We derive tight convergence guarantees by modeling quantization noise as controlled additive perturbations and proving that adaptive schemes preserve linear speedup in the number of workers when compensation is employed. Contributions: (1) An adaptive quantization algorithm with per-coordinate bit allocation rules based on online second-moment estimators. (2) A unified theoretical analysis showing compensated adaptive quantization attains O(1/√T) convergence for smooth nonconvex objectives with explicit dependence on communication budget. (3) Implementation recipes that incur negligible overhead and integrate with synchronized and asynchronous SGD. Results: ADAPT-Comp reduces communicated bits by up to 20× while matching accuracy of 32-bit baselines across transformer pretraining, distributed ResNet training, and large-scale recommendation models; it outperforms fixed-bit compressors and previous error-compensated schemes. Impact: ADAPT-Comp enables more efficient distributed training with rigorous guarantees, facilitating scalable experimentation and deployment of large ICLR-scale models under tight communication constraints.",ICLR,optimization,gpt-5-mini,True,7095,Test-time Adaptation for Image Compression with Distribution Regularization,"Current test- or compression-time adaptation image compression (TTA-IC) approaches, which leverage both latent and decoder refinements as a two-step adaptation scheme, have potentially enhanced the rate-distortion (R-D) performance of learned image compression models on cross-domain compression tasks, \textit{e.g.,} from natural to screen content images.  However, compared with the emergence of various decoder refinement variants, the latent refinement, as an inseparable ingredient, is barely
 tailored to cross-domain scenarios. To this end, we are interested in developing an advanced latent refinement method by extending the effective hybrid latent refinement (HLR) method, which is designed for \textit{in-domain} inference improvement but shows noticeable degradation of the rate cost in \textit{cross-domain} tasks. Specifically, we first provide theoretical analyses, in a cue of marginalization approximation from in- to cross-domain scenarios,  to uncover that the vanilla HLR suffers from an underlying mismatch between refined Gaussian conditional and hyperprior distributions, leading to deteriorated joint probability approximation of marginal distribution with increased rate consumption. To remedy this issue, we introduce a simple Bayesian approximation-endowed \textit{distribution regularization} to encourage learning a better joint probability approximation in a plug-and-play manner. Extensive experiments on six in- and cross-domain datasets demonstrate that our proposed method not only improves the R-D performance compared with other latent refinement counterparts, but also can be flexibly integrated into existing TTA-IC methods with incremental benefits.",ICLR.cc/2025/Conference,6.0,True,0.8426,adapt comp dynamically adjusts quantization granularity gradient statistics minimizing communication subject target optimization error adapt comp reduces communicated bits while matching bit baselines across transformer pretraining distributed resnet training and large scale recommendation models outperforms fixed bit compressors and previous error compensated schemes,current compression time adaptation image compression tta approaches which leverage both latent and decoder refinements two step adaptation scheme have potentially enhanced the rate distortion learned image compression models cross domain compression tasks textit remedy this issue simple bayesian approximation endowed textit distribution regularization encourage learning better joint probability approximation plug and play manner,2025-08-26T00:35:16.354276
8,SaddleSurf: Adaptive Negative-Curvature Exploitation for Fast Nonconvex Escapes,"Motivation: Nonconvex optimization in deep learning is dominated by saddle points whose slow escape often limits convergence speed and solution quality. Existing escape techniques either rely on isotropic perturbations or expensive eigenvector computations, leading to unstable performance or high cost. Approach: We introduce SaddleSurf, an adaptive algorithm that detects negative curvature using low-cost stochastic directional probes and performs curvature-aware steps that combine subspace-aware cubic regularization with variance-reduced gradient information. The method adaptively chooses when to exploit negative curvature versus following gradient descent, guided by a local curvature-confidence statistic computed from minibatch Hessian-vector products. Contributions: (1) A principled negative-curvature exploitation strategy that balances descent and curvature-driven escapes with provable complexity. (2) A stochastic detection mechanism that uses O(1) Hessian-vector products per probe and an adaptive cubic step admitting closed-form trust-region-like control. (3) Nonasymptotic guarantees: under standard smoothness and bounded-variance assumptions, SaddleSurf finds an (ε,γ)-approximate second-order stationary point using O(ε^-2 + γ^-3) stochastic gradient/Hessian-vector work, improving constants over prior approaches. Results: Empirically, SaddleSurf accelerates convergence and attains lower final loss versus SGD with isotropic noise, perturbed gradient methods, and cubic-regularized baselines on deep autoencoders, ResNets, and transformer fine-tuning, with modest extra per-step cost. Impact: SaddleSurf offers a practical, theoretically grounded mechanism to escape saddles efficiently in large-scale nonconvex training, enabling faster and more reliable optimization for ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,9725,Type-II Saddles and Probabilistic Stability of Stochastic Gradient Descent,"Characterizing and understanding the dynamics of stochastic gradient descent (SGD) around saddle points remains an open problem in neural network optimization. We identify two distinct types of saddle points, demonstrating that Type-II saddles pose a significant challenge due to vanishing gradient noise, which makes them particularly difficult for SGD to escape. We show that the dynamics around these saddles can be effectively modeled by a random matrix product process, allowing us to apply concepts from probabilistic stability and Lyapunov exponents. By leveraging ergodic theory, we establish that saddle points can be either attractive or repulsive for SGD, leading to a classification of four distinct dynamic phases based on the gradient's signal-to-noise ratio near the saddle. We apply the theory to the training at the initial stage of neural networks, explaining an intriguing phenomenon that neural networks are prone to be stuck at the initialization point at a larger learning rate. Our results offer a novel theoretical framework for understanding the intricate behavior of SGD around saddle points, with implications for improving optimization strategies in deep learning.",ICLR.cc/2025/Conference,4.75,False,0.8512,motivation nonconvex optimization deep learning dominated saddle points whose slow escape often limits convergence speed and solution quality stochastic detection mechanism that uses hessian vector products per probe and adaptive cubic step admitting closed form trust region like control empirically saddlesurf accelerates convergence and attains lower final loss versus sgd isotropic noise perturbed gradient methods and cubic regularized baselines deep autoencoders resnets and transformer fine tuning modest extra per step cost impact saddlesurf offers practical theoretically grounded mechanism escape saddles large scale nonconvex training enabling faster and more reliable optimization for iclr scale models,characterizing and understanding the dynamics stochastic gradient descent sgd around saddle points remains open problem neural network optimization leveraging ergodic theory establish that saddle points can either attractive repulsive for sgd leading classification four distinct dynamic phases the gradient signal noise ratio near the saddle apply the theory the training the initial stage neural networks explaining intriguing phenomenon that neural networks are prone stuck the initialization point larger learning rate our offer theoretical for understanding the intricate behavior sgd around saddle points implications for improving optimization strategies deep learning,2025-08-26T00:35:16.354287
9,HyperGrad Flow: Continuous-Time Implicit Hyperparameter Optimization with Convergence Guarantees,"Motivation: Hyperparameter tuning for large neural networks is costly and often decoupled from training dynamics, limiting reproducibility and adaptivity. Implicit differentiation enables efficient hypergradient computation but can suffer from instability and high memory use in long horizons. Approach: We propose HyperGrad Flow, a continuous-time formulation of bilevel hyperparameter optimization where inner training dynamics are modeled by a controlled ODE. We derive an implicit function theorem in infinite-dimensional trajectory space to compute hypergradients via adjoint sensitivity with checkpointed reversible integration. We introduce stabilized discretizations and a residual-corrected backward pass that reduce bias and numerical instability. Contributions: (1) A continuous-time bilevel framework that yields hypergradients with provable consistency as discretization refines. (2) A numerically stable adjoint estimator using residual correction and reversible integrators that lowers memory footprint and error accumulation. (3) Convergence theory showing that hyperparameter updates obtained via our estimator converge to stationary points of the bilevel objective under mild smoothness and ergodicity assumptions. Results: HyperGrad Flow speeds up tuning of learning rates, weight decay schedules, and data-augmentation magnitudes on CNNs and transformers, outperforming truncated backpropagation and implicit differentiation baselines in validation improvement per compute. Impact: By marrying continuous-time analysis with practical adjoint estimators, HyperGrad Flow provides a scalable, principled pathway for online hyperparameter optimization in large-scale deep learning.",ICLR,optimization,gpt-5-mini,True,168,Provable Data-driven Hyperparameter Tuning for Deep Neural Networks,"Modern machine learning algorithms, especially deep learning-based techniques, typically involve careful
hyperparameter tuning to achieve the best performance. Despite the surge of intense interest in practical
techniques like Bayesian optimization and random search-based approaches to automating this laborious and
compute-intensive task, the fundamental learning-theoretic complexity of tuning hyperparameters for deep
neural networks is poorly understood. Inspired by this glaring gap, we initiate the formal study of hyperparameter tuning complexity in deep learning through a recently introduced lens of data-driven algorithm design. We assume that we have a series of deep learning tasks, and we have to tune hyperparameters to do well on average over the distribution of
tasks. A major difficulty is that the loss as a function of the hyperparameter is very volatile and furthermore,
it is given implicitly by an optimization problem over the model parameters. This is unlike previous work
in data-driven design, where one can typically explicitly model the algorithmic behavior as a function of
the hyperparameters. To tackle this we introduce a new technique to characterize the discontinuities and 
oscillations of the loss function on any fixed problem instance as we vary the hyperparameter; our analysis 
relies on subtle concepts including tools from differential geometry and constrained optimization. This can be 
used to show that the intrinsic complexity of the corresponding family of loss functions is bounded. We instantiate 
our results and provide the first precise sample complexity bounds for concrete applications—tuning a hyperparameter that interpolates neural activation functions and setting the kernel 
parameter in graph neural networks.",ICLR.cc/2025/Conference,5.666666666666667,False,0.8568,motivation hyperparameter tuning for large neural networks costly and often decoupled from training dynamics limiting reproducibility and adaptivity hypergrad flow continuous time formulation bilevel hyperparameter optimization where inner training dynamics are modeled controlled ode hypergrad flow speeds tuning learning rates weight decay schedules and data augmentation magnitudes cnns and transformers outperforming truncated backpropagation and implicit differentiation baselines validation improvement per compute impact marrying continuous time analysis practical adjoint estimators hypergrad flow provides scalable principled pathway for online hyperparameter optimization large scale deep learning,modern machine learning algorithms deep learning based techniques involve careful hyperparameter tuning achieve the best despite the surge intense interest practical techniques like bayesian optimization and random search based approaches automating this laborious and compute intensive task the fundamental learning theoretic complexity tuning hyperparameters for deep neural networks poorly understood inspired this glaring gap initiate the formal hyperparameter tuning complexity deep learning recently introduced lens data driven assume that have series deep learning tasks and have tune hyperparameters well average over the distribution tasks major difficulty that the loss function the hyperparameter very volatile and furthermore given implicitly optimization problem over the parameters tackle this characterize the discontinuities and oscillations the loss function any fixed problem instance vary the hyperparameter our analysis relies subtle concepts including tools from differential geometry and constrained optimization instantiate our and provide the first precise sample complexity bounds for concrete applications tuning hyperparameter that interpolates neural activation functions and setting the kernel parameter graph neural networks,2025-08-26T00:35:16.354298
10,PrivSAGA: Differentially Private Variance-Reduced Optimization with Tight Utility-Privacy Trade-offs,"Motivation: Differential privacy (DP) is essential for training models on sensitive data, yet DP-SGD suffers from large utility loss and slow convergence. Variance-reduction methods (e.g., SAGA) accelerate optimization but their interplay with privacy mechanisms is poorly understood. Approach: We design PrivSAGA, a differentially private variance-reduced algorithm that integrates per-sample gradient memorization with noise-calibrated update aggregation. Privacy is enforced via a novel clustered Gaussian mechanism that leverages variance-reduction statistics to inject minimal noise while satisfying (ε,δ)-DP. We develop privacy accounting that tightly tracks sensitivity reductions afforded by control variates and prove utility bounds reflecting both variance reduction and privacy noise. Contributions: (1) An algorithmic scheme combining SAGA-style control variates with a clustered noise mechanism that reduces required perturbation magnitude. (2) A refined sensitivity analysis and advanced composition proof yielding improved (ε,δ)-DP guarantees for variance-reduced methods. (3) Nonasymptotic utility bounds demonstrating that PrivSAGA attains faster convergence rates than DP-SGD under the same privacy budget. Results: On logistic regression, image classification, and clinical prediction tasks, PrivSAGA outperforms DP-SGD and private momentum baselines, achieving higher accuracy at strong privacy settings (ε ≤ 1) and reducing privacy-induced accuracy degradation by 20–40%. Impact: PrivSAGA narrows the gap between private and non-private training by exploiting variance reduction to reduce noise injection, enabling practical privacy-preserving optimization for sensitive ICLR-scale applications.",ICLR,optimization,gpt-5-mini,True,9711,Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model,"Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we focus on a threat model where the adversary has access only to the final model, with no visibility into intermediate updates. In the literature, this ``hidden state'' threat model exhibits a significant gap between the lower bound from empirical privacy auditing and the theoretical upper bound provided by privacy accounting. To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence designed to maximize the privacy loss of the final model without relying on intermediate updates. Our experiments show that this approach consistently outperforms previous attempts at auditing the hidden state model. Furthermore, our results advance the understanding of achievable privacy guarantees within this threat model. Specifically, when the crafted gradient is inserted at every optimization step, we show that concealing the intermediate model updates in DP-SGD does not enhance the privacy guarantees. The situation is more complex when the crafted gradient is not inserted at every step: our auditing lower bound matches the privacy upper bound only for an adversarially-chosen loss landscape and a sufficiently large batch size. This suggests that existing privacy upper bounds can be improved in certain regimes.",ICLR.cc/2025/Conference,6.5,True,0.8344,saga accelerate optimization but their interplay privacy mechanisms poorly understood logistic regression image classification and clinical prediction tasks privsaga outperforms sgd and private momentum baselines achieving higher strong privacy settings and reducing privacy induced degradation impact privsaga narrows the gap between private and non private training exploiting variance reduction reduce noise injection enabling practical privacy preserving optimization for sensitive iclr scale applications,machine learning models can trained formal privacy guarantees differentially private optimizers such sgd when the crafted gradient inserted every optimization step that concealing the intermediate updates sgd does not enhance the privacy guarantees,2025-08-26T00:35:16.354302
11,Spectral Sharpness Regularization: A Hessian-Spectrum Approach to Generalization-Aware Optimization,"Motivation: Sharp minima correlate with poor generalization, yet most sharpness-aware methods rely on local perturbations that are heuristic and expensive. Direct control of Hessian spectra during training could improve generalization but is challenging to estimate and regularize at scale. Approach: We introduce Spectral Sharpness Regularization (SSR), a scalable regularizer that targets dominant Hessian eigenvalues via stochastic Lanczos approximations and efficient spectral penalties. SSR applies a differentiable spectral shaping term to the loss, penalizing the top-k eigenvalues through trace-penalty surrogates computable with a few Hessian-vector products per minibatch. We couple SSR with adaptive damping to preserve optimization stability. Contributions: (1) A principled spectral penalty that directly controls sharp directions and admits unbiased stochastic estimates. (2) An efficient implementation using randomized Lanczos and Hutchinson estimators with theoretical bounds on estimator variance and bias. (3) Convergence analysis showing that SSR-equipped SGD converges to stationary points while reducing top-spectrum mass. Results: SSR consistently improves test performance and calibration versus SAM, mixup, and weight decay across CIFAR, ImageNet-pruned networks, and transformer finetuning, particularly under label noise and small-batch regimes. Ablations demonstrate direct correlation between reduced top eigenvalues and generalization gains. Impact: SSR offers a tractable, theory-backed avenue to shape loss landscapes for better generalization, presenting a new class of optimization-aware regularizers for deep learning.",ICLR,optimization,gpt-5-mini,True,4059,Meta-Learning for Dynamic Synaptic Plasticity in Spiking Neural Networks,"Adaptive optimization algorithms, such as Adam Kingma & Ba (2015) and RM-SProp Tieleman & Hinton (2012), have become integral to training deep neu-ral networks, yet their stability properties and impact on generalization remain poorly understood Wilson et al. (2017). This paper extends linear stability anal-ysis to adaptive optimizers, providing a theoretical framework that explains their behavior in relation to loss surface geometry Wu et al. (2022); Jastrz˛ebski et al.(2019). We introduce a novel generalized coherence measure that quantifies the interaction between the adaptive preconditioner and the Hessian of the loss func-tion. This measure yields necessary and sufficient conditions for linear stability near stationary points, offering insights into why adaptive methods may converge to sharper minima with poorer generalization.
Our analysis leads to practical guidelines for hyperparameter tuning, demon-strating how to improve the generalization performance of adaptive optimizers. Through extensive experiments on benchmark datasets and architectures, includ-ing ResNet He et al. (2016) and Vision Transformers Dosovitskiy et al. (2020), we validate our theoretical predictions, showing that aligning the adaptive precon-ditioner with the loss surface geometry through careful parameter selection can narrow the generalization gap between adaptive methods and SGD Loshchilov & Hutter (2018).",ICLR.cc/2025/Conference,4.2,False,0.8241,couple ssr adaptive damping preserve optimization stability ssr consistently improves and calibration versus sam mixup and weight decay across cifar imagenet pruned networks and transformer finetuning under label noise and small batch regimes impact ssr offers tractable theory backed avenue shape loss landscapes for better generalization presenting class optimization aware regularizers for deep learning,adaptive optimization algorithms such adam kingma and sprop tieleman hinton have become integral training deep neu ral networks yet their stability properties and impact generalization remain poorly understood wilson and vision transformers dosovitskiy,2025-08-26T00:35:16.354314
12,Riemannian Block Coordinate Descent for Low-Rank Deep Models with Global Convergence,"Motivation: Low-rank parameterizations reduce model size and computation, but optimizing factors constrained on manifolds (Stiefel, fixed-rank) is challenging at deep learning scale. Existing Riemannian methods are costly or lack scalability. Approach: We propose RBCD-LR, a Riemannian block coordinate descent algorithm that alternates efficient manifold-aware updates across factor blocks using retraction-based proximal steps and variance-reduced stochastic gradients. Each block update solves a compact subproblem on the corresponding manifold using closed-form Cayley or QR retractions, enabling minibatch compatibility and low memory overhead. We establish a global convergence theory by integrating stochastic block-coordinate descent analysis with Riemannian geometry tools. Contributions: (1) A scalable Riemannian block-coordinate framework tailored to low-rank deep models, supporting constraints like fixed-rank, orthogonality, and SPD factors. (2) Theoretical guarantees: almost-sure convergence to stationary points and explicit rates under Lipschitz and bounded variance assumptions. (3) Practical techniques for warm-starting, adaptive block scheduling, and maintaining numerical stability. Results: RBCD-LR trains low-rank ResNets, attention layers, and matrix-factorized embeddings with comparable accuracy to dense models while reducing parameters and FLOPs, outperforming naive factorized SGD and full-manifold gradient methods in wall-clock time. Impact: RBCD-LR enables principled, scalable manifold-constrained optimization for compact deep models, facilitating deployment in resource-limited settings without sacrificing theoretical rigor.",ICLR,optimization,gpt-5-mini,True,3399,Efficient Learning with Sine-Activated Low-Rank Matrices,"Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.",ICLR.cc/2025/Conference,7.0,True,0.8352,motivation low rank parameterizations reduce size and computation but optimizing factors constrained manifolds stiefel fixed rank challenging deep learning scale contributions scalable riemannian block coordinate tailored low rank deep models supporting constraints like fixed rank orthogonality and spd factors rbcd trains low rank resnets attention layers and matrix factorized embeddings comparable dense models while reducing parameters and flops outperforming naive factorized sgd and full manifold gradient methods wall clock time impact rbcd enables principled scalable manifold constrained optimization for compact deep models facilitating deployment resource limited settings sacrificing theoretical rigor,low rank decomposition has emerged vital tool for enhancing parameter efficiency neural network architectures gaining traction across diverse applications machine learning our proves plug enhancement for existing low rank models evidenced its successful application vision transformers vit large language models llms neural radiance fields nerf and shape modelling,2025-08-26T00:35:16.354321
13,MetaOptNet: Provable Meta-Learning via Online Convexification and Task-Aware Regularization,"Motivation: Meta-learning aims to rapidly adapt to new tasks, but theoretical understanding of generalization and convergence in nonconvex meta-training is limited. Empirical methods often lack guarantees and can overfit meta-training tasks. Approach: We introduce MetaOptNet, a meta-learning optimization framework that convexifies inner-task objectives via local variational quadratic models and incorporates task-aware regularizers to stabilize meta-updates. The inner loop solves a convex surrogate efficiently, while the outer loop employs stochastic mirror descent with adaptive meta-regularization informed by task heterogeneity statistics. We provide a unified analysis combining online convex optimization and bilevel stability to obtain meta-convergence results in nonconvex settings. Contributions: (1) A practical meta-training algorithm that replaces inner nonconvex solves with certified convex surrogates to improve stability. (2) Theoretical results: bounds on meta-regret and generalization transfer that scale with task heterogeneity and surrogate fidelity. (3) Empirical design for few-shot classification, reinforcement learning adaptation, and continual learning with reduced catastrophic forgetting. Results: MetaOptNet achieves faster adaptation and improved out-of-distribution generalization versus MAML, Reptile, and recent probabilistic meta-learners on standard few-shot benchmarks and RL adaptation tasks, with clearer robustness to meta-overfitting. Impact: By blending convexification with task-aware regularization and rigorous analysis, MetaOptNet advances provable, scalable meta-learning suitable for ICLR research.",ICLR,optimization,gpt-5-mini,True,7971,ConML: A Universal Meta-Learning Framework with Task-Level Contrastive Learning,"Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. To emulate this human-like rapid learning and enhance alignment and discrimination abilities, we propose ConML, a universal meta-learning framework that can be applied to various meta-learning algorithms without relying on specific model architectures nor target models. The core of ConML is task-level contrastive learning, which extends contrastive learning from the representation space in unsupervised learning to the model space in meta-learning. By leveraging task identity as an additional supervision signal during meta-training, we contrast the outputs of the meta-learner in the model space, minimizing inner-task distance (between models trained on different subsets of the same task) and maximizing inter-task distance (between models from different tasks). We demonstrate that ConML integrates seamlessly with optimization-based, metric-based, and amortization-based meta-learning algorithms, as well as in-context learning, resulting in performance improvements across diverse few-shot learning tasks.",ICLR.cc/2025/Conference,4.0,False,0.8643,metaoptnet meta learning optimization that convexifies inner task objectives local variational quadratic models and incorporates task aware regularizers stabilize meta updates provide unified analysis combining online convex optimization and bilevel stability obtain meta convergence nonconvex settings theoretical bounds meta regret and generalization transfer that scale task heterogeneity and surrogate fidelity empirical for few shot classification reinforcement learning adaptation and continual learning reduced catastrophic forgetting metaoptnet achieves faster adaptation and improved out distribution generalization versus maml reptile and recent probabilistic meta learners standard few shot benchmarks and adaptation tasks clearer robustness meta overfitting,meta learning enables learning systems adapt quickly tasks similar humans emulate this human like rapid learning and enhance alignment and discrimination abilities conml universal meta learning that can applied various meta learning algorithms relying specific architectures nor target models the core conml task level contrastive learning which extends contrastive learning from the representation space unsupervised learning the space meta learning that conml integrates seamlessly optimization based metric based and amortization based meta learning algorithms well context learning resulting improvements across diverse few shot learning tasks,2025-08-26T00:35:16.354324
14,Compositional Stochastic Optimization with Biased Oracles: Theory and Practical Algorithms,"Motivation: Many modern learning objectives are compositional, involving nested expectations (e.g., risk-sensitive learning, reinforcement learning, and certain robust objectives). Practical oracles (e.g., simulators, truncated rollouts) produce biased estimates, breaking convergence of standard compositional solvers. Approach: We present Biased-Coco, a compositional optimization framework that explicitly models oracle bias and variance. Our algorithm employs corrected control variates and an adaptive debiasing schedule that gradually reduces dependence on biased oracles while leveraging cheap biased queries for acceleration. We derive a novel Lyapunov analysis that quantifies the interplay between bias, variance, and step-sizes, and propose bias-aware momentum to mitigate error accumulation. Contributions: (1) A principled model for biased compositional oracles and an algorithm that provably converges under bounded bias drift. (2) Nonasymptotic complexity bounds that separate contributions of oracle bias and stochastic variance, offering trade-off prescriptions for practical sampling. (3) Implementation strategies for RL policy evaluation, risk-averse optimization, and nested estimation with truncated simulators. Results: Biased-Coco outperforms unbiased-heavy baselines and naive biased methods in sample efficiency and final performance across portfolio optimization, policy-gradient RL with truncated rollouts, and distributional robustness tasks. Impact: By providing theory and algorithms that tolerate realistic oracle bias, Biased-Coco broadens the applicability of compositional optimization to practical, resource-constrained ICLR problems.",ICLR,optimization,gpt-5-mini,True,11510,Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding,"Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we propose a neat weight embedding method to learn weight-specific representations, which captures weight-instance interaction for the subproblems and was overlooked by most current methods. We demonstrate the potentials of our method in two instantiations. First, we introduce a succinct addition model to learn weight-specific node embeddings, which surpassed most existing neural methods. Second, we design an enhanced conditional attention model to simultaneously learn the weight embedding and node embeddings, which yielded new state-of-the-art performance. Experimental results on classic MOCO problems verified the superiority of our method. Remarkably, our method also exhibits favorable generalization performance across problem sizes, even outperforming the neural method specialized for boosting size generalization.",ICLR.cc/2025/Conference,7.0,True,0.8085,motivation many modern learning objectives are compositional involving nested expectations risk sensitive learning reinforcement learning and certain robust objectives biased coco compositional optimization that explicitly models oracle bias and variance biased coco outperforms unbiased heavy baselines and naive biased methods sample efficiency and final across portfolio optimization policy gradient truncated rollouts and distributional robustness tasks impact providing theory and algorithms that tolerate realistic oracle bias biased coco broadens the applicability compositional optimization practical resource constrained iclr problems,recent decomposition based neural multi objective combinatorial optimization moco methods struggle achieve desirable even equipped complex learning techniques they often suffer from significant optimality gaps weight specific subproblems address this challenge neat weight embedding learn weight specific representations which captures weight instance interaction for the subproblems and was overlooked most current methods first succinct addition learn weight specific node embeddings which surpassed most existing neural methods second enhanced conditional attention simultaneously learn the weight embedding and node embeddings which yielded state the art remarkably our also exhibits favorable generalization across problem sizes even outperforming the neural specialized for boosting size generalization,2025-08-26T00:35:16.354330
15,Sparse-SGD+: Probabilistic Gradient Sparsification with Error-Controlled Convergence,"Motivation: Gradient sparsification reduces communication and computation but often sacrifices convergence guarantees or requires high variance in updates. Deterministic top-k schemes are biased and expensive to coordinate in distributed settings. Approach: We introduce Sparse-SGD+, a probabilistic sparsification method that selects coordinates according to adaptive importance sampling and applies error-compensated updates with provable bias control. Selection probabilities are derived from online second-moment estimates and curvature proxies, balancing variance and sparsity. We analyze the combined effect of probabilistic sampling and error feedback through a martingale decomposition and show that controlled randomness yields unbiased compressed gradients in expectation. Contributions: (1) An adaptive importance-sampling sparsifier with per-coordinate probability rules supporting target communication budgets. (2) A convergence theorem proving O(1/√T) rates for nonconvex smooth objectives with explicit dependence on sparsity and estimator variance, and a bias-variance decomposition that guides parameter tuning. (3) Practical distributed implementations that avoid global sorting and synchronize efficiently. Results: Sparse-SGD+ achieves up to 15× communication reduction while matching full-gradient baselines on transformer pretraining, distributed image classification, and recommendation systems, outperforming top-k and fixed-random sparsifiers in stability and final accuracy. Impact: Sparse-SGD+ provides a theoretically grounded, practical approach to gradient sparsification that preserves convergence and enables scalable distributed training for large ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,6999,Mitigating Gradient Interference for Efficient Sparse Fine-Tuning of Large Language Models,"Large Language Model (LLM) sparsification plays a crucial role in model compression. 
Among various methods, training-free approaches are highly efficient but often result in accuracy loss, while full fine-tuning requires substantial computational resources. 
Recent works have begun exploring sparse Parameter-Efficient Fine-Tuning (PEFT) methods, but lack theoretical guidance.
This study presents the first comprehensive theoretical framework for efficient sparse fine-tuning, addressing a critical gap in the literature. 
Specifically, we identify gradient conflict as the primary issue in PEFT sparse methods, wherein masked pretrained weights and corresponding PEFT weights exhibit competing optimization objectives during fine-tuning, potentially compromising model performance.
We theoretically model this phenomenon and identify three key factors influencing the efficacy of fine-tuning in sparsified LLMs: (1) error introduced by weight norms, (2) error composition from PEFT structures, and (3) error accumulation during fine-tuning.
Leveraging these theoretical insights, we propose a novel iterative sparse fine-tuning scheme that systematically addresses each identified factor. 
We implement an iterative process alternating between sparsity and fine-tuning to mitigate accumulated error in single turn of finetuning. 
We employ pooling instead of low-rank decomposition to reduce error composition from PEFT structures. 
We apply normalization to PEFT modules during fine-tuning, constraining error values by limiting weight norms while preserving representational capacity. 
Additionally, we utilize Centered Kernel Alignment based information similarity assessment for adaptive allocation of layer-level sparsity and PEFT parameter quantities, addressing layer-specific redundancy.
Empirical evaluation on a 50\% sparse LLaMA-2 7B model demonstrates the superiority of our approach, achieving lossless compression.",ICLR.cc/2025/Conference,4.5,False,0.8089,sparse sgd achieves communication reduction while matching full gradient baselines transformer pretraining distributed image classification and recommendation systems outperforming top and fixed random sparsifiers stability and final,large language llm sparsification plays crucial role compression identify gradient conflict the primary issue peft sparse methods wherein masked pretrained weights and corresponding peft weights exhibit competing optimization objectives during fine tuning potentially compromising,2025-08-26T00:35:16.354334
16,AdaMP: Adaptive Mirror-Prox for Stable Adversarial and Minimax Training,"Motivation: Training minimax models (GANs, adversarial defenders, robust learners) remains unstable due to operator non-monotonicity and mismatched geometry. Standard optimizers do not exploit problem structure, leading to cycling or slow convergence. Approach: We propose AdaMP, an adaptive Mirror-Prox algorithm that automatically selects Bregman geometries based on online curvature and operator skewness estimates. AdaMP alternates proximal mirror steps with an extrapolation that uses a geometry selector derived from localized Fisher and Hessian proxies. By adaptively tuning mirror maps and step sizes, AdaMP balances monotone-like behavior and curvature-aware correction while remaining amenable to minibatch stochasticity. Contributions: (1) A novel adaptive mirror-prox framework that unifies geometry selection with extrapolation for stochastic minimax problems. (2) A practical estimator for geometry choice using low-cost matrix-free probes and a principled rule linking geometry to stability. (3) Theoretical guarantees: under Lipschitz and bounded-variance assumptions AdaMP attains O(1/√T) convergence in saddle-point residuals and demonstrates improved dependence on operator skewness versus fixed-geometry variants. Results: Empirically, AdaMP stabilizes GAN training across CIFAR-10 and FFHQ, reduces mode collapse, and improves FID by 10–25% relative to Adam and extragradient baselines. In adversarial training benchmarks, AdaMP yields faster robustness improvement and lower computational overhead than spectral-normalized alternatives. Impact: AdaMP provides a principled, scalable optimizer tailored to minimax and adversarial settings, offering practical stability gains and opening avenues for geometry-aware training in complex ICLR-scale problems.",ICLR,optimization,gpt-5-mini,True,2106,Conflict-Aware Adversarial Training,"Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for the standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between the gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named \textbf{Conflict-Aware Adversarial Training~(CA-AT)}. Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient finetuning.",ICLR.cc/2025/Conference,5.75,False,0.8289,adversarial training benchmarks adamp yields faster robustness improvement and lower computational overhead than spectral normalized alternatives,adversarial training the most effective obtain adversarial robustness for deep neural networks directly involving adversarial samples the training procedure this argue that the weighted average does not provide the best tradeoff for the standard and adversarial robustness comprehensive experimental that consistently offers superior trade off between standard and adversarial robustness under the settings adversarial training from scratch and parameter efficient finetuning,2025-08-26T00:35:16.354341
17,AutoBatch: Online Batch-Size Scheduling via Gradient-Noise Complexity Estimation,"Motivation: Choosing minibatch size is a critical but poorly automated hyperparameter. Fixed or heuristic schedules can waste compute or slow convergence, especially when gradient noise and curvature evolve during training. Approach: We introduce AutoBatch, an online batch-size scheduler that estimates stochastic gradient complexity — the relationship between variance, curvature, and step-size — from streaming minibatch statistics. AutoBatch uses a lightweight estimator for the signal-to-noise ratio and local Lipschitz curvature, then solves an optimization problem that minimizes wall-clock time subject to a target convergence rate by increasing batch sizes only when beneficial. Contributions: (1) A formal complexity-driven criterion for batch size selection derived from stochastic optimization bounds, connecting variance reduction to gradient norm and curvature estimates. (2) An online algorithm that adapts batch sizes with provable guarantees: AutoBatch preserves convergence rates while achieving near-optimal compute vs. variance trade-offs under smooth nonconvex assumptions. (3) System-level integration strategies that amortize estimator overhead and coordinate with learning-rate schedules. Results: AutoBatch reduces epochs-to-target and wall-clock time by 1.5–3× versus fixed-batch baselines on ResNet/ImageNet training, transformer pretraining, and reinforcement-learning policy gradients; it also stabilizes training in sharp-curvature phases. Impact: AutoBatch automates a major resource-management decision in large-scale training, enabling more efficient utilization of compute and reducing hyperparameter tuning burden for ICLR-scale experiments.",ICLR,optimization,gpt-5-mini,True,4442,Improving the convergence of SGD through adaptive batch sizes,"Mini-batch stochastic gradient descent (SGD) and variants thereof
    approximate the objective function's gradient with a small number of
    training examples, aka the batch size.  Small batch sizes require little
    computation for each model update but can yield high-variance gradient
    estimates, which poses some challenges for optimization. Conversely, large
    batches require more computation but can yield higher precision gradient
    estimates. This work presents a method to adapt the batch size to the
    model's training loss. For various function classes, we show that our
    method requires the same order of model updates as gradient descent while
    requiring the same order of gradient computations as SGD. This method
    requires evaluating the model's loss on the entire dataset every model
    update. However, the required computation is greatly reduced by
    approximating the training loss. We provide experiments that illustrate our
    methods require fewer model updates without increasing the total amount of
    computation.",ICLR.cc/2025/Conference,2.0,nan,0.8054,autobatch uses lightweight estimator for the signal noise ratio and local lipschitz curvature then solves optimization problem that minimizes wall clock time subject target convergence rate increasing batch sizes only when beneficial contributions formal complexity driven criterion for batch size selection derived from stochastic optimization bounds connecting variance reduction gradient norm and curvature estimates autobatch reduces epochs target and wall clock time versus fixed batch baselines resnet imagenet training transformer pretraining and reinforcement learning policy gradients also stabilizes training sharp curvature phases,small batch sizes require little computation for each update but can yield high variance gradient estimates which poses some challenges for optimization,2025-08-26T00:35:16.354351
18,Learned Trust Regions: Neural Surrogates for Efficient Globalized Nonconvex Steps,"Motivation: Trust-region methods provide robust globalization for nonconvex optimization but require expensive inner model solves and Hessian access, limiting their use in deep learning. Approach: We propose Learned Trust Regions (LTR), which replaces costly quadratic models with compact neural surrogates trained online to mimic local loss landscapes. LTR fits a lightweight surrogate using recent trajectory data, uses it to propose trust-region steps, and employs a rigorous acceptance test on the true loss to adapt region radii. We augment surrogates with uncertainty estimates via ensembles to guard against model bias and incorporate corrective backtracking when surrogate predictions err. Contributions: (1) A practical framework that learns and adapts local surrogates for trust-region proposals, reducing reliance on Hessian information. (2) Theoretical analysis showing that, under bounded surrogate error and uncertainty calibration, LTR maintains global convergence to stationary points with quantifiable overhead. (3) Implementation techniques for minibatch training, surrogate regularization, and computationally efficient propose-accept cycles. Results: LTR accelerates convergence and improves robustness on CIFAR, ImageNet transfer, and variational autoencoder objectives, matching second-order solvers’ quality at a fraction of cost and outperforming line-search and fixed-step baselines. Impact: By combining learned local models with classical trust-region guarantees, LTR bridges model-based optimization and deep learning, offering a scalable path to reliable globalization in large-scale nonconvex training.",ICLR,optimization,gpt-5-mini,True,8639,Simple Policy Optimization,"Model-free reinforcement learning algorithms have seen remarkable progress, but key challenges remain. Trust Region Policy Optimization (TRPO) is known for ensuring monotonic policy improvement through conservative updates within a trust region, backed by strong theoretical guarantees. However, its reliance on complex second-order optimization limits its practical efficiency. Proximal Policy Optimization (PPO) addresses this by simplifying TRPO's approach using ratio clipping, improving efficiency but sacrificing some theoretical robustness. This raises a natural question: Can we combine the strengths of both methods? In this paper, we introduce Simple Policy Optimization (SPO), a novel unconstrained first-order algorithm. SPO integrates the surrogate objective with Total Variation (TV) divergence instead of Kullback-Leibler (KL) divergence, achieving a balance between the theoretical rigor of TRPO and the efficiency of PPO. Our new objective improves upon ratio clipping, offering stronger theoretical properties and better constraining the probability ratio within the trust region. Empirical results demonstrate that SPO outperforms PPO with a simple implementation, particularly for training large, complex network architectures end-to-end.",ICLR.cc/2025/Conference,6.25,nan,0.8118,motivation trust region methods provide robust globalization for nonconvex optimization but require expensive inner solves and hessian access limiting their use deep learning learned trust regions ltr which replaces costly quadratic models compact neural surrogates trained online mimic local loss landscapes ltr accelerates convergence and improves robustness cifar imagenet transfer and variational autoencoder objectives matching second order solvers quality fraction cost and outperforming line search and fixed step baselines impact combining learned local models classical trust region guarantees ltr bridges model based optimization and deep learning offering scalable path reliable globalization large scale nonconvex training,model free reinforcement learning algorithms have seen remarkable progress but key challenges remain trust region policy optimization trpo known for ensuring monotonic policy improvement conservative updates within trust region backed strong theoretical guarantees however its reliance complex second order optimization limits its practical efficiency proximal policy optimization ppo addresses this simplifying trpo ratio clipping improving efficiency but sacrificing some theoretical robustness this simple policy optimization spo unconstrained first order empirical that spo outperforms ppo simple implementation for training large complex network architectures end end,2025-08-26T00:35:16.354356
19,SubspaceNewton: Randomized Subspace Newton with Global Convergence and Low-Rank Guarantees,"Motivation: Newton methods achieve rapid local convergence but are impractical in high dimensions due to expensive Hessians. Randomized subspace methods reduce cost but often lack global convergence guarantees or control over curvature approximation quality. Approach: We introduce SubspaceNewton, which constructs adaptive low-dimensional subspaces via randomized range finding on Hessian sketches and computes Newton-like updates restricted to these subspaces. A trust-region-based globalization and a probabilistic curvature test ensure sufficient descent and control approximation error. We provide a principled rank-adaptation rule that increases subspace dimension only when curvature information is insufficient. Contributions: (1) A randomized subspace Newton algorithm with efficient Hessian-vector sketches that scales to deep networks. (2) A globalization mechanism and rank-adaptation strategy yielding probabilistic guarantees of descent and eventual convergence to stationary points. (3) Complexity bounds that quantify trade-offs between subspace dimension, sketching cost, and convergence speed under smoothness assumptions. Results: SubspaceNewton attains faster loss reduction and improved final accuracy versus first-order and naive low-rank Newton baselines on image models and transformer finetuning, often achieving quasi-Newton rates with low-rank subspaces. Impact: SubspaceNewton makes second-order acceleration tractable for large models by marrying randomized linear algebra with rigorous optimization control, offering a scalable alternative for ICLR-scale training.",ICLR,optimization,gpt-5-mini,True,6790,Subspace Optimiztion for Large Language Models with Convergence Guarantees,"Subspace optimization algorithms, with GaLore (Zhao et al., 2024) as a representative method, have gained popularity for pre-training or fine-tuning large language models (LLMs) due to their memory efficiency. However, their convergence guarantees remain unclear, particularly in stochastic settings. In this paper, we unexpectedly discover that GaLore does not always converge to the optimal solution and substantiate this finding with an explicit counterexample. We then investigate the conditions under which GaLore can achieve convergence, demonstrating that it does so either in deterministic scenarios or when using a sufficiently large mini-batch size. More significantly, we introduce **GoLore** (**G**radient rand**o**m **Lo**w-**r**ank proj**e**ction), a novel variant of GaLore that provably converges in stochastic settings, even with standard batch sizes. Our convergence analysis can be readily extended to other sparse subspace optimization algorithms. Finally, we conduct numerical experiments to validate our theoretical results and empirically explore the proposed mechanisms.",ICLR.cc/2025/Conference,5.25,False,0.8164,contributions randomized subspace newton efficient hessian vector sketches that scales deep networks subspacenewton attains faster loss reduction and improved final versus first order and naive low rank newton baselines image models and transformer finetuning often achieving quasi newton rates low rank subspaces impact subspacenewton makes second order acceleration tractable for large models marrying randomized linear algebra rigorous optimization control offering scalable alternative for iclr scale training,subspace optimization algorithms galore representative have gained popularity for pre training fine tuning large language models llms due their memory efficiency our convergence analysis can readily extended other sparse subspace optimization algorithms,2025-08-26T00:35:16.354366
20,Delay-Adaptive Consensus: Robust Decentralized Optimization under Heterogeneous Latencies,"Motivation: Decentralized training is attractive for privacy and scalability but suffers from communication delays and heterogeneity that degrade consensus and convergence. Existing algorithms often assume bounded synchronous delays or sacrifice performance under asynchrony. Approach: We present Delay-Adaptive Consensus (DAC), a decentralized optimizer that models per-edge latency statistics and compensates updates via delay-aware mixing matrices and momentum correction terms. DAC employs adaptive weighting that discounts stale information proportionally to estimated delay distributions and integrates local correction gradients to mitigate drift from asynchrony. We analyze the algorithm via time-varying consensus theory and stochastic approximation with delayed feedback. Contributions: (1) A delay-adaptive decentralized protocol that tolerates heterogeneous, stochastic delays without global synchronization. (2) Theoretical convergence: DAC provably converges to stationary points with explicit dependence on delay moments and network topology under smooth, nonconvex objectives. (3) Practical mechanisms for online delay estimation, compression-friendly communication, and fault tolerance. Results: On decentralized ImageNet training, graph-based recommendation, and federated-like edge learning tasks, DAC outperforms standard gossip and asynchronous SGD variants in convergence speed and robustness to stragglers and intermittent links. Impact: DAC broadens the applicability of decentralized optimization to real-world networks with variable latency, enabling reliable, efficient distributed learning in ICLR-relevant deployments.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8159,theoretical convergence dac provably converges stationary points explicit dependence delay moments and network topology under smooth nonconvex objectives decentralized imagenet training graph based recommendation and federated like edge learning tasks dac outperforms standard gossip and asynchronous sgd variants convergence speed and robustness stragglers and intermittent links impact dac broadens the applicability decentralized optimization real world networks variable latency enabling reliable efficient distributed learning iclr relevant deployments,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354372
21,ProbCD: Probabilistic Coordinate Descent with Bayesian Importance for Sparse Overparameterized Models,"Motivation: Coordinate descent scales poorly with model dimension in overparameterized settings, and naive sparsification or pruning often harms final performance. Guiding coordinate updates by principled importance estimates can accelerate optimization and induce sparsity, but existing heuristics lack probabilistic guarantees. Approach: We introduce ProbCD, a probabilistic coordinate descent framework that samples coordinates according to a Bayesian posterior over parameter relevance derived from local gradient and curvature evidence. ProbCD maintains lightweight local priors updated via online variational updates; sampling probabilities favor coordinates with high expected marginal utility. Updates incorporate elastic corrections to control bias and a shrinkage prior to induce sparsity. Contributions: (1) A Bayesian importance-sampling scheme for coordinate selection with theoretical analysis of expected progress per communication. (2) Convergence results for nonconvex smooth objectives showing that ProbCD attains comparable stationary guarantees while performing fewer coordinate updates in expectation. (3) Algorithms for scalable posterior approximation, compatibility with minibatches, and sparsity-inducing regularization. Results: ProbCD accelerates training and yields compact models on overparameterized CNNs and embedding-heavy recommendation systems, achieving up to 5× fewer parameter updates with negligible accuracy loss and improved compression vs. magnitude pruning. Impact: ProbCD provides a principled probabilistic route to coordinate-efficient training and structured sparsification, offering optimization and model-compression benefits for large-scale ICLR applications.",ICLR,optimization,gpt-5-mini,False,,Nonmyopic Bayesian Optimization in Dynamic Cost Settings,"Bayesian optimization (BO) is a popular framework for optimizing black-box functions, leveraging probabilistic models such as Gaussian processes. However, conventional BO assumes static query costs, which limits its applicability to real-world problems with dynamic cost structures, such as geological surveys or biological sequence design, where query costs vary based on previous actions. To address this, we propose a cost-constrained nonmyopic BO algorithm that incorporates dynamic cost models. Our method employs a neural network policy for variational optimization over multi-step lookahead horizons to plan ahead in dynamic cost environments. Empirically, we benchmark our method on synthetic functions exhibiting a variety of dynamic cost structures. Furthermore, we apply our method to a real-world application in protein sequence design using a large language model-based policy, demonstrating its scalability and effectiveness in handling multi-step planning in a large and complex query space. Our nonmyopic BO algorithm consistently outperforms its myopic counterparts in both synthetic and real-world settings, achieving significant improvements in both efficiency and solution quality.",ICLR.cc/2025/Conference,4.25,False,0.7861,guiding coordinate updates principled importance estimates can accelerate optimization and induce sparsity but existing heuristics lack probabilistic guarantees impact probcd provides principled probabilistic route coordinate efficient training and structured sparsification offering optimization and model compression benefits for large scale iclr applications,bayesian optimization popular for optimizing black box functions leveraging probabilistic models such gaussian processes our employs neural network policy for variational optimization over multi step lookahead horizons plan ahead dynamic cost environments furthermore apply our real world application protein sequence large language model based policy demonstrating its scalability and effectiveness handling multi step planning large and complex query space,2025-08-26T00:35:16.354383
22,HeavyTailSGD: Robust Acceleration under Heavy-Tailed Gradient Noise,"Motivation: Empirical SGD gradients in deep learning often exhibit heavy-tailed distributions, undermining classical variance-based analyses and causing instability for accelerated methods. Standard optimizers tuned for sub-Gaussian noise can diverge or converge slowly. Approach: We introduce HeavyTailSGD, an optimization algorithm that replaces variance-dependent steps with robust estimators using trimmed-mean and Catoni-style M-estimators for gradient aggregation and adaptive step control. We design a momentum variant that accounts for heavy-tailed increments via robustified velocity updates and derive step-size rules based on tail-index estimation. Contributions: (1) A robust optimization framework tailored to heavy-tailed stochastic gradients with practical aggregation and momentum schemes. (2) Theoretical guarantees: under α-stable tail models, HeavyTailSGD attains provable convergence rates that smoothly degrade with tail-index α and surpass standard SGD when tails are heavy. (3) Estimators and algorithms for online tail-index estimation and computationally efficient robust aggregation compatible with minibatch sampling. Results: HeavyTailSGD improves stability and final performance across vision and language tasks where empirical gradient tails are heavy (estimated α < 2), preventing catastrophic gradient spikes and reducing required tuning. It outperforms Adam and SGD with gradient clipping in heavy-tailed regimes while matching their performance when tails are light. Impact: HeavyTailSGD brings robust-statistics principles into practical deep learning optimization, offering a reliable alternative for environments with non-sub-Gaussian noise commonly encountered in ICLR-scale training.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8470,motivation empirical sgd gradients deep learning often exhibit heavy tailed distributions undermining classical variance based analyses and causing instability for accelerated methods heavytailsgd optimization that replaces variance dependent steps robust estimators trimmed mean and catoni style estimators for gradient aggregation and adaptive step control contributions robust optimization tailored heavy tailed stochastic gradients practical aggregation and momentum schemes heavytailsgd improves stability and final across vision and language tasks where empirical gradient tails are heavy estimated preventing catastrophic gradient spikes and reducing required tuning impact heavytailsgd brings robust statistics principles into practical deep learning optimization offering reliable alternative for environments non sub gaussian noise encountered iclr scale training,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354384
23,Constraint-Imitation Optimizer: Learning-to-Optimize with Hard Constraint Guarantees,"Motivation: Learned optimizers accelerate training but often ignore hard constraints (safety, resource limits, fairness), making them unsuitable for constrained deployment. Enforcing constraints post hoc undermines learned optimizer benefits. Approach: We propose the Constraint-Imitation Optimizer (CIO), a meta-learned optimizer trained via imitation of an oracle constrained optimizer. CIO uses a differentiable projection module and a constrained imitation loss that penalizes deviation from oracle primal-dual trajectories. During meta-training, we regularize policy updates with KKT-consistency losses and incorporate trust-region steps to ensure generalization across constraint sets. Contributions: (1) A meta-learning framework that produces learned optimizers respecting hard equality and inequality constraints with provable feasibility guarantees under mild oracle approximation. (2) Theoretical analysis connecting imitation fidelity to constraint satisfaction and convergence of the downstream optimization process. (3) Practical designs for scalable meta-training, constraint encoding, and online correction for distribution shift. Results: CIO accelerates constrained training in resource-aware model compression, fairness-aware classification, and bounded-control RL, reducing iterations to feasibility and improving objective performance versus projected SGD and penalty-method-trained learned optimizers. Impact: CIO enables practical deployment of learned optimizers in constrained settings, marrying the speed of meta-optimization with reliability and safety guarantees relevant to ICLR applications.",ICLR,optimization,gpt-5-mini,False,,Goal Achievement Guided Exploration: Mitigating Premature Convergence in Reinforcement Learning,"Premature convergence to suboptimal policies remains a significant challenge in reinforcement learning (RL), particularly in tasks with sparse rewards or non-convex reward landscapes. Existing work usually utilizes reward shaping, such as curiosity-based internal rewards, to encourage exploring promising spaces. However, this may inadvertently introduce new local optima and impair the optimization for the actual target reward. To address this issue, we propose Goal Achievement Guided Exploration (GAGE), a novel approach that incorporates an agent's goal achievement as a dynamic criterion for balancing exploration and exploitation. GAGE adaptively adjusts the exploitation level based on the agent's current performance relative to an estimated optimal performance, thereby mitigating premature convergence. Extensive evaluations demonstrate that GAGE substantially improves learning outcomes across various challenging tasks by adapting convergence based on task success. Applicable to both continuous and discrete tasks, GAGE seamlessly integrates into existing RL frameworks, highlighting its potential as a versatile tool for enhancing exploration strategies in RL.",ICLR.cc/2025/Conference,5.5,False,0.7837,theoretical analysis connecting imitation fidelity constraint satisfaction and convergence the downstream optimization process,premature convergence suboptimal policies remains significant challenge reinforcement learning tasks sparse rewards non convex reward landscapes however this may inadvertently local optima and impair the optimization for the actual target reward extensive evaluations that gage improves learning outcomes across various challenging tasks adapting convergence task success,2025-08-26T00:35:16.354387
24,Causal-Invariant Optimization: Projected Gradient Methods for Out-of-Distribution Generalization,"Motivation: Models trained by empirical risk minimization often exploit spurious correlations that fail under distribution shift. Existing domain-generalization methods are mostly heuristic and lack optimization-theoretic guarantees that explicitly enforce causal invariance during training. Approach: We introduce Causal-Invariant Optimization (CIO), a projected-gradient framework that constrains updates to a learned causal subspace estimated from multi-environment moments. At each step CIO performs a standard stochastic gradient update followed by a projection onto an invariant subspace recovered by solving a small generalized eigenproblem on environment-wise gradient covariance matrices. The projection is implemented efficiently via randomized sketches, allowing minibatch compatibility. Contributions: (1) The CIO algorithm that operationalizes invariance constraints via gradient-space projections. (2) A theoretical analysis proving that, under an identifiable multi-environment linear invariance condition and smooth nonconvexity, CIO converges to stationary points satisfying invariance constraints and reduces reliance on spurious directions. (3) Practical estimators for subspace recovery with finite-sample concentration bounds and low-memory randomized implementations. Results: Empirically, CIO improves worst-case and average accuracy under covariate and label shifts on synthetic causal tasks, domain-adaptation vision benchmarks, and medical datasets with systematic biases, outperforming ERM, IRM, and invariant risk heuristics while maintaining comparable training cost. Impact: CIO provides a principled, optimization-centric route to enforce causal invariance in large-scale learning, offering provable robustness to distribution shifts relevant to ICLR applications.",ICLR,optimization,gpt-5-mini,True,6446,Demystifying amortized causal discovery with transformers,"Supervised learning for causal discovery from observational data often achieves competitive performance despite seemingly avoiding the explicit assumptions that traditional methods require for identifiability. In this work, we analyze CSIvA (Ke et al., 2023b) on bivariate causal models, a transformer architecture for amortized inference promising to train on synthetic data and transfer to real ones. First, we bridge the gap with identifiability theory, showing that the training distribution implicitly defines a prior on the causal model of the test observations: consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. Second, we find that CSIvA can not generalize to classes of causal models unseen during training: to overcome this limitation, we show that learning on datasets generated from different types of causal models, unambiguously identifiable in isolation, improves the test generalization. We analyze this empirical evidence with theory, illustrating that the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur. Overall, we find that amortized causal discovery still adheres to identifiability theory, violating the previous hypothesis from Lopez-Paz et al. (2015) that supervised learning methods could overcome its restrictions.",ICLR.cc/2025/Conference,5.0,False,0.8324,causal invariant optimization cio projected gradient that constrains updates learned causal subspace estimated from multi environment moments empirically cio improves worst case and average under covariate and label shifts synthetic causal tasks domain adaptation vision benchmarks and medical datasets systematic biases outperforming erm irm and invariant risk heuristics while maintaining comparable training cost impact cio provides principled optimization centric route enforce causal invariance large scale learning offering provable robustness distribution shifts relevant iclr applications,supervised learning for causal discovery from observational data often achieves competitive despite seemingly avoiding the explicit assumptions that traditional methods require for identifiability this csiva bivariate causal models transformer for amortized inference promising train synthetic data and transfer real ones second find that csiva can not generalize classes causal models unseen during training overcome this limitation that learning datasets generated from different types causal models unambiguously identifiable isolation improves the generalization that supervised learning methods could overcome its restrictions,2025-08-26T00:35:16.354398
25,Quantile-Aware SGD: Tail-Risk Minimization with Differentiable Quantile Gradients,"Motivation: Standard training minimizes expected loss and overlooks tail risk; in safety-critical and fairness-sensitive applications, controlling high-loss tails (worst-case samples) is crucial. Existing robust objectives are either non-differentiable or computationally expensive. Approach: We propose Quantile-Aware SGD (QASGD), a scalable optimization algorithm that directly minimizes conditional value-at-risk (CVaR) and higher quantile risks by constructing differentiable surrogate quantile losses via smoothed order-statistics estimators. QASGD computes quantile gradients using a differentiable sorting approximation combined with importance-weighted minibatch sampling that emphasizes high-loss examples. We derive variance-reduction schemes tailored to the quantile estimators and an adaptive quantile schedule to trade off robustness and average performance. Contributions: (1) A differentiable, minibatch-compatible quantile loss estimator enabling direct stochastic optimization of tail risk. (2) Convergence theory establishing O(1/√T) rates for smoothed CVaR objectives under Lipschitz and smoothness assumptions, with bounds linking smoothing and estimator variance. (3) Practical techniques: importance sampling, adaptive quantile annealing, and per-sample reweight stabilization. Results: QASGD reduces tail error rates and improves worst-group accuracy on imbalanced and corrupted-label benchmarks (CIFAR-C, Waterbirds, CelebA), achieves lower CVaR than reweighting and robust loss baselines, and maintains competitive average accuracy. Impact: QASGD offers a principled, efficient tool for minimizing tail risk in deep learning, enabling safer and fairer models for deployment in ICLR-scale tasks.",ICLR,optimization,gpt-5-mini,True,7282,Tilted Losses in Training Quantum Neural Networks,"Empirical risk minimization is a fundamental paradigm in the optimization process of machine learning (ML) models. Several techniques extend this idea by introducing parameters which further regularize this strategy in training these models. One of these paradigms is the so-called tilted empirical risk minimization (TERM), which uses a tilted hyperparameter to penalize the presence of outliers, which represent data samples that differ significantly from the rest of the dataset. Quantum machine learning (QML) models have been studied and benchmarked across various criteria stemming from classical ML, including their training via the parameter-shift rule. Therefore, it is natural to extend the concept of TERM in training QML models, namely the type of models known as quantum neural networks (QNNs). In this work, we examine the impact of a tilted loss function in training a class of QNNs, specifically for binary classification tasks involving two different datasets with induced class imbalance. In the first dataset, the Iris dataset, we show that varying the value of the tilted hyperparameter modifies the decision boundary leading to reduced importance of outliers and better training accuracy --- highlighting the importance of using tilted risk minimization. Additionally, in a synthetic dataset we validate that the training accuracy can be improved using the tilted parameter. Analytically, we extend the parameter-shift training method to accommodate weighted inputs by introducing the tilted hyperparameter for training QNNs. These results highlight the significance of incorporating regularization techniques from ML models into QML models.",ICLR.cc/2025/Conference,3.6666666666666665,nan,0.8039,quantile aware sgd qasgd scalable optimization that directly minimizes conditional value risk cvar and higher quantile risks constructing differentiable surrogate quantile losses smoothed order statistics estimators derive variance reduction schemes tailored the quantile estimators and adaptive quantile schedule trade off robustness and average contributions differentiable minibatch compatible quantile loss estimator enabling direct stochastic optimization tail risk impact qasgd offers principled efficient tool for minimizing tail risk deep learning enabling safer and fairer models for deployment iclr scale tasks,empirical risk minimization fundamental paradigm the optimization process machine learning models quantum machine learning qml models have been studied and benchmarked across various criteria stemming from classical including their training the parameter shift rule therefore natural extend the concept term training qml models namely the type models known quantum neural networks qnns this examine the impact tilted loss function training class qnns for binary classification tasks involving two different datasets induced class imbalance,2025-08-26T00:35:16.354404
26,AsyncVR: Staleness-Resilient Asynchronous Variance Reduction for Heterogeneous Systems,"Motivation: Asynchronous distributed training accelerates throughput but suffers from gradient staleness and worker heterogeneity; variance-reduced methods (SVRG/SAGA) can accelerate convergence but are typically synchronous. There is a need for asynchronous variance reduction that tolerates heterogeneity while retaining theoretical speedups. Approach: We present AsyncVR, an asynchronous variance-reduced algorithm that maintains control variates with staleness-corrected updates. Each worker holds local snapshots and periodically pushes compact correction vectors to a parameter server; the server employs timestamped aggregation and applies explicit staleness compensation via decaying weights derived from staleness models. We model asynchrony using bounded-delay and random-delay frameworks and design update rules ensuring unbiasedness in expectation. Contributions: (1) An asynchronous variance-reduction protocol with lightweight communication and staleness-aware aggregation suited for heterogeneous clusters. (2) Theoretical convergence: under smooth nonconvexity and bounded delays, AsyncVR attains improved sample complexity over asynchronous SGD, with explicit dependence on delay moments and variance terms. (3) Robust implementation strategies including adaptive snapshot intervals and compressed corrections to reduce overhead. Results: AsyncVR achieves near-linear speedup in wall-clock time across heterogeneous GPU clusters and edge devices, outperforming async-SGD and synchronous VR baselines in convergence speed and final accuracy on transformer pretraining and large CNNs. Impact: AsyncVR reconciles variance reduction with practical asynchrony, enabling faster, more reliable distributed optimization in real-world heterogeneous environments relevant to ICLR-scale training.",ICLR,optimization,gpt-5-mini,True,7152,DeMo: Decoupled Momentum Optimization,"Training large scale neural networks typically involves sharing the gradients between all accelerators, which necessitates specialized high-speed interconnects. Taking cues from signal processing, we show that it is not necessary to share or synchronize the full optimizer states and model parameters during training. By decoupling the momentum and allowing divergence in the optimizer states across accelerators, it is possible to even improve convergence compared to previous state of the art optimizers.
From this, we introduce a Decoupled Momentum optimization algorithm (DeMo) that reduces the communication requirements by several orders of magnitude, potentially enabling future training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. Furthermore, our method is agnostic to the network topology and neural network architecture, and supports scalable clock-synchronous distributed training with negligible compute and memory overhead.
Empirically, we show that models trained with DeMo match or surpass the performance of equal models trained with AdamW, entirely bypassing the need for high-speed interconnects for pre-training large scale foundation models.",ICLR.cc/2025/Conference,2.6,nan,0.8096,asyncvr achieves near linear speedup wall clock time across heterogeneous gpu clusters and edge devices outperforming async sgd and synchronous baselines convergence speed and final transformer pretraining and large cnns impact asyncvr reconciles variance reduction practical asynchrony enabling faster more reliable distributed optimization real world heterogeneous environments relevant iclr scale training,training large scale neural networks involves sharing the gradients between all accelerators which necessitates specialized high speed interconnects from this decoupled momentum optimization demo that reduces the communication requirements several orders magnitude potentially enabling future training large neural networks slow internet bandwidths heterogeneous networking hardware furthermore our agnostic the network topology and neural network and supports scalable clock synchronous distributed training negligible compute and memory overhead,2025-08-26T00:35:16.354411
27,ImplicitMeta: Scalable Implicit Differentiation for Long-Horizon Meta-Learning,"Motivation: Meta-learning often requires differentiating through long inner optimization trajectories, incurring heavy memory and instability when using unrolled backpropagation. Implicit differentiation offers memory savings but its application to nonconvex, stochastic inner problems at scale lacks robust, efficient solvers. Approach: We introduce ImplicitMeta, a scalable implicit-differentiation framework that approximates hypergradients for stochastic nonconvex inner problems using a randomized conjugate-residual solver with variance-reduction preconditioning. The method leverages an online Hessian-vector product estimator and Jacobian-free implicit linear system solves, combined with checkpointed reversible integration to control discretization error. Contributions: (1) A practical implicit differentiation pipeline tailored to stochastic, nonconvex inner loops with provable consistency under mild stability and ergodicity assumptions. (2) A randomized linear solver with variance-reduced preconditioning that achieves fast convergence of hypergradient estimates at low memory cost. (3) Rigorous error bounds linking solver residuals, stochasticity, and meta-gradient bias, guiding trade-offs in compute and accuracy. Results: ImplicitMeta accelerates hyperparameter and initialization meta-learning for few-shot classification and RL adaptation, reducing memory by an order of magnitude and matching or improving validation performance versus truncated backprop and naive implicit baselines while enabling longer inner horizons. Impact: ImplicitMeta makes implicit differentiation practical for large-scale meta-learning, unlocking long-horizon adaptation with provable hypergradient quality for ICLR applications.",ICLR,optimization,gpt-5-mini,True,10632,Generalized Greedy Gradient-Based Hyperparameter Optimization,"Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization.  Approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps these difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. Additionally, we demonstrate that under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning and data hyper-cleaning tasks support our findings.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8590,motivation meta learning often requires differentiating long inner optimization trajectories incurring heavy memory and instability when unrolled backpropagation implicitmeta accelerates hyperparameter and initialization meta learning for few shot classification and adaptation reducing memory order magnitude and matching improving validation versus truncated backprop and naive implicit baselines while enabling longer inner horizons impact implicitmeta makes implicit differentiation practical for large scale meta learning unlocking long horizon adaptation provable hypergradient quality for iclr applications,bilevel optimization blo widely used that has numerous applications including hyperparameter optimization meta learning reverse mode differentiation suffers from high memory requirements while the methods the implicit function theorem require the convergence the inner optimization approximations that consider truncated inner optimization trajectory suffer from short horizon bias accumulate the short horizon approximations from each step the inner optimization trajectory experimental few shot meta learning and data hyper cleaning tasks support our,2025-08-26T00:35:16.354418
28,TopoCompress: Topology-Aware Gradient Compression for Networked Distributed Optimization,"Motivation: Communication compression is vital for scalable distributed training, but typical compressors ignore network topology resulting in suboptimal throughput and convergence when links have heterogeneous bandwidth/latency. Approach: We propose TopoCompress, a topology-aware compression scheme that coordinates compressors across workers based on network graph properties. TopoCompress assigns compression rates per edge by solving a lightweight constrained optimization that trades per-link bandwidth with compressor variance impact on convergence. Gradient updates are encoded with locality-preserving sketches and routed with error-feedback mechanisms adapted to link capacities. Contributions: (1) A formal framework linking compression-induced variance to network-aware rate allocation, with convex formulations solvable online. (2) A distributed protocol that integrates per-edge compression, error compensation, and adaptive rebalancing under changing network metrics. (3) Convergence analysis: under smooth nonconvex objectives, TopoCompress provably retains O(1/√T) rates with explicit dependence on network-aware variance budgets. Results: On multi-datacenter setups and simulated heterogeneous links, TopoCompress achieves 2–10× end-to-end speedup versus topology-agnostic compressors while matching test accuracy on ImageNet and recommendation tasks. Impact: TopoCompress enables efficient, reliable distributed optimization by accounting for network heterogeneity, facilitating large-scale ICLR experiments across diverse infrastructure.",ICLR,optimization,gpt-5-mini,True,8884,Markovian Compression: Looking to the Past Helps Accelerate the Future,"This paper deals with distributed optimization problems that use compressed communication to achieve efficient performance and mitigate the communication bottleneck. We propose a family of compression schemes in which operators transform vectors fed to their input according to a Markov chain, i.e., the stochasticity of the compressors depends on previous iterations. Intuitively, this should accelerate the convergence of optimization methods, as considering previous iterations seems more natural and robust. The compressors are implemented in the vanilla Quantized Stochastic Gradient Descent (QSGD) algorithm. To further improve efficiency and convergence rate, we apply the momentum acceleration method. We prove convergence results for our algorithms with Markovian compressors and show theoretically that the accelerated method converges faster than the basic version. The analysis covers non-convex, Polyak-Lojasiewicz (PL), and strongly convex cases. Experiments are conducted to demonstrate the applicability of the results to distributed data-parallel optimization problems. Practical results demonstrate the superiority of methods utilizing our compressors design over several existing optimization algorithms.",ICLR.cc/2025/Conference,5.25,False,0.8471,motivation communication compression vital for scalable distributed training but typical compressors ignore network topology resulting suboptimal throughput and convergence when links have heterogeneous bandwidth latency topocompress topology aware compression scheme that coordinates compressors across workers network graph properties topocompress assigns compression rates per edge solving lightweight constrained optimization that trades per link bandwidth compressor variance impact convergence distributed protocol that integrates per edge compression error compensation and adaptive rebalancing under changing network metrics impact topocompress enables efficient reliable distributed optimization accounting for network heterogeneity facilitating large scale iclr experiments across diverse infrastructure,this deals distributed optimization problems that use compressed communication achieve efficient and mitigate the communication bottleneck intuitively this should accelerate the convergence optimization methods considering previous iterations seems more natural and robust experiments are conducted the applicability the distributed data parallel optimization problems practical the superiority methods utilizing our compressors over several existing optimization algorithms,2025-08-26T00:35:16.354421
29,SpectralInit: Data-Dependent Initialization via Low-Rank Spectral Proxies for Faster Convergence,"Motivation: Initialization profoundly affects nonconvex optimization trajectories; popular random schemes ignore dataset geometry, leading to slow early-phase convergence and brittle training in low-data regimes. Approach: We introduce SpectralInit, a data-dependent initialization strategy that computes low-rank spectral proxies of layer-wise input covariances using streaming randomized SVD on minibatches. SpectralInit initializes weights to align dominant singular directions with principal data modes and scales singular values to match desired local curvature characteristics. The method is lightweight, privacy-preserving via sketching, and compatible with convolutional and attention layers. Contributions: (1) A principled initialization that leverages local data geometry to reduce initial loss and condition number. (2) Theoretical analysis showing that SpectralInit yields smaller expected gradient norm in early iterations and improves the condition of Gauss-Newton approximations under mild assumptions. (3) Practical streaming and compressed algorithms with complexity comparable to a few forward passes. Results: SpectralInit accelerates convergence in low-data, transfer-learning, and small-batch training regimes across vision and language models, reducing epochs-to-target by 20–50% and improving final generalization when combined with standard optimizers. Impact: SpectralInit provides a principled, scalable way to harness dataset geometry at initialization, improving optimization efficiency and robustness in ICLR-scale settings where data efficiency and stability matter.",ICLR,optimization,gpt-5-mini,True,1009,On the Crucial Role of Initialization for Matrix Factorization,"This work revisits the classical low-rank matrix factorization problem and unveils the critical role of initialization in shaping convergence rates for such nonconvex and nonsmooth optimization. We introduce Nystrom initialization, which significantly improves the global convergence of Scaled Gradient Descent (ScaledGD) in both symmetric and asymmetric matrix factorization tasks. Specifically, we prove that ScaledGD with Nystrom initialization achieves quadratic convergence in cases where only linear rates were previously known. Furthermore, we extend this initialization to low-rank adapters (LoRA) commonly used for finetuning foundation models. Our approach, NoRA, i.e., LoRA with Nystrom initialization, demonstrates superior performance across various downstream tasks and model scales, from 1B to 7B parameters, in large language and diffusion models.",ICLR.cc/2025/Conference,6.5,True,0.8086,motivation initialization profoundly affects nonconvex optimization trajectories popular random schemes ignore geometry leading slow early phase convergence and brittle training low data regimes the lightweight privacy preserving sketching and compatible convolutional and attention layers spectralinit accelerates convergence low data transfer learning and small batch training regimes across vision and language models reducing epochs target and improving final generalization when combined standard optimizers impact spectralinit provides principled scalable way harness geometry initialization improving optimization efficiency and robustness iclr scale settings where data efficiency and stability matter,this revisits the classical low rank matrix factorization problem and unveils the critical role initialization shaping convergence rates for such nonconvex and nonsmooth optimization lora nystrom initialization demonstrates superior across various downstream tasks and scales from parameters large language and diffusion models,2025-08-26T00:35:16.354431
30,LabelShift-SGD: Importance-Weighted Stochastic Optimization for Adaptive Label Distribution Shift,"Motivation: In many realistic deployments, label distributions shift between training and deployment (label shift), degrading model performance. Most optimization procedures ignore such shifts at training time and lack mechanisms to adapt efficiently. Approach: We propose LabelShift-SGD, an online stochastic optimization method that jointly estimates label-ratio shifts and performs importance-weighted gradient updates to minimize target-domain risk. The algorithm alternates between a lightweight label-ratio estimator using confusion-matrix adjustments and a variance-reduced importance-weighted SGD with adaptive clipping to control variance from high weights. We analyze bias-variance trade-offs and introduce regularized weight smoothing to improve stability. Contributions: (1) A practical estimator-optimizer loop for label-shift adaptation with provable convergence to stationary points of the importance-weighted objective under bounded estimation error. (2) Theoretical bounds quantifying the impact of label-ratio estimation error on optimization bias and excess risk. (3) Techniques for robustifying importance weights (clipping, smoothing) with explicit guarantees. Results: LabelShift-SGD yields significant robustness to synthetic and realistic label shifts on CIFAR-10-C, ImageNet-corrupted, and clinical prediction tasks, improving target-domain accuracy and calibration compared to ERM, reweighting with static estimates, and importance-sampling baselines. Impact: LabelShift-SGD equips practitioners with an optimization-aware approach to mitigate label distribution shifts, improving deployment reliability in ICLR-relevant domain-adaptation scenarios.",ICLR,optimization,gpt-5-mini,True,1559,Optimizing importance weighting in the presence of sub-population shifts,"A distribution shift between the training and test data can severely harm performance of machine learning models. Importance weighting addresses this issue by assigning different weights to data points during training. We argue that existing heuristics for determining the weights are suboptimal, as they neglect the increase of the variance of the estimated model due to the limited sample size of the training data. We interpret the optimal weights in terms of a bias-variance trade-off,  and  propose a bi-level optimization procedure in which the weights and model parameters are optimized simultaneously. We apply this framework to existing importance weighting techniques for last-layer retraining of deep neural networks in the presence of sub-population shifts and show empirically that optimizing weights significantly improves generalization performance.",ICLR.cc/2025/Conference,7.0,True,0.8267,most optimization procedures ignore such shifts training time and lack mechanisms adapt labelshift sgd online stochastic optimization that jointly estimates label ratio shifts and performs importance weighted gradient updates minimize target domain risk contributions practical estimator optimizer loop for label shift adaptation provable convergence stationary points the importance weighted objective under bounded estimation error theoretical bounds quantifying the impact label ratio estimation error optimization bias and excess risk labelshift sgd yields significant robustness synthetic and realistic label shifts cifar imagenet corrupted and clinical prediction tasks improving target domain and calibration compared erm reweighting static estimates and importance sampling baselines,distribution shift between the training and data can severely harm machine learning models interpret the optimal weights terms bias variance trade off and level optimization procedure which the weights and parameters are optimized simultaneously apply this existing importance weighting techniques for last layer retraining deep neural networks the presence sub population shifts and empirically that optimizing weights improves generalization,2025-08-26T00:35:16.354437
31,SurroQuasi: Surrogate-Aware Quasi-Newton Methods for Spiking Neural Network Training,"Motivation: Spiking Neural Networks (SNNs) promise energy-efficient inference but are difficult to optimize due to nondifferentiable spike events; surrogate gradients enable training but suffer from poor curvature estimates and slow convergence. Approach: We introduce SurroQuasi, a quasi-Newton optimizer tailored to surrogate-gradient SNN training. SurroQuasi models the surrogate loss curvature using low-rank approximations of smoothed Jacobians computed via efficient adjoint surrogates, and applies a damped quasi-Newton update that accounts for surrogate bias. We derive correction factors that compensate for surrogate-induced gradient errors and develop a blockwise update rule that respects temporal and neuronal structure to keep computations tractable. Contributions: (1) A quasi-Newton framework specifically adapted to surrogate gradients with theoretical bounds on the impact of surrogate bias on curvature estimates. (2) A computationally efficient low-rank temporal-block approximation leveraging event sparsity in SNNs, with provable descent under smooth surrogate assumptions. (3) Practical schemes for damping, surrogate calibration, and integration with surrogate families (sigmoid, fast sigmoid). Results: SurroQuasi accelerates training speed and improves spike efficiency on neuromorphic benchmarks (NMNIST, DVS-CIFAR), yields higher accuracy and sparser spiking patterns than SGD and Adam with surrogates, and reduces epochs-to-target by 2–4×. Impact: SurroQuasi bridges optimization theory and neuromorphic practice, offering a principled way to train energy-efficient SNNs with faster convergence and better resource-use, advancing optimization methods for alternative computing paradigms in ICLR research.",ICLR,optimization,gpt-5-mini,True,10106,Discretized Quadratic Integrate-and-Fire Neuron Model for Direct Training of Spiking Neural Networks,"Spiking Neural Networks (SNNs) are a promising alternative to traditional artificial neural networks, offering significant energy-saving potential. Conventional SNN approaches typically utilize the Leaky Integrate-and-Fire (LIF) neuron model, where voltage decays linearly, decreasing proportionally to its current value. However, this linear decay can inadvertently increase energy consumption and reduce model performance due to extraneous spiking activity. To address these limitations, we introduce the discretized Quadratic Integrate-and-Fire (QIF) neuron model, which applies a non-linear transformation to the voltage proportional to its magnitude. The QIF neuron model achieves substantial energy reductions, ranging from $1.43 - 4.21\times$ compared to the LIF neuron model. On static datasets (CIFAR-10, CIFAR-100) and neuromorphic datasets (CIFAR-10 DVS, N-Caltech-101, N-Cars, DVS128-Gesture), the QIF neuron model demonstrates competitive performance and improved accuracy over state-of-the-art results. Furthermore, the QIF neuron model produces smoother loss landscapes and larger local minima, leading to faster training convergence. Our findings suggest that the QIF neuron model offers a promising alternative to the widely adopted LIF neuron model.",ICLR.cc/2025/Conference,4.6,nan,0.8911,motivation spiking neural networks snns promise energy efficient inference but are difficult optimize due nondifferentiable spike events surrogate gradients enable training but suffer from poor curvature estimates and slow convergence impact surroquasi bridges optimization theory and neuromorphic practice offering principled way train energy efficient snns faster convergence and better resource use advancing optimization methods for alternative computing paradigms iclr,spiking neural networks snns are promising alternative traditional artificial neural networks offering significant energy saving potential,2025-08-26T00:35:16.354446
32,Stochastic Proximal Point with Implicit Regularization for Nonconvex Nonsmooth Learning,"Motivation: Modern models often involve nonsmooth or composite objectives (e.g., ReLU networks with regularizers) where stochastic gradient methods struggle to provide stability and implicit regularization guarantees. Proximal-point methods offer robustness but are traditionally too expensive or poorly understood in stochastic nonconvex settings. Approach: We propose SPIR (Stochastic Proximal-point with Implicit Regularization), a minibatch-friendly proximal scheme that solves proximal subproblems approximately via a short inner stochastic solver and leverages implicit regularization arising from proximal geometry. SPIR adaptively controls inner-solve accuracy using a variance-aware stopping rule and injects a curvature-aware proximal metric to bias solutions toward flat regions. Contributions: (1) Algorithmic design: a practical SPIR algorithm combining cheap inner solves, adaptive accuracy, and metric selection to scale to deep models. (2) Theory: nonasymptotic convergence to first-order stationary points for smooth nonconvex composite objectives, with complexity bounds separating inner-solve error, stochastic variance, and proximal strength; we characterize how proximal geometry induces implicit regularization that favors flat minima. (3) Implementation: memory-efficient proximal metrics via diagonal-plus-low-rank approximations and heuristics for inner-solver scheduling. Results: SPIR reduces variance and stabilizes training across CNNs, residual networks with ℓ1 or total-variation regularizers, and sparse autoencoders, achieving faster wall-clock convergence and improved generalization versus SGD, proximal SGD, and ADMM-like baselines. Impact: SPIR provides a scalable bridge between proximal theory and deep learning practice, offering robust optimization for nonsmooth, regularized objectives with principled implicit regularization advantages suitable for ICLR-scale tasks.",ICLR,optimization,gpt-5-mini,True,1420,A Unified Theory of Stochastic Proximal Point Methods without Smoothness,"This paper presents a comprehensive analysis of a broad range of variations of the stochastic proximal point method (SPPM). Proximal point methods have attracted considerable interest owing to their numerical stability and robustness against imperfect tuning, a trait not shared by the dominant stochastic gradient descent (SGD) algorithm. A framework of assumptions that we introduce encompasses methods employing techniques such as variance reduction and arbitrary sampling. A cornerstone of our general theoretical approach is a parametric assumption on the iterates, correction and control vectors. We establish a single theorem that ensures linear convergence under this assumption and $\mu$-strong convexity of the loss function, and without the need to invoke smoothness. This integral theorem reinstates best known complexity and convergence guarantees for several existing methods, which demonstrates the robustness of our approach. We expand our study by developing three new variants of SPPM, and through numerical experiments elucidate various properties inherent to them.",ICLR.cc/2025/Conference,4.5,False,0.8668,proximal point methods offer robustness but are traditionally too expensive poorly understood stochastic nonconvex settings contributions algorithmic practical spir combining cheap inner solves adaptive and selection scale deep models impact spir provides scalable bridge between proximal theory and deep learning practice offering robust optimization for nonsmooth regularized objectives principled implicit regularization advantages suitable for iclr scale tasks,proximal point methods have attracted considerable interest owing their numerical stability and robustness against imperfect tuning trait not shared the dominant stochastic gradient descent sgd this integral theorem reinstates best known complexity and convergence guarantees for several existing methods which demonstrates the robustness our,2025-08-26T00:35:16.354451
33,ParetoMD: Mirror Descent for Generating High-Fidelity Neural Pareto Fronts,"Motivation: Multi-objective learning (e.g., accuracy vs. latency, fairness vs. utility) requires constructing Pareto fronts, but current approaches either solve scalarized objectives repeatedly or lack theoretical guarantees and scalability for deep networks. Approach: We introduce ParetoMD, a mirror-descent based optimizer that computes a continuous set of Pareto-optimal solutions in a single run by maintaining distributional weights over objectives and performing geometry-aware mirror updates on parameters and weight simplex. ParetoMD uses adaptive mirror maps tailored to per-objective curvature and employs entropy-regularized averaging to trace high-fidelity fronts. We derive a stochastic approximation scheme that estimates objective gradients and curvature proxies with controlled variance. Contributions: (1) A unified algorithm that outputs a continuous Pareto curve by evolving parameter-weight pairs, avoiding costly grid-search scalarization. (2) Convergence analysis establishing subsequential convergence to Pareto-stationary solutions in nonconvex smooth settings and finite-sample guarantees on approximating the front under bounded variance. (3) Practical extensions: low-memory checkpointing to extract multiple front points, and bias-correction for imbalanced objectives. Results: ParetoMD efficiently generates smooth Pareto fronts for trade-offs in accuracy vs. FLOPs, fairness vs. accuracy, and robustness vs. clean accuracy on vision and NLP tasks, outperforming repeated scalarization and multi-objective evolutionary baselines in sample efficiency and front coverage. Impact: ParetoMD offers a theoretically grounded and scalable approach for multi-objective neural optimization, enabling practitioners to explore principled trade-offs without repeated costly retraining.",ICLR,optimization,gpt-5-mini,True,5104,Many-Objective Multi-Solution Transport,"Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce ''Many-objective multi-solution Transport (MosT)'', a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between objectives and solutions. Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives. On a range of applications in federated learning, multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives.",ICLR.cc/2025/Conference,6.166666666666667,True,0.8665,motivation multi objective learning latency fairness utility requires constructing pareto fronts but current approaches either solve scalarized objectives repeatedly lack theoretical guarantees and scalability for deep networks flops fairness and robustness clean vision and nlp tasks outperforming repeated scalarization and multi objective evolutionary baselines sample efficiency and front coverage impact paretomd offers theoretically grounded and scalable for multi objective neural optimization enabling practitioners principled trade offs repeated costly retraining,optimizing the many objectives instantiated tasks clients jointly few pareto stationary solutions models critical machine learning however previous multi objective optimization methods often focus few objectives and cannot scale many objectives that outnumber the solutions leading either subpar ignored objectives our insight seek multiple solutions each performing domain expert and focusing specific subset objectives while collectively covering all them most formulates the problem level optimization weighted objectives for each solution where the weights are defined optimal transport between objectives and solutions range applications federated learning multi task learning and mixture prompt learning for llms most distinctly outperforms strong baselines delivering high quality diverse solutions that profile the entire pareto frontier thus ensuring balanced trade offs across many objectives,2025-08-26T00:35:16.354457
34,Spectral Learning Rate Scheduling via Layerwise Neural Jacobian Gaps,"Motivation: Learning-rate schedules remain largely heuristic and insensitive to layerwise dynamical behavior; this leads to suboptimal convergence when curvature and training signal vary across depth and phases. Approach: We propose SpecSched, a data- and model-driven learning-rate scheduler that estimates layerwise spectral gaps of the neural Jacobian using randomized probing and modifies per-layer step sizes according to a spectral-adaptivity law. The scheduler monitors the leading singular values and the gap to the bulk spectrum to infer stability margins and adaptively increases learning rates in well-conditioned layers while damping those with small spectral gaps prone to instability. Contributions: (1) A practical spectral estimator using O(1) Jacobian-vector products per layer per epoch, scalable to large architectures via randomized sketching. (2) A principled scheduling rule derived from local linearization analysis that links spectral gap to optimal per-layer step-size scaling, with theoretical justification under smoothness and local quadratic approximations. (3) Robustness mechanisms including smoothing, momentum-compatible adjustments, and automatic freeze thresholds when spectra indicate spurious directions. Results: SpecSched accelerates convergence and improves final performance on deep convolutional and transformer models, reducing required learning-rate tuning and improving stability under large-batch regimes; ablations show spectral gap correlates with optimal layerwise rates. Impact: By introducing spectral-awareness into schedule design, SpecSched provides a theoretically motivated tool to automate and improve training dynamics, reducing manual tuning for large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,8167,Gradient descent with generalized Newton’s method,"We propose the generalized Newton's method (GeN) --- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the intensive tuning of the learning rate scheduler. In practice, our method is easily implementable, since it only requires additional forward passes with almost zero computational overhead (in terms of training time and memory cost), if the overhead is amortized over many iterations. We present extensive experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that GeN optimizers match the state-of-the-art performance, which was achieved with carefully tuned learning rate schedulers.",ICLR.cc/2025/Conference,6.25,True,0.8058,specsched data and model driven learning rate scheduler that estimates layerwise spectral gaps the neural jacobian randomized probing and modifies per layer step sizes according spectral adaptivity law the scheduler monitors the leading singular values and the gap the bulk spectrum infer stability margins and adaptively increases learning rates well conditioned layers while damping those small spectral gaps prone instability robustness mechanisms including smoothing momentum compatible adjustments and automatic freeze thresholds when spectra indicate spurious directions specsched accelerates convergence and improves final deep convolutional and transformer models reducing required learning rate tuning and improving stability under large batch regimes ablations spectral gap correlates optimal layerwise rates,our automatically and dynamically selects the learning rate that accelerates the convergence the intensive tuning the learning rate scheduler extensive experiments language and vision tasks gpt and resnet showcase that gen optimizers match the state the art which was achieved carefully tuned learning rate schedulers,2025-08-26T00:35:16.354464
35,Fair Federated Aggregation via Utility-Balanced Momentum (U-BM),"Motivation: Federated learning deployments must balance aggregate performance with per-client fairness, but standard aggregation (FedAvg) prioritizes global loss minimization and can amplify client-level disparities. Approach: We introduce U-BM, a federated optimizer that incorporates utility-balanced momentum into server aggregation to equalize client influence over time. Each client computes local momentum-tracked updates; the server rescales and aggregates them using an adaptive balancing function that amplifies underperforming clients while damping overcontributing ones. U-BM also adjusts local learning rates via a fairness-aware control loop estimating cross-client regret. Contributions: (1) Algorithm: U-BM integrates momentum with an online utility-balancing rule to steer global optimization toward equitable utility distributions. (2) Theory: convergence analysis under heterogeneous client objectives showing that U-BM converges to stationary points while improving worst-client utility, with explicit trade-offs between fairness gain and global objective slowdown. (3) Practical protocols for privacy preservation, compressed communication, and partial participation robustness. Results: U-BM reduces worst-client loss and variance across clients on language modeling, medical imaging, and recommendation federated benchmarks, improving fairness metrics (equalized accuracy, loss variance) with modest impact on global accuracy compared to FedAvg, FedProx, and fairness-focused baselines. Impact: U-BM offers a principled, scalable method to reconcile optimization efficiency with client-level fairness in federated settings, supporting equitable deployment of models in resource-diverse environments.",ICLR,optimization,gpt-5-mini,True,2915,Entropy-Based Aggregation for Fair and Effective Federated Learning,"Federated Learning (FL) enables collaborative model training across distributed devices while preserving data privacy. Nonetheless, the heterogeneity of edge devices often leads to inconsistent performance of the globally trained models, resulting in unfair outcomes among users. Existing federated fairness algorithms strive to enhance fairness but often fall short in maintaining the overall performance of the global model, typically measured by the average accuracy across all clients. To address this issue, we propose a novel algorithm that leverages entropy-based aggregation combined with model and gradient alignments to simultaneously optimize fairness and global model performance. Our method employs a bi-level optimization framework, where we derive an analytic solution to the aggregation probability in the inner loop, making the optimization process computationally efficient. Additionally, we introduce an innovative alignment update and an adaptive strategy in the outer loop to further balance global model's performance and fairness. Theoretical analysis indicates that our approach guarantees convergence even in non-convex FL settings and demonstrates significant fairness improvements in generalized regression and strongly convex models. Empirically, our approach surpasses state-of-the-art federated fairness algorithms, ensuring consistent performance among clients while improving the overall performance of the global model.",ICLR.cc/2025/Conference,6.0,False,0.9036,motivation federated learning deployments must balance aggregate per client fairness but standard aggregation fedavg prioritizes global loss minimization and can amplify client level disparities also adjusts local learning rates fairness aware control loop estimating cross client regret contributions integrates momentum online utility balancing rule steer global optimization toward equitable utility distributions theory convergence analysis under heterogeneous client objectives showing that converges stationary points while improving worst client utility explicit trade offs between fairness gain and global objective slowdown practical protocols for privacy preservation compressed communication and partial participation robustness reduces worst client loss and variance across clients language modeling medical imaging and recommendation federated benchmarks improving fairness metrics equalized loss variance modest impact global compared fedavg fedprox and fairness focused baselines impact offers principled scalable reconcile optimization efficiency client level fairness federated settings supporting equitable deployment models resource diverse environments,federated learning enables collaborative training across distributed devices while preserving data privacy existing federated fairness algorithms strive enhance fairness but often fall short maintaining the overall the global measured the average across all clients address this issue that leverages entropy based aggregation combined and gradient alignments simultaneously optimize fairness and global our employs level optimization where derive analytic solution the aggregation probability the inner loop making the optimization process computationally efficient additionally innovative alignment update and adaptive strategy the outer loop further balance global model and fairness theoretical analysis indicates that our guarantees convergence even non convex settings and demonstrates significant fairness improvements generalized regression and strongly convex models empirically our surpasses state the art federated fairness algorithms ensuring consistent among clients while improving the overall the global,2025-08-26T00:35:16.354470
36,Mixed-Precision Stabilization by Adaptive Numerical Regularization (AMPR),"Motivation: Mixed-precision training unlocks computational speedups but suffers from instability due to reduced mantissa precision, overflow/underflow, and poor optimizer dynamics in low-precision accumulators. Existing fixes are ad hoc (loss scaling, selective casting) and do not adapt to evolving training statistics. Approach: We propose AMPR, an optimizer-integrated stabilization technique that applies adaptive numerical regularization: dynamic loss scaling informed by gradient magnitude distribution, local curvature-aware accumulation scaling, and stochastic rounding with controlled bias correction. AMPR models precision-induced perturbations as additive noise and designs regularizers to counteract bias accumulation. It jointly optimizes numerical hyperparameters via a small meta-controller that minimizes a proxy stability loss computed from gradient variance and overflow indicators. Contributions: (1) A unified framework for mixed-precision stability combining adaptive scaling, curvature-aware accumulation, and bias-corrected stochastic rounding. (2) Formal analysis modeling quantization and rounding as perturbations, proving that AMPR preserves convergence guarantees of the baseline optimizer under bounded numerical noise and provides conditions for stable accumulation. (3) Engineering implementations that integrate with PyTorch/AMP, require minimal overhead, and preserve hardware acceleration. Results: AMPR enables consistent mixed-precision training of transformers, GANs, and large CNNs without manual loss-scaling schedules, reducing numerical failures and matching full-precision final accuracy while improving throughput by 1.5–2×. Impact: AMPR provides a principled, automated approach to safe mixed-precision training, reducing engineer burden and widening applicability of low-precision optimization in large ICLR workloads.",ICLR,optimization,gpt-5-mini,False,,SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation,"As advancements in large language models (LLMs) continue and the demand for personalized models increases, parameter-efficient fine-tuning (PEFT) methods (e.g., LoRA) become essential due to their efficiency in reducing computation costs.
However, recent studies have raised alarming concerns that LoRA fine-tuning could potentially compromise the safety alignment in LLMs, posing significant risks for the model owner.
In this paper, we first investigate the underlying mechanism by analyzing the changes in safety alignment related features before and after fine-tuning.
Then, we propose a fixed safety module calculated by safety data and a task-specific initialization for trainable parameters in low-rank adaptations, termed Safety-alignment preserved Low-Rank Adaptation (SaLoRA). 
Unlike previous LoRA methods and their variants, SaLoRA enables targeted modifications to LLMs without disrupting their original alignments. 
Our experiments show that SaLoRA outperforms various adapters-based approaches across various evaluation metrics in different fine-tuning tasks.",ICLR.cc/2025/Conference,6.5,True,0.7800,impact ampr provides principled automated safe mixed precision training reducing engineer burden and widening applicability low precision optimization large iclr workloads,advancements large language models llms continue and the demand for personalized models increases parameter efficient fine tuning peft methods then fixed safety module calculated safety data and task specific initialization for trainable parameters low rank adaptations termed safety alignment preserved low rank adaptation salora,2025-08-26T00:35:16.354473
37,HybridBayesOpt: Combining Bayesian Global Search with Gradient-Based Local Refinement for Hyperparameter Tuning,"Motivation: Hyperparameter optimization is crucial for model performance; Bayesian optimization (BO) excels in global exploration but scales poorly to high-dimensional hyperparameter spaces and cannot exploit gradient information when available. Approach: We introduce HybridBayesOpt, a two-phase hyperparameter optimizer that couples BO for global exploration with gradient-based local refinement via implicit hypergradient estimation. BO proposes promising regions; within each region, we perform efficient bilevel local refinement using an implicit-differentiation solver that computes hypergradients with controlled approximation and low memory. We design acquisition functions that account for local refinement potential, and a scheduler that allocates compute between exploration and refinement based on expected improvement. Contributions: (1) A hybrid algorithmic architecture uniting BO and gradient-based hyperparameter optimization, with mechanisms for acquisition design and compute allocation. (2) Theoretical results: convergence to local optima of the hyperparameter landscape under mild assumptions, and bounds showing improved sample efficiency compared to pure BO in high-dimensional settings when gradients exist. (3) Practical components: surrogate warm-starting, parallelization strategies, and robustness to noisy validation metrics. Results: HybridBayesOpt outperforms standalone BO and gradient-only hyper-tuning on neural architecture hyperparameters, learning-rate schedules, and data-augmentation parameters across CIFAR, ImageNet transfer, and transformer tuning, achieving faster validation improvement per compute and finding better optima in fewer trials. Impact: HybridBayesOpt provides a scalable, practical strategy to leverage both global and local information for hyperparameter search, reducing tuning costs in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,9795,Glocal Hypergradient Estimation with Koopman Operator,"Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose *glocal* hypergradient estimation, blending ""global"" quality with ""local"" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneously. Through numerical experiments of hyperparameter optimization, including optimization of optimizers, we demonstrate the effectiveness of the glocal hypergradient estimation.",ICLR.cc/2025/Conference,5.0,False,0.8432,motivation hyperparameter optimization crucial for bayesian optimization excels global exploration but scales poorly high dimensional hyperparameter spaces and cannot exploit gradient information when available practical components surrogate warm starting parallelization strategies and robustness noisy validation metrics hybridbayesopt outperforms standalone and gradient only hyper tuning neural hyperparameters learning rate schedules and data augmentation parameters across cifar imagenet transfer and transformer tuning achieving faster validation improvement per compute and better optima fewer trials,gradient based hyperparameter optimization methods update hyperparameters hypergradients gradients meta criterion respect hyperparameters numerical experiments hyperparameter optimization including optimization optimizers the effectiveness the glocal hypergradient estimation,2025-08-26T00:35:16.354482
38,Projection-Free Policy Optimization: Frank-Wolfe for Constrained Reinforcement Learning,"Motivation: Constrained reinforcement learning (e.g., safety, energy budgets) often requires solving constrained policy optimization problems where projections onto feasible policy sets are costly or infeasible. Projection-free algorithms like Frank-Wolfe (FW) provide an alternative but are underdeveloped in stochastic, nonconvex policy spaces. Approach: We develop FW-PO, a projection-free policy optimization method applying stochastic Frank-Wolfe updates on the policy parameterization with linear minimization oracles implemented via policy search subroutines (e.g., trust-region actor queries). FW-PO integrates variance reduction for gradient estimates and a constraint-tightening schedule to handle sampling noise. We extend classical FW analysis to nonconvex, stochastic settings with policy-specific geometry (probability simplex or parametric manifolds). Contributions: (1) A projection-free RL optimizer that enables constrained policy learning without expensive projections, compatible with on-policy and off-policy data. (2) Theoretical analysis: convergence to stationary points of constrained objectives with bounded constraint violation, and rates that account for oracle inexactness and sampling variance. (3) Practical implementations demonstrating efficient linear oracle design using off-the-shelf policy solvers and constraint smoothing techniques. Results: FW-PO attains comparable rewards with lower constraint violation on safety-constrained Mujoco tasks, energy-limited robotic control, and constrained resource allocation, outperforming projection-based baselines in wall-clock efficiency and sample complexity in several regimes. Impact: FW-PO opens a projection-free avenue for constrained policy optimization, simplifying implementation and reducing computational cost for safety-critical RL applications investigated at ICLR.",ICLR,optimization,gpt-5-mini,True,8639,Simple Policy Optimization,"Model-free reinforcement learning algorithms have seen remarkable progress, but key challenges remain. Trust Region Policy Optimization (TRPO) is known for ensuring monotonic policy improvement through conservative updates within a trust region, backed by strong theoretical guarantees. However, its reliance on complex second-order optimization limits its practical efficiency. Proximal Policy Optimization (PPO) addresses this by simplifying TRPO's approach using ratio clipping, improving efficiency but sacrificing some theoretical robustness. This raises a natural question: Can we combine the strengths of both methods? In this paper, we introduce Simple Policy Optimization (SPO), a novel unconstrained first-order algorithm. SPO integrates the surrogate objective with Total Variation (TV) divergence instead of Kullback-Leibler (KL) divergence, achieving a balance between the theoretical rigor of TRPO and the efficiency of PPO. Our new objective improves upon ratio clipping, offering stronger theoretical properties and better constraining the probability ratio within the trust region. Empirical results demonstrate that SPO outperforms PPO with a simple implementation, particularly for training large, complex network architectures end-to-end.",ICLR.cc/2025/Conference,6.25,nan,0.8463,motivation constrained reinforcement learning safety energy budgets often requires solving constrained policy optimization problems where projections onto feasible policy sets are costly infeasible projection free policy optimization applying stochastic frank wolfe updates the policy parameterization linear minimization oracles implemented policy search subroutines contributions projection free optimizer that enables constrained policy learning expensive projections compatible policy and off policy data,model free reinforcement learning algorithms have seen remarkable progress but key challenges remain trust region policy optimization trpo known for ensuring monotonic policy improvement conservative updates within trust region backed strong theoretical guarantees however its reliance complex second order optimization limits its practical efficiency proximal policy optimization ppo addresses this simplifying trpo ratio clipping improving efficiency but sacrificing some theoretical robustness this simple policy optimization spo unconstrained first order empirical that spo outperforms ppo simple implementation for training large complex network architectures end end,2025-08-26T00:35:16.354483
39,Restarted Accelerated Variance Reduction: Adaptive Restarts for Robust Acceleration in Stochastic Optimization,"Motivation: Acceleration combined with variance reduction yields powerful convergence in convex settings, but in stochastic nonconvex training, acceleration can amplify noise leading to instability; existing restart heuristics are empirical and lack theory in stochastic regimes. Approach: We introduce RAVR (Restarted Accelerated Variance Reduction), a principled restart scheduler for accelerated variance-reduced algorithms. RAVR monitors a Lyapunov surrogate combining gradient norm, momentum alignment, and variance estimates; when misalignment or excessive variance is detected, RAVR triggers a local restart that resets momentum and adapts snapshot frequency. We provide adaptive restart thresholds derived from concentration inequalities and link restart frequency to stochastic signal-to-noise ratios. Contributions: (1) A theoretically justified restart mechanism tailored to stochastic accelerated variance-reduction that balances acceleration benefits and noise amplification. (2) Convergence guarantees: RAVR recovers accelerated rates in low-noise phases and safe nonaccelerated rates under high noise, with explicit bounds on total gradient complexity accounting for restarts. (3) Practical integration recipes with popular VR methods (SVRG, SAGA) and momentum schemes for deep learning. Results: RAVR stabilizes accelerated VR methods on large-scale nonconvex problems (transformer pretraining, GANs, deep matrix factorization), achieving faster convergence and lower final loss than non-restarted accelerations and matching robustness of nonaccelerated VR in noisy phases. Impact: RAVR provides a bridge between theoretical acceleration and practical stochastic optimization, enabling robust, adaptive acceleration for complex ICLR-scale training tasks.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8084,practical integration recipes popular methods svrg saga and momentum schemes for deep learning ravr stabilizes accelerated methods large scale nonconvex problems transformer pretraining gans deep matrix factorization achieving faster convergence and lower final loss than non restarted accelerations and matching robustness nonaccelerated noisy phases,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354484
40,Entropy-Adaptive Gradient Descent: Provable Implicit Regularization for Generalization,"Motivation: Adaptive optimizers accelerate training but can harm generalization; theoretical understanding of how adaptivity interacts with implicit regularization remains incomplete. There is a need for optimizers that retain adaptivity while providing provable generalization benefits. Approach: We introduce Entropy-Adaptive Gradient Descent (EAGD), a family of per-coordinate adaptive methods that incorporate an entropy-based regularizer into the update rule. EAGD adjusts learning rates using second-moment estimates while simultaneously encouraging parameter distributions with higher entropy through a tractable proximal correction. We analyze training as stochastic dynamics under an entropic potential and derive implicit regularization effects from the stationary distribution. Contributions: (1) The EAGD algorithm: a practical, minibatch-compatible adaptive optimizer with an entropy-proximal term implementable with low overhead. (2) A theoretical framework proving that EAGD induces an implicit bias towards flat minima, yielding tighter generalization bounds compared to standard adaptive methods under smooth nonconvex losses. (3) Nonasymptotic convergence guarantees to first-order stationary points, and characterization of entropy strength versus optimization speed trade-offs. Results: Experiments on ResNet, transformer fine-tuning, and small-data regimes demonstrate that EAGD matches Adam's training speed while improving test accuracy and calibration; entropy regularization reduces sharpness metrics and mitigates overfitting across diverse tasks. Impact: EAGD bridges adaptivity and principled regularization, offering a theoretically grounded optimizer for practitioners seeking both fast convergence and robust generalization in large-scale ICLR-style models.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8236,eagd adjusts learning rates second moment estimates while simultaneously encouraging parameter distributions higher entropy tractable proximal correction nonasymptotic convergence guarantees first order stationary points and characterization entropy strength versus optimization speed trade offs experiments resnet transformer fine tuning and small data regimes that eagd matches adam training speed while improving and calibration entropy regularization reduces sharpness metrics and mitigates overfitting across diverse tasks,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354486
41,Stochastic Mirror Bilevel: Scalable Bilevel Optimization with Nonconvex Inner Problems,"Motivation: Bilevel problems appear in hyperparameter tuning, meta-learning, and architecture search, but existing stochastic bilevel solvers either assume convex inner problems or require expensive unrolled differentiation, limiting scalability. Approach: We propose Stochastic Mirror Bilevel (SMB), a bilevel optimizer that uses mirror-prox updates on the outer variables and mirror-descent with stochastic proximal steps for inner variables, allowing nonconvex inner objectives and general geometry. SMB employs implicit-gradient estimators derived from sensitivity equations solved via randomized sketching and variance reduction. Mirror maps are chosen per-block to exploit geometry (e.g., simplex, positive-definite cones), and inner solves use adaptive accuracy to balance compute and hypergradient bias. Contributions: (1) An algorithmic blueprint enabling scalable bilevel optimization with nonconvex inner problems and geometry-aware updates. (2) Theoretical guarantees: SMB converges to ε-stationary points of the bilevel objective with explicit complexity bounds that decouple inner problem condition, outer smoothness, and variance. (3) Practical constructs: randomized implicit solvers, adaptive inner tolerance scheduling, and mirror-map selection heuristics. Results: On hyperparameter tuning for data augmentation, meta-initialization for few-shot tasks, and architecture search surrogates, SMB outperforms truncated unroll and naive implicit baselines in validation improvement per compute, while using substantially less memory. Impact: SMB extends bilevel optimization to practical nonconvex settings with rigorous analysis and scalable implementations, enabling broader adoption in ICLR research areas where nested objectives are central.",ICLR,optimization,gpt-5-mini,True,10632,Generalized Greedy Gradient-Based Hyperparameter Optimization,"Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization.  Approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps these difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. Additionally, we demonstrate that under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning and data hyper-cleaning tasks support our findings.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8658,contributions algorithmic blueprint enabling scalable bilevel optimization nonconvex inner problems and geometry aware updates hyperparameter tuning for data augmentation meta initialization for few shot tasks and search surrogates smb outperforms truncated unroll and naive implicit baselines validation improvement per compute while less memory impact smb extends bilevel optimization practical nonconvex settings rigorous analysis and scalable implementations enabling broader adoption iclr areas where nested objectives are central,bilevel optimization blo widely used that has numerous applications including hyperparameter optimization meta learning reverse mode differentiation suffers from high memory requirements while the methods the implicit function theorem require the convergence the inner optimization approximations that consider truncated inner optimization trajectory suffer from short horizon bias accumulate the short horizon approximations from each step the inner optimization trajectory experimental few shot meta learning and data hyper cleaning tasks support our,2025-08-26T00:35:16.354488
42,DistillFed: Consensus-Free Federated Optimization via Server-Side Distillation,"Motivation: Federated averaging requires strong client consensus and frequent synchronization, which is costly under heterogeneity and intermittent connectivity. A communication-efficient alternative that avoids strict parameter averaging is desirable. Approach: We introduce DistillFed, a federated optimization paradigm where clients send only logits or small distilled representations to the server; the server aggregates via knowledge-distillation targets and broadcasts compact synthetic examples for local refinement. DistillFed replaces parameter consensus with semantic alignment via a distillation loss, and uses adaptive client weighting to correct for label-shift and system heterogeneity. We analyze convergence by modeling distillation as a surrogate loss linking local and global objectives and incorporate variance reduction across rounds. Contributions: (1) A consensus-free federated protocol based on logit distillation that dramatically reduces communication and synchronization demands. (2) Theoretical analysis showing convergence to stationary points of an implicit global objective under bounded distillation approximation error and stochastic variability; we quantify trade-offs between compression and optimization bias. (3) Practical components: client-side refinement schedules, synthetic exemplar generation, and privacy-preserving distillation via noise-aware aggregation. Results: DistillFed achieves comparable or superior test accuracy to FedAvg with up to 10× lower uplink cost on language modeling, image classification, and medical datasets with severe client heterogeneity and partial participation. It is robust to stragglers and variable client compute. Impact: DistillFed provides a scalable, communication-efficient federated optimization alternative that relaxes consensus requirements, expanding feasible federated deployments for ICLR-scale applications.",ICLR,optimization,gpt-5-mini,True,8567,Neighborhood and Global Perturbations Supported SAM in Federated Learning:  From Local Tweaks To Global Awareness,"Federated Learning (FL) can be coordinated under the orchestration of a central server to build a privacy-preserving model without collaborative data exchange.
However, participant data heterogeneity leads to local optima divergence, affecting convergence outcomes. Recent research focused on global sharpness-aware minimization (SAM) and dynamic regularization to enhance consistency between global and local generalization and optimization objectives. Nonetheless, the estimation of global SAM introduces additional computational and memory overhead. At the same time, the local dynamic regularizer cannot capture the global update state due to training isolation.
This paper proposes a novel FL algorithm, FedTOGA, designed to consider optimization and generalization objectives while maintaining minimal uplink communication overhead. By linking local perturbations to global updates, global generalization consistency is improved. Additionally, linking the local dynamic regularizer to global updates increases the perception of the global gradient and enhances optimization consistency. Global updates are passively received by clients, reducing overhead.
We also propose neighborhood perturbation to approximate local perturbation, analyzing its strengths and working principle. Theoretical analysis shows FedTOGA achieves faster convergence $O(1/T)$ under non-convex functions. Empirical studies demonstrate that FedTOGA outperforms state-of-the-art algorithms, with a 1\% accuracy increase and 30\% faster convergence, achieving state-of-the-art.",ICLR.cc/2025/Conference,4.0,nan,0.8448,distillfed federated optimization paradigm where clients send only logits small distilled representations the server the server aggregates knowledge distillation targets and broadcasts compact synthetic examples for local refinement distillfed replaces parameter consensus semantic alignment distillation loss and uses adaptive client weighting correct for label shift and heterogeneity theoretical analysis showing convergence stationary points implicit global objective under bounded distillation approximation error and stochastic variability quantify trade offs between compression and optimization bias distillfed achieves comparable superior fedavg lower uplink cost language modeling image classification and medical datasets severe client heterogeneity and partial participation impact distillfed provides scalable communication efficient federated optimization alternative that relaxes consensus requirements expanding feasible federated deployments for iclr scale applications,federated learning can coordinated under the orchestration central server build privacy preserving collaborative data exchange recent focused global sharpness aware minimization sam and dynamic regularization enhance consistency between global and local generalization and optimization objectives this proposes fedtoga designed consider optimization and generalization objectives while maintaining minimal uplink communication overhead additionally linking the local dynamic regularizer global updates increases the perception the global gradient and enhances optimization consistency,2025-08-26T00:35:16.354491
43,Smoothed Minimax Certification: Differentiable Robust Optimization with Provable Guarantees,"Motivation: Certifying robustness against adversarial perturbations or distribution shifts is crucial for trustworthy models, yet many certification procedures are computationally expensive or incompatible with standard training regimes. Approach: We propose Smoothed Minimax Certification (SMC), an optimization framework that integrates randomized smoothing into a differentiable minimax training objective. SMC formulates certification as a smooth outer maximization over perturbation distributions while training an inner model via stochastic gradient descent; the smoothing renders the certification objective differentiable and amenable to efficient stochastic optimization. We derive tight dual certificates using Gaussian smoothing and apply sample-efficient Monte Carlo estimators with control variates. Contributions: (1) A tractable, differentiable certification objective that unifies randomized smoothing with minimax training for robustness certification. (2) Provable robustness guarantees: under Gaussian smoothing, SMC provides certified bounds on adversarial risk with explicit dependence on smoothing scale and optimization error. (3) Algorithmic accelerations: variance-reduced estimators, adaptive smoothing scheduling, and certificate-aware regularization. Results: SMC produces models with state-of-the-art certified accuracy under ℓ2 attacks on CIFAR-10 and provable distribution-shift resilience in domain-robust benchmarks, improving certified radii for comparable un-smoothed defenses and offering favorable compute-certification trade-offs. Impact: SMC enables scalable, optimization-friendly robustness certification suitable for real-world ICLR tasks, bridging certification theory and practical training pipelines.",ICLR,optimization,gpt-5-mini,True,8070,Robust Representation Consistency Model via Contrastive Denoising,"Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3\% on average, with up to 11.6\% at larger radii, while reducing inference costs by 85x on average.",ICLR.cc/2025/Conference,6.75,True,0.8566,motivation certifying robustness against adversarial perturbations distribution shifts crucial for trustworthy models yet many certification procedures are computationally expensive incompatible standard training regimes smoothed minimax certification smc optimization that integrates randomized smoothing into differentiable minimax training objective smc formulates certification smooth outer maximization over perturbation distributions while training inner stochastic gradient descent the smoothing renders the certification objective differentiable and amenable efficient stochastic optimization contributions tractable differentiable certification objective that unifies randomized smoothing minimax training for robustness certification provable robustness guarantees under gaussian smoothing smc provides certified bounds adversarial risk explicit dependence smoothing scale and optimization error impact smc enables scalable optimization friendly robustness certification suitable for real world iclr tasks bridging certification theory and practical training pipelines,robustness essential for deep neural networks security sensitive applications this end randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations,2025-08-26T00:35:16.354494
44,Probabilistic Low-Rank Newton: Stochastic Second-Order Steps with Provable Curvature Control,"Motivation: Second-order optimization can dramatically accelerate convergence but is prohibitive in high dimensions due to Hessian computation and storage. Low-rank approximations alleviate cost but risk poor curvature representation. Approach: We introduce Probabilistic Low-Rank Newton (PLRN), which constructs randomized low-rank Hessian approximations via adaptive sketching and enforces probabilistic curvature guarantees using concentration-based rank selection. PLRN computes Newton-like updates in the sketched subspace and applies a principled damping derived from residual curvature estimates to ensure descent. We integrate variance-reduced stochastic gradients and show how to update sketches incrementally with minibatches. Contributions: (1) A randomized algorithm producing low-rank Newton steps with provable high-probability bounds on curvature approximation accuracy. (2) Globalization: a damping and acceptance test ensuring convergence to stationary points in expectation under smooth nonconvexity and bounded variance. (3) Complexity analysis quantifying the trade-off between sketch rank, sketching cost, and convergence acceleration. Results: PLRN outperforms first-order and heuristic low-rank Newton methods on transformer fine-tuning, large MLPs, and autoencoders, offering significant reductions in epochs-to-target and more stable convergence across batch sizes, while maintaining modest per-iteration overhead. Impact: PLRN provides a practical, theoretically principled pathway to exploit second-order information in large models, making curvature-aware optimization feasible for ICLR-scale applications.",ICLR,optimization,gpt-5-mini,True,9645,Gathering and Exploiting Higher-Order Information when Training Large Structured Models,"When training large models, such as neural networks, 
the full derivatives of order 2 and beyond are usually inaccessible,
due to their computational cost.
This is why, among the second-order optimization methods, it is very common
to bypass the computation of the Hessian by using 
first-order information, such as the gradient of the parameters (e.g., quasi-Newton methods)
or the activations (e.g., K-FAC).

In this paper, we focus on the exact and explicit computation
of projections of the Hessian and higher-order derivatives on
well-chosen subspaces, which are relevant for optimization.
Namely, for a given partition of the set of parameters, 
it is possible to compute tensors which can be seen as
""higher-order derivatives according to the partition"",
at a reasonable cost as long as the number of subsets of 
the partition remains small.

Then, we propose an optimization method exploiting
these tensors at order 2 and 3 with several interesting properties, including:
it outputs a learning rate per subset of parameters, which can
be used for hyperparameter tuning;
it takes into account long-range interactions
between the layers of the trained neural network, 
which is usually not the case in similar methods (e.g., K-FAC);
the trajectory of the optimization is invariant under 
affine layer-wise reparameterization.",ICLR.cc/2025/Conference,5.333333333333333,False,0.8569,motivation second order optimization can dramatically accelerate convergence but prohibitive high dimensions due hessian computation and storage low rank approximations alleviate cost but risk poor curvature representation plrn outperforms first order and heuristic low rank newton methods transformer fine tuning large mlps and autoencoders offering significant reductions epochs target and more stable convergence across batch sizes while maintaining modest per iteration overhead impact plrn provides practical theoretically principled pathway exploit second order information large models making curvature aware optimization feasible for iclr scale applications,when training large models such neural networks the full derivatives order and beyond are usually inaccessible due their computational cost this why among the second order optimization methods very common bypass the computation the hessian first order information such the gradient the parameters this focus the exact and explicit computation projections the hessian and higher order derivatives well chosen subspaces which are relevant for optimization then optimization exploiting these tensors order and several interesting properties including outputs learning rate per subset parameters which can used for hyperparameter tuning takes into account long range interactions between the layers the trained neural network which usually not the case similar methods fac the trajectory the optimization invariant under affine layer wise reparameterization,2025-08-26T00:35:16.354498
45,Differential Compression SGD: Communication Reduction with Statistical Guarantees,"Motivation: Communication compression is essential for distributed training, but aggressive compression can bias updates and harm convergence; statistically principled compression methods with explicit control over bias-variance trade-offs are needed. Approach: We propose Differential Compression SGD (DC-SGD), which compresses gradient differences (deltas) relative to a compressed reference, and applies a bias-corrected corrective mechanism based on online variance estimation. DC-SGD adapts compression levels per-parameter and per-round by solving a constrained optimization that minimizes communication subject to a bound on induced variance. We derive precise bounds linking compression distortion to optimization error and design a monotone error-feedback scheme that guarantees unbiasedness in expectation. Contributions: (1) A differential compression protocol with adaptive, statistically controlled compression budgets that minimize communication while preserving convergence. (2) A theoretical framework providing nonasymptotic convergence rates for smooth nonconvex objectives with explicit dependence on compression error and adaptation strategy. (3) System-level strategies for practical deployment: lightweight per-parameter estimators, quantization primitives, and compatibility with asynchronous updates. Results: DC-SGD reduces communicated bits by up to 30× while matching or improving final accuracy compared to fixed quantization and error-feedback baselines across transformer pretraining, distributed ResNet training, and recommendation models. Impact: DC-SGD offers a principled, adaptive route to extreme communication compression with statistical guarantees, facilitating scalable distributed optimization in constrained-network environments frequented in ICLR settings.",ICLR,optimization,gpt-5-mini,True,10441,Towards Faster Decentralized Stochastic Optimization with Communication Compression,"Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings. In this regime, clients are restricted to transmitting small amounts of compressed information to their neighbors over a communication graph. Numerous endeavors have been made to address this challenging problem by developing algorithms with compressed communication for decentralized non-convex optimization problems. Despite considerable efforts, current theoretical understandings of the problem are still very limited, and existing algorithms all suffer from various limitations. In particular, these algorithms typically rely on strong, and often infeasible assumptions such as bounded data heterogeneity or require large batch access while failing to achieve linear speedup with the number of clients. In this paper, we introduce MoTEF, a novel approach that integrates communication compression with $\textbf{Mo}$mentum $\textbf{T}$racking and $\textbf{E}$rror $\textbf{F}$eedback. MoTEF is the first algorithm to achieve an asymptotic rate matching that of distributed SGD under arbitrary data heterogeneity, hence resolving a long-standing theoretical obstacle in decentralized optimization with compressed communication. We provide numerical experiments to validate our theoretical findings and confirm the practical superiority of MoTEF.",ICLR.cc/2025/Conference,6.6,True,0.8255,sgd adapts compression levels per parameter and per round solving constrained optimization that minimizes communication subject bound induced variance derive precise bounds linking compression distortion optimization error and monotone error feedback scheme that guarantees unbiasedness expectation theoretical providing nonasymptotic convergence rates for smooth nonconvex objectives explicit dependence compression error and adaptation strategy sgd reduces communicated bits while matching improving final compared fixed quantization and error feedback baselines across transformer pretraining distributed resnet training and recommendation models impact sgd offers principled adaptive route extreme communication compression statistical guarantees facilitating scalable distributed optimization constrained network environments frequented iclr settings,communication efficiency has garnered significant attention considered the main bottleneck for large scale decentralized machine learning applications distributed and federated settings numerous endeavors have been made address this challenging problem developing algorithms compressed communication for decentralized non convex optimization problems motef the first achieve asymptotic rate matching that distributed sgd under arbitrary data heterogeneity hence resolving long standing theoretical obstacle decentralized optimization compressed communication,2025-08-26T00:35:16.354503
46,MetaMomentum: Learning Robust Momentum Schedules via Reinforcement Optimization,"Motivation: Momentum hyperparameters critically affect optimization dynamics but are typically fixed or hand-tuned; learned momentum schedules could adapt to task-specific dynamics but pose stability and generalization challenges. Approach: We introduce MetaMomentum, a meta-learning approach that parameterizes momentum schedules as small neural controllers trained with reinforcement-style optimization to maximize downstream validation progress. The controller observes lightweight optimization statistics (gradient norms, curvature proxies, loss history) and outputs momentum multipliers per layer or globally. We stabilize meta-training with trust-region policy updates, variance-reduced reward baselines, and curriculum transfer from synthetic optimization tasks. Contributions: (1) A meta-learning algorithm that learns adaptive, history-aware momentum schedules tailored to model and data dynamics. (2) Theoretical analysis framing the controller update as an approximate policy gradient with bias control, yielding convergence to locally optimal scheduling policies under smoothness and ergodicity assumptions. (3) Robust meta-training recipes enabling generalization across architectures, datasets, and batch sizes. Results: MetaMomentum accelerates convergence and improves final metrics across CNNs, transformers, and GANs, reducing the need for momentum tuning and outperforming static and hand-annealed schedules; learned controllers transfer to unseen architectures with minimal fine-tuning. Impact: MetaMomentum automates a critical but under-explored aspect of optimizer design, enabling adaptive momentum that improves robustness and reduces hyperparameter burden in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,6895,Enhancing Optimizer Stability: Momentum Adaptation of NGN Step-size,"Modern optimization algorithms that incorporate momentum and adaptive step-size offer improved performance in various challenging Deep Learning tasks. However, their effectiveness is often highly sensitive to the choice of hyper-parameters, especially the learning rate.  Tuning these parameters is often difficult, resource-intensive, and time-consuming. State-of-the-art optimization algorithms incorporating momentum and adaptive step size are the algorithms of choice in several challenging Deep Learning domains. However, their effectiveness is frequently dependent on selecting the right hyper-parameters, especially the learning rate. Therefore, recent efforts have been directed toward enhancing the stability of optimizers across a wide range of hyper-parameter choices (Schaipp et al., 2024). In this paper, we introduce an algorithm that matches the performance of state-of-the-art optimizers while improving stability through a novel adaptation of the NGN step-size method (Orvieto & Xiao, 2024). Specifically, we propose a momentum-based version (NGN-M) that attains the standard convergence rate of $\mathcal{O}(1/\sqrt{K})$ under common assumptions, without the need for interpolation condition or assumptions of bounded stochastic gradients or iterates, in contrast to previous approaches. Additionally, we empirically demonstrate that the combination of the NGN step-size with momentum results in high robustness while delivering performance that is comparable to or surpasses other state-of-the-art optimizers.",ICLR.cc/2025/Conference,6.0,False,0.8524,motivation momentum hyperparameters critically affect optimization dynamics but are fixed hand tuned learned momentum schedules could adapt task specific dynamics but pose stability and generalization challenges metamomentum meta learning that parameterizes momentum schedules small neural controllers trained reinforcement style optimization maximize downstream validation progress the controller observes lightweight optimization statistics gradient norms curvature proxies loss history and outputs momentum multipliers per layer globally stabilize meta training trust region policy updates variance reduced reward baselines and curriculum transfer from synthetic optimization tasks metamomentum accelerates convergence and improves final metrics across cnns transformers and gans reducing the need for momentum tuning and outperforming static and hand annealed schedules learned controllers transfer unseen architectures minimal fine tuning impact metamomentum automates critical but under explored aspect optimizer enabling adaptive momentum that improves robustness and reduces hyperparameter burden large scale iclr experiments,modern optimization algorithms that incorporate momentum and adaptive step size offer improved various challenging deep learning tasks however their effectiveness often highly sensitive the choice hyper parameters the learning rate state the art optimization algorithms incorporating momentum and adaptive step size are the algorithms choice several challenging deep learning domains however their effectiveness frequently dependent selecting the right hyper parameters the learning rate this that matches the state the art optimizers while improving stability adaptation the ngn step size additionally empirically that the combination the ngn step size momentum high robustness while delivering that comparable surpasses other state the art optimizers,2025-08-26T00:35:16.354505
47,FairBilevel: Bilevel Optimization for Group-Fair Model Training with Theoretical Guarantees,"Motivation: Ensuring group fairness often involves objectives combining empirical risk and fairness constraints; naive methods lack optimization guarantees and can produce unstable trade-offs between accuracy and fairness. Approach: We propose FairBilevel, a bilevel optimization framework that treats fairness criteria (e.g., group-wise error) as outer-level objectives while training model parameters at the inner level. The outer problem adjusts group-specific importance weights to minimize validation fairness-aware loss; the inner solves a weighted empirical risk minimization. FairBilevel employs stochastic implicit gradients with a proximal outer step to handle nonconvexity and introduces a group-regularized inner solver to stabilize updates. Contributions: (1) A bilevel formulation that directly optimizes fairness-aware validation metrics via adaptive group-weighting, enabling principled trade-offs between accuracy and fairness. (2) Convergence theory: FairBilevel converges to approximate bilevel stationary points under smoothness and bounded-variance assumptions, with explicit bounds relating group heterogeneity and hypergradient estimation error. (3) Practical algorithms supporting partial participation, class imbalance adjustments, and scalable implicit-solver approximations. Results: FairBilevel improves worst-group accuracy and reduces disparity metrics on Waterbirds, CelebA, and synthetic imbalanced benchmarks, outperforming reweighting heuristics, adversarial debiasing, and standard bilevel baselines in both fairness and generalization. Impact: FairBilevel provides a principled, scalable optimization approach to enforce group fairness with theoretical assurances, advancing reliable deployment of equitable models in ICLR-relevant applications.",ICLR,optimization,gpt-5-mini,True,5269,Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation,"Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied. Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly. Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap. However, they either lack theoretical guarantees or suffer from heavy computational costs. To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective. Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle. Extensive experiments conducted using six large-scale RS backbone models on three publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness.",ICLR.cc/2025/Conference,6.6,True,0.8632,motivation ensuring group fairness often involves objectives combining empirical risk and fairness constraints naive methods lack optimization guarantees and can produce unstable trade offs between and fairness fairbilevel bilevel optimization that treats fairness criteria contributions bilevel formulation that directly optimizes fairness aware validation metrics adaptive group weighting enabling principled trade offs between and fairness fairbilevel improves worst group and reduces disparity metrics waterbirds celeba and synthetic imbalanced benchmarks outperforming reweighting heuristics adversarial debiasing and standard bilevel baselines both fairness and generalization impact fairbilevel provides principled scalable optimization enforce group fairness theoretical assurances advancing reliable deployment equitable models iclr relevant applications,group max min fairness mmf used fairness aware recommender systems optimization objective aims protect marginalized item groups and ensures fair competition platform overcome these limitations first theoretically that the mmf constrained objective can essentially reformulated group weighted optimization objective then efficient and effective named fairdual which utilizes dual optimization minimize jensen gap extensive experiments conducted six large scale backbone models three publicly available datasets that fairdual outperforms all baselines terms both and fairness,2025-08-26T00:35:16.354511
48,Optimal-Transport Preconditioned Gradient Flows for Deep Learning,"Motivation: Poorly conditioned loss landscapes slow optimization in deep networks and make hand-designed preconditioners brittle across architectures. Existing preconditioning methods focus on local second-order structure but neglect global distributional geometry of gradients. Approach: We propose OT-Precond, a preconditioning scheme derived from optimal-transport (OT) geometry of minibatch gradient distributions. OT-Precond models the evolution of gradient distributions as a gradient flow in Wasserstein space and constructs a transport-informed linear preconditioner that aligns update directions with dominant transport modes. To make this tractable, we estimate low-rank transport maps via sliced-Wasserstein sketches and compute an efficient per-layer linear transform implemented with diagonal-plus-low-rank factors. Contributions: (1) A novel conceptual link between OT gradient flows and preconditioning that yields transport-aware update directions; (2) a scalable estimator for low-rank transport structure compatible with minibatch stochasticity and memory constraints; (3) theoretical analysis establishing that OT-Precond reduces effective condition number under mild smoothness and distributional regularity assumptions. Results: Empirically, OT-Precond accelerates convergence on ResNet and transformer training, reducing epochs-to-target by 20–40% versus Adam and K-FAC baselines with similar wall-clock overhead. Ablations show robustness to batch size and improved generalization in low-data regimes. Impact: OT-Precond introduces distributional geometry into optimizer design, offering a principled and practical preconditioning alternative that adapts to evolving gradient distributions across ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,4059,Meta-Learning for Dynamic Synaptic Plasticity in Spiking Neural Networks,"Adaptive optimization algorithms, such as Adam Kingma & Ba (2015) and RM-SProp Tieleman & Hinton (2012), have become integral to training deep neu-ral networks, yet their stability properties and impact on generalization remain poorly understood Wilson et al. (2017). This paper extends linear stability anal-ysis to adaptive optimizers, providing a theoretical framework that explains their behavior in relation to loss surface geometry Wu et al. (2022); Jastrz˛ebski et al.(2019). We introduce a novel generalized coherence measure that quantifies the interaction between the adaptive preconditioner and the Hessian of the loss func-tion. This measure yields necessary and sufficient conditions for linear stability near stationary points, offering insights into why adaptive methods may converge to sharper minima with poorer generalization.
Our analysis leads to practical guidelines for hyperparameter tuning, demon-strating how to improve the generalization performance of adaptive optimizers. Through extensive experiments on benchmark datasets and architectures, includ-ing ResNet He et al. (2016) and Vision Transformers Dosovitskiy et al. (2020), we validate our theoretical predictions, showing that aligning the adaptive precon-ditioner with the loss surface geometry through careful parameter selection can narrow the generalization gap between adaptive methods and SGD Loshchilov & Hutter (2018).",ICLR.cc/2025/Conference,4.2,False,0.8403,motivation poorly conditioned loss landscapes slow optimization deep networks and make hand designed preconditioners brittle across architectures empirically precond accelerates convergence resnet and transformer training reducing epochs target versus adam and fac baselines similar wall clock overhead ablations robustness batch size and improved generalization low data regimes,adaptive optimization algorithms such adam kingma and sprop tieleman hinton have become integral training deep neu ral networks yet their stability properties and impact generalization remain poorly understood wilson and vision transformers dosovitskiy,2025-08-26T00:35:16.354513
49,Checkpointed Implicit Differentiation for Memory-Constrained Bilevel Optimization,"Motivation: Bilevel problems underpin hyperparameter tuning and meta-learning but computing hypergradients via implicit differentiation can be memory- and compute-intensive for long inner trajectories or large models, limiting applicability. Approach: We introduce CI-Diff, a checkpointed implicit differentiation framework that trades compute for memory by strategically checkpointing inner trajectory states and solving the sensitivity linear system with a memory-frugal recycled Krylov solver. CI-Diff couples a forward checkpoint schedule with randomized low-memory Jacobian-vector product estimators and a preconditioner built from coarse-grained curvature sketches to accelerate inner linear solves. Contributions: (1) A checkpointing strategy optimized for implicit hypergradient computation that minimizes memory while keeping hypergradient bias controlled; (2) a recycled Krylov solver using compressed curvature preconditioning compatible with stochastic gradients; (3) theoretical guarantees: CI-Diff bounds hypergradient approximation error in terms of checkpoint granularity, solver residual, and stochastic variance. Results: CI-Diff enables hyperparameter tuning and meta-training for transformers and large CNNs with up to 8× memory reduction compared to standard implicit methods, while matching validation improvements per GPU-hour. In few-shot meta-learning and architecture hyperparameter search, CI-Diff achieves comparable or better outer-objective progress than truncated unrolling at substantially lower memory cost. Impact: CI-Diff makes principled bilevel differentiation feasible under strict memory constraints, broadening practical meta-learning and automated tuning possibilities for ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,10632,Generalized Greedy Gradient-Based Hyperparameter Optimization,"Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization.  Approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps these difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. Additionally, we demonstrate that under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning and data hyper-cleaning tasks support our findings.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8108,few shot meta learning and hyperparameter search diff achieves comparable better outer objective progress than truncated unrolling lower memory cost,bilevel optimization blo widely used that has numerous applications including hyperparameter optimization meta learning reverse mode differentiation suffers from high memory requirements while the methods the implicit function theorem require the convergence the inner optimization approximations that consider truncated inner optimization trajectory suffer from short horizon bias accumulate the short horizon approximations from each step the inner optimization trajectory experimental few shot meta learning and data hyper cleaning tasks support our,2025-08-26T00:35:16.354515
50,Quasi-Monte Carlo SGD: Low-Discrepancy Sampling for Faster Stochastic Optimization,"Motivation: Stochastic gradient descent relies on random sampling which can exhibit high variance; quasi-Monte Carlo (QMC) sequences reduce integration error but their application to SGD and nonstationary data streams is underexplored. Approach: We present QMC-SGD, a hybrid optimization algorithm that replaces IID sampling with randomized low-discrepancy sequences adapted to minibatch data orderings. QMC-SGD combines randomized scrambled Sobol point selection with importance reweighting to accommodate nonuniform data distributions and uses drift-correction terms to handle nonstationary objectives. We analyze the effect of low-discrepancy sampling on gradient variance and convergence by deriving discrepancy-to-variance transfer bounds under Lipschitz and bounded-variation assumptions. Contributions: (1) A practical protocol to integrate QMC sampling into minibatch SGD at scale with randomized scrambling and per-epoch reshuffling; (2) theoretical results quantifying variance reduction and improved convergence rates in regimes where data integrands have bounded variation in the sense of Hardy–Krause; (3) methods to combine QMC-SGD with momentum and adaptive learning-rate schedules. Results: QMC-SGD reduces empirical gradient variance and achieves faster early-phase convergence on image classification and regression benchmarks, particularly in low-noise or small-batch regimes; it yields 10–30% fewer epochs to target loss versus standard SGD and maintains robustness under realistic data shuffles. Impact: QMC-SGD introduces deterministic-inspired sampling into stochastic optimization, offering a principled variance-reduction tool applicable to settings where sample efficiency matters in ICLR-scale experiments.",ICLR,optimization,gpt-5-mini,True,8445,"Why DP ""LOCAL"" SGD – Faster Convergence in Less Composition with Clipping Bias Reduction","We argue to apply Differentially-Private Local Stochastic Gradient Descent (DP-LSGD), a generalization of regular DP-SGD with per-sample local iterations, to systematically improve privacy-preserving machine learning. We prove and show the following facts in this paper: a). DP-LSGD with local iterations can produce more concentrated per-sample updates and therefore enables a more efficient exploitation of the clipping budget with a better utility-privacy tradeoff; b). given the same $T$ privacy composition or per-sample update aggregation, with properly-selected local iterations, DP-LSGD can converge faster in $O(1/T)$ to a small neighborhood of (local) optimum compared to $O(1/\sqrt{T})$ in regular DP-SGD, i.e., DP-LSGD produces the same accuracy while consumes less of the privacy budget. From an empirical side, thorough experiments are provided to support our developed theory and we show DP-LSGD produces the best-known performance in various practical deep learning tasks: for example with an $(\epsilon=4,\delta=10^{-5})$-DP guarantee, we successfully train ResNet20 from scratch with test accuracy $74.1\%, 86.5\%$ and $91.7\%$ on CIFAR10, SVHN and EMNIST, respectively. Our code is released in an anonymous GitHub link.",ICLR.cc/2025/Conference,4.25,nan,0.8182,qmc sgd hybrid optimization that replaces iid sampling randomized low discrepancy sequences adapted minibatch data orderings the effect low discrepancy sampling gradient variance and convergence deriving discrepancy variance transfer bounds under lipschitz and bounded variation assumptions qmc sgd reduces empirical gradient variance and achieves faster early phase convergence image classification and regression benchmarks low noise small batch regimes yields fewer epochs target loss versus standard sgd and maintains robustness under realistic data shuffles,argue apply differentially private local stochastic gradient descent lsgd generalization regular sgd per sample local iterations systematically improve privacy preserving machine learning from empirical side thorough experiments are provided support our developed theory and lsgd produces the best known various practical deep learning tasks for example epsilon delta guarantee train resnet20 from scratch and cifar10 svhn and emnist respectively,2025-08-26T00:35:16.354518
51,Probabilistic Quantization with Certified Convergence for Communication-Efficient Training,"Motivation: Aggressive quantization is essential for efficient distributed and edge training, but quantization bias and accumulation of numerical errors threaten convergence and often lack rigorous guarantees. Approach: We propose PQ-Cert, a probabilistic quantization framework that enforces unbiasedness in expectation while providing high-probability convergence certificates. PQ-Cert combines probability-matched stochastic rounding with adaptive per-coordinate variance control and an online calibration mechanism that adjusts quantization granularity based on observed compression-induced drift. We model quantization as additive stochastic perturbations and derive concentration inequalities bounding their cumulative effect. Contributions: (1) A quantization protocol with provable unbiasedness and tunable variance guarantees, suitable for synchronous and asynchronous SGD; (2) a convergence theory: under smooth nonconvex objectives PQ-Cert achieves O(1/√T) convergence with explicit high-probability bounds on optimization error as a function of quantization budget and calibration accuracy; (3) practical calibration algorithms to set per-layer and per-parameter quantization levels using streaming statistics. Results: PQ-Cert lowers communicated bits by up to 16× while preserving convergence and final accuracy on transformer pretraining and distributed CNN training, outperforming deterministic rounding, naive stochastic rounding, and fixed-bit error-feedback schemes in stability and communication-accuracy trade-offs. Impact: PQ-Cert provides rigorous foundations and practical tools for safe, high-compression training, enabling deployment of large models in constrained communication environments central to many ICLR applications.",ICLR,optimization,gpt-5-mini,True,8435,Neural Network Adaptive Quantization based on Bayesian Deep Learning,"We propose a novel approach to solve the adaptive quantization problem in neural networks based on epistemic uncertainty analysis. The quantized model is treated as a Bayesian neural network with stochastic weights, where the mean values are employed to estimate the corresponding weights. Standard deviations serve as an indicator of uncertainty and the number of corresponding bits — i.e., a larger number of bits indicate lower uncertainty, and vice versa. We perform an extensive analysis of several algorithms within a novel framework for different convolutional and fully connected neural networks based on open datasets demonstrating the main advantages of the proposed approach. In particular, we introduce two novel algorithms for mixed-precision quantization. Quantile Inform utilizes uncertainty to allocate bit-width across layers, while Random Bits employs stochastic gradient-based optimization techniques to maximize the full likelihood of quantization. Using our approach, we reduce the average bit-width of the VGG-16 model to 3.05 with the 90.5% accuracy on the CIFAR-10 dataset compared to 91.9% for the non-quantized model. For the LeNet model trained on the MNIST dataset, we reduce the average bit-width to 3.16 and achieve 99.0% accuracy, almost equal to 99.2% for the non-quantized model.",ICLR.cc/2025/Conference,4.0,False,0.8156,contributions quantization protocol provable unbiasedness and tunable variance guarantees suitable for synchronous and asynchronous sgd convergence theory under smooth nonconvex objectives cert achieves convergence explicit high probability bounds optimization error function quantization budget and calibration practical calibration algorithms set per layer and per parameter quantization levels streaming statistics cert lowers communicated bits while preserving convergence and final transformer pretraining and distributed cnn training outperforming deterministic rounding naive stochastic rounding and fixed bit error feedback schemes stability and communication accuracy trade offs,solve the adaptive quantization problem neural networks epistemic uncertainty analysis the quantized treated bayesian neural network stochastic weights where the mean values are employed estimate the corresponding weights perform extensive analysis several algorithms within for different convolutional and fully connected neural networks open datasets demonstrating the main advantages the proposed quantile inform utilizes uncertainty allocate bit width across layers while random bits employs stochastic gradient based optimization techniques maximize the full likelihood quantization,2025-08-26T00:35:16.354521
52,Causal Curriculum Optimization: Learning Schedules by Estimating Sample-Level Influence,"Motivation: Curriculum learning aims to order training samples to accelerate learning, but heuristics (loss-based, difficulty-based) ignore causal relationships and may reinforce spurious correlations. Approach: We introduce Causal Curriculum Optimization (CCO), a curriculum scheduler that estimates each sample’s causal influence on model generalization using counterfactual gradient interventions. CCO performs lightweight influence estimation by computing influence functions approximated via Hessian-vector products and randomized perturbations, then schedules samples to maximize expected validation improvement subject to exploration constraints. We ensure scalability by clustering similar samples and sharing influence estimates, and we incorporate adaptive smoothing to control estimation variance. Contributions: (1) A principled curriculum framework grounded in causal influence estimation that prioritizes samples with high expected marginal benefit for generalization; (2) scalable estimators for sample influence using randomized linear algebra and clustering, with error bounds linking estimator variance to scheduling regret; (3) theoretical analysis connecting scheduling decisions to validation improvement with sublinear regret guarantees under stochastic assumptions. Results: CCO accelerates convergence and improves held-out accuracy on domain-shifted vision tasks, noisy-label benchmarks, and active learning scenarios, outperforming loss-based and self-paced curricula in sample efficiency and robustness to spurious features. Impact: CCO offers a causally informed, scalable approach to curriculum design that enhances generalization and reliability for large-scale ICLR problems where data heterogeneity and spurious correlations are prevalent.",ICLR,optimization,gpt-5-mini,True,10701,Safety-Prioritizing Curricula for Constrained Reinforcement Learning,"Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. 
Although existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must adhere to safety constraints.
Thus, these approaches may generate tasks that cause RL agents to violate safety constraints during training and behave suboptimally after. 
We develop a safe curriculum generation approach (SCG) that aligns the objectives of constrained RL and curriculum learning: improving safety during training and boosting sample efficiency.
SCG generates sequences of tasks where the RL agent can be safe and performant by initially generating tasks with minimum safety violations over high-reward ones.
We empirically show that compared to the state-of-the-art curriculum learning approaches and their naively modified safe versions, SCG achieves optimal performance and the lowest amount of constraint violations during training.",ICLR.cc/2025/Conference,5.25,True,0.8265,motivation curriculum learning aims order training samples accelerate learning but heuristics loss based difficulty based ignore causal relationships and may reinforce spurious correlations causal curriculum optimization cco curriculum scheduler that estimates each sample causal influence generalization counterfactual gradient interventions ensure scalability clustering similar samples and sharing influence estimates and incorporate adaptive smoothing control estimation variance cco accelerates convergence and improves held out domain shifted vision tasks noisy label benchmarks and active learning scenarios outperforming loss based and self paced curricula sample efficiency and robustness spurious features,curriculum learning aims accelerate reinforcement learning generating curricula although existing curriculum generation approaches provide benefits sample efficiency they overlook safety critical settings where agent must adhere safety constraints safe curriculum generation scg that aligns the objectives constrained and curriculum learning improving safety during training and boosting sample efficiency empirically that compared the state the art curriculum learning approaches and their naively modified safe versions scg achieves optimal and the lowest amount constraint violations during training,2025-08-26T00:35:16.354527
53,Riemannian Adam: Adaptive Optimization on Manifolds with Convergence Guarantees,"Motivation: Many learning tasks impose manifold constraints (orthogonality, PSD, low-rank), yet popular adaptive optimizers like Adam lack principled Riemannian extensions with convergence guarantees, limiting performance on constrained models. Approach: We develop RAdam, a Riemannian adaptive optimizer that generalizes Adam’s moment estimates to tangent spaces with geometry-aware retractions and vector transport. RAdam constructs per-coordinate adaptive preconditioners using canonical metrics for common manifolds, applies adaptive bias-correction in manifold charts, and uses a manifold-aware damping strategy to ensure stability. We analyze RAdam under geodesic smoothness and bounded stochastic tangent noise. Contributions: (1) The RAdam algorithmic framework supporting a broad class of Riemannian manifolds via metric-aware adaptation; (2) theoretical convergence: nonasymptotic guarantees to first-order stationary points on manifolds under standard smoothness and bounded variance assumptions, including conditions ensuring monotone descent; (3) practical engineering: efficient implementations for Stiefel, sphere, and fixed-rank manifolds using QR/Cayley retractions and low-overhead transport. Results: RAdam improves convergence speed and final objective on orthogonality-constrained CNNs, low-rank matrix factorization models, and SPD-layer networks versus Riemannian SGD and naive Adam-on-embedded-parameters, often reducing epochs-to-target by 2×. Impact: RAdam equips constrained deep learning with a scalable, theoretically grounded adaptive optimizer, expanding practical optimization tools for structured models relevant to ICLR.",ICLR,optimization,gpt-5-mini,True,8283,Efficient optimization with orthogonality constraint: a randomized Riemannian submanifold method,"Optimization with orthogonality constraints frequently arise in various fields such as machine learning, signal processing and computer vision. Riemannian optimization offers a powerful framework for solving these problems by equipping the constraint set with a Riemannian manifold structure and performing optimization intrinsically on the manifold. This approach typically involves computing a search direction in the tangent space and updating variables via a retraction operation. However, as the size of the variables increases, the computational cost of the retraction can become prohibitively high, limiting the applicability of Riemannian optimization to large-scale problems.  To address this challenge and enhance scalability, we propose a novel approach that restricts each update on a random submanifold, thereby significantly reducing the per-iteration complexity. We introduce two sampling strategies for selecting the random submanifold and theoretically analyze the convergence of the proposed method. We provide convergence results for general nonconvex functions and functions that satisfy Riemannian Polyak–Łojasiewicz condition as well as for stochastic optimization settings. Extensive experiments verify the benefits of the proposed method, showcasing its effectiveness across a wide variety of problem instances.",ICLR.cc/2025/Conference,6.0,False,0.8423,motivation many learning tasks impose manifold constraints orthogonality psd low rank yet popular adaptive optimizers like adam lack principled riemannian extensions convergence guarantees limiting constrained models impact radam equips constrained deep learning scalable theoretically grounded adaptive optimizer expanding practical optimization tools for structured models relevant iclr,optimization orthogonality constraints frequently arise various fields such machine learning signal processing and computer vision riemannian optimization offers powerful for solving these problems equipping the constraint set riemannian manifold structure and performing optimization intrinsically the manifold however the size the variables increases the computational cost the retraction can become prohibitively high limiting the applicability riemannian optimization large scale problems provide convergence for general nonconvex functions and functions that satisfy riemannian polyak łojasiewicz condition well for stochastic optimization settings,2025-08-26T00:35:16.354531
54,Graph-Structured Optimizers: Topology-Aware Updates for Graph Neural Networks,"Motivation: Graph Neural Networks (GNNs) pose optimization challenges due to message-passing coupling and irregular computation graphs; standard optimizers ignore input graph structure, resulting in inefficiencies and over-smoothing. Approach: We introduce GraphOpt, an optimizer that adapts parameter updates based on graph topology and node-centric signal propagation characteristics. GraphOpt estimates localized smoothness and mixing rates via randomized localized probes and uses these estimates to apply spatially varying step sizes, diffusion-aware momentum, and regularization that counteracts oversmoothing. Updates are computed with per-edge or per-node scaling factors implemented via sparse operations compatible with mini-batching. Contributions: (1) A topology-aware optimization strategy that leverages graph spectral and local mixing information to modulate updates; (2) theoretical insights: GraphOpt reduces effective propagation-induced gradient bias and improves stability bounds in message-passing layers under mixing assumptions; (3) practical techniques: online estimation of mixing rates, scalable sparse update kernels, and compatibility with sampling approaches (neighbor sampling, subgraph sampling). Results: GraphOpt accelerates convergence and improves generalization on large-scale node classification, graph regression, and molecular property tasks, mitigating oversmoothing and reducing depth-related degradation compared to Adam and SGD baselines. Impact: GraphOpt aligns optimizer behavior with graph structure, offering principled improvements for training deep GNNs at scale, an important class of problems in the ICLR community.",ICLR,optimization,gpt-5-mini,True,9000,Graph Neural Network Is A Mean Field Game,"In current graph neural networks (GNNs), it is a common practice to apply a pre-defined message passing heuristics to all graph data, even though the stereotypical relational inductive bias (e.g., graph heat diffusion) might not fit the unseen graph topology. Such gross simplification might be responsible for the lack of an in-depth understanding of graph learning principles, which challenges us to push the boundary from crafting application-specific GNNs to embracing a ""meta-learning"" paradigm. In this work, we ratchet the gear of GNN another notch forward by formulating GNN as a *mean field game*, that is, the best learning outcome occurs at the *Nash*-equilibrium when the learned graph inference rationale allows each graph node to find what is the best feature representations for not only the individual node but also the entire graph. Following this spirit, we formulate the search for novel GNN mechanism into a variational framework of *mean-field control* (MFC) problem, where the optimal relational inductive bias is essentially the critical point of mean-field information dynamics. Specifically, we seek for the best characteristic MFC functions of transportation mobility (controlling information exchange throughout the graph) and reaction mobility (controlling feature representation learning on each node), on the fly, that uncover the most suitable learning mechanism for a GNN instance by solving an MFC variational problem through the lens of *Hamiltonian flows* (formed in partial differential equations). In this context, our variational framework brings together existing GNN models into various mean-field games with distinct equilibrium states, each characterized by a unique MFC functional. Furthermore, we present an agnostic end-to-end deep model, coined *Nash-GNN* (in honor of Nobel laureate Dr. John Nash), to jointly carve the nature of the inductive bias and fine-tune the GNN hyper-parameters on top of the elucidated learning mechanism. *Nash-GNN* has achieved SOTA performance on diverse graph data including popular benchmark datasets and human connectomes. More importantly, the mathematical insight of mean-field games provides a new window to understand the foundational principles of graph learning as an interactive dynamical system, which allows us to reshape the idea of designing next-generation GNN models.",ICLR.cc/2025/Conference,5.333333333333333,nan,0.8646,motivation graph neural networks gnns pose optimization challenges due message passing coupling and irregular computation graphs standard optimizers ignore input graph structure resulting inefficiencies and over smoothing contributions topology aware optimization strategy that leverages graph spectral and local mixing information modulate updates theoretical insights graphopt reduces effective propagation induced gradient bias and improves stability bounds message passing layers under mixing assumptions practical techniques online estimation mixing rates scalable sparse update kernels and compatibility sampling approaches neighbor sampling subgraph sampling impact graphopt aligns optimizer behavior graph structure offering principled improvements for training deep gnns scale important class problems the iclr community,current graph neural networks gnns common practice apply pre defined message passing heuristics all graph data even though the stereotypical relational inductive bias such gross simplification might responsible for the lack depth understanding graph learning principles which challenges push the boundary from crafting application specific gnns embracing meta learning paradigm this ratchet the gear gnn another notch forward formulating gnn mean field game that the best learning outcome occurs the nash equilibrium when the learned graph inference rationale allows each graph node find what the best feature representations for not only the individual node but also the entire graph seek for the best characteristic mfc functions transportation mobility controlling information exchange throughout the graph and reaction mobility controlling feature representation learning each node the fly that uncover the most suitable learning mechanism for gnn instance solving mfc variational problem the lens hamiltonian flows formed partial differential equations furthermore agnostic end end deep coined nash gnn honor nobel laureate john nash jointly carve the nature the inductive bias and fine tune the gnn hyper parameters top the elucidated learning mechanism more importantly the mathematical insight mean field games provides window understand the foundational principles graph learning interactive dynamical which allows reshape the idea designing next generation gnn models,2025-08-26T00:35:16.354538
55,Exploration-Regularized Optimizers for Reinforcement Learning Policy Training,"Motivation: Policy optimization in reinforcement learning (RL) often suffers from premature convergence to suboptimal deterministic policies due to insufficient exploration; standard optimizers do not explicitly incorporate exploration incentives in parameter updates. Approach: We propose ER-Opt, an optimizer that augments gradient updates with exploration-regularized perturbations derived from intrinsic reward estimators. ER-Opt integrates an intrinsic-value signal into the optimization objective, computes a target exploration gradient via perturbation-based estimation, and blends it with policy gradient estimates using an adaptive blending factor that depends on exploration sufficiency metrics. We prove that under mild ergodicity and bounded variance assumptions, ER-Opt maintains unbiasedness of expected policy gradients while increasing exploration in low-coverage regimes. Contributions: (1) A novel optimizer design that fuses intrinsic-exploration signals with stochastic gradients to guide parameter updates toward informative behaviors; (2) theoretical guarantees: ER-Opt preserves convergence to stationary points of the regularized objective and provides bounds on coverage improvement rates; (3) practical algorithms: low-variance intrinsic estimators, adaptive blending schedules, and compatibility with off-policy correction. Results: ER-Opt improves sample efficiency and final performance on sparse-reward Mujoco and Atari tasks, accelerating discovery of rewarding behaviors compared to vanilla policy gradient, PPO, and curiosity-driven baselines. Impact: ER-Opt provides a principled route to embed exploration incentives directly into optimization, enhancing RL training dynamics and offering a new lever for tackling hard-exploration problems relevant to ICLR research.",ICLR,optimization,gpt-5-mini,True,85,Goal Achievement Guided Exploration: Mitigating Premature Convergence in Reinforcement Learning,"Premature convergence to suboptimal policies remains a significant challenge in reinforcement learning (RL), particularly in tasks with sparse rewards or non-convex reward landscapes. Existing work usually utilizes reward shaping, such as curiosity-based internal rewards, to encourage exploring promising spaces. However, this may inadvertently introduce new local optima and impair the optimization for the actual target reward. To address this issue, we propose Goal Achievement Guided Exploration (GAGE), a novel approach that incorporates an agent's goal achievement as a dynamic criterion for balancing exploration and exploitation. GAGE adaptively adjusts the exploitation level based on the agent's current performance relative to an estimated optimal performance, thereby mitigating premature convergence. Extensive evaluations demonstrate that GAGE substantially improves learning outcomes across various challenging tasks by adapting convergence based on task success. Applicable to both continuous and discrete tasks, GAGE seamlessly integrates into existing RL frameworks, highlighting its potential as a versatile tool for enhancing exploration strategies in RL.",ICLR.cc/2025/Conference,5.5,False,0.8546,motivation policy optimization reinforcement learning often suffers from premature convergence suboptimal deterministic policies due insufficient exploration standard optimizers not explicitly incorporate exploration incentives parameter updates opt integrates intrinsic value signal into the optimization objective computes target exploration gradient perturbation based estimation and blends policy gradient estimates adaptive blending factor that depends exploration sufficiency metrics,premature convergence suboptimal policies remains significant challenge reinforcement learning tasks sparse rewards non convex reward landscapes however this may inadvertently local optima and impair the optimization for the actual target reward extensive evaluations that gage improves learning outcomes across various challenging tasks adapting convergence task success,2025-08-26T00:35:16.354539
56,BayTR-NAS: Scalable Bayesian Trust-Region Optimization for Neural Architecture Search,"Motivation: Neural architecture search (NAS) demands exploration of vast, high-dimensional design spaces with expensive evaluations. Pure Bayesian optimization provides global sample efficiency but struggles with scalability and noisy training metrics; trust-region methods scale locally but risk poor global exploration. Approach: We introduce BayTR-NAS, a hybrid NAS optimizer that marries Bayesian surrogate modeling with adaptive trust regions. BayTR-NAS maintains an ensemble of cheap surrogate models (heteroscedastic Gaussian processes and neural predictors) to estimate uncertainty, and uses an acquisition function modulated by trust-region radii that restrict proposals to promising local neighborhoods. Trust regions adapt based on surrogate fidelity and observed evaluation noise; global exploration is preserved by occasional large-radius proposals and Pareto-style surrogate selection. Contributions: (1) A principled BayTR framework integrating ensembling, heteroscedastic uncertainty, and trust-region control for NAS. (2) Theoretical characterization of a surrogate-fidelity-driven trust update rule that balances local refinement and global exploration under bounded surrogate error. (3) Practical mechanisms—bandwidth-aware kernels, early-stopping surrogate predictions, and transfer priors—that enable scalable operation on modern architecture search spaces. Results: On standard NAS benchmarks (CIFAR-10 search space, NAS-Bench-201) and a transformer cell search for language modeling, BayTR-NAS finds architectures with superior validation accuracy per compute budget compared to BO, regularized evolution, and local-search baselines, reducing wall-clock evaluation cost by 30–50%. Impact: BayTR-NAS offers a scalable, uncertainty-aware optimization paradigm for NAS, enabling efficient architecture discovery in high-dimensional, noisy evaluation regimes common in ICLR-scale research.",ICLR,optimization,gpt-5-mini,True,1316,POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator,"Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.
Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.
This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.
Results were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.",ICLR.cc/2025/Conference,4.4,False,0.8609,motivation neural search nas demands exploration vast high dimensional spaces expensive evaluations pure bayesian optimization provides global sample efficiency but struggles scalability and noisy training metrics trust region methods scale locally but risk poor global exploration baytr nas maintains ensemble cheap surrogate models heteroscedastic gaussian processes and neural predictors estimate uncertainty and uses acquisition function modulated trust region radii that restrict proposals promising local neighborhoods practical mechanisms bandwidth aware kernels early stopping surrogate predictions and transfer priors that enable scalable operation modern search spaces standard nas benchmarks cifar search space nas bench and transformer cell search for language modeling baytr nas finds architectures superior validation per compute budget compared regularized evolution and local search baselines reducing wall clock evaluation cost impact baytr nas offers scalable uncertainty aware optimization paradigm for nas enabling efficient discovery high dimensional noisy evaluation regimes common iclr scale,neural search nas automates the neural network architectures minimising dependence human expertise and iterative experimentation recently transferable neural search transferable nas has emerged generalising the search process from being dataset dependent task dependent this diffusion based streamlines computation generating architectures optimised for unseen datasets the need for further adaptation this introduces the pareto optimal many objective neural generator pomonag extending diffusionnag many objective diffusion process pomonag optimisation enhanced expanding its training meta dataset applying pareto front filtering generated architectures and refining embeddings for conditional generation were validated two distinct search spaces nasbench201 and mobilenetv3 and evaluated across image classification datasets,2025-08-26T00:35:16.354542
57,BlockFisher: Scalable Block-Diagonal Fisher Preconditioning for Transformer Optimization,"Motivation: Transformers exhibit slow convergence due to ill-conditioned curvature and parameter heterogeneity. Full natural-gradient methods are effective but computationally prohibitive; existing approximations (K-FAC) require fragile factorization heuristics and often do not scale to modern models. Approach: We propose BlockFisher, a block-diagonal Fisher preconditioner that leverages transformer modularity to construct efficient curvature approximations per attention-head and feedforward block. BlockFisher computes online low-rank approximations of expected outer-products via streaming sketches and applies a streaming diagonal correction to stabilize inverses. Crucially, it exploits approximate Kronecker and structured sparsity in multi-head attention to reduce memory and computation, and integrates with mixed-precision arithmetic. Contributions: (1) A block-wise Fisher estimation scheme tailored to transformer architecture that is amenable to minibatch stochasticity and streaming updates. (2) A rigorous damping and diagonal-correction strategy with provable condition-number reduction under mild assumptions on gradient second moments. (3) An implementation blueprint that achieves near-linear computational scaling with controlled memory footprints. Results: On large language model finetuning and transformer pretraining proxies, BlockFisher accelerates convergence up to 2× in iterations and yields improved perplexity within comparable wall-clock time versus AdamW and K-FAC variants; it demonstrates robustness across batch sizes and sequence lengths. Impact: BlockFisher provides a practical second-order preconditioning approach tailored to modern transformer designs, enabling more efficient optimization for large-scale language and vision transformers in ICLR-relevant workflows.",ICLR,optimization,gpt-5-mini,True,8698,DenseAttention: No-Compromise Exact All $N \times N$  Interactions Algorithm with $O(N)$ Space and Time Complexity,"The ubiquitous Transformer architecture suffers from two main bottlenecks: 1) low computational and memory efficiency, leading to suboptimal hardware utilization, and 2) quadratic time complexity with respect to sequence length $N$, making it slow and costly for large data contexts. We propose a novel DenseAttention Network architecture, a straightforward simplification of the standard Transformer block that addresses these issues and serves as a drop-in replacement for language modeling tasks. We eliminate memory-bound components in DenseAttention, including Softmax, masking, one skip connection, and both LayerNorms, as well as key, value, and output projection matrices, as they become redundant. Despite these removals, it maintains exact $N \times N$ pairwise interactions between tokens. By exploiting the associativity of matrix multiplications, DenseAttention can be computed with $O(N^2d)$ or $O(Nd^2)$ time and space complexity, depending on the context. To handle the absence of Softmax and prevent numerical instability, we introduce MaxNormActivation at both ends of the Transformer block. We also devise Cosine Relative Positional Embeddings as a computationally efficient replacement for RoPE, and simple LocalAttention variations of the block to help the model focus on details in extremely long contexts. 

DenseAttention competes with FlashAttention in speed on small sequences and outperforms it by orders of magnitude on large contexts. We pre-train encoder language models on sequences up to 16K in length, which perform similarly or better than baseline BERT-large, while significantly improving speed and efficiency.  Finally, we achieve state-of-the-art on the LRA benchmark among the Transformer-based architectures.",ICLR.cc/2025/Conference,4.5,False,0.8125,blockfisher block diagonal fisher preconditioner that leverages transformer modularity construct efficient curvature approximations per attention head and feedforward block crucially exploits approximate kronecker and structured sparsity multi head attention reduce memory and computation and integrates mixed precision arithmetic contributions block wise fisher estimation scheme tailored transformer that amenable minibatch stochasticity and streaming updates large language finetuning and transformer pretraining proxies blockfisher accelerates convergence iterations and yields improved perplexity within comparable wall clock time versus adamw and fac variants demonstrates robustness across batch sizes and sequence lengths impact blockfisher provides practical second order preconditioning tailored modern transformer designs enabling more efficient optimization for large scale language and vision transformers iclr relevant workflows,the ubiquitous transformer suffers from two main bottlenecks low computational and memory efficiency leading suboptimal hardware utilization and quadratic time complexity respect sequence length making slow and costly for large data contexts denseattention network straightforward simplification the standard transformer block that addresses these issues and serves drop replacement for language modeling tasks handle the absence softmax and prevent numerical instability maxnormactivation both ends the transformer block pre train encoder language models sequences 16k length which perform similarly better than bert large while improving speed and efficiency,2025-08-26T00:35:16.354546
58,Wasserstein Bilevel Robustness: Efficient Nested DRO for Model Certifiable Generalization,"Motivation: Distributionally robust optimization (DRO) enhances worst-case performance but is computationally intensive when nested over complex perturbation sets; naive DRO can be overly pessimistic and hard to scale for deep models. Approach: We introduce Wasserstein Bilevel Robustness (WBR), a bilevel DRO formulation where an inner adversary selects perturbations within a Wasserstein ball while the outer learner minimizes the worst-case risk. WBR approximates the inner maximization with entropic regularization and differentiable Sinkhorn iterations, enabling implicit differentiation through the inner problem and stochastic bilevel updates. We further design an adaptive radius scheduler driven by validation risk to avoid undue conservatism. Contributions: (1) A tractable bilevel DRO framework leveraging entropic smoothing and differentiable optimal transport solvers for scalable robustness training. (2) Theoretical results linking entropic regularization strength to certified robustness bounds and providing convergence guarantees for stochastic implicit-gradient updates under smoothness and bounded-variance assumptions. (3) Practical algorithms: minibatch-friendly Sinkhorn solvers, variance-reduced hypergradient estimates, and adaptive radius control for balanced robustness-generalization trade-offs. Results: WBR yields improved worst-case validation accuracy and tighter certified robustness radii on image classification under covariate shifts and synthetic distributional attacks, outperforming classical DRO, adversarial training, and regularized ERM in both robustness and utility. Impact: WBR makes principled, certificate-aware DRO practical for deep learning, offering deployable robustness guarantees for ICLR-scale models under realistic distributional uncertainty.",ICLR,optimization,gpt-5-mini,True,6913,Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization,"Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense.Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.",ICLR.cc/2025/Conference,6.5,True,0.8872,motivation distributionally robust optimization dro enhances worst case but computationally intensive when nested over complex perturbation sets naive dro can overly pessimistic and hard scale for deep models wasserstein bilevel robustness wbr bilevel dro formulation where inner adversary selects perturbations within wasserstein ball while the outer learner minimizes the worst case risk contributions tractable bilevel dro leveraging entropic smoothing and differentiable optimal transport solvers for scalable robustness training theoretical linking entropic regularization strength certified robustness bounds and providing convergence guarantees for stochastic implicit gradient updates under smoothness and bounded variance assumptions wbr yields improved worst case validation and tighter certified robustness radii image classification under covariate shifts and synthetic distributional attacks outperforming classical dro adversarial training and regularized erm both robustness and utility impact wbr makes principled certificate aware dro practical for deep learning offering deployable robustness guarantees for iclr scale models under realistic distributional uncertainty,wasserstein distributionally robust optimization wdro optimizes against worst case distributional shifts within specified uncertainty set leading enhanced generalization unseen adversarial examples compared standard adversarial training which focuses pointwise adversarial perturbations address this gap proposing robust optimization under uncertainty set for adversarial noise wasserstein distance and statistical error kullback leibler divergence called the statistically robust wdro establish robust generalization bound for the optimization implying that out distribution adversarial least good the statistically robust training loss high probability finally extensive experiments that our mitigates robust overfitting and enhances robustness within the wdro,2025-08-26T00:35:16.354548
59,CodedFed: Straggler-Resilient Federated Optimization via Lightweight Gradient Coding,"Motivation: Federated learning faces ubiquitous stragglers and unreliable participation, which prolong rounds and degrade convergence in heterogeneous networks. Traditional redundancy or synchronous waits reduce efficiency or privacy. Approach: We propose CodedFed, a federated optimizer that applies lightweight gradient coding across client groups to tolerate stragglers while preserving privacy and communication efficiency. Clients compute linear encodings of local updates using random sparse coding matrices; the server decodes aggregated gradients from any sufficiently large subset of responses, enabling continued progress without waiting for slow clients. Codes are designed to be privacy-preserving (randomized masks) and amenable to partial participation. Contributions: (1) A federated coding protocol enabling straggler resilience with sublinear additional computation on clients and server decoding complexity proportional to group size. (2) Analysis showing that coded aggregation retains unbiasedness in expectation and that decoding errors can be bounded with high probability under bounded client variance. (3) System-level integration strategies: adaptive group formation, overlap-aware coding to maintain fairness, and compression-compatible encodings. Results: In simulations and real-world federated setups with heterogeneous mobile clients and variable connectivity, CodedFed reduces stalled rounds by up to 70% and improves convergence time by 1.5–3× compared to standard FedAvg and asynchronous baselines, while preserving per-client privacy through randomized masking. Impact: CodedFed provides a practical, theoretically grounded approach to mitigate stragglers in federated optimization, improving efficiency and robustness in real deployments central to ICLR themes like edge learning and privacy.",ICLR,optimization,gpt-5-mini,True,11020,Overcoming Catastrophic Forgetting in Federated Class-Incremental Learning via Federated Global Twin Generator,"Federated Class-Incremental Learning (FCIL) increasingly becomes essential in the decentralized setting, where it enables multiple participants to collaboratively train a global model to perform well on a sequence of tasks without sharing their private data. In FCIL, conventional Federated Learning algorithms such as FedAvg often suffer from catastrophic forgetting, resulting in significant performance declines on earlier tasks. Recent works based on generative models produce synthetic images to help mitigate this issue across all classes. However, these approaches' testing accuracy in previous classes is still much lower than recent classes, i.e., having better plasticity than stability. To overcome these issues, this paper presents Federated Global Twin Generator (FedGTG), an FCIL framework that exploits generative-model training on the global side without accessing client data. Specifically, the server trains a data generator and a feature generator to create two types of information from all seen classes. Then, it sends the synthetic data to the client. The clients then use feature-direction-controlling losses to make the local models retain knowledge and learn new tasks well. We extensively analyze the robustness of FedGTG on natural images and its ability to converge to flat local minima and achieve better predicting confidence (calibration). Experimental results on CIFAR-10, CIFAR-100, and tiny-ImageNet demonstrate the improvements in accuracy and forgetting measures of FedGTG as well as the robustness of domain shifts compared to previous frameworks.",ICLR.cc/2025/Conference,5.0,False,0.8641,motivation federated learning faces ubiquitous stragglers and unreliable participation which prolong rounds and degrade convergence heterogeneous networks impact codedfed provides practical theoretically grounded mitigate stragglers federated optimization improving efficiency and robustness real deployments central iclr themes like edge learning and privacy,federated class incremental learning fcil increasingly becomes essential the decentralized setting where enables multiple participants collaboratively train global perform well sequence tasks sharing their private data fcil conventional federated learning algorithms such fedavg often suffer from catastrophic forgetting resulting significant declines earlier tasks the server trains data generator and feature generator create two types information from all seen classes the clients then use feature direction controlling losses make the local models retain knowledge and learn tasks well extensively the robustness fedgtg natural images and its ability converge flat local minima and achieve better predicting confidence calibration experimental cifar cifar and tiny imagenet the improvements and forgetting measures fedgtg well the robustness domain shifts compared previous frameworks,2025-08-26T00:35:16.354556
60,ConvexProxy: Learning Differentiable Convex Surrogates for Constrained Deep Optimization,"Motivation: Training deep models subject to complex constraints (safety, resource budgets, logical rules) is challenging because constraints are often nonconvex or non-differentiable, impeding reliable optimization. Approach: We introduce ConvexProxy, a meta-optimization pipeline that learns smooth, convex surrogate constraints from data and oracle evaluations. ConvexProxy fits a parameterized convex surrogate family via constrained regression to approximate feasibility sets, then uses these surrogates in a projected-stochastic-gradient framework. Surrogates are trained with Lipschitz and curvature regularization to ensure stable projections and are periodically refined with counterexample-guided sampling. Contributions: (1) A methodology to construct differentiable convex proxies that approximate complex constraints while enabling efficient projection and optimization. (2) Theoretical guarantees: under approximation error bounds, projected SGD on surrogates converges to points that approximately satisfy original constraints with quantified feasibility violation. (3) Practical procedures for surrogate training, counterexample generation, and low-overhead projection implementation in minibatch settings. Results: ConvexProxy enforces safety constraints in RL control, resource budgets in model compression, and fairness constraints in classification, achieving high feasibility with less computational cost than exact projection or penalty methods, and often improving downstream utility versus heuristic approximations. Impact: By learning tractable convex surrogates, ConvexProxy expands the scope of constrained deep optimization, making disciplined enforcement of complex constraints practical for ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,1602,Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning,"In practical applications, the underlying constraint knowledge is often unknown and difficult to specify. To address this issue, recent advances in Inverse Constrained Reinforcement Learning (ICRL) have focused on inferring these constraints from expert demonstrations. However, the ICRL approach typically characterizes constraint learning as a tri-level optimization problem, which is inherently complex due to its interdependent variables and multiple layers of optimization.
Considering these challenges, a critical question arises: *Can we implicitly embed constraint signals into reward functions and effectively solve this problem using a classic reward inference algorithm?* The resulting method, known as Inverse Reward Correction (IRC), merits investigation. In this work, we conduct a theoretical analysis comparing the sample complexities of both solvers. Our findings confirm that the IRC solver achieves lower sample complexity than its ICRL counterpart.
Nevertheless, this reduction in complexity comes at the expense of generalizability. Specifically, in the target environment, the reward correction terms may fail to guarantee the safety of the resulting policy, whereas this issue can be effectively mitigated by transferring the constraints via the ICRL solver. 
Advancing our inquiry, we investigate conditions under which the ICRL solver ensures $\epsilon$-optimality when transferring to new environments. Empirical results across various environments validate our theoretical findings, underscoring the nuanced trade-offs between complexity reduction and generalizability in safety-critical applications.",ICLR.cc/2025/Conference,5.75,True,0.8315,motivation training deep models subject complex constraints safety resource budgets logical rules challenging because constraints are often nonconvex non differentiable impeding reliable optimization contributions methodology construct differentiable convex proxies that approximate complex constraints while enabling efficient projection and optimization convexproxy enforces safety constraints control resource budgets compression and fairness constraints classification achieving high feasibility less computational cost than exact projection penalty methods and often improving downstream utility versus heuristic approximations impact learning tractable convex surrogates convexproxy expands the scope constrained deep optimization making disciplined enforcement complex constraints practical for iclr scale models,practical applications the underlying constraint knowledge often unknown and difficult specify address this issue recent advances inverse constrained reinforcement learning icrl have focused inferring these constraints from expert demonstrations however the icrl characterizes constraint learning tri level optimization problem which inherently complex due its interdependent variables and multiple layers optimization,2025-08-26T00:35:16.354560
61,Online Control-Lyapunov Scheduling: Provable Adaptive Learning-Rate Controllers,"Motivation: Static learning rates or heuristic schedules often underperform across phases of training and architectures. Control-theoretic approaches promise principled adaptation, but existing controllers lack stochastic robustness and theoretical nonconvex guarantees. Approach: We propose Control-Lyapunov Scheduling (CLS), an online adaptive learning-rate controller that treats SGD dynamics as a stochastic control system and selects step sizes by approximately minimizing a Lyapunov drift bound estimated from minibatch statistics. CLS computes closed-form learning-rate updates using local quadratic approximations and an uncertainty-aware attenuation term to handle stochastic gradients. The controller is provably stable under bounded noise and integrates seamlessly with momentum and adaptive normalization. Contributions: (1) A practical CLS algorithm deriving learning-rate updates from online Lyapunov drift minimization with explicit noise adaptation. (2) Theoretical analysis proving almost-sure stability and nonasymptotic convergence to stationary points for smooth nonconvex objectives under bounded variance and appropriate controller gains. (3) Implementation details: robust estimation of quadratic model parameters, computationally cheap attenuation heuristics, and compatibility with large-batch distributed training. Results: CLS reduces hyperparameter tuning needs, accelerates convergence, and improves final validation across vision and NLP tasks compared to hand-tuned schedules and adaptive optimizers; CLS particularly benefits long-horizon training and transfer-learning scenarios. Impact: CLS provides a principled, theoretically grounded mechanism to automate learning-rate adaptation, enhancing robustness and efficiency in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,8167,Gradient descent with generalized Newton’s method,"We propose the generalized Newton's method (GeN) --- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the intensive tuning of the learning rate scheduler. In practice, our method is easily implementable, since it only requires additional forward passes with almost zero computational overhead (in terms of training time and memory cost), if the overhead is amortized over many iterations. We present extensive experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that GeN optimizers match the state-of-the-art performance, which was achieved with carefully tuned learning rate schedulers.",ICLR.cc/2025/Conference,6.25,True,0.8401,motivation static learning rates heuristic schedules often underperform across phases training and architectures control theoretic approaches promise principled adaptation but existing controllers lack stochastic robustness and theoretical nonconvex guarantees contributions practical cls deriving learning rate updates from online lyapunov drift minimization explicit noise adaptation cls reduces hyperparameter tuning needs accelerates convergence and improves final validation across vision and nlp tasks compared hand tuned schedules and adaptive optimizers cls benefits long horizon training and transfer learning scenarios impact cls provides principled theoretically grounded mechanism automate learning rate adaptation enhancing robustness and efficiency large scale iclr experiments,our automatically and dynamically selects the learning rate that accelerates the convergence the intensive tuning the learning rate scheduler extensive experiments language and vision tasks gpt and resnet showcase that gen optimizers match the state the art which was achieved carefully tuned learning rate schedulers,2025-08-26T00:35:16.354561
62,SequentialStat Momentum: Statistical-Tested Restarts for Stochastic Momentum Acceleration,"Motivation: Momentum accelerates convergence but can amplify stochastic noise, leading to oscillations or divergence. Restart heuristics improve robustness but are ad hoc and lack statistical justification. Approach: We present SequentialStat Momentum (SSM), a restart mechanism that employs sequential hypothesis tests on minibatch gradient streams to detect momentum misalignment and trigger principled restarts. SSM evaluates alignment via a running inner-product statistic between momentum and current gradient, applies sequential probability ratio tests to detect statistically significant negative alignment, and resets momentum with controlled momentum decay when warranted. The tests account for minibatch variance and dependency via martingale concentration bounds. Contributions: (1) A statistically principled restart policy for momentum that adapts to noise levels and provides false-alarm control. (2) Theoretical guarantees: SSM maintains accelerated convergence in low-noise phases and prevents divergence or oscillation with bounded detection delay under stochastic models with bounded moments. (3) Practical recipes: lightweight estimators, confidence parameter tuning, and integration with Adam-like adaptive moments. Results: Across CNNs, transformers, and GAN training, SSM reduces instability incidents, shortens time-to-accuracy under noisy gradients, and outperforms fixed-restart heuristics in both stability and final performance, particularly in small-batch and high-variance regimes. Impact: SSM delivers a rigorous, low-overhead mechanism to harness momentum acceleration safely in stochastic optimization, reducing manual tuning and improving reliability for ICLR-scale training.",ICLR,optimization,gpt-5-mini,False,,Revisiting the Relation Between Robustness and Universality,"The *modified universality hypothesis* proposed by Jones et al. (2022) suggests that adversarially robust models trained for a given task are highly similar. We revisit the hypothesis and test its generality. We find that predictive behavior does not converge with increasing robustness and thus is not universal. Further, with additional similarity measures, we uncover differences in the representations that were invisible with the measures used in prior work. While robust models tend to be more similar than standard models, robust models remain distinct in important aspects. Moreover, the importance of similarity measures when comparing representations is highlighted as the absolute level of similarity---and thus the assessment of universality---is heavily dependent on the measure used.",ICLR.cc/2025/Conference,2.0,nan,0.7949,restart heuristics improve robustness but are hoc and lack statistical justification theoretical guarantees ssm maintains accelerated convergence low noise phases and prevents divergence oscillation bounded detection delay under stochastic models bounded moments,find that predictive behavior does not converge increasing robustness and thus not universal,2025-08-26T00:35:16.354568
63,Neural Coordinate Sampler: Learned Importance Sampling for Sparse Model Optimization,"Motivation: Coordinate methods and sparse-update strategies accelerate training for very large, sparse models (e.g., embeddings) but require effective coordinate selection heuristics. Fixed heuristics (random, magnitude) fail to exploit evolving training dynamics. Approach: We introduce Neural Coordinate Sampler (NCS), a small meta-model that learns to predict coordinate importance from lightweight per-coordinate features (recent gradients, access frequency, embedding norms). NCS outputs sampling probabilities used in a stochastic coordinate update scheme with importance-weighted unbiased estimators. The sampler is trained online to maximize expected immediate decrease in a surrogate loss estimated via shadow updates, with additional regularization to balance exploration and fairness. Contributions: (1) A practical learned importance-sampling mechanism for coordinate descent that adapts to nonstationary training dynamics. (2) Theoretical results demonstrating that under mild assumptions NCS achieves better expected progress per update than uniform sampling and preserves unbiasedness via importance weighting. (3) Engineering contributions: low-overhead feature maintenance, compression-friendly storage, and convergence-safe update rules. Results: NCS accelerates training and reduces update counts on large-scale recommendation embedding tables and sparse NLP layers, achieving up to 5× fewer coordinate updates for comparable validation performance and improving cold-start and long-tailed item learning versus heuristic samplers. Impact: NCS offers a scalable, data-driven approach to coordinate selection, enabling more efficient optimization of sparse and overparameterized models important in many ICLR-relevant systems.",ICLR,optimization,gpt-5-mini,True,2564,Unveiling Neural Combinatorial Optimization Model Representations Through Probing,"Neural combinatorial optimization (NCO) models have achieved remarkable performance, yet their learned underlying representations remain largely unclear. This hinders real-world application, as industrial stakeholders may want a deeper understanding of NCO models before committing resources. In this paper, we make the first step towards interpreting NCO models by investigating embeddings learned by various architectures through three probing tasks. Specifically, we analyze representative and state-of-the-art attention-based models, including AM, POMO, and LEHD, on the representative Traveling Salesman Problem and Capacitated Vehicle Routing Problem. Our findings reveal that NCO models encode linear representations of Euclidean distances between nodes, while also capturing additional knowledge that help avoid making myopic decisions. Furthermore, we show that architectural choices affect the ability of deep models to accurately represent Euclidean distances and to incorporate non-myopic decision-making strategies. We also verify to what extent NCO models understand the feasibility of constraints. Our work represents an initial effort to interpret NCO models, enhance understanding of why certain architectures outperform others, and demonstrate probing as a valuable tool for analyzing their internal mechanisms.",ICLR.cc/2025/Conference,5.25,False,0.8009,neural coordinate sampler ncs small meta model that learns predict coordinate importance from lightweight per coordinate features recent gradients access frequency embedding norms the sampler trained online maximize expected immediate decrease surrogate loss estimated shadow updates additional regularization balance exploration and fairness engineering contributions low overhead feature maintenance compression friendly storage and convergence safe update rules ncs accelerates training and reduces update counts large scale recommendation embedding tables and sparse nlp layers achieving fewer coordinate updates for comparable validation and improving cold start and long tailed item learning versus heuristic samplers impact ncs offers scalable data driven coordinate selection enabling more efficient optimization sparse and overparameterized models important many iclr relevant systems,neural combinatorial optimization nco models have achieved remarkable yet their learned underlying representations remain largely unclear our reveal that nco models encode linear representations euclidean distances between nodes while also capturing additional knowledge that help avoid making myopic decisions furthermore that architectural choices affect the ability deep models accurately represent euclidean distances and incorporate non myopic decision making strategies,2025-08-26T00:35:16.354572
64,Federated Newton-Sketch: Communication-Efficient Second-Order Methods for Heterogeneous Clients,"Motivation: Federated learning with heterogeneous clients suffers from slow convergence when using first-order methods, while full Newton methods are communication- and computation-heavy. There is a gap for scalable second-order federated optimizers that tolerate client heterogeneity and limited bandwidth. Approach: We propose Federated Newton-Sketch (FedNS), a federated second-order scheme that forms low-dimensional randomized sketches of local Hessian information at clients and transmits compact curvature summaries to a server. The server aggregates sketches to build a global sketched Newton direction, applies a trust-region-inspired damping to handle nonconvexity and heterogeneity, and broadcasts compressed preconditioned updates. Clients use local correction steps to ensure unbiasedness in expectation. Contributions: (1) A novel sketch-based federated second-order algorithm that reduces per-round communication to O(r·d) where r ≪ d. (2) A rigorous analysis showing convergence to stationary points under standard smoothness and bounded-variance assumptions, with explicit dependence on sketch rank, heterogeneity, and compression error. (3) Practical mechanisms for adaptive sketch-rank selection, privacy-preserving randomization, and efficient local linear solves using conjugate gradients. Results: Empirically on language and vision finetuning across heterogeneous client distributions, FedNS reduces rounds-to-target by 2–4× versus FedAvg and matches wall-clock speed of first-order optimizers while using comparable communication. Ablations demonstrate robustness to client drop-out and sketch rank selection. Impact: FedNS makes second-order acceleration practical for federated settings, offering a principled path to faster, communication-efficient optimization in privacy-sensitive, heterogeneous deployments typical of ICLR-scale applications.",ICLR,optimization,gpt-5-mini,True,11649,Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization,"Federated Learning (FL) faces significant challenges related to communication efficiency and heterogeneity. To address these issues, we explore the potential of using low-rank updates. Our theoretical analysis reveals that client's loss exhibits a higher rank structure (gradients span higher rank subspaces of Hessian) compared to the server's loss. Based on this insight, we hypothesize that constraining client-side optimization to a low-rank subspace could provide an implicit regularization effect. Consequently, we propose FedLoRU, a general low-rank update framework for FL. Our framework enforces low-rank client-side updates and accumulates these updates to form a higher-rank model. Additionally, variants of FedLoRU can adapt to environments with statistical and model heterogeneity by employing multiple or hierarchical low-rank updates. Experimental results demonstrate that FedLoRU performs comparably to full-rank algorithms and exhibits robustness to heterogeneous and large numbers of clients.",ICLR.cc/2025/Conference,4.0,False,0.8765,motivation federated learning heterogeneous clients suffers from slow convergence when first order methods while full newton methods are communication and computation heavy empirically language and vision finetuning across heterogeneous client distributions fedns reduces rounds target versus fedavg and matches wall clock speed first order optimizers while comparable communication ablations robustness client drop out and sketch rank selection impact fedns makes second order acceleration practical for federated settings offering principled path faster communication efficient optimization privacy sensitive heterogeneous deployments typical iclr scale applications,federated learning faces significant challenges related communication efficiency and heterogeneity this insight hypothesize that constraining client side optimization low rank subspace could provide implicit regularization effect experimental that fedloru performs comparably full rank algorithms and exhibits robustness heterogeneous and large numbers clients,2025-08-26T00:35:16.354577
65,Convexifying Flows: Local Quasi-Convexification for Stable Nonconvex Optimization,"Motivation: Nonconvex objectives create landscapes with traps and instability; global convexification is infeasible, yet local transformations that render regions quasi-convex could stabilize optimization and improve convergence. Approach: We introduce Convexifying Flows (CF), an algorithmic framework that learns local smooth bijections that push the objective toward quasi-convexity in neighborhoods visited during training. CF fits short-time invertible flows (parameterized by shallow neural maps) to minimize curvature asymmetry measures estimated from minibatch Hessian-vector products. Optimization alternates between (i) updating the flow to reduce nonconvexity in a trust region and (ii) performing gradient steps on the transformed coordinates with backtransformation. We regularize flows to preserve global topology and control Jacobian conditioning. Contributions: (1) A novel local quasi-convexification method enabling transformed-gradient steps with improved descent properties. (2) Theoretical results: under smoothness and bounded Jacobian distortion, CF yields improved local descent guarantees and reduces expected time to reach ε-stationarity compared to baseline SGD in regions of high negative curvature. (3) Practical recipe: lightweight flow architectures, randomized curvature estimators, and schemes to bound transformation overhead. Results: CF accelerates training and reduces sensitivity to learning-rate tuning on MLPs, ResNets, and small transformers, particularly in early-phase optimization and on tasks with rugged loss surfaces; it consistently attains lower final loss and improved stability versus SGD and adaptive baselines. Impact: Convexifying Flows offer a new paradigm—transforming optimization geometry on the fly—to make nonconvex training more robust and efficient for large-scale ICLR models.",ICLR,optimization,gpt-5-mini,True,11666,Nesterov acceleration in benignly non-convex landscapes,"While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a 'benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.",ICLR.cc/2025/Conference,6.75,True,0.8426,motivation nonconvex objectives create landscapes traps and instability global convexification infeasible yet local transformations that render regions quasi convex could stabilize optimization and improve convergence fits short time invertible flows parameterized shallow neural maps minimize curvature asymmetry measures estimated from minibatch hessian vector products optimization alternates between updating the flow reduce nonconvexity trust region and performing gradient steps the transformed coordinates backtransformation accelerates training and reduces sensitivity learning rate tuning mlps resnets and small transformers early phase optimization and tasks rugged loss surfaces consistently attains lower final loss and improved stability versus sgd and adaptive baselines impact convexifying flows offer paradigm transforming optimization geometry the fly make nonconvex training more robust and efficient for large scale iclr models,while momentum based optimization algorithms are used the notoriously non convex optimization problems deep learning their analysis has historically been restricted the convex and strongly convex setting this article partially close this gap between theory and practice and that virtually identical guarantees can obtained optimization problems benign non convexity that these weaker geometric assumptions are well justified overparametrized deep learning least locally,2025-08-26T00:35:16.354583
66,Projection-Free Variance-Reduced Frank-Wolfe for Constrained Deep Learning,"Motivation: Many practical problems require constrained optimization (budgeted models, fairness, safety). Projection-based methods are costly in high dimensions, while projection-free Frank-Wolfe (FW) methods struggle with high stochastic variance in deep learning settings. Approach: We propose VR-FW, a stochastic Frank-Wolfe algorithm augmented with variance-reduction and adaptive linear oracle caching. VR-FW maintains control variates for gradient estimates, updates a momentum-corrected FW direction, and uses a lightweight cached oracle to warm-start linear minimization oracles. We introduce a curvature-aware step-size rule that leverages variance estimates to stabilize updates and an anytime restart mechanism that reinitializes control variates when drift accumulates. Contributions: (1) A projection-free stochastic optimizer with explicit variance-reduction tailored to deep nonconvex constraints. (2) Convergence theory: VR-FW achieves O(1/√T) convergence to stationary points under smoothness and bounded-variance assumptions, with improved constants from variance control and caching. (3) Practical contributions: oracle caching strategies, minibatch-compatible variance estimators, and methods for common constraint families (ℓ1, simplex, spectral norm). Results: VR-FW outperforms projection-based SGD and vanilla FW in constrained training of sparse networks, fairness-constrained classifiers, and low-rank models, reducing projection overhead and achieving better constraint adherence at comparable or faster wall-clock time. Impact: VR-FW enables scalable, projection-free constrained optimization for deep learning, expanding feasible constrained formulations for ICLR research without heavy projection costs.",ICLR,optimization,gpt-5-mini,True,7430,A Computation and Communication Efficient Projection-free Algorithm for Decentralized Constrained Optimization,"Decentralized constrained optimization problems arise in numerous real-world applications, where a major challenge lies in the computational complexity of projecting onto complex sets, especially in large-scale systems. 
The projection-free method, Frank-Wolfe (FW), is popular for the constrained optimization problem with complex sets due to its efficiency in tackling the projection process. 
However, when applying FW methods to decentralized constrained finite-sum optimization problems, previous studies provide suboptimal incremental first-order oracle (IFO) bounds in both convex and non-convex settings. 
In this paper, we propose a stochastic algorithm named Decentralized Variance Reduction Gradient Tracking Frank-Wolfe ($\texttt{DVRGTFW}$), which incorporates the techniques of variance reduction, gradient tracking, and multi-consensus in the FW update to obtain tight bounds. 
We present a novel convergence analysis, diverging from previous decentralized FW methods, and demonstrating $\tilde{\mathcal{O}}(n+\sqrt{\frac{n}{m}}L\varepsilon^{-1})$ and $\mathcal{O}(\sqrt{\frac{n}{m}}L^2\varepsilon^{-2})$ IFO complexity bounds in convex and non-convex settings, respectively. 
To the best of our knowledge, these bounds are the best achieved in the literature to date. Besides, in the non-convex case, $\texttt{DVRGTFW}$ achieves $\mathcal{O}(\frac{L^2\varepsilon^{-2}}{\sqrt{1-\lambda_2(W)}})$ communication complexity which is closed to the lower bound $\Omega(\frac{L\varepsilon^{-2}}{\sqrt{1-\lambda_2(W)}})$. 
Empirical results validate the convergence properties of $\texttt{DVRGTFW}$ and highlight its superior performance over other related methods.",ICLR.cc/2025/Conference,4.666666666666667,nan,0.8613,motivation many practical problems require constrained optimization budgeted models fairness safety projection based methods are costly high dimensions while projection free frank wolfe methods struggle high stochastic variance deep learning settings contributions projection free stochastic optimizer explicit variance reduction tailored deep nonconvex constraints impact enables scalable projection free constrained optimization for deep learning expanding feasible constrained formulations for iclr heavy projection costs,decentralized constrained optimization problems arise numerous real world applications where major challenge lies the computational complexity projecting onto complex sets large scale systems the projection free frank wolfe popular for the constrained optimization problem complex sets due its efficiency tackling the projection process however when applying methods decentralized constrained finite sum optimization problems previous studies provide suboptimal incremental first order oracle ifo bounds both convex and non convex settings,2025-08-26T00:35:16.354590
67,Learn2Compress: Meta-Learned Compression Operators for Communication-Constrained Optimization,"Motivation: Gradient compression is crucial for communication-limited training, but fixed compressors (quantization, sparsification) are suboptimal across tasks and change during training. Approach: We present Learn2Compress, a meta-learning framework that trains compact neural compressors to minimize communication subject to optimization error budgets. During meta-training, compressors are optimized end-to-end on surrogate tasks to minimize downstream validation loss given a bit-budget constraint; they are parameterized to support low-cost encoding/decoding and incorporate error-feedback compatibility. At deployment, compressors adapt to live gradient statistics through a small online fine-tuning loop that preserves meta-learned priors. Contributions: (1) A meta-optimization pipeline that jointly learns compressor architectures and coding policies optimized for actual optimization dynamics. (2) Theoretical characterization linking compressor distortion to optimization error and guiding meta-objective design under information constraints. (3) System-level design for lightweight encoder/decoder implementations suitable for edge and distributed settings. Results: Learn2Compress reduces transmitted bits by up to 20× while matching or improving final accuracy compared to rule-based compressors and learned-but-unconstrained methods on image classification and recommendation models. Online adaptation yields robustness to changing gradient distributions and improves convergence speed versus static learned compressors. Impact: Learn2Compress offers a practical, principled way to automate communication compression, delivering adaptive, optimizer-aware compressors that improve efficiency in distributed training scenarios common in ICLR-scale systems.",ICLR,optimization,gpt-5-mini,True,2612,Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding,"Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose  Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.",ICLR.cc/2025/Conference,7.2,True,0.8343,learn2compress meta learning that trains compact neural compressors minimize communication subject optimization error budgets contributions meta optimization pipeline that jointly learns compressor architectures and coding policies optimized for actual optimization dynamics theoretical characterization linking compressor distortion optimization error and guiding meta objective under information constraints learn2compress reduces transmitted bits while matching improving final compared rule based compressors and learned but unconstrained methods image classification and recommendation models online adaptation yields robustness changing gradient distributions and improves convergence speed versus static learned compressors,neural compression has brought tremendous progress designing lossy compressors good rate distortion low complexity thus far neural compression involves transforming the source latent vector which then rounded integers and entropy coded integer rounding the latent space the quantization regions induced neural transformations remain square like and fail match those optimal vector quantization real world sources ltc improves upon standard neural compressors,2025-08-26T00:35:16.354592
68,Hessian Spectral Clipping: Eigenvalue Capping for Robust Deep Optimization,"Motivation: Large positive eigenvalues of the Hessian can produce unstable large steps while negative curvature causes slow escape; existing heuristics (gradient clipping, damped Newton) do not selectively control spectral extremes. Approach: We introduce Hessian Spectral Clipping (HSC), a practical optimizer add-on that estimates top and bottom Hessian eigenvalues via randomized Lanczos probes and applies adaptive spectral caps to gradient updates through a spectral filtering operator implemented with low-rank projections. HSC combines eigenvalue capping with curvature-aware step rescaling and provides mechanisms to cap only positive spikes or amplify negative directions for escape. We derive stability conditions linking clipping thresholds to step sizes and estimator variance. Contributions: (1) A novel spectral clipping mechanism to control extreme curvature directions during training without full Hessian inversion. (2) Theoretical analysis showing HSC preserves convergence to stationary points and bounds on step-size enlargement permitted by spectral caps under stochastic estimation error. (3) Efficient implementations using O(k) Hessian-vector products per epoch, with strategies for amortizing cost across layers. Results: HSC stabilizes training and accelerates convergence across ResNets, transformers, and deep autoencoders, particularly in large-batch and mixed-precision regimes; it reduces catastrophic loss spikes and improves final generalization compared to clipping heuristics and second-order damped methods. Impact: HSC offers a targeted spectral tool to modulate curvature during optimization, enabling safer and faster training of large models by explicitly managing Hessian extremes—a useful primitive for ICLR practitioners.",ICLR,optimization,gpt-5-mini,True,4059,Meta-Learning for Dynamic Synaptic Plasticity in Spiking Neural Networks,"Adaptive optimization algorithms, such as Adam Kingma & Ba (2015) and RM-SProp Tieleman & Hinton (2012), have become integral to training deep neu-ral networks, yet their stability properties and impact on generalization remain poorly understood Wilson et al. (2017). This paper extends linear stability anal-ysis to adaptive optimizers, providing a theoretical framework that explains their behavior in relation to loss surface geometry Wu et al. (2022); Jastrz˛ebski et al.(2019). We introduce a novel generalized coherence measure that quantifies the interaction between the adaptive preconditioner and the Hessian of the loss func-tion. This measure yields necessary and sufficient conditions for linear stability near stationary points, offering insights into why adaptive methods may converge to sharper minima with poorer generalization.
Our analysis leads to practical guidelines for hyperparameter tuning, demon-strating how to improve the generalization performance of adaptive optimizers. Through extensive experiments on benchmark datasets and architectures, includ-ing ResNet He et al. (2016) and Vision Transformers Dosovitskiy et al. (2020), we validate our theoretical predictions, showing that aligning the adaptive precon-ditioner with the loss surface geometry through careful parameter selection can narrow the generalization gap between adaptive methods and SGD Loshchilov & Hutter (2018).",ICLR.cc/2025/Conference,4.2,False,0.8178,hsc stabilizes training and accelerates convergence across resnets transformers and deep autoencoders large batch and mixed precision regimes reduces catastrophic loss spikes and improves final generalization compared clipping heuristics and second order damped methods,adaptive optimization algorithms such adam kingma and sprop tieleman hinton have become integral training deep neu ral networks yet their stability properties and impact generalization remain poorly understood wilson and vision transformers dosovitskiy,2025-08-26T00:35:16.354594
69,Random-Feature Quasi-Newton: Scalable Curvature Estimation via Kernel Sketching,"Motivation: Quasi-Newton methods require curvature approximations that are costly in high-dimensional parameter spaces. Random feature expansions provide low-dimensional embeddings that can capture important function geometry, but their use for quasi-Newton curvature estimates is underexplored. Approach: We propose RF-QuasiNewton, which projects gradients into a learned random-feature space approximating the neural tangent kernel, constructs low-rank Hessian approximations in that space, and computes Newton-like updates mapped back to parameter space. The random-feature map is updated online via ridge regression to track changing tangent geometry. RF-QuasiNewton uses efficient sketching for feature maintenance and applies a damping rule to ensure global descent. Contributions: (1) A method that exploits kernel approximations to obtain tractable curvature estimates for quasi-Newton updates in deep models. (2) Convergence analysis demonstrating descent and eventual convergence to stationary points under smoothness and bounded projection error, with complexity depending on feature dimension rather than parameter dimension. (3) Practical components: incremental feature updates, low-memory implementations, and guidelines for feature dimension selection. Results: RF-QuasiNewton accelerates convergence in MLPs and transformers on medium-scale tasks, often achieving quasi-Newton rates with feature dimensions several orders smaller than parameter count, and demonstrates robustness to noisy gradients. Impact: RF-QuasiNewton brings efficient curvature-informed updates to large models via kernel sketching, providing a scalable alternative to classical quasi-Newton and second-order methods for optimization in ICLR settings.",ICLR,optimization,gpt-5-mini,False,,Optimizing Learning for Robust Hyperbolic Deep Learning in Computer Vision,"Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the  data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on improving stability for curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across direct classification, generation, and hierarchical metric learning tasks while allowing for larger hyperbolic models.",ICLR.cc/2025/Conference,4.4,False,0.7984,random feature expansions provide low dimensional embeddings that can capture important function geometry but their use for quasi newton curvature estimates underexplored quasinewton which projects gradients into learned random feature space approximating the neural tangent kernel constructs low rank hessian approximations that space and computes newton like updates mapped back parameter space quasinewton uses efficient sketching for feature maintenance and applies damping rule ensure global descent contributions that exploits kernel approximations obtain tractable curvature estimates for quasi newton updates deep models convergence analysis demonstrating descent and eventual convergence stationary points under smoothness and bounded projection error complexity depending feature dimension rather than parameter dimension practical components incremental feature updates low memory implementations and guidelines for feature dimension selection quasinewton accelerates convergence mlps and transformers medium scale tasks often achieving quasi newton rates feature dimensions several orders smaller than parameter count and demonstrates robustness noisy gradients impact quasinewton brings efficient curvature informed updates large models kernel sketching providing scalable alternative classical quasi newton and second order methods for optimization iclr settings,hyperbolic deep learning has become growing direction computer vision for the unique properties afforded the alternate embedding space current riemannian optimizers not account for changes the manifold which greatly harms and forces lower learning rates minimize projection errors our focuses improving stability for curvature learning introducing improved schema for popular learning algorithms and providing normalization constrain embeddings within the variable representative radius the manifold additionally formulation for riemannian adamw and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations greatly reducing the computational penalty the hyperbolic embedding space our demonstrates consistent improvements across direct classification generation and hierarchical learning tasks while allowing for larger hyperbolic models,2025-08-26T00:35:16.354598
70,Byzantine-Resilient Aggregation with Certified Influence Bounds,"Motivation: Distributed optimization is vulnerable to Byzantine failures where malicious clients send arbitrary updates; many robust aggregators lack provable certificates of robustness tied to influence on the global model. Approach: We propose Certified-Influence Aggregation (CIA), a robust aggregation scheme that combines trimmed-mean-style aggregation with per-client influence certificates computed via randomized influence estimators. CIA computes a robust central estimate and provides a high-probability bound on each client’s potential influence by measuring sensitivity through randomized directional tests. Aggregation then downweights or excludes clients whose certified influence exceeds a threshold, guaranteeing bounded model deviation even under adversarial corruption. Contributions: (1) A certified aggregation protocol that outputs both an aggregate update and per-client influence bounds with probabilistic guarantees. (2) A theoretical framework tying certified influence thresholds to worst-case model deviation and convergence, offering conditions under which optimization is resilient to a given number of Byzantine clients. (3) Efficient randomized estimators and implementation compatible with partial participation and compressed updates. Results: On federated benchmarks with simulated Byzantine attackers and heterogeneous data, CIA maintains stable convergence and model utility where naive averaging and common robust heuristics fail, providing explicit certificates that identify malicious clients with high precision. Impact: CIA advances reliable distributed optimization by offering provable, actionable guarantees against adversarial participants, improving trustworthiness of federated and decentralized ICLR-scale deployments.",ICLR,optimization,gpt-5-mini,True,11141,Towards General Certified Robustness of Combinatorial Optimization Solvers,"Combinatorial optimization (CO), driven by algorithmic advancements, now spans applications like network design and bioinformatics, crucial for optimizing complex systems and tackling NP-hard problems efficiently across various industries.
Nonetheless, the study for robustness, especially certified robustness in the CO domain which ensures optimization consistency among different data distributions, persists as an unexplored domain.
In this study, we explore the certified robustness and robustness enhancement strategy for CO solvers.
Experiments across datasets and solvers illustrate that our proposed certification definition can achieve a solid robustness guarantee and the enhancement method significantly amplifies the model’s immunity to perturbations in practice.",ICLR.cc/2025/Conference,3.75,nan,0.8077,motivation distributed optimization vulnerable byzantine failures where malicious clients send arbitrary updates many robust aggregators lack provable certificates robustness tied influence the global theoretical tying certified influence thresholds worst case deviation and convergence offering conditions under which optimization resilient given number byzantine clients impact cia advances reliable distributed optimization offering provable actionable guarantees against adversarial participants improving trustworthiness federated and decentralized iclr scale deployments,combinatorial optimization driven algorithmic advancements now spans applications like network and bioinformatics crucial for optimizing complex systems and tackling hard problems across various industries nonetheless the for robustness certified robustness the domain which ensures optimization consistency among different data distributions persists unexplored domain this the certified robustness and robustness enhancement strategy for solvers experiments across datasets and solvers illustrate that our proposed certification definition can achieve solid robustness guarantee and the enhancement amplifies the model immunity perturbations practice,2025-08-26T00:35:16.354603
71,Gradient Entropy Scheduling: Controlling Exploration via Information-Theoretic Step Sizes,"Motivation: Exploration-exploitation trade-offs in optimization affect convergence and generalization: overly aggressive updates can overfit, while overly cautious updates slow learning. Existing schedules focus on magnitude heuristics without explicit information control. Approach: We introduce Gradient Entropy Scheduling (GES), an information-theoretic controller that sets per-step effective step sizes by targeting a desired decrease in the entropy of the stochastic gradient distribution. GES estimates gradient entropy via plug-in estimators on minibatch gradients and computes step sizes that balance expected empirical risk reduction with controlled reduction in gradient diversity. The controller includes variance-aware regularization and integrates naturally with momentum and adaptive optimizers. Contributions: (1) A principled scheduling mechanism grounded in entropy control that links step-size dynamics to information content in gradients. (2) Theoretical analysis showing GES stabilizes optimization by preventing premature collapse of gradient diversity and provides bounds on convergence rate under controlled entropy decay. (3) Practical estimators and smoothing strategies for robust entropy estimation at minibatch scale. Results: GES reduces overfitting and improves generalization across vision and NLP tasks, particularly in low-data and transfer-learning scenarios; it achieves comparable or faster convergence while yielding better calibration and robustness to label noise compared to fixed and adaptive schedules. Impact: GES offers a novel information-theoretic perspective for optimizer scheduling, affording principled control over exploration during training and enhancing reliability and performance of large-scale models in the ICLR ecosystem.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8297,motivation exploration exploitation trade offs optimization affect convergence and generalization overly aggressive updates can overfit while overly cautious updates slow learning theoretical analysis showing ges stabilizes optimization preventing premature collapse gradient diversity and provides bounds convergence rate under controlled entropy decay ges reduces overfitting and improves generalization across vision and nlp tasks low data and transfer learning scenarios achieves comparable faster convergence while yielding better calibration and robustness label noise compared fixed and adaptive schedules,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354605
72,SketchNewton: Low-Rank Hessian Sketching for Scalable Second-Order Optimization,"Motivation: Second-order methods promise rapid convergence in nonconvex optimization but are prohibitively expensive for modern deep networks due to Hessian storage and inversion. Practical low-rank approximations exist but lack rigorous control of curvature approximation quality in stochastic settings. Approach: We introduce SketchNewton, a scalable second-order optimizer that constructs adaptive low-rank Hessian sketches via randomized range finding on streaming Hessian-vector products. At each iteration, SketchNewton forms a compact sketched curvature matrix, computes a damped Newton step in the sketch subspace, and applies a provable residual correction to control orthogonal curvature. The sketch rank adapts online using concentration-based criteria that trade computational cost against approximation fidelity. Contributions: (1) A novel algorithmic pipeline combining randomized Hessian sketching, adaptive rank selection, and residual correction for stable second-order updates in minibatch regimes. (2) A theoretical analysis proving high-probability bounds on curvature approximation error and showing convergence to first-order stationary points under standard smoothness and bounded-variance assumptions, with explicit dependence on sketch rank and noise. (3) Practical implementation details—incremental sketch updates, damping rules, and complexity bounds—enabling efficient GPU execution. Results: Across transformer fine-tuning, ResNet training, and autoencoder objectives, SketchNewton reduces epochs-to-target by 2×–4× relative to AdamW and matches or outperforms K-FAC while incurring modest extra per-step cost. Ablations verify rank-adaptation and residual correction are key to stability. Impact: SketchNewton brings provable, efficient curvature exploitation to large-scale deep learning, offering a practical second-order alternative that bridges randomized linear algebra with modern stochastic optimization.",ICLR,optimization,gpt-5-mini,True,9645,Gathering and Exploiting Higher-Order Information when Training Large Structured Models,"When training large models, such as neural networks, 
the full derivatives of order 2 and beyond are usually inaccessible,
due to their computational cost.
This is why, among the second-order optimization methods, it is very common
to bypass the computation of the Hessian by using 
first-order information, such as the gradient of the parameters (e.g., quasi-Newton methods)
or the activations (e.g., K-FAC).

In this paper, we focus on the exact and explicit computation
of projections of the Hessian and higher-order derivatives on
well-chosen subspaces, which are relevant for optimization.
Namely, for a given partition of the set of parameters, 
it is possible to compute tensors which can be seen as
""higher-order derivatives according to the partition"",
at a reasonable cost as long as the number of subsets of 
the partition remains small.

Then, we propose an optimization method exploiting
these tensors at order 2 and 3 with several interesting properties, including:
it outputs a learning rate per subset of parameters, which can
be used for hyperparameter tuning;
it takes into account long-range interactions
between the layers of the trained neural network, 
which is usually not the case in similar methods (e.g., K-FAC);
the trajectory of the optimization is invariant under 
affine layer-wise reparameterization.",ICLR.cc/2025/Conference,5.333333333333333,False,0.8485,motivation second order methods promise rapid convergence nonconvex optimization but are prohibitively expensive for modern deep networks due hessian storage and inversion across transformer fine tuning resnet training and autoencoder objectives sketchnewton reduces epochs target relative adamw and matches outperforms fac while incurring modest extra per step cost impact sketchnewton brings provable efficient curvature exploitation large scale deep learning offering practical second order alternative that bridges randomized linear algebra modern stochastic optimization,when training large models such neural networks the full derivatives order and beyond are usually inaccessible due their computational cost this why among the second order optimization methods very common bypass the computation the hessian first order information such the gradient the parameters this focus the exact and explicit computation projections the hessian and higher order derivatives well chosen subspaces which are relevant for optimization then optimization exploiting these tensors order and several interesting properties including outputs learning rate per subset parameters which can used for hyperparameter tuning takes into account long range interactions between the layers the trained neural network which usually not the case similar methods fac the trajectory the optimization invariant under affine layer wise reparameterization,2025-08-26T00:35:16.354606
73,FlowImplicit: Continuous-Time Implicit Differentiation for Long-Horizon Bilevel Optimization,"Motivation: Bilevel problems underlie hyperparameter tuning, meta-learning, and data-centric objectives; implicit differentiation scales better than unrolling but suffers from numerical instability and memory issues when inner dynamics span long horizons. Approach: We present FlowImplicit, a continuous-time implicit differentiation framework that models inner optimization as an ODE flow and computes hypergradients via stabilized adjoint solves. FlowImplicit employs reversible integrators for forward trajectories and a residual-corrected backward solver that leverages randomized checkpointing to control memory. We derive discretization-consistent implicit gradient estimators with variance-aware regularization to counter stochastic minibatch noise. Contributions: (1) A principled continuous-time perspective for implicit differentiation yielding hypergradients consistent across discretization refinements. (2) Numerical techniques: reversible integrators, randomized low-memory checkpoint schedules, and residual-corrected adjoint solves that mitigate backward instability and stochastic bias. (3) Rigorous analysis: error bounds relating hypergradient bias to solver residuals, discretization error, and minibatch variance; complexity trade-offs for memory vs. compute. Results: FlowImplicit enables hyperparameter and meta-parameter optimization with inner horizons 5–20× longer than standard implicit approaches while using comparable memory. On few-shot meta-learning, long-horizon MAML variants trained with FlowImplicit achieve improved adaptation and generalization versus truncated unrolling and naive adjoint methods. Impact: FlowImplicit makes long-horizon bilevel optimization practical and stable for large-scale deep problems, expanding the effective horizon for meta-training and hyperparameter tuning in ICLR-relevant applications.",ICLR,optimization,gpt-5-mini,True,3082,Optimization Insights into Deep Diagonal Linear Networks,"Overparameterized models trained with (stochastic) gradient descent are ubiquitous in modern machine learning. These large models achieve unprecedented performance on test data, but their theoretical understanding is still limited. In this paper, we take a step towards filling this gap by adopting an optimization perspective. More precisely, we study the implicit regularization properties of the gradient flow “algorithm” for estimating the parameters of a deep diagonal neural network. Our main contribution is showing that this gradient flow induces a mirror flow dynamic on the model, meaning that it is biased towards a specific solution of the problem depending on the initialization of the network. Along the way, we prove several properties of the trajectory.",ICLR.cc/2025/Conference,3.0,nan,0.8402,flowimplicit continuous time implicit differentiation that models inner optimization ode flow and computes hypergradients stabilized adjoint solves flowimplicit enables hyperparameter and meta parameter optimization inner horizons longer than standard implicit approaches while comparable memory few shot meta learning long horizon maml variants trained flowimplicit achieve improved adaptation and generalization versus truncated unrolling and naive adjoint methods impact flowimplicit makes long horizon bilevel optimization practical and stable for large scale deep problems expanding the effective horizon for meta training and hyperparameter tuning iclr relevant applications,overparameterized models trained stochastic gradient descent are ubiquitous modern machine learning this take step towards filling this gap adopting optimization perspective more precisely the implicit regularization properties the gradient flow algorithm for estimating the parameters deep diagonal neural network our main showing that this gradient flow induces mirror flow dynamic the meaning that biased towards specific solution the problem depending the initialization the network,2025-08-26T00:35:16.354613
74,HeteroProx: Proximal Personalization for Federated Learning with Theoretical Guarantees,"Motivation: Client heterogeneity and statistical non-IIDness in federated learning degrade global models and client experience; naive averaging fails to reconcile personal and global objectives. Approach: We propose HeteroProx, a proximal personalization framework that decomposes client objectives into shared global parameters and lightweight personalized adapters. Each client performs proximal SGD on personalized parameters while contributing compressed proximal corrections for shared parameters; the server aggregates corrections with variance-aware weighting to update the global model. HeteroProx leverages an adaptive proximal coefficient driven by local curvature proxies to balance personalization and consensus. Contributions: (1) A scalable personalization algorithm integrating proximal updates with server-side variance-aware aggregation and communication compression. (2) Convergence theory: HeteroProx provably converges to a neighborhood of stationary points of a coupled global-personalized objective under bounded heterogeneity and stochastic noise, with explicit bounds on personalization quality vs. communication. (3) Practical mechanisms: adapter architectures, adaptive proximal scheduling, and lossy compression compatible with privacy-preserving masks. Results: On cross-device language modeling, medical imaging with patient-specific distributions, and recommendation systems, HeteroProx improves per-client accuracy and fairness metrics compared to FedAvg, FedProx, and multi-task baselines, while reducing uplink communication by up to 8×. Ablations confirm adaptive proximal tuning yields superior personalization-utility trade-offs. Impact: HeteroProx provides a theoretically grounded, efficient personalization strategy for federated learning, improving client experiences in heterogeneous and resource-constrained settings central to ICLR's applied research.",ICLR,optimization,gpt-5-mini,True,6341,Debiasing Federated Learning with Correlated Client Participation,"In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice.  Existing analyses of FedAvg usually assume the participating clients are independently sampled in each round from a uniform distribution, which does not reflect real-world scenarios. This paper introduces a theoretical framework that models client participation in FL as a Markov chain to study optimization convergence when clients have non-uniform and correlated participation across rounds. 
We apply this framework to analyze a more practical pattern: every client must wait a minimum number of $R$ rounds (minimum separation) before re-participating. We theoretically prove and empirically observe that increasing minimum separation reduces the bias induced by intrinsic non-uniformity of client availability in cross-device FL systems. 
Furthermore, we develop an effective debiasing algorithm for FedAvg that provably converges to the unbiased optimal solution under arbitrary minimum separation and unknown client availability distribution.",ICLR.cc/2025/Conference,6.75,True,0.8739,motivation client heterogeneity and statistical non iidness federated learning degrade global models and client experience naive averaging fails reconcile personal and global objectives cross device language modeling medical imaging patient specific distributions and recommendation systems heteroprox improves per client and fairness metrics compared fedavg fedprox and multi task baselines while reducing uplink communication,cross device federated learning millions mobile clients only small subset clients participate training every communication round and federated averaging fedavg the most popular practice this introduces theoretical that models client participation markov chain optimization convergence when clients have non uniform and correlated participation across rounds,2025-08-26T00:35:16.354614
75,Wasserstein-SGD: Distributional Robustness via Entropic Optimal Transport Regularization,"Motivation: Models trained by ERM are vulnerable to distributional shifts; classical DRO offers robustness but is computationally heavy or overly conservative for deep learning. Approach: We introduce Wasserstein-SGD, a stochastic optimization algorithm that integrates entropic regularization of Wasserstein perturbations into minibatch training. At each iteration, Wasserstein-SGD solves a smoothed adversarial inner problem via Sinkhorn iterations to compute distributionally worst-case reweighting over samples and then performs a stochastic gradient step w.r.t. the reweighted loss. Entropic smoothing yields differentiability and efficient GPU-friendly implementations. Contributions: (1) A tractable DRO formulation using entropic OT that is compatible with minibatch stochastic optimization and yields differentiable adversarial reweightings. (2) Theoretical results: convergence to stationary points of the smoothed DRO objective under standard smoothness and variance assumptions, and certified bounds on worst-case risk as functions of the Sinkhorn regularizer and transport radius. (3) Practical accelerations: warm-started Sinkhorn, low-rank transport approximations, and adaptive transport-radius scheduling to mitigate conservatism. Results: Wasserstein-SGD enhances robustness to covariate shift and label corruption on CIFAR-C, ImageNet-C, and real-world medical datasets, improving worst-group accuracy and calibration while incurring modest computational overhead. Compared to adversarial training and classical DRO, it achieves tighter trade-offs between robustness and utility. Impact: Wasserstein-SGD makes principled, transport-based robustness accessible at scale, providing certifiable improvements against realistic distributional uncertainties relevant to ICLR applications.",ICLR,optimization,gpt-5-mini,True,6913,Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization,"Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense.Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.",ICLR.cc/2025/Conference,6.5,True,0.8629,motivation models trained erm are vulnerable distributional shifts classical dro offers robustness but computationally heavy overly conservative for deep learning wasserstein sgd stochastic optimization that integrates entropic regularization wasserstein perturbations into minibatch training contributions tractable dro formulation entropic that compatible minibatch stochastic optimization and yields differentiable adversarial reweightings wasserstein sgd enhances robustness covariate shift and label corruption cifar imagenet and real world medical datasets improving worst group and calibration while incurring modest computational overhead compared adversarial training and classical dro achieves tighter trade offs between robustness and utility impact wasserstein sgd makes principled transport based robustness accessible scale providing certifiable improvements against realistic distributional uncertainties relevant iclr applications,wasserstein distributionally robust optimization wdro optimizes against worst case distributional shifts within specified uncertainty set leading enhanced generalization unseen adversarial examples compared standard adversarial training which focuses pointwise adversarial perturbations address this gap proposing robust optimization under uncertainty set for adversarial noise wasserstein distance and statistical error kullback leibler divergence called the statistically robust wdro establish robust generalization bound for the optimization implying that out distribution adversarial least good the statistically robust training loss high probability finally extensive experiments that our mitigates robust overfitting and enhances robustness within the wdro,2025-08-26T00:35:16.354616
76,LyapunovTune: Online Stability-Guided Hyperparameter Control for Deep Training,"Motivation: Hyperparameter schedules (learning rate, momentum) critically affect optimization stability and performance; manual tuning is expensive and brittle across tasks. Approach: LyapunovTune is an online controller that adaptively adjusts learning-rate and momentum by estimating a Lyapunov drift from minibatch gradient statistics. The controller computes a local quadratic surrogate and chooses hyperparameters to minimize an estimated upper bound on the Lyapunov decrease subject to stability constraints. We implement robust estimators for curvature and gradient noise and include safeguards against over-aggressive adjustments via conservative projection. Contributions: (1) A practical controller that automatically stabilizes and accelerates training by minimizing an estimated Lyapunov upper bound at each step. (2) Convergence guarantees: LyapunovTune preserves convergence to stationary points under bounded estimation error and stochastic variance, and reduces sensitivity to initial hyperparameters. (3) Engineering insights: low-overhead curvature estimation using diagonal-plus-low-rank sketches, and integration with common optimizers (SGD, Adam) and distributed training. Results: LyapunovTune reduces hyperparameter tuning budget and accelerates convergence across vision and NLP tasks, matching hand-tuned schedules and outperforming adaptive optimizers in stability-critical regimes like GAN training and mixed-precision runs. Impact: LyapunovTune offers a theoretically motivated, deployable solution to automate hyperparameter control, improving robustness and efficiency in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,8167,Gradient descent with generalized Newton’s method,"We propose the generalized Newton's method (GeN) --- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the intensive tuning of the learning rate scheduler. In practice, our method is easily implementable, since it only requires additional forward passes with almost zero computational overhead (in terms of training time and memory cost), if the overhead is amortized over many iterations. We present extensive experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that GeN optimizers match the state-of-the-art performance, which was achieved with carefully tuned learning rate schedulers.",ICLR.cc/2025/Conference,6.25,True,0.8481,motivation hyperparameter schedules learning rate momentum critically affect optimization stability and manual tuning expensive and brittle across tasks lyapunovtune reduces hyperparameter tuning budget and accelerates convergence across vision and nlp tasks matching hand tuned schedules and outperforming adaptive optimizers stability critical regimes like gan training and mixed precision runs impact lyapunovtune offers theoretically motivated deployable solution automate hyperparameter control improving robustness and efficiency large scale iclr experiments,our automatically and dynamically selects the learning rate that accelerates the convergence the intensive tuning the learning rate scheduler extensive experiments language and vision tasks gpt and resnet showcase that gen optimizers match the state the art which was achieved carefully tuned learning rate schedulers,2025-08-26T00:35:16.354618
77,LearnCompress: End-to-End Meta-Learning of Gradient Compressors for Distributed Optimization,"Motivation: Communication bottlenecks limit distributed training; fixed compressors (quantization, sparsification) are suboptimal in diverse training dynamics. Approach: LearnCompress frames compressor design as a meta-learning problem: a compact neural compressor is trained end-to-end to minimize downstream optimization loss under bit-budget constraints. During meta-training on proxy tasks, compressors learn task- and optimizer-aware encoding strategies. At deployment, the compressor adapts online with minimal fine-tuning to current gradient statistics. We enforce unbiasedness via importance weighting and incorporate error-feedback compatibility. Contributions: (1) A meta-learning pipeline producing learned compressors optimized for real optimization dynamics and communication budgets. (2) Theoretical analysis linking compressor distortion to expected optimization slowdown and guiding the meta-objective regularization. (3) Practical architectures and quantization-aware encoders that are lightweight, parallelizable, and privacy-preserving. Results: LearnCompress reduces communicated bits by up to 25× while preserving accuracy on transformer pretraining, distributed ResNet training, and recommendation models; online adaptation yields robustness to distributional shifts and outperforms fixed learned compressors and classical schemes in convergence per bit. Impact: LearnCompress automates generation of efficient, optimizer-aware compressors, enabling more scalable distributed optimization for ICLR-scale workloads with rigorous performance trade-offs.",ICLR,optimization,gpt-5-mini,True,2612,Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding,"Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose  Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.",ICLR.cc/2025/Conference,7.2,True,0.8307,learncompress frames compressor meta learning problem compact neural compressor trained end end minimize downstream optimization loss under bit budget constraints contributions meta learning pipeline producing learned compressors optimized for real optimization dynamics and communication budgets theoretical analysis linking compressor distortion expected optimization slowdown and guiding the meta objective regularization learncompress reduces communicated bits while preserving transformer pretraining distributed resnet training and recommendation models online adaptation yields robustness distributional shifts and outperforms fixed learned compressors and classical schemes convergence per bit impact learncompress automates generation efficient optimizer aware compressors enabling more scalable distributed optimization for iclr scale workloads rigorous trade offs,neural compression has brought tremendous progress designing lossy compressors good rate distortion low complexity thus far neural compression involves transforming the source latent vector which then rounded integers and entropy coded integer rounding the latent space the quantization regions induced neural transformations remain square like and fail match those optimal vector quantization real world sources ltc improves upon standard neural compressors,2025-08-26T00:35:16.354620
78,SmoothProxy: Differentiable Surrogates for Optimizing Nondifferentiable Objectives,"Motivation: Many important objectives—ranking metrics, intersection-over-union, and certain fairness measures—are nondifferentiable, hindering direct gradient-based optimization and requiring proxies that may misalign with the true metric. Approach: We propose SmoothProxy, a framework to construct differentiable surrogates with provable fidelity guarantees to target nondifferentiable objectives. SmoothProxy systematically derives smoothed approximations via mollified indicator functions and adaptive temperature schemes, then calibrates surrogate parameters using a calibration loss that bounds the discrepancy between surrogate and true objective under perturbations. Contributions: (1) A general recipe to design differentiable surrogates with explicit fidelity controls and adaptive smoothing schedules. (2) Theoretical guarantees: uniform approximation bounds linking surrogate smoothness and sample size to the gap in objective and gradient alignment, and convergence guarantees when optimizing surrogates under stochastic gradients. (3) Practical procedures for temperature annealing, variance reduction, and plug-in surrogate libraries for common nondifferentiable tasks. Results: SmoothProxy improves true metric performance (mAP, IoU, group fairness gap) versus standard surrogates on object detection, retrieval, and fairness benchmarks, with smoother optimization dynamics and reduced hyperparameter sensitivity. Impact: SmoothProxy enables principled optimization for nondifferentiable objectives prevalent in ICLR applications, reducing reliance on ad hoc proxies and improving alignment between training objectives and evaluation metrics.",ICLR,optimization,gpt-5-mini,True,2328,LORA-MaOO: Learning Ordinal Relations and Angles for Expensive Many-Objective Optimization,"Many-objective optimization (MaOO) simultaneously optimizes many conflicting objectives to identify the Pareto front - a set of diverse solutions that represent different optimal balances between conflicting objectives. For expensive MaOO problems, due to their costly function evaluations, computationally cheap surrogates have been widely used in MaOO to save evaluation budget. However, as the number of objectives $M$ increases, the cost of using surrogates increases rapidly as many optimization algorithms need maintain $M$ surrogates. In addition, a large $M$ indicates a high-dimensional objective space, increasing the difficulty of maintaining solution diversity. 
It is a challenge to reach diverse optimal solutions with a relatively low cost of using surrogates for MaOO problems. 
To handle this challenge, we propose LORA-MaOO, a surrogate-assisted MaOO algorithm that learns $M$ surrogates from spherical coordinates, including an ordinal-regression-based surrogate that learns the ordinal relations between solutions (denoted as radial surrogate) and $M$-1 regression-based surrogates that trained on angular coordinates (denoted as angular surrogates).
In each optimization iteration, model-based search is completed with a single radial surrogate, while $M$-1 angular surrogates are used only once for selecting diverse candidates. Therefore, the frequency of using angular surrogates is largely reduced, lowering the cost of using surrogates. 
In addition, we design a clustering method to quantify artificial ordinal relations for non-dominated solutions and improve the quantification of dominance-based ordinal relations. These ordinal relations are used to train the radial regression surrogate which predicts how desirable the candidate solutions are in terms of convergence. The solution diversity is maintained via angles between solutions instead of pre-defined auxiliary reference vectors, which is parameter-free. Experimental results show that LORA-MaOO significantly outperforms other surrogate-assisted MaOO methods on most MaOO benchmark problems and real-world applications.",ICLR.cc/2025/Conference,4.75,False,0.8291,motivation many important objectives ranking metrics intersection over union and certain fairness measures are nondifferentiable hindering direct gradient based optimization and requiring proxies that may misalign the true smoothproxy improves true map iou group fairness gap versus standard surrogates object detection retrieval and fairness benchmarks smoother optimization dynamics and reduced hyperparameter sensitivity impact smoothproxy enables principled optimization for nondifferentiable objectives prevalent iclr applications reducing reliance hoc proxies and improving alignment between training objectives and evaluation metrics,many objective optimization maoo simultaneously optimizes many conflicting objectives identify the pareto front set diverse solutions that represent different optimal balances between conflicting objectives however the number objectives increases the cost surrogates increases rapidly many optimization algorithms need maintain surrogates each optimization iteration model based search completed single radial surrogate while angular surrogates are used only once for selecting diverse candidates addition clustering quantify artificial ordinal relations for non dominated solutions and improve the quantification dominance based ordinal relations,2025-08-26T00:35:16.354626
79,Riemannian Quasi-Adam: Adaptive Optimization on Manifolds with Adaptive Preconditioning,"Motivation: Manifold-constrained parameters (orthogonal weights, low-rank factors, SPD matrices) appear in modern architectures, but popular adaptive optimizers lack principled extensions that respect manifold geometry and guarantee convergence. Approach: We develop Riemannian Quasi-Adam (RQ-Adam), an adaptive optimizer that generalizes Adam-style moment estimates to tangent spaces and constructs quasi-Newton preconditioners respecting Riemannian metrics. RQ-Adam uses vector transport for momentum, retraction-compatible bias correction, and manifold-aware damping to ensure stability. A quasi-Newton update in a low-dimensional tangent subspace improves curvature awareness while preserving manifold feasibility. Contributions: (1) A practical Riemannian adaptive optimizer integrating momentum, quasi-Newton preconditioning, and geometry-aware corrections. (2) Convergence theory: nonasymptotic guarantees to Riemannian stationary points under geodesic smoothness and bounded stochastic tangent noise, including conditions for monotone decrease. (3) Implementations and optimizations for common manifolds (Stiefel, sphere, SPD) using efficient retractions and low-overhead transport. Results: RQ-Adam accelerates convergence and improves final performance in orthogonality-constrained CNNs, low-rank embedding models, and manifold-structured generative models, outperforming Riemannian SGD and naive Adam-in-ambient-space baselines. Impact: RQ-Adam furnishes a robust, adaptive optimization tool for manifold-constrained learning, expanding optimization capabilities for structured models central to ICLR research.",ICLR,optimization,gpt-5-mini,True,1076,Continuous-Time Analysis of Adaptive Optimization and Normalization,"Adaptive optimization algorithms, particularly Adam and its variant AdamW, are fundamental to modern deep learning, however, their training dynamics lack comprehensive theoretical understanding, with limited insight into why common practices—such as specific hyperparameter choices and normalization layers—contribute to successful generalization. This work presents a continuous-time formulation of Adam and AdamW, facilitating a tractable analysis of training dynamics that can shed light on such practical questions. We theoretically derive a stable region for Adam's hyperparameters $(\beta, \gamma)$ that ensures bounded updates, empirically verifying these predictions by observing unstable exponential growth of parameter updates outside this region. Furthermore, we theoretically justify the success of normalization layers by uncovering an implicit meta-adaptive effect of scale-invariant architectural components. This insight leads to an explicit optimizer, $2$-Adam, which we generalize to $k$-Adam—an optimizer that applies an adaptive normalization procedure $k$ times, encompassing Adam (corresponding to $k=1$) and Adam with a normalization layer (corresponding to $k=2$). Overall, our continuous-time formulation of Adam facilitates a principled analysis, offering deeper understanding of optimal hyperparameter choices and architectural decisions in modern deep learning.",ICLR.cc/2025/Conference,4.25,False,0.8491,adam accelerates convergence and improves final orthogonality constrained cnns low rank embedding models and manifold structured generative models outperforming riemannian sgd and naive adam ambient space baselines impact adam furnishes robust adaptive optimization tool for manifold constrained learning expanding optimization capabilities for structured models central iclr,adaptive optimization algorithms adam and its variant adamw are fundamental modern deep learning however their training dynamics lack comprehensive theoretical understanding limited insight into why common practices such specific hyperparameter choices and normalization layers contribute successful generalization overall our continuous time formulation adam facilitates principled analysis offering deeper understanding optimal hyperparameter choices and architectural decisions modern deep learning,2025-08-26T00:35:16.354630
80,AutoLips: Online Local Lipschitz Estimation for Adaptive Step-Size Control,"Motivation: Selecting appropriate step sizes remains a central challenge in stochastic nonconvex optimization; fixed schedules or global Lipschitz assumptions are brittle when curvature varies across parameters and training phases. Approach: We propose AutoLips, an online procedure that estimates local smoothness (Lipschitz constant) with confidence intervals from streaming gradient and function-difference probes. AutoLips constructs lightweight local quadratic surrogates using sequential gradient-difference sketches and sub-sampled directional finite differences; it then computes per-parameter or per-block adaptive step sizes by solving a constrained trust-region rule informed by the estimated Lipschitz bounds. The estimator incorporates variance correction to remain robust under minibatch noise. Contributions: (1) An online Lipschitz estimator with provable concentration bounds under bounded variance and smoothness; (2) a principled step-size controller that guarantees sufficient decrease in expectation and adapts per-block learning rates; (3) implementation strategies (sketching, amortized probing) enabling low overhead in large models. Results: On convolutional networks, transformers, and GAN training, AutoLips achieves faster early-phase convergence and reduces tuning budget: it attains target validation loss in 20–40% fewer epochs versus tuned schedules and adaptive optimizers, and stabilizes training under large-batch and mixed-precision regimes. Impact: AutoLips provides a theoretically grounded, practical mechanism to automate step-size selection based on local curvature and noise, reducing manual tuning and improving robustness for large-scale ICLR-style optimization tasks.",ICLR,optimization,gpt-5-mini,True,11055,Clipping Improves Adam and AdaGrad when the Noise Is Heavy-Tailed,"Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for training modern Deep Learning models, especially Large Language Models. Typically, the noise in the stochastic gradients is heavy-tailed for the later ones. Gradient clipping provably helps to achieve good high-probability convergence for such noises. However, despite the similarity between AdaGrad/Adam and Clip-SGD, the current understanding of the high-probability convergence of AdaGrad/Adam-type methods is limited in this case. In this work, we prove that AdaGrad/Adam (and their delayed version) can have provably bad high-probability convergence if the noise is heavy-tailed. We also show that gradient clipping fixes this issue, i.e., we derive new high-probability convergence bounds with polylogarithmic dependence on the confidence level for AdaGrad and Adam with clipping and with/without delay for smooth convex/non-convex stochastic optimization with heavy-tailed noise. Our empirical evaluations highlight the superiority of clipped versions of AdaGrad/Adam in handling the heavy-tailed noise.",ICLR.cc/2025/Conference,5.25,False,0.8164,contributions online lipschitz estimator provable concentration bounds under bounded variance and smoothness principled step size controller that guarantees sufficient decrease expectation and adapts per block learning rates implementation strategies sketching amortized probing enabling low overhead large models impact autolips provides theoretically grounded practical mechanism automate step size selection local curvature and noise reducing manual tuning and improving robustness for large scale iclr style optimization tasks,methods adaptive stepsizes such adagrad and adam are essential for training modern deep learning models large language models derive high probability convergence bounds polylogarithmic dependence the confidence level for adagrad and adam clipping and with without delay for smooth convex non convex stochastic optimization heavy tailed noise,2025-08-26T00:35:16.354633
81,SafeAugLagr: Augmented Lagrangian with Certified Feasibility for Constrained Deep Learning,"Motivation: Enforcing hard constraints (safety, resource budgets, fairness) during deep model training is critical for deployment, but existing constrained optimizers either sacrifice feasibility guarantees or are impractical at scale. Approach: We present SafeAugLagr, an augmented Lagrangian framework that integrates differentiable certification checks into the outer multiplier updates and uses a stochastic proximal inner solver. SafeAugLagr augments the classical primal-dual loop with certificate-aware penalty adaptation: before accepting a multiplier update, the method computes high-confidence feasibility certificates via concentration bounds over minibatch constraint estimates and adjusts penalties to guarantee probabilistic constraint satisfaction. Contributions: (1) A certificate-driven augmented Lagrangian algorithm that yields high-probability feasibility guarantees under sampling noise; (2) convergence analysis demonstrating subsequence convergence to KKT points with controlled feasibility violation depending on sample size and penalty schedule; (3) scalable implementations leveraging implicit differentiation for multiplier gradients and variance-reduced proximal inner solves. Results: SafeAugLagr enforces tight resource and fairness constraints in tasks including energy-constrained model compression, fairness-aware classification, and safety-constrained RL, outperforming penalty-only and projection baselines in constraint satisfaction and final task utility; empirical certificates matched observed feasibility across datasets. Impact: SafeAugLagr operationalizes certified constrained training for deep models, enabling reliable enforcement of deployment-critical constraints in ICLR-relevant systems while maintaining optimization efficiency.",ICLR,optimization,gpt-5-mini,True,11099,COSTAR: Dynamic Safety Constraints Adaptation in Safe Reinforcement Learning,"Recent advancements in safe reinforcement learning (safe RL) have focused on developing agents that maximize rewards while satisfying predefined safety constraints. However, the challenge of learning policies capable of generalizing to dynamic safety requirements has rarely been explored. To this end, we propose a novel COntrastive Safe TAsk Representation (COSTAR) framework for safe RL, which can boost existing algorithm's generalization to dynamic safety constraints, including variable cost functions and safety thresholds.In COSTAR, we employ a Safe Task Encoder to extract safety-specific representations from trajectory contexts, effectively distinguishing between various safety constraints with contrastive learning. It is noteworthy that our framework can integrate with existing safe RL algorithms and possesses zero-shot adaptation capability to varying safety constraints during deployment. Extensive experiments demonstrate that our COSTAR framework consistently achieves high rewards while maintaining low costs, and exhibits robust generalization capabilities when dealing with out-of-distribution (OOD) tasks.",ICLR.cc/2025/Conference,3.0,nan,0.8312,motivation enforcing hard constraints safety resource budgets fairness during deep training critical for deployment but existing constrained optimizers either sacrifice feasibility guarantees are impractical scale safeauglagr enforces tight resource and fairness constraints tasks including energy constrained compression fairness aware classification and safety constrained outperforming penalty only and projection baselines constraint satisfaction and final task utility empirical certificates matched observed feasibility across datasets impact safeauglagr operationalizes certified constrained training for deep models enabling reliable enforcement deployment critical constraints iclr relevant systems while maintaining optimization efficiency,recent advancements safe reinforcement learning safe have focused developing agents that maximize rewards while satisfying predefined safety constraints however the challenge learning policies capable generalizing dynamic safety requirements has rarely been explored this end contrastive safe task representation costar for safe which can boost existing algorithm generalization dynamic safety constraints including variable cost functions and safety thresholds costar employ safe task encoder extract safety specific representations from trajectory contexts distinguishing between various safety constraints contrastive learning noteworthy that our can integrate existing safe algorithms and possesses zero shot adaptation capability varying safety constraints during deployment,2025-08-26T00:35:16.354636
82,FedVar: Variance-Adaptive Aggregation for Robust Federated Optimization,"Motivation: Client heterogeneity and stochastic variability hinder federated convergence and can bias global models, particularly under partial participation. Approach: We introduce FedVar, a server-side aggregation scheme that adaptively weights client updates by online estimates of their update variance and staleness. Each client sends compact variance sketches along with parameter deltas; the server computes per-client credible intervals and applies variance-adaptive weights that downweight noisy or highly stale contributions while preserving unbiasedness in expectation. FedVar further incorporates a stabilization term that injects a global control variate estimated from frequent large-batch snapshots to reduce aggregate variance. Contributions: (1) A practical variance-adaptive aggregation protocol with low communication overhead using sketching; (2) theoretical guarantees: FedVar converges to stationary points under heterogeneous objectives and arbitrary partial participation, with explicit rates showing improved dependence on variance heterogeneity; (3) mechanisms for robustness to adversarial clients via variance-based anomaly detection. Results: On cross-device language modeling, medical imaging, and highly skewed recommendation datasets, FedVar accelerates convergence and improves both global and per-client metrics versus FedAvg, FedProx, and SCAFFOLD; it is particularly effective when client noise and participation vary. Impact: FedVar offers a principled, implementable way to mitigate variance-induced inefficiencies in federated learning, enabling more robust and efficient training under realistic system heterogeneity.",ICLR,optimization,gpt-5-mini,True,6341,Debiasing Federated Learning with Correlated Client Participation,"In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice.  Existing analyses of FedAvg usually assume the participating clients are independently sampled in each round from a uniform distribution, which does not reflect real-world scenarios. This paper introduces a theoretical framework that models client participation in FL as a Markov chain to study optimization convergence when clients have non-uniform and correlated participation across rounds. 
We apply this framework to analyze a more practical pattern: every client must wait a minimum number of $R$ rounds (minimum separation) before re-participating. We theoretically prove and empirically observe that increasing minimum separation reduces the bias induced by intrinsic non-uniformity of client availability in cross-device FL systems. 
Furthermore, we develop an effective debiasing algorithm for FedAvg that provably converges to the unbiased optimal solution under arbitrary minimum separation and unknown client availability distribution.",ICLR.cc/2025/Conference,6.75,True,0.8414,contributions practical variance adaptive aggregation protocol low communication overhead sketching theoretical guarantees fedvar converges stationary points under heterogeneous objectives and arbitrary partial participation explicit rates showing improved dependence variance heterogeneity mechanisms for robustness adversarial clients variance based anomaly detection cross device language modeling medical imaging and highly skewed recommendation datasets fedvar accelerates convergence and improves both global and per client metrics versus fedavg fedprox and scaffold effective when client noise and participation vary,cross device federated learning millions mobile clients only small subset clients participate training every communication round and federated averaging fedavg the most popular practice this introduces theoretical that models client participation markov chain optimization convergence when clients have non uniform and correlated participation across rounds,2025-08-26T00:35:16.354638
83,Kronecker-Free Quasi-Newton for Attention Layers with Low-Rank Curvature Updates,"Motivation: Attention-rich models (transformers) pose challenges for quasi-Newton and natural-gradient methods because standard Kronecker factorizations are expensive or inaccurate for attention blocks. Approach: We propose KFQN-Attn, a Kronecker-free quasi-Newton optimizer that builds low-rank curvature approximations specifically tailored to multi-head attention. KFQN-Attn computes structured low-rank sketches of layerwise Gauss-Newton blocks via randomized probing across attention heads and time steps, forms a compact solver in the sketched subspace, and applies a stable damping and residual correction for the orthogonal complement. The method avoids explicit Kronecker decompositions, reducing memory and enabling straightforward GPU parallelism. Contributions: (1) A novel attention-aware low-rank curvature construction that captures temporal and head-wise correlations efficiently; (2) convergence analysis showing descent and eventual convergence to stationary points under standard smoothness and bounded-noise conditions, with complexity depending on sketch rank; (3) practical techniques for amortized Hessian-vector probing and attention-specific damping heuristics. Results: On BERT finetuning and transformer language-model pretraining proxies, KFQN-Attn speeds convergence in epochs by 1.5–3× compared to AdamW and yields improved early perplexity while maintaining comparable wall-clock time due to efficient low-rank computations. Impact: KFQN-Attn makes quasi-Newton acceleration practical for attention-dominant architectures, providing a deployable optimizer to reduce training iterations for large ICLR-scale transformer models.",ICLR,optimization,gpt-5-mini,True,942,Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data,"Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism.  In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. 
To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation.
We validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.",ICLR.cc/2025/Conference,3.75,False,0.8410,motivation attention rich models transformers pose challenges for quasi newton and natural gradient methods because standard kronecker factorizations are expensive inaccurate for attention blocks kfqn attn kronecker free quasi newton optimizer that builds low rank curvature approximations tailored multi head attention kfqn attn computes structured low rank sketches layerwise gauss newton blocks randomized probing across attention heads and time steps forms compact solver the sketched subspace and applies stable damping and residual correction for the orthogonal complement bert finetuning and transformer language model pretraining proxies kfqn attn speeds convergence epochs compared adamw and yields improved early perplexity while maintaining comparable wall clock time due efficient low rank computations impact kfqn attn makes quasi newton acceleration practical for attention dominant architectures providing deployable optimizer reduce training iterations for large iclr scale transformer models,transformers are now ubiquitous for sequence modeling tasks but their extension multi dimensional data remains challenge due the quadratic cost the attention mechanism address the computational challenges associated high order tensor attention kronecker factorized attention mechanism that reduces the attention cost quadratic each axis dimension rather than quadratic the total size the input tensor this strategy maintains the model expressiveness while enabling scalable attention computation the effectiveness hot two high dimensional tasks including multivariate time series forecasting and medical image classification,2025-08-26T00:35:16.354645
84,DRAS: Distributionally Robust Active Sampling for Label-Efficient Robust Training,"Motivation: Building robust models under distribution shift often requires many labeled examples in worst-case regions, making robustness expensive in labeling cost. Approach: We propose DRAS, a sampling strategy that unifies distributionally robust optimization (DRO) with active learning: DRAS selects unlabeled samples for annotation that maximally reduce the worst-case risk under a Wasserstein-style uncertainty set. At each round, the method estimates the worst-case reweighting using a smoothed DRO inner solver, computes per-sample value-of-information scores via influence-function approximations, and queries labels for high-impact points. Contributions: (1) A principled active sampling algorithm targeting DRO objectives to maximize robustness per label; (2) theoretical guarantees linking sample-query selection to reductions in the certified worst-case risk and bounds on sample complexity under smoothness and bounded-variation assumptions; (3) scalable estimators using randomized influence approximations and minibatch-friendly DRO solvers. Results: DRAS achieves substantial label-efficiency: on corrupted CIFAR variants and domain-shifted vision tasks, it attains the same certified robustness levels as random or uncertainty sampling with 40–70% fewer labels. DRAS also improves worst-group accuracy in fairness-oriented datasets with limited labeling budgets. Impact: DRAS enables cost-effective robust model training by directing labeling effort to samples that most reduce distributional vulnerability, offering a valuable tool for practitioners aiming for certified robustness under limited annotation resources.",ICLR,optimization,gpt-5-mini,True,6913,Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization,"Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense.Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.",ICLR.cc/2025/Conference,6.5,True,0.8671,motivation building robust models under distribution shift often requires many labeled examples worst case regions making robustness expensive labeling cost dras sampling strategy that unifies distributionally robust optimization dro active learning dras selects unlabeled samples for annotation that maximally reduce the worst case risk under wasserstein style uncertainty set contributions principled active sampling targeting dro objectives maximize robustness per label theoretical guarantees linking sample query selection reductions the certified worst case risk and bounds sample complexity under smoothness and bounded variation assumptions scalable estimators randomized influence approximations and minibatch friendly dro solvers dras achieves substantial label efficiency corrupted cifar variants and domain shifted vision tasks attains the same certified robustness levels random uncertainty sampling fewer labels impact dras enables cost effective robust training directing labeling effort samples that most reduce distributional vulnerability offering valuable tool for practitioners aiming for certified robustness under limited annotation resources,wasserstein distributionally robust optimization wdro optimizes against worst case distributional shifts within specified uncertainty set leading enhanced generalization unseen adversarial examples compared standard adversarial training which focuses pointwise adversarial perturbations address this gap proposing robust optimization under uncertainty set for adversarial noise wasserstein distance and statistical error kullback leibler divergence called the statistically robust wdro establish robust generalization bound for the optimization implying that out distribution adversarial least good the statistically robust training loss high probability finally extensive experiments that our mitigates robust overfitting and enhances robustness within the wdro,2025-08-26T00:35:16.354646
85,Randomized Implicit Hypergradients: Scalable Hyperparameter Optimization without Unrolling,"Motivation: Hyperparameter optimization for large models is hindered by the cost of unrolling long inner optimization trajectories; implicit differentiation scales better but often relies on expensive exact linear solves. Approach: We introduce Randomized Implicit Hypergradients (RIH), an approximate implicit differentiation method that estimates hypergradients by solving the sensitivity linear system via randomized sketching and stochastic Lanczos methods. RIH constructs compact sketch subspaces that capture dominant Jacobian directions with a handful of Hessian-vector products and computes an approximate inverse-vector product with controlled bias. We combine this with variance-reduced stochastic gradients and checkpointed reversible integration to limit memory. Contributions: (1) An efficient randomized linear solver for hypergradient computation with provable bounds on approximation bias and variance relative to exact implicit differentiation. (2) Analysis linking sketch dimension, solver iterations, and hypergradient quality, with practical rules to balance compute and hypergradient bias. (3) Integration recipes for hyperparameter tuning, meta-initialization, and bilevel learning at scale. Results: RIH accelerates hyperparameter search for learning-rate schedules, weight decay, and data-augmentation parameters in transformers and CNNs, achieving better validation improvement per GPU-hour than truncated unrolling and exact implicit methods under memory constraints. Impact: RIH makes high-quality implicit hypergradients tractable for large-scale models, reducing memory and compute barriers to practical bilevel and meta-optimization in ICLR workflows.",ICLR,optimization,gpt-5-mini,True,10632,Generalized Greedy Gradient-Based Hyperparameter Optimization,"Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization.  Approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps these difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. Additionally, we demonstrate that under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning and data hyper-cleaning tasks support our findings.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8674,motivation hyperparameter optimization for large models hindered the cost unrolling long inner optimization trajectories implicit differentiation scales better but often relies expensive exact linear solves integration recipes for hyperparameter tuning meta initialization and bilevel learning scale,bilevel optimization blo widely used that has numerous applications including hyperparameter optimization meta learning reverse mode differentiation suffers from high memory requirements while the methods the implicit function theorem require the convergence the inner optimization approximations that consider truncated inner optimization trajectory suffer from short horizon bias accumulate the short horizon approximations from each step the inner optimization trajectory experimental few shot meta learning and data hyper cleaning tasks support our,2025-08-26T00:35:16.354648
86,QuantBias: Quantization-Aware Optimizers with Online Bias Correction,"Motivation: Aggressive gradient quantization reduces communication but introduces bias that degrades convergence; standard error-feedback mitigates bias over time but can accumulate error under nonstationary gradients. Approach: We propose QuantBias, an optimizer-integrated quantization framework that performs online bias estimation and corrective compensation. QuantBias maintains compact running estimates of per-coordinate quantization bias using control-variate-style statistics collected from recent quantized updates; it subtracts estimated bias before compression and applies a corrected error-feedback update to the local accumulator. The scheme is lightweight, compatible with common compressors, and includes confidence-weighted bias decay to prevent over-correction under noise. Contributions: (1) A general-purpose online bias estimation and correction method that reduces long-term quantization bias while preserving low communication. (2) Theoretical analysis showing QuantBias yields unbiased gradient estimates in expectation up to bounded estimator error and attains O(1/√T) convergence for smooth nonconvex objectives with explicit dependence on bias estimator variance. (3) Implementation guidelines for memory-efficient bias sketches and interplay with asynchronous updates. Results: QuantBias reduces communication by up to 16× with minimal accuracy loss on distributed transformer pretraining and large CNNs, outperforming standard error-feedback and naive stochastic rounding in stability and final performance, especially under nonstationary training dynamics. Impact: QuantBias bridges practical compression and rigorous bias control, enabling more aggressive communication reduction without sacrificing convergence—beneficial for large-scale distributed ICLR experiments.",ICLR,optimization,gpt-5-mini,False,,BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM inference,"Post-training quantization (PTQ) is a promising approach to reducing the storage and computational requirements of large language models (LLMs) without additional training cost. Recent PTQ studies have primarily focused on quantizing only weights to sub-8-bits while maintaining activations at 8-bits or higher. Accurate sub-8-bit quantization for both weights and activations without relying on quantization-aware training remains a significant challenge. In this work, we introduce a novel quantization method called block clustered quantization (BCQ) wherein each operand tensor is decomposed into blocks (a block is a group of contiguous scalars), blocks are clustered based on their statistics, and a dedicated optimal quantization codebook is designed for each cluster. We propose a PTQ algorithm called Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block clustering and codebook design to greedily minimize the quantization mean squared error. When weight and activation scalars are encoded to W4A4 format (with 0.5-bits of overhead for storing scaling factors and codebook selectors), we advance the current state-of-the-art by demonstrating <1% loss in inference accuracy across several LLMs and downstream tasks.",ICLR.cc/2025/Conference,5.5,False,0.7954,quantbias reduces communication minimal loss distributed transformer pretraining and large cnns outperforming standard error feedback and naive stochastic rounding stability and final under nonstationary training dynamics,post training quantization ptq promising reducing the storage and computational requirements large language models llms additional training cost ptq called locally optimal bcq bcq that iterates between the steps block clustering and codebook greedily minimize the quantization mean squared error,2025-08-26T00:35:16.354650
87,Value-of-Information Curriculum: Bandit-Driven Scheduling for Faster Generalization,"Motivation: Curriculum strategies can speed learning but most heuristics (loss/margin-based) lack principled criteria and may neglect long-term generalization gains. Approach: We propose Value-of-Information Curriculum (VoIC), a bandit-based scheduling framework that selects mini-batches to maximize estimated long-term validation improvement. At each scheduling decision VoIC computes a value-of-information (VoI) score for candidate batches by simulating one-step parameter perturbations and estimating downstream validation change via a learned regression model; a contextual bandit balances exploration and exploitation across batches, with reward delayed and credit-assigned via importance-weighted returns. Contributions: (1) A principled VoI formulation for curriculum scheduling rooted in decision-theoretic expected utility that targets validation improvement. (2) Algorithmic contributions: efficient one-step lookahead estimators, a scalable contextual bandit for batch selection, and variance-reduction for delayed rewards. (3) Theoretical insights: regret bounds for the bandit scheduler and conditions under which VoIC improves sample complexity relative to uniform sampling. Results: VoIC accelerates convergence and improves final generalization on image classification, robustness tasks, and RL curricula, reducing labeled-data requirements and outperforming loss-based and learned-scoring curricula in both speed and validation performance. Impact: VoIC provides a decision-theoretic, scalable approach to curriculum design that prioritizes long-term generalization, offering a practical tool to enhance training efficiency for a broad range of ICLR-relevant applications.",ICLR,optimization,gpt-5-mini,False,,Safety-Prioritizing Curricula for Constrained Reinforcement Learning,"Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. 
Although existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must adhere to safety constraints.
Thus, these approaches may generate tasks that cause RL agents to violate safety constraints during training and behave suboptimally after. 
We develop a safe curriculum generation approach (SCG) that aligns the objectives of constrained RL and curriculum learning: improving safety during training and boosting sample efficiency.
SCG generates sequences of tasks where the RL agent can be safe and performant by initially generating tasks with minimum safety violations over high-reward ones.
We empirically show that compared to the state-of-the-art curriculum learning approaches and their naively modified safe versions, SCG achieves optimal performance and the lowest amount of constraint violations during training.",ICLR.cc/2025/Conference,5.25,True,0.7906,motivation curriculum strategies can speed learning but most heuristics loss margin based lack principled criteria and may neglect long term generalization gains voic accelerates convergence and improves final generalization image classification robustness tasks and curricula reducing labeled data requirements and outperforming loss based and learned scoring curricula both speed and validation,curriculum learning aims accelerate reinforcement learning generating curricula although existing curriculum generation approaches provide benefits sample efficiency they overlook safety critical settings where agent must adhere safety constraints safe curriculum generation scg that aligns the objectives constrained and curriculum learning improving safety during training and boosting sample efficiency empirically that compared the state the art curriculum learning approaches and their naively modified safe versions scg achieves optimal and the lowest amount constraint violations during training,2025-08-26T00:35:16.354651
88,Lyapunov-Controlled Momentum: Online Stability Guarantees for Accelerated Stochastic Optimization,"Motivation: Momentum accelerates stochastic optimization but can destabilize training when noise or curvature changes, leading to bursts or divergence. Existing remedies (clipping, restarts) are heuristic and lack online stability certificates suitable for nonconvex deep learning. Approach: We introduce Lyapunov-Controlled Momentum (LCM), an optimizer that selects momentum coefficients online by minimizing an estimated Lyapunov drift bound computed from minibatch gradients and local curvature probes. LCM models parameter iterates as a discrete-time stochastic dynamical system and derives a quadratic Lyapunov candidate whose expected one-step decrease yields analytic constraints on allowable momentum. The controller solves a small closed-form optimization per step to choose momentum and step-size that maximize progress while guaranteeing nonpositive Lyapunov drift in expectation. Contributions: (1) A principled control-theoretic momentum scheduler with provable stability under bounded-gradient noise and Lipschitz smoothness. (2) Efficient estimators for Lyapunov terms using only low-cost directional Hessian-vector products and minibatch statistics, making LCM practical for large networks. (3) Nonasymptotic convergence: we prove that LCM ensures almost-sure boundedness of iterates and convergence to first-order stationary points with controllable trade-offs between speed and conservatism. Results: Empirically, LCM accelerates convergence and reduces instability on CNNs, transformers, and GANs across small- and large-batch regimes; it reduces catastrophic loss spikes and matches or improves final accuracy compared to tuned momentum schedules. Impact: LCM provides a theoretically grounded, low-overhead mechanism to safely leverage momentum across diverse ICLR-scale optimization scenarios, reducing manual tuning and improving reliability.",ICLR,optimization,gpt-5-mini,True,2739,"HOME-3: HIGH-ORDER MOMENTUM ESTIMATOR USING THIRD-POWER GRADIENT FOR CONVEX, SMOOTH NONCONVEX, AND NONSMOOTH NONCONVEX OPTIMIZATION","Momentum-based gradients are critical for optimizing advanced machine learning models, as they not only accelerate convergence but also help gradient-based optimizers overcome stationary points. While most state-of-the-art momentum techniques rely on lower-power gradients, such as the squared first-order gradient, there has been limited exploration into the potential of higher-power gradients—those raised to powers greater than two, such as the third-power first-order gradient. In this work, we introduce the concept of high-order momentum, where
momentum is constructed using higher-power gradients, with a specific focus on the third-power first-order gradient as a representative example. Our research offers both theoretical and empirical evidence of the benefits of this novel approach. From a theoretical standpoint, we demonstrate that incorporating third-power gradients into momentum can improve the convergence bounds of gradient-based optimizers
for both convex and smooth nonconvex problems. To validate these findings, we conducted extensive empirical experiments across convex, smooth nonconvex, and nonsmooth nonconvex optimization tasks. The results consistently showcase that high-order momentum outperforms traditional momentum-based optimizers, providing superior performance and more efficient optimization.",ICLR.cc/2025/Conference,3.5,nan,0.8485,motivation momentum accelerates stochastic optimization but can destabilize training when noise curvature changes leading bursts divergence existing remedies clipping restarts are heuristic and lack online stability certificates suitable for nonconvex deep learning the controller solves small closed form optimization per step choose momentum and step size that maximize progress while guaranteeing nonpositive lyapunov drift expectation impact lcm provides theoretically grounded low overhead mechanism safely leverage momentum across diverse iclr scale optimization scenarios reducing manual tuning and improving reliability,momentum based gradients are critical for optimizing advanced machine learning models they not only accelerate convergence but also help gradient based optimizers overcome stationary points these conducted extensive empirical experiments across convex smooth nonconvex and nonsmooth nonconvex optimization tasks the consistently showcase that high order momentum outperforms traditional momentum based optimizers providing superior and more efficient optimization,2025-08-26T00:35:16.354653
89,Sparse-BFGS: Memory-Efficient Quasi-Newton Updates for Extreme-Scale Sparse Models,"Motivation: Large-scale recommendation and language models rely on embedding tables with extremely high dimensionality and sparsity. First-order optimizers are slow to converge on such parameters, while classical quasi-Newton methods are infeasible due to density and memory. Approach: We present Sparse-BFGS, a quasi-Newton framework that constructs and applies low-memory curvature updates tailored to sparse parameter blocks. Sparse-BFGS maintains compact, blockwise limited-memory secant pairs only for frequently updated coordinates and uses randomized sampling to approximate curvature for rarely touched entries. Updates are applied via sparse low-rank transforms that preserve sparsity in forward/backward passes. An adaptive eviction policy controls which coordinates retain curvature history based on access frequency and estimated utility. Contributions: (1) A novel sparse quasi-Newton algorithm that yields curvature-informed updates while maintaining sparse parameter structures and linear memory scaling in active set size. (2) Theoretical guarantees: under blockwise smoothness and bounded sampling variance, Sparse-BFGS achieves improved local convergence constants compared to SGD with the same update budget. (3) Practical system designs: locality-aware caching, efficient sparse linear algebra kernels, and heuristics for frequency-adaptive history maintenance. Results: On large-scale recommendation systems and language-model embedding-training tasks, Sparse-BFGS reduces epochs-to-target by 2–5× over Adam/Adagrad on sparse parameters and yields better hit-rate and perplexity, with negligible extra memory beyond active embeddings. Impact: Sparse-BFGS enables second-order-like acceleration in environments dominated by massive sparse parameter spaces, offering a practical optimization advance for industry-scale ICLR-relevant applications.",ICLR,optimization,gpt-5-mini,True,10158,R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference,"Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50\% model-level sparsity, resulting in a significant 43\% end-to-end efficient improvements with customized kernels.",ICLR.cc/2025/Conference,5.5,True,0.8282,motivation large scale recommendation and language models rely embedding tables extremely high dimensionality and sparsity impact sparse bfgs enables second order like acceleration environments dominated massive sparse parameter spaces offering practical optimization advance for industry scale iclr relevant applications,large language models llms while demonstrating remarkable capabilities across various applications significant challenges during inference due their substantial size when deployed edge devices building this replace the linear layers llms rank aware sparse inference that leverages the sparsity input channels and singular value components eliminating the need for active channel prediction like the output sparsity approaches,2025-08-26T00:35:16.354657
90,InfluencePrune: Data-Driven Pruning via Causal Influence Estimation for Efficient Training,"Motivation: Training large models on massive datasets is costly; pruning redundant or harmful training samples can reduce computation and improve generalization, yet current heuristics (loss, gradient norm) can discard valuable data or retain noisy samples. Approach: We propose InfluencePrune, a principled data pruning strategy that estimates each training example’s causal influence on validation loss using efficient, randomized influence function approximations. InfluencePrune computes per-sample influence scores via low-cost Hessian-vector product sketches and randomized trace estimators, clusters samples to amortize computation, and prunes those with provably negligible or negative marginal validation impact. We incorporate robust statistics to limit sensitivity to heavy-tailed gradients and provide an online variant that updates influence estimates during training. Contributions: (1) A scalable influence-estimation pipeline enabling principled pruning with quantifiable validation impact bounds. (2) Theoretical analysis: high-probability guarantees linking estimator accuracy, pruning threshold, and resulting validation-loss deviation. (3) Practical algorithms: clustering for amortization, streaming updates for online pruning, and safeguards against distributional drift. Results: InfluencePrune reduces training data by 20–60% while preserving or improving validation accuracy across vision and NLP benchmarks, accelerates convergence, and is effective at removing label-noise and distributional outliers better than loss-based pruning and coreset baselines. Impact: InfluencePrune provides a rigorous, scalable method to shrink training workloads without sacrificing performance, enabling more efficient experimentation and deployment for ICLR-scale models.",ICLR,optimization,gpt-5-mini,True,2653,DRoP: Distributionally Robust Data Pruning,"In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets.",ICLR.cc/2025/Conference,7.333333333333333,True,0.8262,practical algorithms clustering for amortization streaming updates for online pruning and safeguards against distributional drift influenceprune reduces training data while preserving improving validation across vision and nlp benchmarks accelerates convergence and effective removing label noise and distributional outliers better than loss based pruning and coreset baselines,the era exceptionally data hungry models careful selection the training data essential mitigate the extensive costs deep learning data pruning offers solution removing redundant uninformative samples from the which yields faster convergence and improved neural scaling laws however little known about its impact classification bias the trained models theoretical analysis the classification risk mixture gaussians argue that choosing appropriate class pruning ratios coupled random pruning within classes has potential improve worst class thus drop distributionally robust pruning and empirically its standard computer vision benchmarks sharp contrast existing algorithms our proposed continues improving distributional robustness tolerable drop average prune more from the datasets,2025-08-26T00:35:16.354660
91,ODE-Precond: Continuous-Time Preconditioning via Learned Neural Vector Fields,"Motivation: Preconditioning transforms can dramatically accelerate optimization, but hand-designed preconditioners often fail to capture complex, data-dependent curvature in deep networks. Approach: We introduce ODE-Precond, a continuous-time preconditioning framework that learns a low-dimensional neural vector field defining a coordinate transformation under which gradient descent exhibits improved conditioning. ODE-Precond parameterizes an invertible residual flow whose Jacobian approximates desirable preconditioning matrices; the flow is trained online to minimize an empirical conditioning objective derived from local quadratic surrogates. Optimization alternates between updating model parameters in transformed coordinates with standard optimizers and updating the flow to maintain favorable conditioning. Contributions: (1) A novel continuous-time learned preconditioner that adapts to evolving loss geometry and is implementable with modest overhead via shallow invertible networks. (2) Analysis linking flow Jacobian properties to effective condition number reduction and providing conditions for stable composition with stochastic optimizers. (3) Practical algorithms: incremental flow updates, stability-regularized flow training, and low-rank approximations for large layers. Results: ODE-Precond accelerates convergence on CNNs, transformers, and recurrent models, reducing iterations to target by 30–60% and improving robustness to learning-rate misspecification, with modest extra compute. Ablations show learned flows capture layer-specific scaling and cross-parameter couplings missed by diagonal schemes. Impact: ODE-Precond introduces a flexible learned preconditioning paradigm, enabling optimizer-aware coordinate transforms that improve efficiency across diverse ICLR training regimes.",ICLR,optimization,gpt-5-mini,True,8358,Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers,"Large linear systems are ubiquitous in modern computational science and engineering. The main recipe for solving them is the use of Krylov subspace iterative methods with well-designed preconditioners. Deep learning models can be used as nonlinear preconditioners during the iteration of linear solvers such as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners with predefined sparsity patterns. Recently, GNNs have been shown to be a promising tool for designing preconditioners to reduce the overall computational cost of iterative methods by constructing them more efficiently than with classical linear algebra techniques. However, preconditioners designed with these approaches cannot outperform those designed with classical methods in terms of the number of iterations in CG. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN to obtain preconditioners that reduce the condition number of the system more significantly. Numerical experiments show that our approach outperforms both classical and neural network-based methods for an important class of parametric partial differential equations. We also provide a heuristic justification for the loss function used and show that preconditioners obtained by learning with this loss function reduce the condition number in a more desirable way for CG.",ICLR.cc/2025/Conference,4.75,False,0.8263,motivation preconditioning transforms can dramatically accelerate optimization but hand designed preconditioners often fail capture complex data dependent curvature deep networks ode precond continuous time preconditioning that learns low dimensional neural vector field defining coordinate transformation under which gradient descent exhibits improved conditioning optimization alternates between updating parameters transformed coordinates standard optimizers and updating the flow maintain favorable conditioning ode precond accelerates convergence cnns transformers and recurrent models reducing iterations target and improving robustness learning rate misspecification modest extra compute,deep learning models can used nonlinear preconditioners during the iteration linear solvers such the conjugate gradient neural network models require enormous number parameters approximate well this setup another take advantage small graph neural networks gnns construct preconditioners predefined sparsity patterns numerical experiments that our outperforms both classical and neural network based methods for important class parametric partial differential equations also provide heuristic justification for the loss function used and that preconditioners obtained learning this loss function reduce the condition number more desirable way for,2025-08-26T00:35:16.354663
92,Meta-Ensemble Optimizers: Online Bandit Selection of Optimizer Primitives,"Motivation: No single optimizer uniformly outperforms others across tasks and phases of training; practitioners resort to manual switching or extensive tuning. Approach: We propose Meta-Ensemble Optimizers (MEO), an online meta-controller that dynamically composes an ensemble of optimizer primitives (SGD, momentum, Adam, quasi-Newton steps) using a contextual bandit that selects per-layer or per-block primitives to maximize short-term validation improvement. MEO collects lightweight performance signals via shadow updates and uses variance-reduced reward estimates and confidence-driven exploration. Controller policies are learned online and can be warm-started from meta-trained priors across tasks. Contributions: (1) A modular optimizer architecture enabling adaptive composition of heterogeneous optimizer primitives with theoretical bandit-based regret control. (2) Analysis connecting controller regret to downstream convergence, with conditions ensuring the meta-controller does not destabilize base optimizers. (3) Practical strategies: low-overhead shadow-update estimation, layer-wise context features (gradient statistics, curvature proxies), and safe fallback to baseline optimizers. Results: MEO improves wall-clock validation progress across diverse benchmarks (vision, language, reinforcement learning) by adaptively exploiting complementary strengths of primitives; it reduces total tuning time and outperforms static best-of baselines and naive switching heuristics. Impact: MEO offers a principled route to optimizer adaptability, automating optimizer selection and composition to improve efficiency and robustness in large-scale ICLR experiments.",ICLR,optimization,gpt-5-mini,True,1117,BLIMEY: Towards Better Routing Methods in Sparse Mixture of Experts,"Mixture of Experts (MoE) architectures offer a promising avenue for scaling neural networks by facilitating parameter-efficient model expansion while optimizing FLOP utilization. However, several challenges persist, including sub-optimal expert utilization, vanishing gradients, sub-par expert specialization, and inconsistent routing decisions. We introduce BLIMEY, a novel diagnostic framework that systematically quantifies these critical issues, enabling design and implementation of more robust and efficient expert routing algorithms. BLIMEY equips researchers with granular, interpretable metrics on MoE dynamics, explaining both the performance advantages over monolithic architectures and the persistent algorithmic bottlenecks, thereby offering a comprehensive diagnostic framework for MoE optimization. Leveraging BLIMEY, we establish new scaling laws, that surpass established benchmarks like Chinchilla, achieving 3x reductions in FLOPs and 5\% performance gains. Furthermore, our framework unveils significant optimization potential in routing algorithms, revealing sub-optimal expert specification and load imbalances in current methodologies. To accelerate innovation in MoE routing and computational efficiency, we have open-sourced BLIMEY framework, including its diagnostic tools and implementation libraries.",ICLR.cc/2025/Conference,2.0,nan,0.8049,meo improves wall clock validation progress across diverse benchmarks vision language reinforcement learning adaptively exploiting complementary strengths primitives reduces total tuning time and outperforms static best baselines and naive switching heuristics impact meo offers principled route optimizer adaptability automating optimizer selection and composition improve efficiency and robustness large scale iclr experiments,mixture experts moe architectures offer promising avenue for scaling neural networks facilitating parameter efficient expansion while optimizing flop utilization blimey equips researchers granular interpretable metrics moe dynamics explaining both the advantages over monolithic architectures and the persistent algorithmic bottlenecks thereby offering comprehensive diagnostic for moe optimization furthermore our unveils significant optimization potential routing algorithms revealing sub optimal expert specification and load imbalances current methodologies,2025-08-26T00:35:16.354666
93,InfoCert-Compress: Compression with Information-Theoretic Certificates for Distributed Optimization,"Motivation: Aggressive model update compression reduces communication but risks degrading convergence; practitioners lack principled guarantees linking compression to optimization error. Approach: We introduce InfoCert-Compress, a compression framework that provides information-theoretic certificates bounding the mutual information loss between original and compressed updates and translates these bounds into high-probability optimization error guarantees. The compressor adaptively selects quantization levels per block by solving a constrained rate-allocation problem that minimizes certified risk subject to a bit budget. We derive concentration-based certificates for common compressors (stochastic quantization, top-k sparsification) and present efficient algorithms to compute per-round allocations using streaming variance estimates. Contributions: (1) A unifying information-theoretic certification methodology that relates compression rate to optimization degradation with provable high-probability guarantees. (2) Adaptive per-block rate allocation algorithms that optimize communication while satisfying a target certificate. (3) Implementation techniques for low-latency encoding and practical certificate monitoring. Results: InfoCert-Compress achieves significant communication reductions (10–30×) while maintaining certified optimization error bounds across transformer pretraining and distributed image classification; empirically it matches or exceeds baselines that lack certificates, and certificates correlate well with observed accuracy degradation. Impact: InfoCert-Compress empowers distributed training with principled communication-accuracy trade-offs and deployable certificates, improving reliability and efficiency for ICLR-scale distributed optimization.",ICLR,optimization,gpt-5-mini,True,8884,Markovian Compression: Looking to the Past Helps Accelerate the Future,"This paper deals with distributed optimization problems that use compressed communication to achieve efficient performance and mitigate the communication bottleneck. We propose a family of compression schemes in which operators transform vectors fed to their input according to a Markov chain, i.e., the stochasticity of the compressors depends on previous iterations. Intuitively, this should accelerate the convergence of optimization methods, as considering previous iterations seems more natural and robust. The compressors are implemented in the vanilla Quantized Stochastic Gradient Descent (QSGD) algorithm. To further improve efficiency and convergence rate, we apply the momentum acceleration method. We prove convergence results for our algorithms with Markovian compressors and show theoretically that the accelerated method converges faster than the basic version. The analysis covers non-convex, Polyak-Lojasiewicz (PL), and strongly convex cases. Experiments are conducted to demonstrate the applicability of the results to distributed data-parallel optimization problems. Practical results demonstrate the superiority of methods utilizing our compressors design over several existing optimization algorithms.",ICLR.cc/2025/Conference,5.25,False,0.8178,motivation aggressive update compression reduces communication but risks degrading convergence practitioners lack principled guarantees linking compression optimization error infocert compress compression that provides information theoretic certificates bounding the mutual information loss between original and compressed updates and translates these bounds into high probability optimization error guarantees contributions unifying information theoretic certification methodology that relates compression rate optimization degradation provable high probability guarantees infocert compress achieves significant communication reductions while maintaining certified optimization error bounds across transformer pretraining and distributed image classification empirically matches exceeds baselines that lack certificates and certificates correlate well observed degradation impact infocert compress empowers distributed training principled communication accuracy trade offs and deployable certificates improving reliability and efficiency for iclr scale distributed optimization,this deals distributed optimization problems that use compressed communication achieve efficient and mitigate the communication bottleneck intuitively this should accelerate the convergence optimization methods considering previous iterations seems more natural and robust experiments are conducted the applicability the distributed data parallel optimization problems practical the superiority methods utilizing our compressors over several existing optimization algorithms,2025-08-26T00:35:16.354669
94,TrustBilevel: Trust-Region Surrogates for Stable Bilevel Optimization,"Motivation: Bilevel optimization problems (hyperparameter tuning, meta-learning) are sensitive to inner-solver error; naive outer updates using approximate hypergradients can diverge. Approach: We propose TrustBilevel, a trust-region-based bilevel solver that constructs local convex quadratic surrogates for the outer objective by implicitly differentiating a regularized inner problem solved to adaptive accuracy. Each outer step fits a trust-region model using probe evaluations of the approximate hypergradient and accepts steps based on predicted vs. actual validation decrease, with inner-solve tolerances adjusted by a fidelity controller that balances compute and hypergradient quality. Contributions: (1) A trust-region framework tailored to bilevel optimization that ensures robust outer progress despite stochastic inner solves. (2) Theoretical results: convergence to bilevel stationary points under smoothness and controlled inner-solve residuals, with complexity trade-offs between inner accuracy and outer iterations. (3) Practical advances: variance-reduced hypergradient estimation, efficient implicit solvers, and a fidelity control law that adaptively allocates inner iterations. Results: TrustBilevel outperforms truncated unrolling and naive implicit methods on hyperparameter tuning and meta-learning benchmarks, achieving more stable outer-step acceptance and faster validation improvement per compute. Impact: TrustBilevel provides a robust, theoretically grounded approach for scalable bilevel problems, making reliable meta-optimization feasible in large-scale ICLR applications.",ICLR,optimization,gpt-5-mini,True,10632,Generalized Greedy Gradient-Based Hyperparameter Optimization,"Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization.  Approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps these difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. Additionally, we demonstrate that under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning and data hyper-cleaning tasks support our findings.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8803,motivation bilevel optimization problems hyperparameter tuning meta learning are sensitive inner solver error naive outer updates approximate hypergradients can diverge contributions trust region tailored bilevel optimization that ensures robust outer progress despite stochastic inner solves,bilevel optimization blo widely used that has numerous applications including hyperparameter optimization meta learning reverse mode differentiation suffers from high memory requirements while the methods the implicit function theorem require the convergence the inner optimization approximations that consider truncated inner optimization trajectory suffer from short horizon bias accumulate the short horizon approximations from each step the inner optimization trajectory experimental few shot meta learning and data hyper cleaning tasks support our,2025-08-26T00:35:16.354671
95,Manifold Primal-Dual Descent: Constrained Optimization on Riemannian Parameter Spaces,"Motivation: Constraints naturally arise in many learning tasks (orthogonality, spectral norms, fairness), and sometimes the parameter domain itself is a manifold; existing primal-dual methods do not fully exploit Riemannian geometry, reducing efficiency and convergence robustness. Approach: We introduce Manifold Primal-Dual Descent (MPDD), a primal-dual optimizer that operates directly on Riemannian manifolds with dual variables in Euclidean space. MPDD performs Riemannian gradient descent on primal variables using geometry-aware retractions, and dual ascent with proximal updates for inequality constraints. The algorithm integrates vector transport and manifold-aware linearization in the dual updates, and employs a merit function-based linesearch to ensure global progress. Contributions: (1) A principled primal-dual method for constrained optimization where primal variables lie on manifolds, with careful handling of geometry in dual updates. (2) Convergence theory: subsequential convergence to KKT points under geodesic smoothness and bounded stochastic tangent noise, and explicit feasibility-violation rates. (3) Implementations for common manifolds (Stiefel, Grassmann, SPD) with efficient retractions and transport primitives. Results: MPDD efficiently enforces orthogonality, low-rank, and spectral constraints in deep models, outperforming ambient-space penalty and projection heuristics in constraint satisfaction and final task utility on vision and representation-learning benchmarks. Impact: MPDD extends constrained optimization capabilities to manifold-structured parameter spaces with theoretical rigor and practical efficiency, broadening modeling choices for ICLR-scale problems.",ICLR,optimization,gpt-5-mini,True,8283,Efficient optimization with orthogonality constraint: a randomized Riemannian submanifold method,"Optimization with orthogonality constraints frequently arise in various fields such as machine learning, signal processing and computer vision. Riemannian optimization offers a powerful framework for solving these problems by equipping the constraint set with a Riemannian manifold structure and performing optimization intrinsically on the manifold. This approach typically involves computing a search direction in the tangent space and updating variables via a retraction operation. However, as the size of the variables increases, the computational cost of the retraction can become prohibitively high, limiting the applicability of Riemannian optimization to large-scale problems.  To address this challenge and enhance scalability, we propose a novel approach that restricts each update on a random submanifold, thereby significantly reducing the per-iteration complexity. We introduce two sampling strategies for selecting the random submanifold and theoretically analyze the convergence of the proposed method. We provide convergence results for general nonconvex functions and functions that satisfy Riemannian Polyak–Łojasiewicz condition as well as for stochastic optimization settings. Extensive experiments verify the benefits of the proposed method, showcasing its effectiveness across a wide variety of problem instances.",ICLR.cc/2025/Conference,6.0,False,0.8652,motivation constraints naturally arise many learning tasks orthogonality spectral norms fairness and sometimes the parameter domain itself manifold existing primal dual methods not fully exploit riemannian geometry reducing efficiency and convergence robustness contributions principled primal dual for constrained optimization where primal variables lie manifolds careful handling geometry dual updates mpdd enforces orthogonality low rank and spectral constraints deep models outperforming ambient space penalty and projection heuristics constraint satisfaction and final task utility vision and representation learning benchmarks impact mpdd extends constrained optimization capabilities manifold structured parameter spaces theoretical rigor and practical efficiency broadening modeling choices for iclr scale problems,optimization orthogonality constraints frequently arise various fields such machine learning signal processing and computer vision riemannian optimization offers powerful for solving these problems equipping the constraint set riemannian manifold structure and performing optimization intrinsically the manifold however the size the variables increases the computational cost the retraction can become prohibitively high limiting the applicability riemannian optimization large scale problems provide convergence for general nonconvex functions and functions that satisfy riemannian polyak łojasiewicz condition well for stochastic optimization settings,2025-08-26T00:35:16.354672
96,Lyapunov-Controlled Momentum: Online Stability and Acceleration for Stochastic Nonconvex Optimization,"Motivation: Momentum schemes accelerate stochastic optimization but are sensitive to noisy gradients and changing curvature, causing oscillations, loss spikes, or divergence. Existing fixes (clipping, restarts) are heuristic and do not provide online stability guarantees for nonconvex deep learning. Approach: We present Lyapunov-Controlled Momentum (LCM), an optimizer that adaptively selects momentum and step-size by minimizing an estimated Lyapunov drift bound computed from minibatch gradients and low-cost curvature probes. LCM models parameter updates as a stochastic dynamical system and constructs a quadratic Lyapunov candidate whose expected one-step decrease yields analytic constraints on admissible momentum. The controller computes closed-form adjustments per iteration that maximize progress subject to a nonpositive Lyapunov drift in expectation. Contributions: (1) A principled control-theoretic momentum scheduler with provable stability under bounded-gradient noise and Lipschitz smoothness. (2) Efficient estimators for Lyapunov terms using directional Hessian-vector products and streaming gradient statistics, keeping overhead minimal. (3) Nonasymptotic analysis establishing boundedness of iterates and convergence to first-order stationary points, with explicit trade-offs between aggressiveness and conservatism. Results: Empirically, LCM reduces catastrophic loss spikes, accelerates convergence across CNNs, transformers, and GANs, and matches or improves final generalization versus tuned momentum baselines, particularly in large-batch and mixed-precision regimes. Impact: LCM operationalizes Lyapunov-based control for mainstream stochastic optimizers, reducing manual tuning and improving reliability of accelerated methods in large-scale ICLR applications.",ICLR,optimization,gpt-5-mini,True,2739,"HOME-3: HIGH-ORDER MOMENTUM ESTIMATOR USING THIRD-POWER GRADIENT FOR CONVEX, SMOOTH NONCONVEX, AND NONSMOOTH NONCONVEX OPTIMIZATION","Momentum-based gradients are critical for optimizing advanced machine learning models, as they not only accelerate convergence but also help gradient-based optimizers overcome stationary points. While most state-of-the-art momentum techniques rely on lower-power gradients, such as the squared first-order gradient, there has been limited exploration into the potential of higher-power gradients—those raised to powers greater than two, such as the third-power first-order gradient. In this work, we introduce the concept of high-order momentum, where
momentum is constructed using higher-power gradients, with a specific focus on the third-power first-order gradient as a representative example. Our research offers both theoretical and empirical evidence of the benefits of this novel approach. From a theoretical standpoint, we demonstrate that incorporating third-power gradients into momentum can improve the convergence bounds of gradient-based optimizers
for both convex and smooth nonconvex problems. To validate these findings, we conducted extensive empirical experiments across convex, smooth nonconvex, and nonsmooth nonconvex optimization tasks. The results consistently showcase that high-order momentum outperforms traditional momentum-based optimizers, providing superior performance and more efficient optimization.",ICLR.cc/2025/Conference,3.5,nan,0.8386,motivation momentum schemes accelerate stochastic optimization but are sensitive noisy gradients and changing curvature causing oscillations loss spikes divergence existing fixes clipping restarts are heuristic and not provide online stability guarantees for nonconvex deep learning,momentum based gradients are critical for optimizing advanced machine learning models they not only accelerate convergence but also help gradient based optimizers overcome stationary points these conducted extensive empirical experiments across convex smooth nonconvex and nonsmooth nonconvex optimization tasks the consistently showcase that high order momentum outperforms traditional momentum based optimizers providing superior and more efficient optimization,2025-08-26T00:35:16.354674
97,CodedFed: Lightweight Gradient Coding for Straggler-Resilient Federated Optimization,"Motivation: Federated learning must operate over heterogeneous devices with variable compute and connectivity; stragglers and partial participation substantially slow rounds and impede convergence. Traditional redundancy techniques are communication- or privacy-expensive. Approach: We introduce CodedFed, a federated optimization protocol that applies sparse randomized linear codes to client updates, enabling the server to recover an aggregate gradient from any sufficiently large subset of client responses. Clients locally encode update shards using low-cost sparse random matrices and send compact coded vectors; the server decodes global aggregates without waiting for stragglers, while randomized masking preserves client-level privacy. Contributions: (1) A practical coding design tailored to federated constraints: low client overhead, sublinear server decoding cost, and compatibility with compression and secure aggregation. (2) Theoretical guarantees: unbiasedness in expectation, high-probability bounds on decoding error under bounded client variance, and convergence analysis showing preserved stationary guarantees with explicit dependence on coding redundancy and participation patterns. (3) System-level algorithms for adaptive group formation, redundancy scheduling based on observed latency, and robustness to client drop-out. Results: In simulated and realistic federated setups (variable-latency mobile clusters, medical data partitions), CodedFed reduces stalled rounds by up to 70% and accelerates wall-clock convergence by 1.5–3× relative to synchronous FedAvg and asynchronous baselines, while maintaining comparable model utility and privacy properties. Impact: CodedFed provides a principled, low-overhead path to straggler resilience in federated optimization, improving efficiency and robustness for real-world ICLR-scale deployments.",ICLR,optimization,gpt-5-mini,True,6341,Debiasing Federated Learning with Correlated Client Participation,"In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice.  Existing analyses of FedAvg usually assume the participating clients are independently sampled in each round from a uniform distribution, which does not reflect real-world scenarios. This paper introduces a theoretical framework that models client participation in FL as a Markov chain to study optimization convergence when clients have non-uniform and correlated participation across rounds. 
We apply this framework to analyze a more practical pattern: every client must wait a minimum number of $R$ rounds (minimum separation) before re-participating. We theoretically prove and empirically observe that increasing minimum separation reduces the bias induced by intrinsic non-uniformity of client availability in cross-device FL systems. 
Furthermore, we develop an effective debiasing algorithm for FedAvg that provably converges to the unbiased optimal solution under arbitrary minimum separation and unknown client availability distribution.",ICLR.cc/2025/Conference,6.75,True,0.8744,motivation federated learning must operate over heterogeneous devices variable compute and connectivity stragglers and partial participation slow rounds and impede convergence codedfed federated optimization protocol that applies sparse randomized linear codes client updates enabling the server recover aggregate gradient from any sufficiently large subset client responses system level algorithms for adaptive group formation redundancy scheduling observed latency and robustness client drop out impact codedfed provides principled low overhead path straggler resilience federated optimization improving efficiency and robustness for real world iclr scale deployments,cross device federated learning millions mobile clients only small subset clients participate training every communication round and federated averaging fedavg the most popular practice this introduces theoretical that models client participation markov chain optimization convergence when clients have non uniform and correlated participation across rounds,2025-08-26T00:35:16.354675
98,BiasedCompositional: Robust Optimization for Nested Objectives with Biased Oracles,"Motivation: Compositional objectives—nested expectations—appear in risk-aware learning, reinforcement learning, and robust estimation. Practical oracles (truncated rollouts, simulator truncations, importance sampling) often return biased estimates, breaking assumptions of existing compositional solvers and causing convergence failure. Approach: We propose BiasedCompositional, an algorithmic framework that explicitly models oracle bias and counteracts it with adaptive debiasing, control variates, and bias-aware step-size schedules. The method maintains auxiliary correction variables estimated via low-cost probes and gradually anneals reliance on biased oracles while leveraging their speed for early progress. A Lyapunov-style analysis quantifies interactions between bias, variance, and learning rates. Contributions: (1) A principled optimization algorithm that tolerates bounded, time-varying oracle bias and exploits biased queries for acceleration without sacrificing convergence. (2) Nonasymptotic complexity bounds separating contributions of bias and variance to optimization error, and prescriptive rules for bias-variance trade-offs and debiasing schedules. (3) Practical procedures for RL truncated-rollout scenarios, nested expectation risk objectives, and simulation-based inverse problems, including online bias estimators and variance-reduction primitives. Results: BiasedCompositional outperforms unbiased-heavy baselines and naive biased methods in sample efficiency and final performance on portfolio optimization, policy-gradient tasks with truncated rollouts, and distributional robustness objectives. It achieves faster empirical convergence while maintaining provable guarantees under realistic bias magnitudes. Impact: By enabling reliable compositional optimization with biased oracles, BiasedCompositional broadens practical applicability of nested objectives in ICLR domains where full unbiased sampling is costly or infeasible.",ICLR,optimization,gpt-5-mini,True,3722,FEDERATED COMPOSITIONAL OPTIMIZATION: THE IMPACT OF TWO-SIDED LEARNING RATES ON COMMUNICATION EFFICIENCY,"Compositional optimization (CO) has recently gained popularity due to its applications in distributionally robust optimization (DRO), meta-learning, reinforcement learning, and many other machine learning applications. The large-scale and distributed nature of data necessitates efficient federated learning (FL) algorithms for CO, but the compositional structure of the objective poses significant challenges. Current methods either rely on large batch gradients (which are impractical) or suffer from suboptimal communication efficiency. To address these challenges, we propose efficient FedAvg-type algorithms for solving non-convex CO in the FL setting. We first establish that standard FedAvg fails in solving the federated CO problems due to data heterogeneity, which amplifies bias in local gradient estimates. Our analysis establishes that either {\em additional communication} or {\em two-sided learning rate-based} algorithms are required to control this bias. To this end, we develop two algorithms for solving the federated CO problem. First, we propose FedDRO that utilizes the compositional problem structure to design a communication strategy that allows FedAvg to control the bias in the estimation of the compositional gradient, achieving $\mathcal{O}(\epsilon^{-2})$ sample and $\mathcal{O}(\epsilon^{-3/2})$ communication complexity. Then we propose DS-FedDRO, a two-sided learning rate algorithm, that eliminates the need for additional communication and achieves the optimal $\mathcal{O}(\epsilon^{-2})$ sample and $\mathcal{O}(\epsilon^{-1})$ communication complexity, highlighting the importance of two-sided learning rate algorithms for solving federated CO problems. The proposed algorithms avoid the need for large batch gradients and achieve linear speedup with the number of clients. We corroborate our theoretical findings with empirical studies on large-scale DRO problems.",ICLR.cc/2025/Conference,4.666666666666667,nan,0.8320,motivation compositional objectives nested expectations appear risk aware learning reinforcement learning and robust estimation lyapunov style analysis quantifies interactions between bias variance and learning rates contributions principled optimization that tolerates bounded time varying oracle bias and exploits biased queries for acceleration sacrificing convergence nonasymptotic complexity bounds separating contributions bias and variance optimization error and prescriptive rules for bias variance trade offs and debiasing schedules biasedcompositional outperforms unbiased heavy baselines and naive biased methods sample efficiency and final portfolio optimization policy gradient tasks truncated rollouts and distributional robustness objectives impact enabling reliable compositional optimization biased oracles biasedcompositional broadens practical applicability nested objectives iclr domains where full unbiased sampling costly infeasible,compositional optimization has recently gained popularity due its applications distributionally robust optimization dro meta learning reinforcement learning and many other machine learning applications the large scale and distributed nature data necessitates efficient federated learning algorithms for but the compositional structure the objective poses significant challenges our analysis establishes that either additional communication two sided learning rate based algorithms are required control this bias then feddro two sided learning rate that eliminates the need for additional communication and achieves the optimal mathcal epsilon sample and mathcal epsilon communication complexity highlighting the importance two sided learning rate algorithms for solving federated problems,2025-08-26T00:35:16.354678
99,Spectral Sharpness Regularization: Controlling Hessian Spectrum for Generalization-Aware Training,"Motivation: Sharp minima correlate with poor generalization, yet most sharpness-aware methods use heuristic perturbations that are costly and indirectly control curvature. Directly shaping Hessian spectra during training offers promise but is challenging to estimate and regularize at scale. Approach: We introduce Spectral Sharpness Regularization (SSR), an efficient spectral penalty that targets dominant Hessian eigenvalues via stochastic Lanczos and Hutchinson estimators. SSR computes differentiable surrogates of top-k eigenvalues and applies a smooth spectral penalty to the loss, coupled with adaptive damping to maintain optimization stability. The estimators require only a few Hessian-vector products per minibatch and incorporate variance control to be robust under stochastic gradients. Contributions: (1) A principled spectral regularizer directly reducing top-spectrum mass with unbiased stochastic estimators and provable variance bounds. (2) Theoretical analysis demonstrating that SGD with SSR converges to stationary points while decreasing top-eigenvalue mass under mild smoothness and bounded-variance assumptions. (3) Practical techniques: randomized Lanczos implementation for minibatch settings, adaptive penalty scheduling, and compatibility with common optimizers. Results: SSR consistently improves test accuracy, calibration, and robustness versus SAM, weight decay, and other baselines across CIFAR, ImageNet transfer, and transformer finetuning; ablation studies reveal strong correlation between reduced top eigenvalues and generalization gains. Impact: SSR provides a tractable, theoretically grounded mechanism to shape loss landscape spectra for better generalization, enabling targeted curvature control in large-scale deep learning tasks relevant to ICLR.",ICLR,optimization,gpt-5-mini,True,8713,Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",ICLR.cc/2025/Conference,7.0,True,0.8071,ssr computes differentiable surrogates top eigenvalues and applies smooth spectral penalty the loss coupled adaptive damping maintain optimization stability ssr consistently improves calibration and robustness versus sam weight decay and other baselines across cifar imagenet transfer and transformer finetuning ablation studies reveal strong correlation between reduced top eigenvalues and generalization gains impact ssr provides tractable theoretically grounded mechanism shape loss landscape spectra for better generalization enabling targeted curvature control large scale deep learning tasks relevant iclr,despite the vast empirical evidence supporting the efficacy adaptive optimization methods deep learning their theoretical understanding far from complete our analysis signsgd highlights noteworthy and precise contrast sgd terms convergence speed stationary distribution and robustness heavy tail noise crucially support our theoretical analysis experimental evidence verifying our insights this includes numerically integrating our sdes euler maruyama discretization various neural network architectures such mlps cnns resnets and transformers,2025-08-26T00:35:16.354679
