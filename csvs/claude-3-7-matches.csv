ai_index,ai_title,ai_abstract,ai_conference,ai_domain,ai_model,match_found,human_index,human_title,human_abstract,human_venue,human_overall_score,human_accepted,similarity_score,ai_processed_text,human_processed_text,generated_at
0,**Adaptive Subnetwork Activation: Dynamic Pathway Optimization for Efficient Inference**,"Deep neural networks often use substantially more computation than necessary for many input examples. We introduce Adaptive Subnetwork Activation (ASA), a novel architecture that dynamically determines which computational pathways to execute based on input complexity. Our approach employs a lightweight gating mechanism that learns to activate only the relevant subnetworks within a larger architecture, substantially reducing computation without sacrificing accuracy. By formulating pathway selection as a differentiable operation using Gumbel-Softmax relaxation, ASA can be trained end-to-end with standard backpropagation. We overcome the inherent challenges of unstable training dynamics and posterior collapse through a carefully designed regularization scheme and progressive pathway pruning. Extensive experiments on image classification, natural language processing, and reinforcement learning tasks demonstrate that ASA reduces FLOPs by 65-78% compared to fixed architectures while maintaining or improving accuracy. Notably, our method achieves state-of-the-art efficiency on ImageNet, outperforming existing dynamic computation approaches by 2.4% in accuracy with comparable computational costs. ASA represents a significant step toward more efficient deep learning by intelligently allocating computational resources based on the complexity of each input.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,11198,Effective Interplay between Sparsity and Quantization: From Theory to Practice,"The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.",ICLR.cc/2025/Conference,7.5,True,0.8103,deep neural networks often use more computation than necessary for many input examples extensive experiments image classification natural language processing and reinforcement learning tasks that asa reduces flops compared fixed architectures while maintaining improving asa represents significant step toward more efficient deep learning intelligently allocating computational resources the complexity each input,the increasing size deep neural networks dnns necessitates effective compression reduce their computational and memory footprints corroborate these experiments spanning range large language models including the opt and llama families 125m parameters and vision models like vit and resnet,2025-08-26T01:56:21.867160
1,**NeuroSparse: Neuromorphic-Inspired Sparsity for Robust and Energy-Efficient Neural Networks**,"Neural networks with biologically plausible architectures offer intriguing advantages in energy efficiency and robustness, yet their integration with modern deep learning remains challenging. We present NeuroSparse, a novel architecture that incorporates neuromorphic principles of sparse, event-driven computation into standard neural networks. Our approach introduces dendrite-inspired activation patterns that enable dynamic, input-dependent sparsity through lateral inhibition mechanisms. We develop a temporal coding scheme where information is encoded in the timing and frequency of sparse activations rather than continuous values. By formalizing these biological principles within a mathematically rigorous framework, NeuroSparse networks can be trained using standard backpropagation while preserving neuromorphic characteristics. Experiments across vision and speech recognition tasks show that NeuroSparse achieves up to 95% activation sparsity while maintaining competitive accuracy. The architecture demonstrates remarkable robustness to noise and adversarial attacks, with a 47% reduction in susceptibility compared to standard networks. Additionally, NeuroSparse achieves a 3.1× energy efficiency improvement when deployed on neuromorphic hardware. Our work bridges the gap between neuromorphic computing and deep learning, offering new directions for developing efficient, robust neural architectures inspired by biological principles.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8949,Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency,"The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.
In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. 
The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.
Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.",ICLR.cc/2025/Conference,5.75,True,0.8543,neural networks biologically plausible architectures offer intriguing advantages energy efficiency and robustness yet their integration modern deep learning remains challenging neurosparse that incorporates neuromorphic principles sparse event driven computation into standard neural networks experiments across vision and speech recognition tasks that neurosparse achieves activation sparsity while maintaining competitive the demonstrates remarkable robustness noise and adversarial attacks reduction susceptibility compared standard networks our bridges the gap between neuromorphic computing and deep learning offering directions for developing efficient robust neural architectures inspired biological principles,the human brain utilizes spikes for information transmission and dynamically reorganizes its network structure boost energy efficiency and cognitive capabilities throughout its lifespan drawing inspiration from this spike based computation spiking neural networks snns have been developed construct event driven models that emulate this efficiency despite these advances deep snns continue suffer from over parameterization during training and inference stark contrast the brain ability self organize this two stage dynamic structure learning for deep snns aimed maintaining effective sparse training from scratch while optimizing compression efficiency this improves the exploration sparse structures training deep snns adapting sparsity dynamically from the point view compression efficiency our experiments that this sparse training not only aligns the current deep snns models but also improves the efficiency compressing sparse snns,2025-08-26T01:56:21.867189
2,**Liquid Transformer Networks: Continuous Time Computation with Dynamic Attention**,"Transformer architectures have demonstrated impressive performance but suffer from fundamental limitations in handling continuous-time processes and dynamic data. We introduce Liquid Transformer Networks (LTNs), a novel architecture that reformulates attention mechanisms within a continuous-time dynamical systems framework. Our approach represents hidden states as evolving manifolds and models attention as fluid interactions between representation trajectories, allowing the network to naturally process irregularly-sampled temporal data. By leveraging recent advances in neural ordinary differential equations, LTNs can adapt their computational depth dynamically based on input complexity. The key innovation lies in our formulation of ""liquid attention,"" where attention weights evolve according to learned differential equations rather than being computed through fixed operations. This enables more flexible adaptation to temporal dependencies across varying timescales. Experiments on irregularly-sampled time series prediction, continuous control tasks, and event-based visual processing demonstrate that LTNs outperform discrete Transformers and neural ODE approaches, reducing prediction error by 24-31%. Our architecture offers a promising direction for neural networks that can naturally process continuous signals while maintaining the expressiveness of attention mechanisms.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7214,Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning,"Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary differential equations (ODEs). Our proposed model parameterizes all weights of attention and feed-forward blocks through neural networks, expressing these weights as functions of a continuous layer index. Through spectral analysis of the model's dynamics, we uncover an increase in eigenvalue magnitude that challenges the weight-sharing assumption prevalent in existing theoretical studies. We also leverage the Lyapunov exponent to examine token-level sensitivity, enhancing model interpretability. Our neural ODE transformer demonstrates performance comparable to or better than vanilla transformers across various configurations and datasets, while offering flexible fine-tuning capabilities that can adapt to different architectural constraints.",ICLR.cc/2025/Conference,5.75,True,0.8571,transformer architectures have demonstrated impressive but suffer from fundamental limitations handling continuous time processes and dynamic data liquid transformer networks ltns that reformulates attention mechanisms within continuous time dynamical systems our represents hidden states evolving manifolds and models attention fluid interactions between representation trajectories allowing the network naturally process irregularly sampled temporal data leveraging recent advances neural ordinary differential equations ltns can adapt their computational depth dynamically input complexity the key innovation lies our formulation liquid attention where attention weights evolve according learned differential equations rather than being computed fixed operations this enables more flexible adaptation temporal dependencies across varying timescales experiments irregularly sampled time series prediction continuous control tasks and event based visual processing that ltns outperform discrete transformers and neural ode approaches reducing prediction error our offers promising direction for neural networks that can naturally process continuous signals while maintaining the expressiveness attention mechanisms,recent advancements large language models llms transformer architectures have sparked significant interest understanding their inner workings this modeling transformer architectures highly flexible non autonomous neural ordinary differential equations odes our proposed parameterizes all weights attention and feed forward blocks neural networks expressing these weights functions continuous layer index also leverage the lyapunov exponent examine token level sensitivity enhancing interpretability our neural ode transformer demonstrates comparable better than vanilla transformers across various configurations and datasets while offering flexible fine tuning capabilities that can adapt different architectural constraints,2025-08-26T01:56:21.867214
3,**HyperMorph: Topology-Adaptive Neural Architectures with Dynamic Hypergraph Structure**,"Standard neural networks maintain fixed computational graphs, limiting their ability to model complex structural relationships between features. We present HyperMorph, a novel architecture that dynamically learns and adapts its computational topology through differentiable hypergraph transformations. Our approach represents neural network layers as hypergraphs where hyperedges connect variable-sized feature subsets, enabling the model to discover and leverage higher-order patterns beyond pairwise relationships. The key innovation is a learned morphing mechanism that adapts the hypergraph structure during inference based on input characteristics. Through carefully designed regularization and a novel backpropagation algorithm for hypergraph structures, HyperMorph networks maintain stable training despite their dynamic nature. Experiments on graph-structured data, molecular property prediction, and complex reasoning tasks demonstrate that HyperMorph consistently outperforms traditional graph neural networks and Transformer architectures, with particularly striking improvements on problems requiring relational reasoning (15-23% accuracy increase). The topology-adaptive properties of HyperMorph provide new insights into how neural architectures can dynamically reorganize their computational structure to better match the inherent structure of the problem domain.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,5664,Training-Free Message Passing for Learning on Hypergraphs,"Hypergraphs are crucial for modelling higher-order interactions in real-world data. Hypergraph neural networks (HNNs) effectively utilise these structures by message passing to generate informative node features for various downstream tasks like node classification. However, the message passing module in existing HNNs typically requires a computationally intensive training process, which limits their practical use. To tackle this challenge, we propose an alternative approach by decoupling the usage of hypergraph structural information from the model learning stage. This leads to a novel training-free message passing module, named TF-MP-Module, which can be precomputed in the data preprocessing stage, thereby reducing the computational burden. We refer to the hypergraph neural network equipped with our TF-MP-Module as TF-HNN. We theoretically support the efficiency and effectiveness of TF-HNN by showing that: 1) It is more training-efficient compared to existing HNNs; 2) It utilises as much information as existing HNNs for node feature generation; and 3) It is robust against the oversmoothing issue while using long-range interactions. Experiments based on seven real-world hypergraph benchmarks in node classification and hyperlink prediction show that, compared to state-of-the-art HNNs, TF-HNN exhibits both competitive performance and superior training efficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN outperforms the node classification accuracy of the best baseline by 10% with just 1% of the training time of that baseline.",ICLR.cc/2025/Conference,6.5,True,0.8545,standard neural networks maintain fixed computational graphs limiting their ability complex structural relationships between features our represents neural network layers hypergraphs where hyperedges connect variable sized feature subsets enabling the discover and leverage higher order patterns beyond pairwise relationships experiments graph structured data molecular property prediction and complex reasoning tasks that hypermorph consistently outperforms traditional graph neural networks and transformer architectures striking improvements problems requiring relational reasoning increase the topology adaptive properties hypermorph provide insights into how neural architectures can dynamically reorganize their computational structure better match the inherent structure the problem domain,hypergraph neural networks hnns utilise these structures message passing generate informative node features for various downstream tasks like node classification tackle this challenge alternative decoupling the usage hypergraph structural information from the learning stage refer the hypergraph neural network equipped our module hnn theoretically support the efficiency and effectiveness hnn showing that more training efficient compared existing hnns utilises much information existing hnns for node feature generation and robust against the oversmoothing issue while long range interactions experiments seven real world hypergraph benchmarks node classification and hyperlink prediction that compared state the art hnns hnn exhibits both competitive and superior training efficiency the large scale trivago hnn outperforms the node classification the best just the training time that,2025-08-26T01:56:21.867223
4,**Memory-Augmented Gaussian Processes for Efficient Nonparametric Deep Learning**,"Deep neural networks typically require massive parameterization to achieve state-of-the-art performance, leading to significant computational and memory costs. We propose Memory-Augmented Gaussian Processes (MAGPs), a novel hybrid architecture that combines the expressiveness of deep learning with the sample efficiency of nonparametric Bayesian methods. Our approach introduces a differentiable external memory structure indexed by learned feature representations, which enables efficient storage and retrieval of previously observed patterns. By coupling this with inducing point Gaussian processes, MAGPs can perform Bayesian inference over feature representations while maintaining tractable computation. The key innovation lies in our memory addressing mechanism that adaptively determines when to use parametric computation versus nonparametric pattern retrieval based on the familiarity of input patterns. Experiments across classification, regression, and few-shot learning tasks demonstrate that MAGPs achieve comparable performance to much larger neural networks while using 65-80% fewer parameters. Notably, our approach exhibits strong uncertainty calibration and resilience to distribution shifts. MAGPs represent a promising direction for developing more parameter-efficient architectures that leverage episodic memory to reduce the reliance on extensive parameterization.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8601,Bayesian Learning with Deep Q-Exponential Process,"Motivated by deep neural networks, the deep Gaussian process (DGP) generalizes the standard GP by stacking multiple layers of GPs. Despite the enhanced expressiveness, GP, as an $L_2$ regularization prior, tends to be over-smooth and sub-optimal for inhomogeneous subjects, such as images with edges. Recently, Q-exponential process (Q-EP) has been proposed as an $L_q$ relaxation to GP and demonstrated with more desirable regularization properties through a parameter $q>0$ with $q=2$ corresponding to GP. Sharing the similar tractability of posterior and predictive distributions with GP, Q-EP can also be stacked to improve its modeling flexibility. In this paper, we generalize Q-EP to deep Q-EP to enjoy both proper regularization and improved expressiveness. The generalization is realized by introducing shallow Q-EP as a latent variable model and then building a hierarchy of the shallow Q-EP layers. Sparse approximation by inducing points and scalable variational strategy are applied to facilitate the inference. We demonstrate the numerical advantages of the proposed deep Q-EP model by comparing with multiple state-of-the-art deep probabilistic models.",ICLR.cc/2025/Conference,3.6666666666666665,False,0.8505,deep neural networks require massive parameterization achieve state the art leading significant computational and memory costs memory augmented gaussian processes magps hybrid that combines the expressiveness deep learning the sample efficiency nonparametric bayesian methods our introduces differentiable external memory structure indexed learned feature representations which enables efficient storage and retrieval previously observed patterns coupling this inducing point gaussian processes magps can perform bayesian inference over feature representations while maintaining tractable computation experiments across classification regression and few shot learning tasks that magps achieve comparable much larger neural networks while fewer parameters,motivated deep neural networks the deep gaussian process dgp generalizes the standard stacking multiple layers gps this generalize deep enjoy both proper regularization and improved expressiveness the numerical advantages the proposed deep comparing multiple state the art deep probabilistic models,2025-08-26T01:56:21.867233
5,**Fractal Neural Architectures: Scale-Invariant Computation through Self-Similar Structures**,"Modeling hierarchical patterns at multiple scales remains a fundamental challenge in deep learning. We introduce Fractal Neural Architectures (FNAs), a novel approach that embeds the mathematical principle of self-similarity into network design. Our architecture recursively applies identical transformation patterns across different scales and abstraction levels, enabling more efficient parameter sharing while maintaining expressivity. The key innovation is a differentiable fractal generation process that constructs the computational graph through iterative expansion of base patterns, controlled by learned parameters. This allows the network to adaptively determine the appropriate computational depth and branching structure for different input regions. We provide theoretical analysis demonstrating that FNAs possess favorable inductive biases for hierarchical data and achieve exponential expressivity with respect to parameter count. Experiments on image classification, hierarchical reinforcement learning, and recursive natural language processing tasks show that FNAs consistently outperform conventional architectures of similar parameter counts by 3.5-7.2%. Furthermore, FNAs demonstrate exceptional transfer learning capabilities when scaling to larger datasets. Our work establishes a new paradigm for neural architecture design based on mathematical principles of self-similarity and scale invariance.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,11412,Fractal-Inspired Message Passing Neural Networks with Fractal Nodes,"Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but they struggle to balance local and global information processing. While graph Transformers aim to address these issues, they often neglect the inherent locality of Message Passing Neural Networks (MPNNs). Inspired by the fractal nature of real-world networks, we propose a novel concept, '*fractal nodes*', that addresses the limitations of both MPNN and graph Transformer. The approach draws insights from renormalization techniques to design a message-passing scheme that captures both local and global structural information. Our method enforces feature self-similarity into nodes by creating fractal nodes that coexist with the original nodes. Fractal nodes adaptively summarize subgraph information and are integrated into MPNN. We show that fractal nodes alleviate an over-squashing problem by providing direct shortcuts to pass fractal information over long distances. Experiments show that our method achieves comparable or better performance to the graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.",ICLR.cc/2025/Conference,5.5,False,0.8198,modeling hierarchical patterns multiple scales remains fundamental challenge deep learning fractal neural architectures fnas that embeds the mathematical principle self similarity into network the key innovation differentiable fractal generation process that constructs the computational graph iterative expansion base patterns controlled learned parameters this allows the network adaptively determine the appropriate computational depth and branching structure for different input regions experiments image classification hierarchical reinforcement learning and recursive natural language processing tasks that fnas consistently outperform conventional architectures similar parameter counts furthermore fnas exceptional transfer learning capabilities when scaling larger datasets our establishes paradigm for neural mathematical principles self similarity and scale invariance,graph neural networks gnns have emerged powerful tools for learning graph structured data but they struggle balance local and global information processing while graph transformers aim address these issues they often neglect the inherent locality message passing neural networks mpnns inspired the fractal nature real world networks concept fractal nodes that addresses the limitations both mpnn and graph transformer our enforces feature self similarity into nodes creating fractal nodes that coexist the original nodes,2025-08-26T01:56:21.867240
6,**Tensor-Train Transformers: Compressing Attention Mechanisms with Higher-Order Decomposition**,"Transformer models achieve remarkable performance but suffer from quadratic complexity in self-attention mechanisms, limiting their scalability. We introduce Tensor-Train Transformers (T3), a novel architecture that reformulates attention using higher-order tensor decompositions. Our approach represents the attention weight matrix as a tensor-train factorization, dramatically reducing the parameter count while preserving model expressivity. By leveraging the hierarchical structure of tensor-train decomposition, T3 models can capture long-range dependencies more efficiently than standard Transformers. We develop specialized initialization schemes and optimization techniques that stabilize the training of tensor-decomposed networks, addressing challenges of previous tensor-based approaches. Experiments on language modeling, machine translation, and long-sequence understanding tasks demonstrate that T3 achieves comparable or superior performance to vanilla Transformers with up to 85% parameter reduction and 73% inference speedup. Notably, our architecture scales more favorably to longer sequences, with complexity growing linearly rather than quadratically with sequence length. T3 represents a significant advance in making Transformer architectures more efficient through higher-order decomposition techniques, enabling their deployment in resource-constrained environments.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8958,transformer models achieve remarkable but suffer from quadratic complexity self attention mechanisms limiting their scalability tensor train transformers that reformulates attention higher order tensor decompositions our represents the attention weight matrix tensor train factorization dramatically reducing the parameter count while preserving expressivity specialized initialization schemes and optimization techniques that stabilize the training tensor decomposed networks addressing challenges previous tensor based approaches experiments language modeling machine translation and long sequence understanding tasks that achieves comparable superior vanilla transformers parameter reduction and inference speedup represents significant advance making transformer architectures more efficient higher order decomposition techniques enabling their deployment resource constrained environments,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T01:56:21.867248
7,**Graph-Guided Neural Architecture Search: Topology-Aware Exploration of the Architecture Space**,"Neural Architecture Search (NAS) has enabled the automated discovery of high-performance architectures, but current approaches treat architecture components as independent entities, ignoring their topological relationships. We introduce Graph-Guided Neural Architecture Search (G²NAS), a novel framework that explicitly models the architecture search space as a graph of architectural motifs with learned relationship embeddings. Our approach employs a hierarchical graph neural network to reason about component compatibility and predict the performance of candidate architectures based on their topological structure. By formulating architecture generation as a sequential graph construction process guided by the learned compatibility model, G²NAS efficiently explores promising regions of the architecture space. Experiments across vision, language, and multi-modal tasks demonstrate that G²NAS discovers architectures that outperform both hand-crafted designs and previous NAS methods by 1.8-3.4% while requiring 5-8× less search time. Theoretical analysis shows that our graph-guided exploration provides provably better sample efficiency than standard NAS approaches. G²NAS represents a fundamental shift in architecture search methodology by explicitly modeling the relational structure of neural components, enabling more efficient discovery of innovative architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8253,Causal-aware Graph Neural Architecture Search under Distribution Shifts,"Graph neural architecture search (Graph NAS) has emerged as a promising approach for autonomously designing graph neural network architectures by leveraging the correlations between graphs and architectures. However, the existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. In this paper, we propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with the following critical challenges: 1) how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, 2) how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose a novel approach, Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed CARNAS achieves advanced out-of-distribution generalization ability by discovering the causal relationship between graphs and architectures during the search process.",ICLR.cc/2025/Conference,5.0,False,0.8784,neural search nas has enabled the automated discovery high performance architectures but current approaches treat components independent entities ignoring their topological relationships graph guided neural search g²nas that explicitly models the search space graph architectural motifs learned relationship embeddings our employs hierarchical graph neural network reason about component compatibility and predict the candidate architectures their topological structure formulating generation sequential graph construction process guided the learned compatibility g²nas explores promising regions the space g²nas represents fundamental shift search methodology explicitly modeling the relational structure neural components enabling more efficient discovery innovative architectures,graph neural search graph nas has emerged promising for autonomously designing graph neural network architectures leveraging the correlations between graphs and architectures address these challenges causal aware graph neural search carnas which able capture the causal graph architecture relationship during the search process and discover the generalized graph under distribution shifts disentangled causal subgraph identification capture the causal subgraphs that have stable prediction abilities across distributions then graph embedding intervention intervene causal subgraphs within the latent space ensuring that these subgraphs encapsulate essential features for prediction while excluding non causal elements,2025-08-26T01:56:21.867258
8,**Quantum-Inspired Tensor Networks for Efficient Representation Learning**,"Quantum computing concepts offer unique perspectives for addressing limitations in classical neural networks. We introduce Quantum-Inspired Tensor Networks (QTNets), a novel architecture that adapts concepts from quantum tensor networks to classical deep learning. Our approach represents high-dimensional feature spaces using matrix product states and tree tensor networks, enabling exponentially more efficient representation of entangled feature relationships. The key innovation is a differentiable tensor contraction layer that dynamically adjusts its entanglement structure based on input complexity, allowing flexible trade-offs between expressivity and computational efficiency. We develop specialized initialization schemes and regularization techniques that preserve the theoretical advantages of tensor networks while enabling stable training with standard backpropagation. Experiments on quantum state tomography, many-body physical systems, and entangled image datasets demonstrate that QTNets achieve superior sample efficiency compared to conventional neural networks, requiring 60-75% fewer parameters to achieve comparable performance. Notably, our architecture demonstrates particular advantages for problems with intricate correlation structures that confound traditional architectures. QTNets represent a promising direction for incorporating quantum-inspired computational principles into practical neural architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,9445,Optimizer-Dependent Generalization Bound for Quantum Neural Networks,"Quantum neural networks (QNNs) play a pivotal role in addressing complex tasks within quantum machine learning, analogous to classical neural networks in deep learning. Ensuring consistent performance across diverse datasets is crucial for understanding and optimizing QNNs in both classical and quantum machine learning tasks, but remains a challenge as QNN's generalization properties have not been fully explored. In this paper, we investigate the generalization properties of QNNs through the lens of learning algorithm stability, circumventing the need to explore the entire hypothesis space and providing insights into how classical optimizers influence QNN performance. By establishing a connection between QNNs and quantum combs, we examine the general behaviors of QNN models from a quantum information theory perspective. Leveraging the uniform stability of the stochastic gradient descent algorithm, we propose a generalization error bound determined by the number of trainable parameters, data uploading times, dataset dimension, and classical optimizer hyperparameters. Numerical experiments validate this comprehensive understanding of QNNs and align with our theoretical conclusions. As the first exploration into understanding the generalization capability of QNNs from a unified perspective of design and training, our work offers practical insights for applying QNNs in quantum machine learning.",ICLR.cc/2025/Conference,6.0,False,0.8582,quantum computing concepts offer unique perspectives for addressing limitations classical neural networks quantum inspired tensor networks qtnets that adapts concepts from quantum tensor networks classical deep learning our represents high dimensional feature spaces matrix product states and tree tensor networks enabling exponentially more efficient representation entangled feature relationships experiments quantum state tomography many body physical systems and entangled image datasets that qtnets achieve superior sample efficiency compared conventional neural networks requiring fewer parameters achieve comparable qtnets represent promising direction for incorporating quantum inspired computational principles into practical neural architectures,quantum neural networks qnns play pivotal role addressing complex tasks within quantum machine learning analogous classical neural networks deep learning ensuring consistent across diverse datasets crucial for understanding and optimizing qnns both classical and quantum machine learning tasks but remains challenge qnn generalization properties have not been fully explored this the generalization properties qnns the lens learning stability circumventing the need the entire hypothesis space and providing insights into how classical optimizers influence qnn the first exploration into understanding the generalization capability qnns from unified perspective and training our offers practical insights for applying qnns quantum machine learning,2025-08-26T01:56:21.867268
9,**Neural Sheaves: Topological Structures for Information Flow in Deep Networks**,"Understanding information flow in deep networks remains a significant challenge. We introduce Neural Sheaves, a novel architecture that formalizes neural computation using sheaf theory, a branch of mathematics that models local-to-global information flow over topological spaces. Our approach represents feature activations as sections of sheaves over a cellular complex, with neural operations formulated as sheaf restrictions and extensions. This framework enables precise control over how information propagates between different representational subspaces. The key innovation is a learned sheaf Laplacian that adaptively determines information diffusion patterns based on input characteristics, allowing for dynamic, context-dependent computation. We provide theoretical results demonstrating that Neural Sheaves generalize both graph neural networks and attention mechanisms while offering enhanced expressivity through higher-order topological structures. Experiments on graph classification, dynamical systems modeling, and geometric deep learning benchmarks show that Neural Sheaves consistently outperform traditional architectures by 4.7-8.2% while providing interpretable visualizations of information flow. Our architecture establishes a new theoretical foundation for neural networks based on algebraic topology, offering insights into representation learning through the lens of sheaf theory.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10745,Bundle Neural Network for message diffusion on graphs,"The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via *message diffusion* on *flat vector bundles* — geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks.",ICLR.cc/2025/Conference,7.333333333333333,True,0.8042,understanding information flow deep networks remains significant challenge neural sheaves that formalizes neural computation sheaf theory branch mathematics that models local global information flow over topological spaces our represents feature activations sections sheaves over cellular complex neural operations formulated sheaf restrictions and extensions provide theoretical demonstrating that neural sheaves generalize both graph neural networks and attention mechanisms while offering enhanced expressivity higher order topological structures experiments graph classification dynamical systems modeling and geometric deep learning benchmarks that neural sheaves consistently outperform traditional architectures while providing interpretable visualizations information flow our establishes theoretical foundation for neural networks algebraic topology offering insights into representation learning the lens sheaf theory,the dominant paradigm for learning graphs message passing address these issues bundle neural networks bunns graph neural network that operates message diffusion flat vector bundles geometrically inspired structures that assign each node vector space and orthogonal map bunn layer evolves node features diffusion type partial differential equation where its discrete form acts special case the recently introduced sheaf neural network snn alleviating over smoothing establish the universality bunns approximating feature transformations infinite families graphs injective positional encodings marking the first positive expressivity its kind empirically bunns perform strongly heterophilic and long range tasks which demonstrates their robustness diverse range challenging real world tasks,2025-08-26T01:56:21.867275
10,**Equivariant Neural Operators: Symmetry-Preserving Architectures for Continuous Physical Systems**,"Deep learning models for physical systems often struggle to preserve fundamental physical symmetries and invariances. We present Equivariant Neural Operators (ENOs), a novel architecture that guarantees equivariance to desired symmetry groups while operating directly on continuous functions. Our approach extends neural operator frameworks with group-theoretic constraints, enabling the learning of mappings between function spaces that inherently respect physical symmetries such as translation, rotation, and scaling. The key innovation is a continuous steerable kernel mechanism that enforces equivariance constraints throughout the computational graph without discretization artifacts. By incorporating Lie algebra representations into the network design, ENOs can handle continuous symmetry groups beyond the capabilities of conventional equivariant networks. Experiments on fluid dynamics, electromagnetic field prediction, and quantum chemistry tasks demonstrate that ENOs achieve 30-45% lower prediction error compared to non-equivariant baselines and 15-20% improvement over discretized equivariant models. Notably, our architecture generalizes remarkably well to out-of-distribution scenarios, maintaining accuracy under extreme transformations where traditional models fail completely. ENOs represent a significant advance in incorporating physical symmetries into deep learning for continuous domains.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8735,Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups,"The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models.",ICLR.cc/2025/Conference,6.5,True,0.8331,deep learning models for physical systems often struggle preserve fundamental physical symmetries and invariances equivariant neural operators enos that guarantees equivariance desired symmetry groups while operating directly continuous functions our extends neural operator frameworks group theoretic constraints enabling the learning mappings between function spaces that inherently respect physical symmetries such translation rotation and scaling incorporating lie algebra representations into the network enos can handle continuous symmetry groups beyond the capabilities conventional equivariant networks experiments fluid dynamics electromagnetic field prediction and quantum chemistry tasks that enos achieve lower prediction error compared non equivariant baselines and improvement over discretized equivariant models enos represent significant advance incorporating physical symmetries into deep learning for continuous domains,the quest for robust and generalizable machine learning models has driven recent interest exploiting symmetries equivariant neural networks the context pde solvers recent works have shown that lie point symmetries can useful inductive bias for physics informed neural networks pinns data and loss augmentation this lie algebra canonicalization lielac that exploits only the action infinitesimal generators the symmetry group circumventing the need for knowledge the full group structure finally showcase lielac efficacy tasks invariant image classification and lie point symmetry equivariant neural pde solvers pre trained models,2025-08-26T01:56:21.867281
11,**Curriculum Dropout: Progressive Structural Regularization for Robust Generalization**,"Standard dropout techniques apply uniform regularization throughout training, failing to adapt to the evolving needs of the learning process. We introduce Curriculum Dropout, a novel architectural regularization approach that progressively structures the dropout pattern based on the network's learning dynamics. Our method begins with aggressive, randomized dropout and gradually transitions to a deterministic, structure-preserving dropout scheme informed by neural pathway importance. The key innovation is a meta-learning framework that adaptively determines optimal dropout schedules for different network components based on gradient sensitivity and feature diversity metrics. By formalizing dropout as a curriculum learning problem, our approach overcomes the limitations of static regularization strategies. Comprehensive experiments across vision, language, and reinforcement learning tasks demonstrate that Curriculum Dropout consistently improves generalization performance by 2.8-4.5% compared to standard dropout techniques while providing greater robustness to distribution shifts. Theoretical analysis reveals that our approach effectively navigates the exploration-exploitation trade-off during training, initially encouraging diverse feature learning before focusing on refinement of critical pathways. Curriculum Dropout represents a fundamental rethinking of structural regularization as an adaptive process aligned with the natural progression of neural network learning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8479,Enhancing Efficiency and Regularization in Convolutional Neural Networks: Strategies for Optimized Dropout,"This study explores dropout optimization in Convolutional Neural Networks (CNNs), aiming to beat traditional approaches in regularization and efficiency. We introduce dynamic, context-aware strategies, embodied by Probabilistic Feature Importance Dropout (PFID). This method modifies dropout rates to the unique learning phase of CNNs, integrating adaptive, structured, and contextual dropout techniques. Experimentation, benchmarked against current state-of-the-art methods, demonstrates improvements in network performance, particularly in generalization and training efficiency. We also discuss our findings. The findings represent an advancement in dropout techniques, offering more adaptable and robust CNN models for complex datasets and computational landscapes.",ICLR.cc/2025/Conference,2.0,nan,0.8541,standard dropout techniques apply uniform regularization throughout training failing adapt the evolving needs the learning process curriculum dropout architectural regularization that progressively structures the dropout pattern the network learning dynamics our begins aggressive randomized dropout and gradually transitions deterministic structure preserving dropout scheme informed neural pathway importance the key innovation meta learning that adaptively determines optimal dropout schedules for different network components gradient sensitivity and feature diversity metrics formalizing dropout curriculum learning problem our overcomes the limitations static regularization strategies comprehensive experiments across vision language and reinforcement learning tasks that curriculum dropout consistently improves generalization compared standard dropout techniques while providing greater robustness distribution shifts theoretical analysis reveals that our navigates the exploration exploitation trade off during training initially encouraging diverse feature learning before focusing refinement critical pathways curriculum dropout represents fundamental rethinking structural regularization adaptive process aligned the natural progression neural network learning,this explores dropout optimization convolutional neural networks cnns aiming beat traditional approaches regularization and efficiency dynamic context aware strategies embodied probabilistic feature importance dropout pfid this modifies dropout rates the unique learning phase cnns integrating adaptive structured and contextual dropout techniques experimentation benchmarked against current state the art methods demonstrates improvements network generalization and training efficiency,2025-08-26T01:56:21.867285
12,**Implicit Neural Representations with Compositional Fourier Features**,"Implicit neural representations have shown promise for continuous signal modeling, but struggle with representing high-frequency details and complex compositional structures. We introduce Compositional Fourier Feature Networks (CFFNs), a novel architecture that enhances implicit representations through hierarchical composition of frequency-domain features. Our approach represents signals as compositions of learnable Fourier basis functions organized in a multi-scale hierarchy, enabling more efficient modeling of complex patterns across different frequency bands. The key innovation is a frequency attention mechanism that adaptively selects and composes relevant frequency components based on spatial coordinates, allowing the network to allocate representational capacity according to local signal complexity. We develop specialized initialization schemes and positional encodings that stabilize training across the frequency spectrum. Experiments on high-resolution image representation, 3D shape modeling, and physics-based simulation demonstrate that CFFNs achieve 15-30% lower reconstruction error than previous implicit representation methods while using 40-60% fewer parameters. Notably, our architecture exhibits superior performance on signals with discontinuities and sharp features that challenge existing approaches. CFFNs represent a significant advancement in implicit neural representations through principled incorporation of compositional frequency-domain structure.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10338,FreSh: Frequency Shifting for Accelerated Neural Representation Learning,"Implicit Neural Representations (INRs) have recently gained attention as a powerful approach for continuously representing signals such as images, videos, and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to exhibit a low-frequency bias, limiting their ability to capture high-frequency details accurately. This limitation is typically addressed by incorporating high-frequency input embeddings or specialized activation layers. In this work, we demonstrate that these embeddings and activations are often configured with hyperparameters that perform well on average but are suboptimal for specific input signals under consideration, necessitating a costly grid search to identify optimal settings. Our key observation is that the initial frequency spectrum of an untrained model's output correlates strongly with the model's eventual performance on a given target signal. Leveraging this insight, we propose frequency shifting (or FreSh), a method that selects embedding hyperparameters to align the frequency spectrum of the model’s initial output with that of the target signal. We show that this simple initialization technique improves performance across various neural representation methods and tasks, achieving results comparable to extensive hyperparameter sweeps but with only marginal computational overhead compared to training a single model with default hyperparameters.",ICLR.cc/2025/Conference,7.0,True,0.8379,implicit neural representations have shown promise for continuous signal modeling but struggle representing high frequency details and complex compositional structures compositional fourier feature networks cffns that enhances implicit representations hierarchical composition frequency domain features the key innovation frequency attention mechanism that adaptively selects and composes relevant frequency components spatial coordinates allowing the network allocate representational capacity according local signal complexity experiments high resolution image representation shape modeling and physics based simulation that cffns achieve lower reconstruction error than previous implicit representation methods while fewer parameters cffns represent significant advancement implicit neural representations principled incorporation compositional frequency domain structure,implicit neural representations inrs have recently gained attention powerful for continuously representing signals such images videos and shapes multilayer perceptrons mlps leveraging this insight frequency shifting fresh that selects embedding hyperparameters align the frequency spectrum the model initial output that the target signal that this simple initialization improves across various neural representation methods and tasks achieving comparable extensive hyperparameter sweeps but only marginal computational overhead compared training single default hyperparameters,2025-08-26T01:56:21.867292
13,**Homological Message Passing: Exploiting Topological Structure in Graph Neural Networks**,"Graph neural networks typically operate on pairwise node relationships, limiting their ability to capture higher-order topological structures crucial for many complex systems. We introduce Homological Message Passing Networks (HMPNs), a novel architecture that leverages concepts from algebraic topology to model higher-order relationships in graph-structured data. Our approach represents graphs using simplicial complexes and performs message passing across simplices of different dimensions (nodes, edges, triangles, etc.), enabling the capture of topological features beyond pairwise connections. The key innovation is a persistent homology-based attention mechanism that identifies and prioritizes structurally significant features across multiple scales. By incorporating boundary and coboundary operators from homological algebra, HMPNs can reason about topological invariants such as connected components, cycles, and voids. Experiments on molecular property prediction, social network analysis, and topological data analysis tasks demonstrate that HMPNs consistently outperform traditional GNNs by 5.2-9.7%, with particularly striking improvements on problems with intricate topological structure. Our architecture provides new theoretical insights into the expressive power of graph representation learning through the lens of algebraic topology and offers a principled approach to incorporating higher-order structural information.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6374,Improving Graph Neural Networks with Heterophily-based Filtration and Filtration Learning,"Graph neural networks (GNNs) are a powerful method of learning representations of graph-structured data. While they excel at learning class-discriminative representations of nodes in homophilous graphs, where connecting nodes tend to belong to the same class, many GNNs struggle with heterophilous graphs whose inter-class connections can muddy the message passing.  Inspired by this finding, we propose a topological filtration scheme, treating graphs as 1-dimensional simplicial complexes N  with a filter function based on estimated edge heterophily, and introduce two methodologies that use a backbone GNN to learn from the resulting graph filtration. The first trains a GNN on each graph in the filtration sequence consecutively for a portion of the total training time, using embeddings from previous graphs to initialize node embeddings in subsequent graphs. The second approach uses a novel message passing scheme to pass messages jointly within each and between graph levels in the filtration sequence with common nodes. Both methods enhance the influence of early birth adjacent nodes in homophilous subgraphs, yet allow for the model to learn from the full range of heterophilous and homophilous connections in the graph. We further extend our approach to learn a graph filtration sequence of graphs through a learnable node filter function. Experiments show that our heterophily-filtered GNNs achieve superior node classification accuracy on heterophilous and homophilous networks alike.",ICLR.cc/2025/Conference,2.0,nan,0.8954,graph neural networks operate pairwise node relationships limiting their ability capture higher order topological structures crucial for many complex systems the key innovation persistent homology based attention mechanism that identifies and prioritizes structurally significant features across multiple scales experiments molecular property prediction social network analysis and topological data analysis tasks that hmpns consistently outperform traditional gnns striking improvements problems intricate topological structure our provides theoretical insights into the expressive power graph representation learning the lens algebraic topology and offers principled incorporating higher order structural information,graph neural networks gnns are powerful learning representations graph structured data while they excel learning class discriminative representations nodes homophilous graphs where connecting nodes tend belong the same class many gnns struggle heterophilous graphs whose inter class connections can muddy the message passing experiments that our heterophily filtered gnns achieve superior node classification heterophilous and homophilous networks alike,2025-08-26T01:56:21.867300
14,**Neural State Space Models: Bridging Continuous Dynamics and Discrete Computation**,"Modeling complex dynamical systems with neural networks presents a fundamental tension between continuous dynamics and discrete computation. We introduce Neural State Space Models (NSSMs), a novel architecture that unifies continuous-time dynamical systems theory with discrete neural computation. Our approach represents system dynamics using a learned state-space formulation where states evolve according to neural ordinary differential equations while discrete observations and controls interact with the continuous state through learnable interfaces. The key innovation is a hybrid update mechanism that seamlessly integrates continuous evolution with discrete, attention-based corrections, enabling more accurate modeling of complex dynamical systems with mixed continuous-discrete characteristics. We develop specialized training techniques that balance reconstruction accuracy with physical consistency constraints through carefully designed regularization terms. Experiments on chaotic systems prediction, robotic control, and complex physical simulations demonstrate that NSSMs reduce prediction error by 25-40% compared to purely continuous or discrete approaches. Notably, our architecture demonstrates remarkable generalization to longer time horizons and novel initial conditions. NSSMs represent a significant advance in neural architecture design for dynamical systems, bridging the gap between continuous physical models and discrete computational approaches.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,59,Neural ODE with Differentiable Hidden State for Irregular Time Series,"Capturing the continuous underlying dynamics of irregular time series is essential for accurately reflecting the ongoing evolution and intricate correlations within the data. The discrete nature of current models, including RNN-based models and transformer variants, poses challenges when it comes to generalizing to the continuous-time data paradigms, which is necessary for capturing ongoing dynamics of irregular time series. 
Neural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis. However, integrating new information while maintaining the continuity of latent dynamics remains challenging. 
To tackle this problem, we introduce Differentiable Hidden State (DHS) enhanced neural ODE, a data-dependent framework that is capable of effectively capturing temporal dependencies and ensuring the continuity of the hidden process. We leverage the theory of generalized inverses to innovatively compute attention mechanism in reverse and obtain a continuous representation. To capture more accurate temporal relationships, we introduce Hoyer metric and maximize the sparsity of it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our model.",ICLR.cc/2025/Conference,2.0,nan,0.8400,modeling complex dynamical systems neural networks presents fundamental tension between continuous dynamics and discrete computation neural state space models nssms that unifies continuous time dynamical systems theory discrete neural computation our represents dynamics learned state space formulation where states evolve according neural ordinary differential equations while discrete observations and controls interact the continuous state learnable interfaces experiments chaotic systems prediction robotic control and complex physical simulations that nssms reduce prediction error compared purely continuous discrete approaches nssms represent significant advance neural for dynamical systems bridging the gap between continuous physical models and discrete computational approaches,the discrete nature current models including rnn based models and transformer variants poses challenges when comes generalizing the continuous time data paradigms which necessary for capturing ongoing dynamics irregular time series neural ordinary differential equations nodes assume continuous latent dynamic and provide elegant for irregular time series analysis tackle this problem differentiable hidden state dhs enhanced neural ode data dependent that capable capturing temporal dependencies and ensuring the continuity the hidden process leverage the theory generalized inverses innovatively compute attention mechanism reverse and obtain continuous representation,2025-08-26T01:56:21.867306
15,**Neural Tangent Kernel-Guided Architecture Design: Theoretical Foundations for Convergence Guarantees**,"Despite remarkable empirical success, neural architectures often lack theoretical convergence guarantees, limiting our ability to deploy them in high-stakes applications. We introduce a principled framework for designing neural architectures with provable convergence properties by leveraging recent advances in Neural Tangent Kernel (NTK) theory. Our approach systematically constructs architectures whose training dynamics remain stable in the infinite-width limit, while maintaining finite-width expressivity. We identify a novel class of activation functions and initialization schemes that ensure stable NTK evolution throughout training, addressing the traditional gap between theory and practice. Through careful parameterization of residual connections and normalization layers, we construct architectures that provably converge to global minima at polynomial rates. Empirical evaluation across image classification, reinforcement learning, and natural language processing tasks demonstrates that our NTK-guided architectures achieve comparable performance to state-of-the-art models while providing theoretical guarantees. Our method reduces training variance by 43% and improves convergence speed by 28% compared to standard architectures. This work establishes a new paradigm for neural architecture design grounded in theoretical foundations, bridging the gap between mathematical analysis and practical performance.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3416,Revised NTK Analysis of Optimization and Generalization with Its Extensions to Arbitrary Initialization,"Recent theoretical works based on the neural tangent kernel (NTK) have shed light on the optimization and generalization of over-parameterized neural networks, and partially bridge the gap between their practical success and classical learning theory. However, the existing NTK-based analysis has a limitation that the scaling of the initial parameter should decrease with respect to the sample size which is contradictory to the practical initialization scheme. To address this issue, in this paper, we present the revised NTK analysis of optimization and generalization of overparametrized neural networks, which successfully remove the dependency on the sample size of the initialization. Based on our revised analysis, we further extend our theory that allow for arbitrary initialization, not limited to Gaussian initialization. Under our initialization-independent analysis, we propose NTK-based regularizer that can improve the model generalization, thereby illustrating the potential to bridge the theory and practice while also supporting our theory. Our numerical simulations demonstrate that the revised theory indeed can achieve the significantly lower generalization error bound compared to existing error bound. Also importantly, the proposed regularizer also corroborate our theory on the arbitrary initialization with fine-tuning scenario, which takes the first step for NTK theory to be promisingly applied to real-world applications.",ICLR.cc/2025/Conference,3.75,nan,0.8460,despite remarkable empirical success neural architectures often lack theoretical convergence guarantees limiting our ability deploy them high stakes applications principled for designing neural architectures provable convergence properties leveraging recent advances neural tangent kernel ntk theory empirical evaluation across image classification reinforcement learning and natural language processing tasks demonstrates that our ntk guided architectures achieve comparable state the art models while providing theoretical guarantees this establishes paradigm for neural grounded theoretical foundations bridging the gap between mathematical analysis and practical,recent theoretical works the neural tangent kernel ntk have shed light the optimization and generalization over parameterized neural networks and partially bridge the gap between their practical success and classical learning theory address this issue this the revised ntk analysis optimization and generalization overparametrized neural networks which remove the dependency the sample size the initialization,2025-08-26T01:56:21.867316
16,**Learnable Spectral Filtering Networks: Frequency-Selective Feature Extraction with Theoretical Guarantees**,"Convolutional neural networks excel at spatial pattern recognition but lack explicit control over frequency domain properties, limiting their robustness to spectral shifts. We introduce Learnable Spectral Filtering Networks (LSFNets), a novel architecture that operates directly in the frequency domain with theoretically-backed guarantees on spectral selectivity. Our approach replaces standard convolutional layers with learnable spectral filters that adaptively extract frequency-specific features while maintaining spatial localization. The key innovation lies in our parameterization of filter banks as differentiable combinations of orthogonal frequency bases, enabling the network to learn optimal spectral decompositions from data. We develop a specialized initialization scheme and regularization technique that ensures stable gradient flow across frequency bands during training. Experiments on image classification, adversarial robustness, and out-of-distribution generalization demonstrate that LSFNets outperform conventional CNNs by 3.7% in accuracy while showing remarkable resilience to frequency-domain perturbations (68% improvement). Importantly, our approach maintains interpretability through visualization of learned spectral filters, providing insights into the network's decision-making process. LSFNets represent a fundamental advancement in neural architecture design by explicitly incorporating frequency domain priors with theoretical guarantees on spectral properties.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10633,Point-Calibrated Spectral Neural Operators,"Two typical neural models have been extensively studied for operator learning, learning in spatial space via attention mechanism or learning in spectral space via spectral analysis technique such as Fourier Transform. Spatial learning enables point-level flexibility but lacks global continuity constraint, while spectral learning enforces spectral continuity prior but lacks point-wise adaptivity. This work innovatively combines the continuity prior and the point-level flexibility, with the introduced Point-Calibrated Spectral Transform. It achieves this by calibrating the preset spectral eigenfunctions with the predicted point-wise frequency preference via neural gate mechanism. Beyond this, we introduce Point-Calibrated Spectral Neural Operators, which learn operator mappings by approximating functions with the point-level adaptive spectral basis, thereby not only preserving the benefits of spectral prior but also boasting the superior adaptability comparable to the attention mechanis. Comprehensive experiments demonstrate its consistent performance enhancement in extensive PDE solving scenarios.",ICLR.cc/2025/Conference,5.5,False,0.8196,convolutional neural networks excel spatial pattern recognition but lack explicit control over frequency domain properties limiting their robustness spectral shifts learnable spectral filtering networks lsfnets that operates directly the frequency domain theoretically backed guarantees spectral selectivity the key innovation lies our parameterization filter banks differentiable combinations orthogonal frequency bases enabling the network learn optimal spectral decompositions from data importantly our maintains interpretability visualization learned spectral filters providing insights into the network decision making process lsfnets represent fundamental advancement neural explicitly incorporating frequency domain priors theoretical guarantees spectral properties,two typical neural models have been extensively studied for operator learning learning spatial space attention mechanism learning spectral space spectral analysis such fourier transform spatial learning enables point level flexibility but lacks global continuity constraint while spectral learning enforces spectral continuity prior but lacks point wise adaptivity achieves this calibrating the preset spectral eigenfunctions the predicted point wise frequency preference neural gate mechanism beyond this point calibrated spectral neural operators which learn operator mappings approximating functions the point level adaptive spectral basis thereby not only preserving the benefits spectral prior but also boasting the superior adaptability comparable the attention mechanis,2025-08-26T01:56:21.867322
17,**Hierarchical Mixture-of-Experts Transformers with Sparse Router Regularization**,"Scaling transformer models has led to remarkable performance gains but faces significant computational barriers. We present Hierarchical Mixture-of-Experts Transformers (HMoET), a novel architecture that dramatically improves parameter efficiency through conditional computation and sparse routing. Our approach organizes experts in a multi-level tree structure where each node contains a lightweight router that selectively activates only the most relevant sub-branches for a given input. The key innovation lies in our sparse router regularization technique, which enforces structured sparsity constraints on routing decisions while maintaining end-to-end differentiability. By jointly optimizing expert specialization and routing pathways, HMoET achieves adaptive computation depth and width based on input complexity. Experiments on language modeling, machine translation, and multi-modal reasoning demonstrate that HMoET matches the performance of dense transformers with only 14-18% of parameters actively used per sample, enabling 5.7× inference speedup. Notably, our hierarchical routing mechanism effectively mitigates the load balancing and expert collapse problems that plague standard MoE approaches. This work establishes a new direction for scalable transformer architectures that can efficiently adapt their computational footprint to input complexity.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8698,DenseAttention: No-Compromise Exact All $N \times N$  Interactions Algorithm with $O(N)$ Space and Time Complexity,"The ubiquitous Transformer architecture suffers from two main bottlenecks: 1) low computational and memory efficiency, leading to suboptimal hardware utilization, and 2) quadratic time complexity with respect to sequence length $N$, making it slow and costly for large data contexts. We propose a novel DenseAttention Network architecture, a straightforward simplification of the standard Transformer block that addresses these issues and serves as a drop-in replacement for language modeling tasks. We eliminate memory-bound components in DenseAttention, including Softmax, masking, one skip connection, and both LayerNorms, as well as key, value, and output projection matrices, as they become redundant. Despite these removals, it maintains exact $N \times N$ pairwise interactions between tokens. By exploiting the associativity of matrix multiplications, DenseAttention can be computed with $O(N^2d)$ or $O(Nd^2)$ time and space complexity, depending on the context. To handle the absence of Softmax and prevent numerical instability, we introduce MaxNormActivation at both ends of the Transformer block. We also devise Cosine Relative Positional Embeddings as a computationally efficient replacement for RoPE, and simple LocalAttention variations of the block to help the model focus on details in extremely long contexts. 

DenseAttention competes with FlashAttention in speed on small sequences and outperforms it by orders of magnitude on large contexts. We pre-train encoder language models on sequences up to 16K in length, which perform similarly or better than baseline BERT-large, while significantly improving speed and efficiency.  Finally, we achieve state-of-the-art on the LRA benchmark among the Transformer-based architectures.",ICLR.cc/2025/Conference,4.5,False,0.8753,scaling transformer models has led remarkable gains but faces significant computational barriers experiments language modeling machine translation and multi modal reasoning that hmoet matches the dense transformers only parameters actively used per sample enabling inference speedup this establishes direction for scalable transformer architectures that can adapt their computational footprint input complexity,the ubiquitous transformer suffers from two main bottlenecks low computational and memory efficiency leading suboptimal hardware utilization and quadratic time complexity respect sequence length making slow and costly for large data contexts denseattention network straightforward simplification the standard transformer block that addresses these issues and serves drop replacement for language modeling tasks handle the absence softmax and prevent numerical instability maxnormactivation both ends the transformer block pre train encoder language models sequences 16k length which perform similarly better than bert large while improving speed and efficiency,2025-08-26T01:56:21.867326
18,**DiracNets: Unifying Residual and Dense Connections Through Differentiable Skip-Coefficient Learning**,"Skip connections have become essential components in modern neural architectures, yet their optimal configuration remains largely determined by manual design choices. We introduce DiracNets, a novel framework that unifies and generalizes residual and dense connectivity patterns through learnable skip coefficients. Our approach represents layer connections as a differentiable adjacency matrix whose entries are jointly optimized with network weights, enabling the discovery of optimal information flow patterns. The key innovation is a path-normalized gradient flow mechanism that ensures stable training despite the exponential number of potential paths through deep networks. By formulating skip connection learning as a constrained optimization problem with carefully designed regularization terms, DiracNets automatically discover efficient connectivity patterns tailored to specific tasks. Extensive experiments across computer vision, graph analysis, and sequence modeling demonstrate that DiracNets consistently outperform hand-designed architectures by 2.3-4.1% while requiring 25-40% fewer parameters. Analysis of emerged connectivity patterns reveals that DiracNets learn task-specific information routing strategies that balance feature reuse and representational capacity. This work establishes a principled approach to architectural connectivity optimization that reduces reliance on heuristic design choices.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,5820,Hyper-Connections,"We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",ICLR.cc/2025/Conference,6.25,True,0.8313,skip connections have become essential components modern neural architectures yet their optimal configuration remains largely determined manual choices our represents layer connections differentiable adjacency matrix whose entries are jointly optimized network weights enabling the discovery optimal information flow patterns the key innovation path normalized gradient flow mechanism that ensures stable training despite the exponential number potential paths deep networks formulating skip connection learning constrained optimization problem carefully designed regularization terms diracnets automatically discover efficient connectivity patterns tailored specific tasks analysis emerged connectivity patterns reveals that diracnets learn task specific information routing strategies that balance feature reuse and representational capacity this establishes principled architectural connectivity optimization that reduces reliance heuristic choices,this addresses common drawbacks observed residual connection variants such the seesaw effect between gradient vanishing and representation collapse theoretically hyper connections allow the network adjust the strength connections between features different depths and dynamically rearrange layers conduct experiments focusing the pre training large language models including dense and sparse models where hyper connections significant improvements over residual connections additional experiments conducted vision tasks also similar improvements,2025-08-26T01:56:21.867332
19,**Hyperbolic Neural Architectures: Embedding Hierarchical Structures in Non-Euclidean Spaces**,"Traditional neural networks operate in Euclidean space, limiting their ability to efficiently model hierarchical and tree-like structures inherent in many datasets. We present Hyperbolic Neural Architectures (HNAs), a comprehensive framework for designing and training neural networks in hyperbolic space, which provides an inductive bias for hierarchical data. Our approach develops hyperbolic generalizations of fundamental neural building blocks, including fully-connected, convolutional, and attention mechanisms that operate consistently in negatively curved spaces. The key innovation is a geodesic-aware backpropagation algorithm that correctly computes gradients along manifold curvatures, addressing the numerical instabilities that have hindered previous hyperbolic approaches. Through careful reparameterization of exponential and logarithmic maps, HNAs seamlessly integrate with existing deep learning frameworks while preserving hyperbolic geometry. Experiments on hierarchical classification, knowledge graph completion, and tree-structured data modeling demonstrate that HNAs achieve 7-12% accuracy improvements over Euclidean counterparts while using 30-50% fewer dimensions. Most notably, our architectures exhibit exceptional generalization properties when extrapolating to deeper hierarchical structures not seen during training. This work establishes a foundation for neural computation in non-Euclidean spaces that better aligns with the underlying geometry of complex, hierarchical data.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3887,Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces,"Learning in hyperbolic spaces has gained increasing attention due to the superior capability of modeling hierarchical structures. Existing hyperbolic learning methods use a fixed distance measure that assumes a uniform hierarchical structure across all data points. However, this assumption does not always hold in real-world scenarios, considering the diversity of the hierarchical structures of data. This work proposes to learn geometry aware distance measures that dynamically adjust to accommodate diverse hierarchical structures in hyperbolic spaces. We derive geometry aware distance measures by generating projections and curvatures for each pair of samples, which maps each pair to a suitable hyperbolic space. We introduce a revised low-rank decomposition scheme and a hard-pair mining mechanism to reduce the computational cost incurred by the pairwise generation without compromising accuracy. Moreover, we derive an upper bound of the low-rank approximation error via Talagrand concentration inequality to guarantee the effectiveness of our low-rank decomposition scheme. Theoretical analysis and experiments on standard image classification and few-shot learning tasks affirm the effectiveness of our method in refining hyperbolic learning through our geometry aware distance measures.",ICLR.cc/2025/Conference,4.75,nan,0.8630,traditional neural networks operate euclidean space limiting their ability hierarchical and tree like structures inherent many datasets hyperbolic neural architectures hnas comprehensive for designing and training neural networks hyperbolic space which provides inductive bias for hierarchical data our develops hyperbolic generalizations fundamental neural building blocks including fully connected convolutional and attention mechanisms that operate consistently negatively curved spaces careful reparameterization exponential and logarithmic maps hnas seamlessly integrate existing deep learning frameworks while preserving hyperbolic geometry experiments hierarchical classification knowledge graph completion and tree structured data modeling that hnas achieve improvements over euclidean counterparts while fewer dimensions this establishes foundation for neural computation non euclidean spaces that better aligns the underlying geometry complex hierarchical data,learning hyperbolic spaces has gained increasing attention due the superior capability modeling hierarchical structures existing hyperbolic learning methods use fixed distance measure that assumes uniform hierarchical structure across all data points revised low rank decomposition scheme and hard pair mining mechanism reduce the computational cost incurred the pairwise generation compromising theoretical analysis and experiments standard image classification and few shot learning tasks affirm the effectiveness our refining hyperbolic learning our geometry aware distance measures,2025-08-26T01:56:21.867338
20,**Differentiable Neural Architecture Compression via Information Bottleneck Optimization**,"Neural architecture compression is typically performed as a separate process after training, leading to suboptimal trade-offs between model size and performance. We introduce Differentiable Neural Architecture Compression (DNAC), a novel end-to-end framework that jointly optimizes architecture structure and weights through information bottleneck principles. Our approach formulates compression as a constrained optimization problem where the information flow through each architectural component is directly controlled by learnable gates. The key innovation is a variational information bottleneck that explicitly balances minimal architectural complexity against maximal task performance, enabling principled architecture simplification during training. By developing a specialized straight-through estimator and temperature annealing schedule, DNAC overcomes the challenges of optimizing discrete architectural choices in a continuous framework. Experiments across vision, language, and multimodal domains demonstrate that DNAC consistently produces more efficient architectures than post-training compression methods, achieving equivalent performance with 35-58% fewer parameters and 42-67% fewer FLOPs. Importantly, our approach automatically discovers non-uniform compression patterns that preserve critical pathways while aggressively pruning redundant components. This work establishes a new paradigm for neural architecture design where compression constraints are incorporated from the beginning rather than applied as an afterthought.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,5175,Rate/Distortion Constrained Model Quantization for Efficient Storage and Inference,"The proliferation of large pre-trained neural networks has recently revived research in both quantization of network weights (for faster inference), and in their
compression (to reduce file sizes). However, there has so far been little idea transfer between the two lines of research. In this paper, we combine techniques from
quantization and compression to propose an efficient and highly effective post-training compression method for large neural networks. Our method extends the
recently published quantization method OPTQ (Frantar et al., 2023) with a tunable
rate/distortion trade-off by introducing a cost per bit into OPTQ's rounding
operation. Crucially, we estimate the bit rate based on the predictive model used
in the state-of-the-art neural network compression method NNCodec (Becking
et al., 2023). In our experiments with several standard pre-trained networks from
the computer vision community, our method leads to significantly (up to 2.7x)
smaller file sizes than NNCodec at equal model performance, generally compressing to less than half a bit per network weight and implicitly pruning insignificant weights.
Additionally, and in contrast to NNcodec, our method offers the same opportunities for inference speed-ups as OPTQ. By proving that file size and inference
cost can be reduced simultaneously, we hope that our contribution shows a path
towards deploying large neural networks on end-user devices, alleviating privacy
concerns, regulatory constraints, and dependency on large service providers.",ICLR.cc/2025/Conference,5.0,False,0.8464,neural compression performed separate process after training leading suboptimal trade offs between size and differentiable neural compression dnac end end that jointly optimizes structure and weights information bottleneck principles our formulates compression constrained optimization problem where the information flow each architectural component directly controlled learnable gates this establishes paradigm for neural where compression constraints are incorporated from the beginning rather than applied afterthought,the proliferation large pre trained neural networks has recently revived both quantization network weights for faster inference and their compression reduce file sizes however there has far been little idea transfer between the two lines this combine techniques from quantization and compression efficient and highly effective post training compression for large neural networks crucially estimate the bit rate the predictive used the state the art neural network compression nncodec our experiments several standard pre trained networks from the computer vision community our leads smaller file sizes than nncodec equal compressing less than half bit per network weight and implicitly pruning insignificant weights proving that file size and inference cost can reduced simultaneously hope that our shows path towards deploying large neural networks end user devices alleviating privacy concerns regulatory constraints and dependency large service providers,2025-08-26T01:56:21.867344
21,**Continuous Graph Neural Networks via Partial Differential Equation Dynamics**,"Discrete graph neural networks struggle to model continuous dynamics on graphs and cannot easily incorporate physics-informed inductive biases. We introduce Continuous Graph Neural Networks (CGNNs), a novel architecture that reformulates message passing as a system of partial differential equations (PDEs) evolving over a graph manifold. Our approach represents node features as continuous functions that evolve according to learned diffusion-advection-reaction dynamics, enabling more natural modeling of physical processes and temporal phenomena on graphs. The key innovation is a graph differential operator that generalizes continuous PDEs to irregular graph topologies while preserving their mathematical properties. By leveraging neural ODE techniques with specialized graph-based integrators, CGNNs can adapt their computational depth to problem complexity through adaptive step sizing. Experiments on spatio-temporal forecasting, molecular dynamics, and complex physical systems demonstrate that CGNNs outperform discrete GNNs by 18-26% in prediction accuracy while providing physically consistent interpolation between time points. Theoretical analysis proves that our formulation encompasses existing GNN architectures as special cases of discretized PDE systems. This work establishes a principled bridge between graph representation learning and continuous dynamical systems, enabling more faithful modeling of continuous processes on graph-structured domains.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10957,Graphon Neural Differential Equations and Transferabilty of Graph Neural Differential Equations,"Graph Neural Differential Equations (GNDEs) extend Graph Neural Networks (GNNs) to a continuous-depth framework, providing a robust tool for modeling complex network dynamics. In this paper, we investigate the potential of GNDEs for transferring knowledge across different graphs with shared convolutional structures. To bridge the gap between discrete and continuous graph representations, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the continuous limit of GNDEs. Using tools from nonlinear evolution equations and graph limit theory, we rigorously establish this continuum limit and develop a mathematical framework to quantify the approximation error between a GNDE and its corresponding Graphon-NDE, which decreases as the number of nodes increases, ensuring reliable transferability. We further derive specific rates for various graph families, providing practical insights into the performance of GNDEs. These findings extend recent results on GNNs to the continuous-depth setting and reveal a fundamental trade-off between discriminability and transferability in GNDEs.",ICLR.cc/2025/Conference,3.6666666666666665,False,0.8917,discrete graph neural networks struggle continuous dynamics graphs and cannot easily incorporate physics informed inductive biases continuous graph neural networks cgnns that reformulates message passing partial differential equations pdes evolving over graph manifold leveraging neural ode techniques specialized graph based integrators cgnns can adapt their computational depth problem complexity adaptive step sizing experiments spatio temporal forecasting molecular dynamics and complex physical systems that cgnns outperform discrete gnns prediction while providing physically consistent interpolation between time points this establishes principled bridge between graph representation learning and continuous dynamical systems enabling more faithful modeling continuous processes graph structured domains,graph neural differential equations gndes extend graph neural networks gnns continuous depth providing robust tool for modeling complex network dynamics this the potential gndes for transferring knowledge across different graphs shared convolutional structures bridge the gap between discrete and continuous graph representations graphon neural differential equations graphon ndes the continuous limit gndes,2025-08-26T01:56:21.867352
22,**Neural Architecture Evolution Through Differentiable Cellular Encoding**,"Current neural architecture search methods typically explore a predefined search space of human-designed components, limiting their ability to discover truly novel architectural patterns. We introduce Differentiable Cellular Encoding (DCE), a novel approach that evolves neural architectures through biologically-inspired developmental programs. Our method encodes architectures as compact, differentiable growth rules that iteratively transform a simple initial structure into a complex computational graph. The key innovation is a hybrid genetic-gradient optimization algorithm that jointly evolves the discrete structure of developmental rules while optimizing their continuous parameters through gradient descent. By representing architectures implicitly through their generative process rather than explicitly as fixed graphs, DCE can discover highly efficient, recursive architectural motifs beyond traditional human design patterns. Experiments on image classification, reinforcement learning, and natural language processing demonstrate that DCE consistently evolves architectures that outperform both hand-designed networks and traditional NAS approaches by 2.8-4.5% while using 30-45% fewer parameters. Analysis of the evolved architectures reveals emergent motifs that exploit symmetry and recursion in ways not typically found in human-designed networks. This work establishes a new paradigm for neural architecture design inspired by biological morphogenesis rather than engineering principles.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1316,POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator,"Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.
Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.
This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.
Results were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.",ICLR.cc/2025/Conference,4.4,False,0.8445,current neural search methods predefined search space human designed components limiting their ability discover truly architectural patterns differentiable cellular encoding dce that evolves neural architectures biologically inspired developmental programs the key innovation hybrid genetic gradient optimization that jointly evolves the discrete structure developmental rules while optimizing their continuous parameters gradient descent experiments image classification reinforcement learning and natural language processing that dce consistently evolves architectures that outperform both hand designed networks and traditional nas approaches while fewer parameters this establishes paradigm for neural inspired biological morphogenesis rather than engineering principles,neural search nas automates the neural network architectures minimising dependence human expertise and iterative experimentation recently transferable neural search transferable nas has emerged generalising the search process from being dataset dependent task dependent this diffusion based streamlines computation generating architectures optimised for unseen datasets the need for further adaptation this introduces the pareto optimal many objective neural generator pomonag extending diffusionnag many objective diffusion process pomonag optimisation enhanced expanding its training meta dataset applying pareto front filtering generated architectures and refining embeddings for conditional generation were validated two distinct search spaces nasbench201 and mobilenetv3 and evaluated across image classification datasets,2025-08-26T01:56:21.867359
23,**Logical Neural Networks: Integrating Symbolic Reasoning with Differentiable Architecture Learning**,"Deep learning and symbolic reasoning have complementary strengths, yet their integration remains a fundamental challenge. We introduce Logical Neural Networks (LNNs), a novel architecture that seamlessly embeds first-order logic within a differentiable computational framework. Our approach represents logical propositions and rules as specialized neural modules with guaranteed logical consistency while maintaining end-to-end differentiability. The key innovation is a neurosymbolic training procedure that jointly optimizes the logical structure and neural parameters, enabling the architecture to learn both symbolic rules and their exceptions from data. By developing differentiable approximations to logical operations with provable logical equivalence bounds, LNNs combine the interpretability of symbolic systems with the learning capabilities of neural networks. Experiments on knowledge graph reasoning, visual-logical question answering, and rule-based classification demonstrate that LNNs outperform both pure neural approaches and traditional symbolic systems by 7-15%, particularly in low-data regimes and when logical consistency is critical. Importantly, LNNs provide interpretable explanations for their predictions through automatically extracted logical rules. This work establishes a principled foundation for neurosymbolic architectures that integrate the complementary strengths of neural and symbolic approaches.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,4994,MMD-NSL: Mixed Multinomial Distribution-based Neuro-Symbolic Learning,"Neuro-symbolic learning (NSL) aims to integrate neural networks with symbolic reasoning approaches to enhance the interpretability of machine learning models. Existing methods mostly focus on the long dependency problem of symbolic learning. The important challenge of complex categorization is largely overlooked. To bridge this gap, we propose the Mixed Multinomial Distribution-based NSL MMD-NSL framework. It seamlessly integrates the handling of long dependency chains and complex semantic categorization within Knowledge Graphs (KGs). By introducing a continuous Mixed Multinomial Logic Semantic Distribution, we extend traditional Markov Logic Networks (MLN) to incorporate context-aware semantic embeddings. Our theoretical innovations, including a bijective mapping between MLNs and continuous multinomial distributions, enable the capture of intricate dependencies and varied contexts crucial for NSL tasks.
The framework leverages a bilevel optimization strategy, where a transformer-based upper level dynamically learns mixing coefficients akin to attention mechanisms, while the lower level optimizes rule weights for learning both context and rule patterns. Extensive experiments on the DWIE benchmarking datasets demonstrate significant advantages of MMD-NSL over four state-of-the-art approaches. It achieves 10.47% higher F1-scores on average than the best-performing baseline across 23 sub-datasets. It advances continuous probabilistic models for neuro-symbolic reasoning and complex relational tasks.",ICLR.cc/2025/Conference,4.4,False,0.8841,deep learning and symbolic reasoning have complementary strengths yet their integration remains fundamental challenge logical neural networks lnns that seamlessly embeds first order logic within differentiable computational our represents logical propositions and rules specialized neural modules guaranteed logical consistency while maintaining end end differentiability the key innovation neurosymbolic training procedure that jointly optimizes the logical structure and neural parameters enabling the learn both symbolic rules and their exceptions from data developing differentiable approximations logical operations provable logical equivalence bounds lnns combine the interpretability symbolic systems the learning capabilities neural networks experiments knowledge graph reasoning visual logical question answering and rule based classification that lnns outperform both pure neural approaches and traditional symbolic systems low data regimes and when logical consistency critical this establishes principled foundation for neurosymbolic architectures that integrate the complementary strengths neural and symbolic approaches,neuro symbolic learning nsl aims integrate neural networks symbolic reasoning approaches enhance the interpretability machine learning models existing methods mostly focus the long dependency problem symbolic learning seamlessly integrates the handling long dependency chains and complex semantic categorization within knowledge graphs kgs introducing continuous mixed multinomial logic semantic distribution extend traditional markov logic networks mln incorporate context aware semantic embeddings the leverages bilevel optimization strategy where transformer based upper level dynamically learns mixing coefficients akin attention mechanisms while the lower level optimizes rule weights for learning both context and rule patterns advances continuous probabilistic models for neuro symbolic reasoning and complex relational tasks,2025-08-26T01:56:21.867365
24,**Neural Field Architectures: Continuous Representation Networks for Infinite-Resolution Learning**,"Discrete convolutional architectures struggle with continuous signals and multi-scale phenomena due to their inherent resolution limitations. We present Neural Field Architectures (NFAs), a novel framework that represents signals as continuous functions parameterized by neural networks rather than discrete tensors. Our approach replaces traditional layer-wise operations with functional transformations that preserve the continuous nature of the representation throughout the network. The key innovation is a multi-resolution attention mechanism that adaptively focuses computational resources on relevant regions of the continuous domain, enabling efficient modeling of fine details and global structure simultaneously. By developing specialized functional equivalents of convolution, pooling, and normalization, NFAs operate entirely in a resolution-independent manner. Experiments on high-resolution image processing, 3D reconstruction, and continuous physical field prediction demonstrate that NFAs achieve 20-35% lower reconstruction error than discrete architectures while being invariant to input/output resolution. Notably, our architecture allows seamless generalization across different sampling patterns and resolutions without retraining. This work establishes a new paradigm for neural network design based on continuous functional representations rather than discrete tensors, enabling truly resolution-independent deep learning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1383,Grounding Continuous Representations in Geometry: Equivariant Neural Fields,"Conditional Neural Fields (CNFs) are increasingly being leveraged as continuous signal representations, by associating each data-sample with a latent variable that conditions a shared backbone Neural Field (NeF) to reconstruct the sample. However, existing CNF architectures face limitations when using this latent downstream in tasks requiring fine-grained geometric reasoning, such as classification and segmentation. We posit that this results from lack of explicit modelling of geometric information (e.g. locality in the signal or the orientation of a feature) in the latent space of CNFs. As such, we propose Equivariant Neural Fields (ENFs), a novel CNF architecture which uses a geometry-informed cross-attention to condition the NeF on a geometric variable—a latent point cloud of features—that enables an equivariant decoding from latent to field. We show that this approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws: if the field transforms, the latent representation transforms accordingly—and vice versa. Crucially, this equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weight-sharing over similar local patterns, allowing for efficient learning of datasets of fields. We validate these main properties in a range of tasks including classification, segmentation, forecasting, reconstruction and generative modelling, showing clear improvement over baselines with a geometry-free latent space.",ICLR.cc/2025/Conference,7.0,True,0.8396,neural field architectures nfas that represents signals continuous functions parameterized neural networks rather than discrete tensors our replaces traditional layer wise operations functional transformations that preserve the continuous nature the representation throughout the network the key innovation multi resolution attention mechanism that adaptively focuses computational resources relevant regions the continuous domain enabling efficient modeling fine details and global structure simultaneously experiments high resolution image processing reconstruction and continuous physical field prediction that nfas achieve lower reconstruction error than discrete architectures while being invariant input output resolution this establishes paradigm for neural network continuous functional representations rather than discrete tensors enabling truly resolution independent deep learning,conditional neural fields cnfs are increasingly being leveraged continuous signal representations associating each data sample latent variable that conditions shared backbone neural field nef reconstruct the sample however existing cnf architectures face limitations when this latent downstream tasks requiring fine grained geometric reasoning such classification and segmentation such equivariant neural fields enfs cnf which uses geometry informed cross attention condition the nef geometric variable latent point cloud features that enables equivariant decoding from latent field that this induces steerability property which both field and latent are grounded geometry and amenable transformation laws the field transforms the latent representation transforms accordingly and vice versa crucially this equivariance relation ensures that the latent capable representing geometric patterns faitfhully allowing for geometric reasoning latent space weight sharing over similar local patterns allowing for efficient learning datasets fields,2025-08-26T01:56:21.867367
25,**Group Equivariant Self-Attention: A General Framework for Symmetric Neural Architectures**,"Incorporating symmetry constraints into neural architectures remains challenging despite their importance for sample-efficient learning. We introduce Group Equivariant Self-Attention (GESA), a unified framework that generalizes self-attention mechanisms to respect arbitrary symmetry groups. Our approach constructs attention operations that provably maintain equivariance to user-specified transformations, ensuring consistent predictions under input transformations. The key innovation is a steerable attention formulation where queries, keys, and values transform according to group representations, enabling equivariance to complex symmetry groups beyond simple translations and rotations. By developing a general mathematical framework for equivariant attention, GESA subsumes existing equivariant architectures as special cases while enabling novel equivariance types not previously achievable. Experiments on molecular property prediction, protein structure analysis, and physical simulation demonstrate that GESA outperforms non-equivariant baselines by 15-28% in sample efficiency and 8-14% in prediction accuracy. Theoretical analysis provides provable guarantees on the types of symmetries that can be preserved throughout the network. This work establishes a comprehensive framework for incorporating symmetry constraints into attention-based architectures, bridging group theory and deep learning to enable more efficient learning in domains with known invariances.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2267,Learning Molecular Symmetry Breaking via Symmetry-adapted Neural Networks,"E(3)-equivariant neural networks have achieved remarkable performance in molecular modeling. However, the equivariance constraint limits the model's effectiveness in learning tasks involving symmetry breaking, particularly those that violate the celebrated Curie principle. Relaxing the equivariance constraint is essential for addressing these challenges. In this paper, we explore the intricate symmetry relationships between an object and its spontaneously symmetry-broken outcomes. We introduce a relaxed equivariance based on the molecule's inherent symmetries. Additionally, we develop SANN -- a symmetry-adapted neural network architecture that learns symmetry breaking through equivalence classes of atoms. SANN decomposes the molecular point cloud into sets of symmetry-equivalent atoms and performs message-passing both within and across these classes. We demonstrate the advantages of our method over invariant and equivariant models through synthetic tasks and show that SANN effectively learns both equivariance and symmetry breaking in various benchmark molecular modeling tasks.",ICLR.cc/2025/Conference,5.25,False,0.8535,incorporating symmetry constraints into neural architectures remains challenging despite their importance for sample efficient learning our constructs attention operations that provably maintain equivariance user specified transformations ensuring consistent predictions under input transformations the key innovation steerable attention formulation where queries keys and values transform according group representations enabling equivariance complex symmetry groups beyond simple translations and rotations experiments molecular property prediction protein structure analysis and physical simulation that gesa outperforms non equivariant baselines sample efficiency and prediction theoretical analysis provides provable guarantees the types symmetries that can preserved throughout the network this establishes comprehensive for incorporating symmetry constraints into attention based architectures bridging group theory and deep learning enable more efficient learning domains known invariances,equivariant neural networks have achieved remarkable molecular modeling however the equivariance constraint limits the model effectiveness learning tasks involving symmetry breaking those that violate the celebrated curie principle additionally sann symmetry adapted neural network that learns symmetry breaking equivalence classes atoms,2025-08-26T01:56:21.867377
26,**Modular Meta-Learning Architectures with Neural Composition Graphs**,"Standard meta-learning approaches typically employ monolithic architectures that struggle to capture the compositional nature of tasks. We introduce Neural Composition Graphs (NCGs), a novel meta-learning architecture that represents tasks as explicit compositions of reusable neural modules organized in a learnable graph structure. Our approach jointly optimizes a library of specialized neural modules alongside a meta-controller that dynamically assembles these modules into task-specific computational graphs. The key innovation is a differentiable module selection mechanism that enables end-to-end training of both the module parameters and composition policy through gradient-based meta-learning. By developing specialized regularization techniques that encourage module specialization while ensuring composability, NCGs learn to decompose complex tasks into meaningful functional components. Experiments on few-shot learning, compositional generalization, and multi-task reinforcement learning demonstrate that NCGs achieve 12-18% higher accuracy on novel tasks compared to monolithic meta-learning approaches, with particularly striking advantages on compositionally novel task combinations. Analysis reveals that NCGs discover interpretable task decompositions that align with human-intuitive subtask boundaries. This work establishes a new approach to meta-learning based on explicit compositional structure, enabling more efficient adaptation to novel tasks through recombination of learned primitive operations.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1821,When and how are modular networks better?,"Many real-world learning tasks have an underlying hierarchical modular structure, composed of smaller sub-functions. Traditional neural networks (NNs), however, often ignore this structure, leading to inefficiencies in learning and generalization. Leveraging known structural information can enhance performance by aligning the network architecture with the task’s inherent modularity. In this work, we investigate how modular NNs can outperform traditional dense networks by systematically varying the degree of structural knowledge incorporated. We compare architectures ranging from monolithic dense NNs, which assume no prior knowledge, to hierarchically modular NNs with shared modules, which leverage sparsity, modularity, and module reusability. Our experiments demonstrate that incorporating structural knowledge, particularly through module reuse and fixed connectivity, significantly improves learning efficiency and generalization. Hierarchically modular NNs excel in data-scarce scenarios by promoting functional specialization within the modules and reducing redundancy. These findings suggest that task-specific architectural biases can lead to more efficient, interpretable, and effective learning systems.",ICLR.cc/2025/Conference,3.75,False,0.8391,neural composition graphs ncgs meta learning that represents tasks explicit compositions reusable neural modules organized learnable graph structure our jointly optimizes library specialized neural modules alongside meta controller that dynamically assembles these modules into task specific computational graphs experiments few shot learning compositional generalization and multi task reinforcement learning that ncgs achieve higher tasks compared monolithic meta learning approaches striking advantages compositionally task combinations this establishes meta learning explicit compositional structure enabling more efficient adaptation tasks recombination learned primitive operations,many real world learning tasks have underlying hierarchical modular structure composed smaller sub functions traditional neural networks nns however often ignore this structure leading inefficiencies learning and generalization leveraging known structural information can enhance aligning the network the task inherent modularity this how modular nns can outperform traditional dense networks systematically varying the degree structural knowledge incorporated our experiments that incorporating structural knowledge module reuse and fixed connectivity improves learning efficiency and generalization these suggest that task specific architectural biases can lead more efficient interpretable and effective learning systems,2025-08-26T01:56:21.867383
27,**Uncertainty-Aware Neural Architectures with Learnable Stochastic Activations**,"Traditional neural networks provide point estimates without principled uncertainty quantification, limiting their reliability in safety-critical applications. We introduce Learnable Stochastic Activation Networks (LSANs), a novel architecture that intrinsically models predictive uncertainty by replacing deterministic activations with parameterized stochastic processes. Our approach represents each activation as a random variable whose distribution parameters are themselves the outputs of learnable functions, enabling the network to capture heteroscedastic uncertainty throughout its computation graph. The key innovation is a specialized variance-controlling gradient estimator that ensures stable training despite the stochasticity of forward passes. By carefully designing the distribution families and developing layer-specific normalizing flows, LSANs maintain computational efficiency while providing principled uncertainty estimates. Experiments on regression, classification under distribution shift, and active learning demonstrate that LSANs produce well-calibrated uncertainty estimates that outperform both standard ensembles and Bayesian neural networks by 15-23% in calibration metrics while maintaining comparable accuracy. Notably, our single-model approach achieves this without the computational burden of sampling-based methods. This work establishes a new paradigm for uncertainty-aware neural architectures where stochasticity is a fundamental architectural component rather than an inference-time approximation.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7940,Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence,"Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty estimates (e.g. pretrained models predicting only point estimates), popular approaches that operate post-hoc include Laplace’s method and split conformal prediction (split-CP). However, Laplace’s method can be miscalibrated when the model is misspecified and split-CP requires sample splitting, and thus comes at the expense of statistical efficiency. In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. Coupled with linearization of the network, we express the absolute residual nonconformity score as a piecewise linear function of the candidate label allowing for an efficient procedure that avoids the exhaustive search over the output space. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.",ICLR.cc/2025/Conference,6.25,True,0.8492,traditional neural networks provide point estimates principled uncertainty quantification limiting their reliability safety critical applications our represents each activation random variable whose distribution parameters are themselves the outputs learnable functions enabling the network capture heteroscedastic uncertainty throughout its computation graph experiments regression classification under distribution shift and active learning that lsans produce well calibrated uncertainty estimates that outperform both standard ensembles and bayesian neural networks calibration metrics while maintaining comparable this establishes paradigm for uncertainty aware neural architectures where stochasticity fundamental architectural component rather than inference time approximation,uncertainty quantification important prerequisite for the deployment deep learning models safety critical areas yet this hinges the uncertainty estimates being useful the extent the prediction intervals are well calibrated and sharp pretrained models predicting only point estimates popular approaches that operate post hoc include laplace and split conformal prediction split this construct prediction intervals for neural network regressors post hoc held out data this achieved approximating the full conformal prediction full standard regression benchmarks and bounding box localization the resulting prediction intervals are locally adaptive and often tighter than those split,2025-08-26T01:56:21.867392
28,**Tensor Decomposition Networks: Systematic Compression through Low-Rank Factorization**,"Parameter efficiency in deep learning remains a critical challenge for deployment in resource-constrained environments. We introduce Tensor Decomposition Networks (TDNets), a systematic framework that fundamentally redesigns neural architectures using principled tensor factorization methods. Our approach replaces standard dense layers with adaptive combinations of CP, Tucker, and Tensor-Train decompositions, enabling exponential reduction in parameter counts while preserving representational capacity. The key innovation is a learnable decomposition selection mechanism that automatically determines the optimal factorization structure for each layer based on its functional role in the network. By developing specialized initialization schemes and regularization techniques for tensor networks, TDNets overcome the training instabilities that have hindered previous tensor-based approaches. Experiments across computer vision, natural language processing, and recommendation systems demonstrate that TDNets achieve comparable performance to standard architectures while reducing parameter counts by 85-93% and inference time by 67-74%. Theoretical analysis establishes provable bounds on the expressivity of different decomposition schemes relative to parameter count. This work establishes a comprehensive framework for designing inherently efficient neural architectures through systematic tensor algebraic principles rather than post-hoc compression techniques.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2617,A Multi-Decomposition Method for Compressing Larger AI Models Based on Reinforcement Learning,"With the development of modern deep neural network (DNN), the scale of parameters is increasing, making it difficult to deploy models for use on resource-constrained edge devices. To address this issue, model compression is necessary, and using low-rank matrix decomposition to compress DNN models is an effective research approach. However, traditional studies on low-rank decomposition compression typically apply a single matrix decomposition method to each parameter matrix in the neural network, without considering the structural characteristics of each layer in AI models, thus failing to achieve the optimal compression effect. Therefore, this paper proposes, for the first time, a scheme for model compression using multiple decomposition methods, selecting the most suitable decomposition method for each layer in the model. However, to truly implement this approach, it is essential to balance model accuracy and compression cost. To address this, we propose a joint optimization paradigm that simultaneously optimizes model accuracy and compression rate. We also introduce a framework LMFBRL based on reinforcement learning that jointly selects the optimal decomposition method and rank. Tests were conducted on five models such as LeNet-300, ResNet-20, and Vgg-16. Compared to singly using the MF method for compressing the LeNet300 model, our approach has shown an improvement of 3.6% in compression rate and a 1.8% increase in accuracy. The test results validate the effectiveness of the algorithm proposed in this paper.",ICLR.cc/2025/Conference,2.0,nan,0.8445,parameter efficiency deep learning remains critical challenge for deployment resource constrained environments tensor decomposition networks tdnets systematic that fundamentally redesigns neural architectures principled tensor factorization methods the key innovation learnable decomposition selection mechanism that automatically determines the optimal factorization structure for each layer its functional role the network experiments across computer vision natural language processing and recommendation systems that tdnets achieve comparable standard architectures while reducing parameter counts and inference time this establishes comprehensive for designing inherently efficient neural architectures systematic tensor algebraic principles rather than post hoc compression techniques,the development modern deep neural network dnn the scale parameters increasing making difficult deploy models for use resource constrained edge devices however traditional studies low rank decomposition compression apply single matrix decomposition each parameter matrix the neural network considering the structural characteristics each layer models thus failing achieve the optimal compression effect address this joint optimization paradigm that simultaneously optimizes and compression rate also lmfbrl reinforcement learning that jointly selects the optimal decomposition and rank,2025-08-26T01:56:21.867398
29,**Memory-Centric Neural Architectures: Unified Representation Learning with Associative Recall**,"Current neural architectures treat memory as a secondary component rather than a central mechanism for representation learning. We introduce Memory-Centric Neural Architectures (MCNAs), a novel framework that places associative memory operations at the core of neural computation. Our approach represents knowledge through explicit memory matrices that store and retrieve information through learnable key-value associations, enabling more transparent and efficient learning. The key innovation is a differentiable memory read-write mechanism with contrastive key-value binding that ensures memory stability while allowing gradient-based optimization. By developing specialized techniques for memory consolidation and forgetting, MCNAs overcome catastrophic forgetting without requiring explicit replay buffers. Experiments on continual learning, few-shot adaptation, and knowledge-intensive reasoning demonstrate that MCNAs outperform traditional architectures by 10-17% in knowledge retention and 8-13% in sample efficiency. Particularly notable is the ability of MCNAs to perform explicit memory editing, allowing targeted correction of specific memories without complete retraining. This work establishes a new paradigm for neural architecture design centered around explicit, addressable memory operations rather than implicit distributed representations, enabling more interpretable and adaptable models.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8071,An Evolved Universal Transformer Memory,"Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads. NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.",ICLR.cc/2025/Conference,7.0,True,0.8040,current neural architectures treat memory secondary component rather than central mechanism for representation learning memory centric neural architectures mcnas that places associative memory operations the core neural computation our represents knowledge explicit memory matrices that store and retrieve information learnable key value associations enabling more transparent and efficient learning the key innovation differentiable memory read write mechanism contrastive key value binding that ensures memory stability while allowing gradient based optimization experiments continual learning few shot adaptation and knowledge intensive reasoning that mcnas outperform traditional architectures knowledge retention and sample efficiency this establishes paradigm for neural centered around explicit addressable memory operations rather than implicit distributed representations enabling more interpretable and adaptable models,overcome this trade off neural attention memory models namms introducing learned network for memory management that improves both the and efficiency transformers evolve namms atop pre trained transformers provide different latent contexts focusing the most relevant information for individual layers and attention heads namms are universally applicable any self attention they condition exclusively the values the produced attention matrices learning namms small set problems achieve substantial improvements across multiple long context benchmarks while cutting the model input contexts fraction the original sizes the generality our conditioning enables zero shot transfer namms trained only language entirely transformer architectures even across input modalities their benefits carrying over vision and reinforcement learning,2025-08-26T01:56:21.867402
30,**Sparse Mixture of LoRA Experts: Parameter-Efficient Transfer Learning with Conditional Computation**,"Pre-trained foundation models have revolutionized machine learning, but fine-tuning these models for specific tasks remains computationally intensive and parameter-inefficient. We introduce Sparse Mixture of LoRA Experts (SMoLE), a novel architecture that combines conditional computation with parameter-efficient fine-tuning. Our approach decomposes adaptation into multiple specialized low-rank experts that are conditionally activated based on input characteristics. By integrating a differentiable expert routing mechanism with Low-Rank Adaptation (LoRA), SMoLE achieves greater expressivity than standard fine-tuning approaches while maintaining parameter efficiency. We develop specialized regularization techniques that prevent expert collapse and ensure diverse specialization, addressing common challenges in mixture-of-experts systems. Experiments across natural language processing, computer vision, and multimodal tasks demonstrate that SMoLE outperforms standard LoRA by 12-18% on complex tasks while using comparable parameter counts. Notably, our approach enables effective knowledge compartmentalization, allowing single models to specialize in multiple domains without interference. SMoLE represents a significant advancement in parameter-efficient transfer learning by enabling conditional adaptation of pre-trained models through sparse, specialized expert modules.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,9194,The Quest for Winning Tickets in Low-Rank Adapters,"Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method, offers an effective strategy for adapting large pre-trained models to specific tasks with minimal computational overhead. LoRA achieves this by introducing low-rank parameter matrices to the frozen pre-trained models. However, despite their efficiency, LoRA and its variants modify all elements of a parameter block, which is unnecessary as LoRA primarily aims to adjust a small set of subspaces that capture task-specific knowledge. Drawing inspiration from the Lottery Ticket Hypothesis (LTH), which posits that dense neural networks contain sparse subnetworks capable of performing similarly to fully-parameterized models, we investigate whether similar sparse subnetworks exist for low-rank adapters. We demonstrate that such subnetworks, often referred to as ""winning tickets"" in the context of LTH, indeed exist for low-rank adapters. We introduce a method to identify this sparse subset of weights for each layer by relating the top subspaces of the pretrained parameter block to the elements of the corresponding weight matrix. This subset is then fine-tuned using LoRA. We show that this sparse subset is not necessarily unique; as long as sparsity is kept within a certain bound defined by the task, random subnetworks with similar sparsity can act as winning tickets. Building on this discovery, we propose a novel approach called Partial-LoRA, which adds sparse low-rank parameters to pre-trained models. Through extensive experiments on 8 vision and 4 language tasks, we demonstrate that Partial-LoRA can reduce trainable parameters by up to 87% while maintaining or even improving model performance in some cases. Our work thus reduces memory needs and theoretically grounds sparse LoRAs.",ICLR.cc/2025/Conference,5.2,False,0.8871,pre trained foundation models have revolutionized machine learning but fine tuning these models for specific tasks remains computationally intensive and parameter inefficient our decomposes adaptation into multiple specialized low rank experts that are conditionally activated input characteristics integrating differentiable expert routing mechanism low rank adaptation lora smole achieves greater expressivity than standard fine tuning approaches while maintaining parameter efficiency experiments across natural language processing computer vision and multimodal tasks that smole outperforms standard lora complex tasks while comparable parameter counts notably our enables effective knowledge compartmentalization allowing single models specialize multiple domains interference smole represents significant advancement parameter efficient transfer learning enabling conditional adaptation pre trained models sparse specialized expert modules,low rank adaptation lora prominent parameter efficient fine tuning peft offers effective strategy for adapting large pre trained models specific tasks minimal computational overhead however despite their efficiency lora and its variants modify all elements parameter block which unnecessary lora primarily aims adjust small set subspaces that capture task specific knowledge drawing inspiration from the lottery ticket hypothesis lth which posits that dense neural networks contain sparse subnetworks capable performing similarly fully parameterized models whether similar sparse subnetworks exist for low rank adapters extensive experiments vision and language tasks that partial lora can reduce trainable parameters while maintaining even improving some cases,2025-08-26T01:56:21.867408
31,**Neural Algorithmic Reasoning with Mesa-Optimization: Learning to Plan with Differentiable Search Algorithms**,"Neural networks struggle with complex algorithmic reasoning tasks that require explicit planning and search. We introduce Mesa-Optimizers, a novel architecture that embeds differentiable algorithmic primitives as specialized neural modules capable of performing explicit search and planning. Our approach implements common search algorithms (breadth-first, depth-first, A*) as end-to-end differentiable operations, enabling the network to learn when and how to leverage algorithmic reasoning. The key innovation is a meta-controller that dynamically selects and parameterizes appropriate search strategies based on problem characteristics. By incorporating algorithmic inductive biases while maintaining differentiability, Mesa-Optimizers combine the structured reasoning of classical algorithms with the flexibility of learned representations. Experiments on path planning, combinatorial optimization, and sequential decision-making demonstrate that our architecture outperforms standard neural networks by 25-40% on complex reasoning tasks while maintaining competitive performance on pattern recognition problems. Notably, Mesa-Optimizers exhibit strong generalization to larger problem instances and novel environments, suggesting they learn generalizable algorithmic strategies rather than memorizing solutions. This work represents a fundamental step toward neural architectures that can seamlessly integrate algorithmic reasoning with learned representations.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2919,Algorithmic Language Models with Neurally Compiled Libraries,"Important reasoning tasks such as planning are fundamentally algorithmic, meaning that solving these tasks robustly requires inducing the underlying algorithms, rather than shortcuts. Large Language Models lack true algorithmic ability primarily because of the limitations of neural network optimization algorithms, their optimization data and optimization objective, but also due to the inexpressivity of the transformer architecture. To address this lack of algorithmic ability, our paper proposes augmenting LLMs with an internal reasoning module. This module contains a library of fundamental operations and sophisticated differentiable programs, so that common algorithms do not need to be learned from scratch. To accomplish this, we add memory, registers, basic operations, and adaptive recurrence to a billion-parameter scale transformer architecture built on LLaMA3.2. Then, we define a method for directly compiling algorithms into a differentiable starting library, which is used natively and propagates gradients for optimization. In this paper, we study the feasibility of this augmentation by fine-tuning an augmented LLaMA 3.2 on simple algorithmic tasks with variable computational depth, such as a recursive fibonacci algorithm or insertion sort.",ICLR.cc/2025/Conference,3.75,False,0.8714,neural networks struggle complex algorithmic reasoning tasks that require explicit planning and search mesa optimizers that embeds differentiable algorithmic primitives specialized neural modules capable performing explicit search and planning our implements common search algorithms breadth first depth first end end differentiable operations enabling the network learn when and how leverage algorithmic reasoning incorporating algorithmic inductive biases while maintaining differentiability mesa optimizers combine the structured reasoning classical algorithms the flexibility learned representations experiments path planning combinatorial optimization and sequential decision making that our outperforms standard neural networks complex reasoning tasks while maintaining competitive pattern recognition problems this represents fundamental step toward neural architectures that can seamlessly integrate algorithmic reasoning learned representations,important reasoning tasks such planning are fundamentally algorithmic meaning that solving these tasks robustly requires inducing the underlying algorithms rather than shortcuts large language models lack true algorithmic ability primarily because the limitations neural network optimization algorithms their optimization data and optimization objective but also due the inexpressivity the transformer address this lack algorithmic ability our proposes augmenting llms internal reasoning module accomplish this add memory registers basic operations and adaptive recurrence billion parameter scale transformer built llama3 then define for directly compiling algorithms into differentiable starting library which used natively and propagates gradients for optimization,2025-08-26T01:56:21.867417
32,**Continual Architecture Evolution: Self-Modifying Neural Networks Through Differentiable Mutation Operators**,"Neural architecture design typically remains fixed after initial training, limiting adaptation to evolving data distributions and task requirements. We present Continual Architecture Evolution (CAE), a novel framework enabling neural networks to self-modify their architectural structure during deployment. Our approach implements differentiable mutation operators that can add, remove, or transform architectural components through forward-mode gradient-based updates, allowing networks to evolve without interrupting service. The key innovation is a meta-plasticity mechanism that learns when and how to apply architectural mutations based on performance metrics and computational constraints. By formalizing architecture evolution as a differentiable operation, CAE enables continuous adaptation without requiring separate search or retraining phases. Experiments on streaming data, evolving distributions, and multi-task settings demonstrate that CAE networks outperform static architectures by 17-23% in long-term performance while maintaining computational efficiency. Theoretical analysis establishes convergence guarantees under specific conditions despite the non-stationary nature of the architecture. CAE represents a paradigm shift from static to dynamically evolving neural architectures that continuously refine their structure in response to changing requirements and data distributions.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,9273,Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective,"The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. Existing studies have proposed numerous CL methods to achieve this trade-off. However, these methods often overlook the impact of basic architecture on stability and plasticity, thus the trade-off is limited to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Architecture (Dual-Arch), which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments across datasets and CL methods demonstrate that Dual-Arch can enhance the performance of existing CL methods while being up to 87% more compact in terms of parameters than the baselines.",ICLR.cc/2025/Conference,5.2,False,0.8246,neural remains fixed after initial training limiting adaptation evolving data distributions and task requirements continual evolution cae enabling neural networks self modify their architectural structure during deployment formalizing evolution differentiable operation cae enables continuous adaptation requiring separate search retraining phases experiments streaming data evolving distributions and multi task settings that cae networks outperform static architectures long term while maintaining computational efficiency cae represents paradigm shift from static dynamically evolving neural architectures that continuously refine their structure response changing requirements and data distributions,the quest for continual learning seeks empower neural networks the ability learn and adapt incrementally central this pursuit addressing the stability plasticity dilemma which involves striking balance between two conflicting objectives preserving previously learned knowledge and acquiring knowledge each network designed specialized and lightweight tailored its respective objective,2025-08-26T01:56:21.867420
33,**DeepSymbolic: Learning Symbolic Computational Graphs Through Differentiable Program Synthesis**,"Neural networks excel at pattern recognition but struggle to learn interpretable, symbolic operations critical for certain domains. We introduce DeepSymbolic, a novel architecture that learns explicit symbolic computational graphs from data through differentiable program synthesis. Our approach represents computations as directed acyclic graphs of basic operations (arithmetic, logical, and conditional) that are discovered and optimized end-to-end via gradient descent. The key innovation is a continuous relaxation of discrete program structure through a specially designed architecture that parameterizes the space of possible programs. By incorporating domain-specific languages and type systems as architectural constraints, DeepSymbolic enables guided exploration of the program space while maintaining differentiability. Experiments on mathematical reasoning, physical simulation, and algorithmic tasks demonstrate that DeepSymbolic achieves comparable accuracy to black-box neural networks while producing interpretable symbolic programs that generalize perfectly to out-of-distribution inputs. The discovered programs provide insights into underlying patterns and can be formally verified, edited, and transferred across problems. DeepSymbolic bridges the gap between neural and symbolic approaches by learning explicit computational structures that combine the learnability of neural networks with the interpretability and generalizability of symbolic programs.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1657,Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning,"Neurosymbolic learning has emerged as a promising paradigm to incorporate
symbolic reasoning into deep learning models.
However, existing frameworks are limited in scalability with respect to both
the training data and the complexity of symbolic programs.
We propose Dolphin, a framework to scale neurosymbolic learning at a fundamental level by mapping both forward chaining and backward gradient propagation in symbolic programs 
to vectorized computations.
For this purpose, Dolphin introduces a set of abstractions and primitives 
built directly on top of a high-performance deep learning framework like 
PyTorch, effectively enabling symbolic programs to be written as PyTorch modules.
It thereby enables neurosymbolic programs to be written in a language like Python that is familiar to developers and compile them to computation graphs that are amenable to end-to-end differentiation on GPUs.
We evaluate Dolphin on a suite of 13 benchmarks across 5 neurosymbolic tasks that combine deep learning models for
text, image, or video processing with symbolic programs that involve multi-hop 
reasoning, recursion, and even black-box functions like Python `eval()`.
Dolphin achieves comparable or better accuracy on all benchmarks while taking 0.33% -- 61.73% of the time (and 23.23% on average) to train these models on the largest input per task compared to baselines Scallop, ISED, and IndeCateR+, which time out on most of these inputs.",ICLR.cc/2025/Conference,6.0,False,0.8605,neural networks excel pattern recognition but struggle learn interpretable symbolic operations critical for certain domains experiments mathematical reasoning physical simulation and algorithmic tasks that deepsymbolic achieves comparable black box neural networks while producing interpretable symbolic programs that generalize perfectly out distribution inputs deepsymbolic bridges the gap between neural and symbolic approaches learning explicit computational structures that combine the learnability neural networks the interpretability and generalizability symbolic programs,neurosymbolic learning has emerged promising paradigm incorporate symbolic reasoning into deep learning models dolphin scale neurosymbolic learning fundamental level mapping both forward chaining and backward gradient propagation symbolic programs vectorized computations for this purpose dolphin introduces set ions and primitives built directly top high performance deep learning like pytorch enabling symbolic programs written pytorch modules thereby enables neurosymbolic programs written language like python that familiar developers and compile them computation graphs that are amenable end end differentiation gpus dolphin suite benchmarks across neurosymbolic tasks that combine deep learning models for text image video processing symbolic programs that involve multi hop reasoning recursion and even black box functions like python eval,2025-08-26T01:56:21.867425
34,**Probabilistic Circuit Networks: Neural Architectures with Tractable Uncertainty Estimation**,"Deep neural networks lack principled uncertainty quantification, limiting their reliability in critical applications. We present Probabilistic Circuit Networks (PCNs), a novel architecture that guarantees tractable computation of exact probability distributions through a carefully designed computational graph. Our approach organizes differentiable probabilistic circuits in a hierarchical structure that enables closed-form computation of marginals, conditionals, and expectations without sampling or variational approximations. The key innovation is a specialized parameterization that ensures tractability while maintaining expressivity comparable to standard neural networks. By incorporating sum-product networks, hidden Markov models, and Gaussian mixture components as architectural building blocks, PCNs can represent complex multimodal distributions while enabling exact inference. Experiments on classification with rejection, anomaly detection, and probabilistic time-series forecasting demonstrate that PCNs outperform Bayesian neural networks and ensembles by 15-22% in calibration metrics while maintaining competitive accuracy. Notably, PCNs perform inference 50-100× faster than sampling-based approaches due to their closed-form uncertainty calculations. This work establishes a new direction for neural architecture design that provides rigorous uncertainty estimation through architectural constraints rather than approximate inference techniques.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,5589,Scalable Approximate Message Passing for Bayesian Neural Networks,"Bayesian neural networks (BNNs) offer the potential for reliable uncertainty quantification and interpretability, which are critical for trustworthy AI in high-stakes domains. However, existing methods often struggle with issues such as overconfidence, hyperparameter sensitivity, and posterior collapse, leaving room for alternative approaches. In this work, we advance message passing (MP) for BNNs and present a novel framework that models the predictive posterior as a factor graph. To the best of our knowledge, our framework is the first MP method that handles convolutional neural networks and avoids double-counting training data, a limitation of previous MP methods that causes overconfidence. We evaluate our approach on CIFAR-10 with a convolutional neural network of roughly 890k parameters and find that it can compete with the SOTA baselines AdamW and IVON, even having an edge in terms of calibration. On synthetic data, we validate the uncertainty estimates and observe a strong correlation (0.9) between posterior credible intervals and its probability of covering the true data-generating function outside the training range. While our method scales to an MLP with 5.6 million parameters, further improvements are necessary to match the scale and performance of state-of-the-art variational inference methods.",ICLR.cc/2025/Conference,5.0,False,0.8623,deep neural networks lack principled uncertainty quantification limiting their reliability critical applications the key innovation specialized parameterization that ensures tractability while maintaining expressivity comparable standard neural networks experiments classification rejection anomaly detection and probabilistic time series forecasting that pcns outperform bayesian neural networks and ensembles calibration metrics while maintaining competitive this establishes direction for neural that provides rigorous uncertainty estimation architectural constraints rather than approximate inference techniques,bayesian neural networks bnns offer the potential for reliable uncertainty quantification and interpretability which are critical for trustworthy high stakes domains the best our knowledge our the first that handles convolutional neural networks and avoids double counting training data limitation previous methods that causes overconfidence our cifar convolutional neural network roughly 890k parameters and find that can compete the sota baselines adamw and ivon even having edge terms calibration,2025-08-26T01:56:21.867430
35,**Intrinsically Sparse Architectures: Training Dense Networks Without Weight Matrices**,"Standard neural networks represent weights as dense matrices, leading to significant computational and memory inefficiency despite high parameter redundancy. We introduce Intrinsically Sparse Architectures (ISAs), a novel paradigm that fundamentally reimagines neural computation without explicit weight matrices. Our approach replaces traditional linear transformations with parameterized hash functions that implicitly define sparse connectivity patterns through input-dependent computational routing. The key innovation is a differentiable indexing mechanism that computes network activations directly from input features without materializing weight matrices, enabling sub-linear scaling with respect to virtual parameter count. By carefully designing the hash function parameterization and developing specialized gradient estimators, ISAs maintain trainability while achieving extreme sparsity. Experiments across vision, language, and reinforcement learning tasks demonstrate that ISAs achieve performance comparable to traditional architectures while reducing memory requirements by 85-95% and improving inference speed by 3-7×. Theoretical analysis shows that ISAs can approximate standard networks with high precision despite their implicit parameterization. This work establishes a fundamentally new approach to neural architecture design based on algorithmic parameter generation rather than stored weight matrices.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6837,QP-SNN: Quantized and Pruned Spiking Neural Networks,"Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to encode information and operate in an asynchronous event-driven manner, offering a highly energy-efficient paradigm for machine intelligence. However, the current SNN community focuses primarily on performance improvement by developing large-scale models, which limits the applicability of SNNs in resource-limited edge devices. In this paper, we propose a hardware-friendly and lightweight SNN, aimed at effectively deploying high-performance SNN in resource-limited scenarios. Specifically, we first develop a baseline model that integrates uniform quantization and structured pruning, called QP-SNN baseline. While this baseline significantly reduces storage demands and computational costs, it suffers from performance decline. To address this, we conduct an in-depth analysis of the challenges in quantization and pruning that lead to performance degradation and propose solutions to enhance the baseline's performance. For weight quantization, we propose a weight rescaling strategy that utilizes bit width more effectively to enhance the model's representation capability. For structured pruning, we propose a novel pruning criterion using the singular value of spatiotemporal spike activities to enable more accurate removal of redundant kernels. Extensive experiments demonstrate that integrating two proposed methods into the baseline allows QP-SNN to achieve state-of-the-art performance and efficiency, underscoring its potential for enhancing SNN deployment in edge intelligence computing.",ICLR.cc/2025/Conference,6.75,True,0.8214,standard neural networks represent weights dense matrices leading significant computational and memory inefficiency despite high parameter redundancy intrinsically sparse architectures isas paradigm that fundamentally reimagines neural computation explicit weight matrices the key innovation differentiable indexing mechanism that computes network activations directly from input features materializing weight matrices enabling sub linear scaling respect virtual parameter count experiments across vision language and reinforcement learning tasks that isas achieve comparable traditional architectures while reducing memory requirements and improving inference speed this establishes fundamentally neural algorithmic parameter generation rather than stored weight matrices,brain inspired spiking neural networks snns leverage sparse spikes encode information and operate asynchronous event driven manner offering highly energy efficient paradigm for machine intelligence for weight quantization weight rescaling strategy that utilizes bit width more enhance the model representation capability,2025-08-26T01:56:21.867439
36,**Neural Ordinary Differential Operators: Continuous-Depth Architectures for Functional Data Analysis**,"Traditional neural networks discretize computation into fixed-depth layers, limiting their ability to model continuous transformations between function spaces. We present Neural Ordinary Differential Operators (NODOs), a novel architecture that extends Neural ODEs to operate directly on functional data rather than fixed-dimensional vectors. Our approach represents both network states and inputs as functions in infinite-dimensional spaces, with dynamics governed by learned differential operators that transform entire functions continuously. The key innovation is a spectral parameterization of differential operators that enables efficient computation while preserving the functional nature of representations throughout the network. By developing specialized numerical integration schemes for operator equations, NODOs maintain computational tractability despite operating in infinite-dimensional spaces. Experiments on spatiotemporal forecasting, partial differential equation learning, and functional regression demonstrate that NODOs outperform discretized architectures by 18-26% in prediction accuracy while providing resolution-independent processing of continuous signals. Theoretical analysis establishes connections to Green's functions and reproducing kernel Hilbert spaces, providing insights into the expressivity of operator-based architectures. NODOs represent a fundamental advancement in neural architecture design for functional data, enabling direct processing of continuous signals without discretization artifacts.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7323,Accelerating Neural ODEs: A Variational Formulation-based Approach,"Neural Ordinary Differential Equations (Neural ODEs or NODEs) excel at modeling continuous dynamical systems from observational data, especially when the data is irregularly sampled. However, existing training methods predominantly rely on numerical ODE solvers, which are time-consuming and prone to accumulating numerical errors over time due to autoregression. In this work, we propose VF-NODE, a novel approach based on the variational formulation (VF) to accelerate the training of NODEs. Unlike existing training methods, the proposed VF-NODEs implement a series of global integrals, thus evaluating Deep Neural Network (DNN)--based vector fields only at specific observed data points. This strategy drastically reduces the number of function evaluations (NFEs). Moreover, our method eliminates the use of autoregression, thereby reducing error accumulations for modeling dynamical systems. Nevertheless, the VF loss introduces oscillatory terms into the integrals when using the Fourier basis. We incorporate Filon's method to address this issue. To further enhance the performance for noisy and incomplete data, we employ the natural cubic spline regression to estimate a closed-form approximation. We provide a fundamental analysis of how our approach minimizes computational costs. Extensive experiments demonstrate that our approach accelerates NODE training by 10 to 1000 times compared to existing NODE-based methods, while achieving higher or comparable accuracy in dynamical systems. The code is available at https://github.com/ZhaoHongjue/VF-NODE-ICLR2025.",ICLR.cc/2025/Conference,6.5,True,0.8681,traditional neural networks discretize computation into fixed depth layers limiting their ability continuous transformations between function spaces neural ordinary differential operators nodos that extends neural odes operate directly functional data rather than fixed dimensional vectors our represents both network states and inputs functions infinite dimensional spaces dynamics governed learned differential operators that transform entire functions continuously the key innovation spectral parameterization differential operators that enables efficient computation while preserving the functional nature representations throughout the network experiments spatiotemporal forecasting partial differential equation learning and functional regression that nodos outperform discretized architectures prediction while providing resolution independent processing continuous signals nodos represent fundamental advancement neural for functional data enabling direct processing continuous signals discretization artifacts,neural ordinary differential equations neural odes nodes excel modeling continuous dynamical systems from observational data when the data irregularly sampled unlike existing training methods the proposed nodes series global integrals thus evaluating deep neural network dnn based vector fields only specific observed data points,2025-08-26T01:56:21.867442
37,**Adaptive Computation Depth Networks: Instance-Dependent Dynamic Layer Traversal**,"Current deep networks apply identical computational depth to all inputs, resulting in inefficient processing of easy examples and insufficient capacity for difficult ones. We introduce Adaptive Computation Depth Networks (ACDNets), a novel architecture that dynamically determines computational depth on a per-instance basis. Our approach implements a differentiable halting mechanism that decides whether to execute or skip each layer based on the current representation state, optimizing computational efficiency while maintaining task performance. The key innovation is a reinforcement learning-based controller trained with a novel reward function that balances accuracy against computational cost, enabling principled trade-offs between performance and efficiency. By incorporating auxiliary classifiers at intermediate depths, ACDNets can exit early for easily recognizable inputs while utilizing full depth for challenging cases. Experiments across image classification, natural language processing, and reinforcement learning demonstrate that ACDNets reduce average inference FLOPs by 35-60% compared to static networks while maintaining or slightly improving accuracy. Analysis reveals that computation depth correlates strongly with sample difficulty, with ACDNets learning to allocate resources according to instance complexity. This work establishes a new approach to neural architecture design based on dynamic, input-dependent computation paths rather than static, uniform processing.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,824,Adaptive higher order reversible integrators for memory efficient deep learning,"The depth of networks plays a crucial role in the effectiveness of deep learning. However, the memory requirement for backpropagation scales linearly with the number of layers, which leads to memory bottlenecks during training. Moreover, deep networks are often unable to handle time-series data appearing at irregular intervals. These issues can be resolved by considering continuous-depth networks based on the neural ODE framework in combination with reversible integration methods that allow for variable time-steps. Reversibility of the method ensures that the memory requirement for training is independent of network depth, while variable time-steps are required for assimilating time-series data on irregular intervals. However, at present, there are no known higher-order reversible methods with this property. High-order methods are especially important when a high level of accuracy in learning is required or when small time-steps are necessary due to large errors in time integration of neural ODEs, for instance in context of complex dynamical systems such as Kepler systems and molecular dynamics. The requirement of small time-steps when using a low-order method can significantly increase the computational cost of training as well as inference. In this work, we present an approach for constructing high-order reversible methods that allow adaptive time-stepping. Our numerical tests show the advantages in computational speed when applied to the task of learning dynamical systems.",ICLR.cc/2025/Conference,4.666666666666667,False,0.8384,current deep networks apply identical computational depth all inputs resulting inefficient processing easy examples and insufficient capacity for difficult ones our implements differentiable halting mechanism that decides whether execute skip each layer the current representation state optimizing computational efficiency while maintaining task the key innovation reinforcement learning based controller trained reward function that balances against computational cost enabling principled trade offs between and efficiency experiments across image classification natural language processing and reinforcement learning that acdnets reduce average inference flops compared static networks while maintaining slightly improving analysis reveals that computation depth correlates strongly sample difficulty acdnets learning allocate resources according instance complexity this establishes neural dynamic input dependent computation paths rather than static uniform processing,the depth networks plays crucial role the effectiveness deep learning moreover deep networks are often unable handle time series data appearing irregular intervals these issues can resolved considering continuous depth networks the neural ode combination reversible integration methods that allow for variable time steps reversibility the ensures that the memory requirement for training independent network depth while variable time steps are required for assimilating time series data irregular intervals high order methods are important when high level learning required when small time steps are necessary due large errors time integration neural odes for instance context complex dynamical systems such kepler systems and molecular dynamics our numerical tests the advantages computational speed when applied the task learning dynamical systems,2025-08-26T01:56:21.867449
38,**Coherent Gradient Networks: Training Neural Architectures with Natural Gradient Properties**,"Training deep neural networks efficiently requires addressing the challenges of vanishing/exploding gradients and poor conditioning of the loss landscape. We introduce Coherent Gradient Networks (CGNets), a novel architecture specifically designed to maintain consistent gradient flow through principled parameterization of linear and nonlinear components. Our approach implements weight parameterization schemes that implicitly approximate natural gradient descent without explicit Fisher matrix computations, enabling more efficient optimization. The key innovation is a coherence-preserving initialization and normalization strategy that ensures eigenvalues of the network's Jacobian remain close to unity throughout training. By carefully coordinating parameterization across layers through a global conditioning mechanism, CGNets maintain favorable optimization properties even at extreme depths. Experiments on deep supervised learning, generative modeling, and reinforcement learning demonstrate that CGNets converge 3-5× faster than standard architectures while achieving 5-10% better final performance. Theoretical analysis establishes connections to information geometry and provides convergence guarantees under specific conditions. CGNets represent a fundamental rethinking of neural architecture design focused on optimization dynamics rather than representational capacity, resulting in networks that train more efficiently and reliably across diverse tasks.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2237,Gaussian Loss Smoothing Enables Certified Training with Tight Convex Relaxations,"Training neural networks with high certified accuracy against adversarial examples remains an open challenge despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods, perhaps surprisingly, can perform worse than looser relaxations. Prior work hypothesized that this phenomenon is caused by the discontinuity, non-smoothness and perturbation sensitivity of the loss surface induced by tighter relaxations. In this work, we theoretically show that Gaussian Loss Smoothing (GLS) can alleviate these issues. We confirm this empirically by instantiating GLS with two variants: a zeroth-order optimization algorithm called PGPE which allows training with non-differentiable relaxations, and a first-order optimization algorithm, called RGS, which requires gradients of the relaxation, but is much more efficient than PGPE. Extensive experiments show that when combined with tight relaxations, these methods surpass state-of-the-art methods when training on the same network architecture for many settings. Our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks and pave a path towards leveraging tighter relaxations for certified training.",ICLR.cc/2025/Conference,3.3333333333333335,False,0.8438,training deep neural networks requires addressing the challenges vanishing exploding gradients and poor conditioning the loss landscape our implements weight parameterization schemes that implicitly approximate natural gradient descent explicit fisher matrix computations enabling more efficient optimization carefully coordinating parameterization across layers global conditioning mechanism cgnets maintain favorable optimization properties even extreme depths experiments deep supervised learning generative modeling and reinforcement learning that cgnets converge faster than standard architectures while achieving better final cgnets represent fundamental rethinking neural focused optimization dynamics rather than representational capacity resulting networks that train more and reliably across diverse tasks,training neural networks high certified against adversarial examples remains open challenge despite significant efforts confirm this empirically instantiating gls two variants zeroth order optimization called pgpe which allows training non differentiable relaxations and first order optimization called rgs which requires gradients the relaxation but much more efficient than pgpe extensive experiments that when combined tight relaxations these methods surpass state the art methods when training the same network for many settings our clearly the promise gaussian loss smoothing for training certifiably robust neural networks and pave path towards leveraging tighter relaxations for certified training,2025-08-26T01:56:21.867457
39,**Dynamical Isometry Networks: Controlling Signal Propagation Through Structured Matrix Factorization**,"The expressivity of deep neural networks is fundamentally limited by their ability to preserve gradient information during forward and backward passes. We present Dynamical Isometry Networks (DINets), a novel architecture that maintains singular value concentration through carefully designed matrix factorization schemes. Our approach parameterizes weight matrices using specialized orthogonal components with controlled spectral properties, enabling stable signal propagation regardless of network depth. The key innovation is a learnable matrix factorization that adapts its spectral properties during training while maintaining near-isometric transformation properties. By incorporating structured randomness through Haar-distributed orthogonal factors, DINets ensure that gradients neither vanish nor explode, even in extremely deep networks. Experiments on image classification, generative modeling, and recurrent network tasks demonstrate that DINets train 2-3× faster than standard architectures and achieve 3-7% better performance, with particularly dramatic improvements in networks exceeding 100 layers. Theoretical analysis provides tight bounds on the singular value distribution throughout training, explaining the empirical advantages. DINets establish a principled approach to neural architecture design based on spectral control of information flow, enabling more reliable training of ultra-deep networks across diverse applications.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6549,Faster Gradient Descent in Deep Linear Networks: The Advantage of Depth,"Gradient descent dynamics in deep linear networks has been studied under a wide range of settings. These studies have reported some negative results on the role of depth, in that, gradient descent in  deep linear networks: (i) can take exponential number of iterations to converge, (ii) can exhibit sigmoidal learning, i.e., almost no learning in initial phase followed by rapid learning, (iii) can delay convergence with increase in depth. Some of these results are also under stronger assumptions such as whitened data and balanced initialisation. These messages from prior works suggest that depth hurts the speed of convergence.

In this paper, we argue that the negative role of depth in the prior works is due to certain pitfalls which can be carefully avoided. We give a positive message on the role of depth, i.e., seen as an additional resource, depth can always be used to speed up convergence. For this purpose, we consider scalar regression with quadratic loss. In this setting, we propose a novel aligned gradient descent (AGD) algorithm for which we show that (i) linear convergence is always possible (ii) depth accelerates the speed of convergence. In AGD, feature alignment happens in first layer and the deeper layers accelerate by learning the right scale. We show acceleration in AGD happens in finite time for unwhitened data. We provide insights into the {acceleration} mechanism and also show that acceleration happens in phases. We also demonstrate the acceleration due to AGD on synthetic and benchmark datasets. Our main message is not propose AGD as a new algorithm in itself, but to demonstrate that depth is an advantage in linear networks thereby dispelling some of the past negative results on the role of depth.",ICLR.cc/2025/Conference,2.3333333333333335,False,0.8221,the expressivity deep neural networks fundamentally limited their ability preserve gradient information during forward and backward passes our parameterizes weight matrices specialized orthogonal components controlled spectral properties enabling stable signal propagation regardless network depth incorporating structured randomness haar distributed orthogonal factors dinets ensure that gradients neither vanish nor explode even extremely deep networks experiments image classification generative modeling and recurrent network tasks that dinets train faster than standard architectures and achieve better dramatic improvements networks exceeding layers dinets establish principled neural spectral control information flow enabling more reliable training ultra deep networks across diverse applications,gradient descent dynamics deep linear networks has been studied under wide range settings these studies have reported some negative the role depth that gradient descent deep linear networks can take exponential number iterations converge can exhibit sigmoidal learning almost learning initial phase followed rapid learning iii can delay convergence increase depth agd feature alignment happens first layer and the deeper layers accelerate learning the right scale,2025-08-26T01:56:21.867461
40,**HyperCorrelated Networks: Learning Correlations Between Neural Network Parameters**,"Standard neural networks treat parameters as independent variables, ignoring the natural correlations between weights that emerge during learning. We introduce HyperCorrelated Networks (HCNets), a novel architecture that explicitly models and exploits parameter correlations through a hypernetwork-based parameterization. Our approach represents weights as structured outputs of a hypernetwork that captures statistical dependencies between parameters, enabling more efficient parameterization and better generalization. The key innovation is a correlation-aware parameter generation mechanism that factorizes the parameter space according to learned correlation patterns, drastically reducing the effective dimension of the optimization problem. By implementing block-diagonal plus low-rank approximations within the hypernetwork, HCNets maintain computational efficiency while capturing important parameter relationships. Experiments on image classification, language modeling, and few-shot learning demonstrate that HCNets outperform standard networks by 4-8% in generalization performance while using 30-50% fewer independent parameters. Analysis of the learned correlations reveals interesting structural patterns that align with known weight sharing strategies like convolutional filters. HCNets represent a fundamental shift in neural architecture design by explicitly modeling the joint distribution of parameters rather than treating them as independent variables.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,11531,Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models,"Large pre-trained models, or foundation models, have shown impressive performance when adapted to a variety of downstream tasks, often out-performing specialized models. Hypernetworks, neural networks that generate some or all of the parameters of another neural network, have become an increasingly important technique for conditioning and generalizing implicit neural representations (INRs), which represent signals or objects such as audio or 3D shapes using a neural network. However, despite the potential benefits of incorporating foundation models in hypernetwork methods, this research direction has not been investigated, likely due to the dissimilarity of the weight generation task with other visual tasks. To address this gap, we (1) show how foundation models can improve hypernetworks with Transformer-based architectures, (2) provide an empirical analysis of the benefits of foundation models for hypernetworks through the lens of the generalizable INR task, showing that leveraging foundation models improves performance, generalizability, and data efficiency across a variety of algorithms and modalities. We also provide further analysis in examining the design space of foundation model-based hypernetworks, including examining the choice of foundation models, algorithms, and the effect of scaling foundation models.",ICLR.cc/2025/Conference,5.5,True,0.8281,standard neural networks treat parameters independent variables ignoring the natural correlations between weights that emerge during learning the key innovation correlation aware parameter generation mechanism that factorizes the parameter space according learned correlation patterns drastically reducing the effective dimension the optimization problem experiments image classification language modeling and few shot learning that hcnets outperform standard networks generalization while fewer independent parameters hcnets represent fundamental shift neural explicitly modeling the joint distribution parameters rather than treating them independent variables,hypernetworks neural networks that generate some all the parameters another neural network have become increasingly important for conditioning and generalizing implicit neural representations inrs which represent signals objects such audio shapes neural network however despite the potential benefits incorporating foundation models hypernetwork methods this direction has not been investigated likely due the dissimilarity the weight generation task other visual tasks,2025-08-26T01:56:21.867465
41,**Concept Bottleneck Transformers: Interpretable Neural Architectures with Explicit Conceptual Decomposition**,"Modern neural networks achieve impressive performance but lack interpretability, limiting their adoption in high-stakes domains. We present Concept Bottleneck Transformers (CBTs), a novel architecture that enforces decomposition of internal representations into human-understandable concepts while maintaining end-to-end differentiability. Our approach introduces specialized bottleneck layers that map continuous representations to an explicit concept space before recombining them for downstream prediction. The key innovation is a concept projection mechanism that aligns transformer attention patterns with predefined conceptual structures while allowing the model to discover emergent concepts beyond those specified. By incorporating concept-level supervision and developing specialized regularization techniques, CBTs learn representations that are both predictive and semantically meaningful. Experiments on medical diagnosis, scientific reasoning, and image classification demonstrate that CBTs achieve performance within 2-5% of black-box transformers while providing transparent concept-level explanations for predictions. Notably, CBTs enable direct intervention on concept activations, allowing users to correct specific conceptual misunderstandings without retraining. This work establishes a new approach to neural architecture design that explicitly bridges subsymbolic processing with symbolic concepts, enabling more interpretable and trustworthy deep learning models.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3159,Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers,"Balancing high performance with interpretability in increasingly powerful Transformer-based models remains a challenge. While mechanistic interpretability aims to specify neural network computations in explicit, pseudocode-like formats, existing methods often involve laborious manual analysis or struggle to fully elucidate learned internal algorithms. Recent efforts to build intrinsically interpretable models have introduced considerable expressivity and optimization challenges. This work introduces Adaptive Transformer Programs, an enhanced framework building upon RASP language and Transformer Programs to create more robust and interpretable models. The proposed method increases expressivity by redesigning two primary attention modules to improve categorical and numerical reasoning capabilities. To overcome optimization hurdles, we introduce a novel reparameterization scheme that enhances the exploration-exploitation trade-off during training. We validate our approach through extensive experiments on diverse tasks, including in-context learning, algorithmic problems (e.g., sorting and Dyck languages), and NLP benchmarks such as named entity recognition and text classification. Results demonstrate that Adaptive Transformer Programs substantially narrow the performance gap between black-box Transformers and interpretable models, enhancing transparency. This work advances the development of high-performing, transparent AI systems for critical applications, addressing crucial ethical concerns in AI development.",ICLR.cc/2025/Conference,7.0,True,0.8605,modern neural networks achieve impressive but lack interpretability limiting their adoption high stakes domains our introduces specialized bottleneck layers that map continuous representations explicit concept space before recombining them for downstream prediction the key innovation concept projection mechanism that aligns transformer attention patterns predefined conceptual structures while allowing the discover emergent concepts beyond those specified experiments medical diagnosis scientific reasoning and image classification that cbts achieve within black box transformers while providing transparent concept level explanations for predictions this establishes neural that explicitly bridges subsymbolic processing symbolic concepts enabling more interpretable and trustworthy deep learning models,balancing high interpretability increasingly powerful transformer based models remains challenge while mechanistic interpretability aims specify neural network computations explicit pseudocode like formats existing methods often involve laborious manual analysis struggle fully elucidate learned internal algorithms recent efforts build intrinsically interpretable models have introduced considerable expressivity and optimization challenges this introduces adaptive transformer programs enhanced building upon rasp language and transformer programs create more robust and interpretable models the proposed increases expressivity redesigning two primary attention modules improve categorical and numerical reasoning capabilities overcome optimization hurdles reparameterization scheme that enhances the exploration exploitation trade off during training sorting and dyck languages and nlp benchmarks such named entity recognition and text classification that adaptive transformer programs narrow the gap between black box transformers and interpretable models enhancing transparency,2025-08-26T01:56:21.867470
42,**Fractional Neural Architectures: Continuous-Order Operators for Flexible Temporal Dependencies**,"Conventional neural networks struggle to model phenomena with complex temporal dependencies that fall between differencing (order 1) and integration (order -1). We introduce Fractional Neural Architectures (FNAs), a novel approach that incorporates fractional calculus into neural network design, enabling modeling of processes with non-integer order dynamics. Our architecture implements differentiable fractional differential operators with learnable orders, allowing the network to adaptively determine the appropriate differentiation/integration order for each feature dimension. The key innovation is a memory-efficient approximation of fractional operators using adaptive basis functions that capture long-range dependencies without requiring storage of the entire sequence history. By carefully designing initialization and regularization schemes for fractional order parameters, FNAs maintain stable training despite the complex nature of fractional operators. Experiments on time series forecasting, system identification, and anomaly detection demonstrate that FNAs outperform RNNs, Transformers, and Neural ODEs by 15-28% on processes with long-memory properties. Theoretical analysis establishes connections to power-law dynamics commonly observed in natural and economic systems. FNAs represent a fundamental advancement in neural architecture design for temporal modeling by incorporating mathematically principled fractional operators with learnable orders.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,59,Neural ODE with Differentiable Hidden State for Irregular Time Series,"Capturing the continuous underlying dynamics of irregular time series is essential for accurately reflecting the ongoing evolution and intricate correlations within the data. The discrete nature of current models, including RNN-based models and transformer variants, poses challenges when it comes to generalizing to the continuous-time data paradigms, which is necessary for capturing ongoing dynamics of irregular time series. 
Neural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis. However, integrating new information while maintaining the continuity of latent dynamics remains challenging. 
To tackle this problem, we introduce Differentiable Hidden State (DHS) enhanced neural ODE, a data-dependent framework that is capable of effectively capturing temporal dependencies and ensuring the continuity of the hidden process. We leverage the theory of generalized inverses to innovatively compute attention mechanism in reverse and obtain a continuous representation. To capture more accurate temporal relationships, we introduce Hoyer metric and maximize the sparsity of it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our model.",ICLR.cc/2025/Conference,2.0,nan,0.8237,conventional neural networks struggle phenomena complex temporal dependencies that fall between differencing order and integration order fractional neural architectures fnas that incorporates fractional calculus into neural network enabling modeling processes non integer order dynamics our implements differentiable fractional differential operators learnable orders allowing the network adaptively determine the appropriate differentiation integration order for each feature dimension experiments time series forecasting identification and anomaly detection that fnas outperform rnns transformers and neural odes processes long memory properties fnas represent fundamental advancement neural for temporal modeling incorporating mathematically principled fractional operators learnable orders,the discrete nature current models including rnn based models and transformer variants poses challenges when comes generalizing the continuous time data paradigms which necessary for capturing ongoing dynamics irregular time series neural ordinary differential equations nodes assume continuous latent dynamic and provide elegant for irregular time series analysis tackle this problem differentiable hidden state dhs enhanced neural ode data dependent that capable capturing temporal dependencies and ensuring the continuity the hidden process leverage the theory generalized inverses innovatively compute attention mechanism reverse and obtain continuous representation,2025-08-26T01:56:21.867472
43,**Neuromorphic Spiking Transformers: Energy-Efficient Attention Through Temporal Coding**,"Transformer architectures achieve remarkable performance but suffer from prohibitive energy costs, particularly in attention mechanisms. We introduce Neuromorphic Spiking Transformers (NeSTs), a novel architecture that reimagines attention through biologically-inspired temporal spike coding to dramatically improve energy efficiency. Our approach represents continuous values as precise spike timing patterns rather than floating-point activations, enabling computation through sparse, event-driven operations. The key innovation is a temporal coding scheme for attention that encodes relevance in spike timing rather than activation magnitude, allowing energy expenditure to scale with information content rather than model size. By developing specialized surrogate gradient methods and initialization techniques for spiking neurons, NeSTs overcome the non-differentiability challenges that typically hinder spiking neural networks. Experiments on language modeling, time series analysis, and sensory processing demonstrate that NeSTs maintain within 3-5% of the accuracy of standard Transformers while reducing energy consumption by 85-95% when implemented on neuromorphic hardware. Theoretical analysis provides formal guarantees on the approximation capabilities of temporal coding relative to traditional attention. NeSTs establish a new direction for neural architecture design that leverages neuromorphic principles to create extremely energy-efficient versions of attention-based architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6017,Quantized Spike-driven Transformer,"Spiking neural networks (SNNs) are emerging as a promising energy-efficient alternative to traditional artificial neural networks (ANNs) due to their spike-driven paradigm.
However, recent research in the SNN domain has mainly focused on enhancing accuracy by designing large-scale Transformer structures, which typically rely on substantial computational resources, limiting their deployment on resource-constrained devices.
To overcome this challenge, we propose a quantized spike-driven Transformer baseline (QSD-Transformer), which achieves reduced resource demands by utilizing a low bit-width parameter. 
Regrettably, the QSD-Transformer often suffers from severe performance degradation.
In this paper, we first conduct empirical analysis and find that the bimodal distribution of quantized spike-driven self-attention (Q-SDSA) leads to spike information distortion (SID) during quantization, causing significant performance degradation. To mitigate this issue, we take inspiration from mutual information entropy and propose a bi-level optimization strategy to rectify the information distribution in Q-SDSA.
Specifically, at the lower level, we introduce an information-enhanced LIF to rectify the information distribution in Q-SDSA.
At the upper level, we propose a fine-grained distillation scheme for the QSD-Transformer to align the distribution in Q-SDSA with that in the counterpart ANN.
By integrating the bi-level optimization strategy, the QSD-Transformer can attain enhanced energy efficiency without sacrificing its high-performance advantage.
We validate the QSD-Transformer on various visual tasks, and experimental results indicate that our method achieves state-of-the-art results in the SNN domain.
For instance, when compared to the prior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3\% top-1 accuracy, accompanied by significant reductions of 6.0$\times$ and 8.1$\times$ in power consumption and model size, respectively. Code is available at https://github.com/bollossom/QSD-Transformer.",ICLR.cc/2025/Conference,6.0,True,0.8448,transformer architectures achieve remarkable but suffer from prohibitive energy costs attention mechanisms neuromorphic spiking transformers nests that reimagines attention biologically inspired temporal spike coding dramatically improve energy efficiency the key innovation temporal coding scheme for attention that encodes relevance spike timing rather than activation magnitude allowing energy expenditure scale information content rather than size developing specialized surrogate gradient methods and initialization techniques for spiking neurons nests overcome the non differentiability challenges that hinder spiking neural networks experiments language modeling time series analysis and sensory processing that nests maintain within the standard transformers while reducing energy consumption when implemented neuromorphic hardware theoretical analysis provides formal guarantees the approximation capabilities temporal coding relative traditional attention nests establish direction for neural that leverages neuromorphic principles create extremely energy efficient versions attention based architectures,spiking neural networks snns are emerging promising energy efficient alternative traditional artificial neural networks anns due their spike driven paradigm however recent the snn domain has mainly focused enhancing designing large scale transformer structures which rely substantial computational resources limiting their deployment resource constrained devices overcome this challenge quantized spike driven transformer qsd transformer which achieves reduced resource demands utilizing low bit width parameter mitigate this issue take inspiration from mutual information entropy and level optimization strategy rectify the information distribution sdsa integrating the level optimization strategy the qsd transformer can attain enhanced energy efficiency sacrificing its high performance advantage the qsd transformer various visual tasks and experimental indicate that our achieves state the art the snn domain,2025-08-26T01:56:21.867477
44,**Quantum-Inspired Tensor-Train Architectures for Tractable Many-Body Neural Networks**,"Modeling high-dimensional dependencies between many variables remains a fundamental challenge in deep learning due to the exponential growth in parameters. We introduce Quantum-Inspired Tensor-Train Architectures (QITTA), a novel framework that adapts quantum many-body physics techniques to create exponentially more efficient neural representations. Our approach parameterizes high-dimensional functions using tensor networks with matrix product state structures, enabling compact representation of complex correlations between exponentially many variable configurations. The key innovation is a differentiable tensor contraction mechanism that balances expressivity and computational tractability through adaptive bond dimension selection. By incorporating specialized initialization schemes inspired by quantum state preparation and developing efficient backpropagation algorithms for tensor networks, QITTA enables practical training despite the complex parameterization. Experiments on high-dimensional density estimation, quantum many-body simulation, and combinatorial optimization demonstrate that QITTA achieves comparable or superior performance to standard architectures while using exponentially fewer parameters (O(nd²) vs O(n^d)). Theoretical analysis establishes connections to quantum entanglement entropy and provides expressivity bounds based on tensor rank. QITTA represents a fundamental advancement in neural architecture design for high-dimensional problems by adapting quantum-inspired tensor methods to create tractable models for many-body dependencies.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,926,A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning,"Quantum-centric supercomputing presents a compelling framework for large-scale hybrid quantum-classical tasks. Although quantum machine learning (QML) offers theoretical benefits in various applications, challenges such as large-size data encoding in the input stage and the reliance on quantum resources in the inference stage limit its practicality for tasks like fine-tuning large language models (LLMs). Quantum parameter generation, a novel approach of QML, addresses these limitations by using quantum neural networks (QNNs) to generate classical model weights (parameters) exclusively during training, thereby decoupling inference from quantum hardware. In this work, we introduce Quantum Parameter Adaptation (QPA) in the framework of quantum parameter generation, which integrates QNNs with a classical multi-layer perceptron mapping model to generate parameters for fine-tuning methods. Using Gemma-2 and GPT-2 as case studies, QPA demonstrates significant parameter reduction for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), while maintaining comparable or improved performance in text generation tasks. Specifically, QPA reduces the number of  parameters to $52.06\%$ of the original LoRA for GPT-2 with a slight performance gain of $0.75\%$, and to $16.84\%$ for Gemma-2, with a marginal performance improvement of $0.07\%$. These results highlight QPA’s ability to achieve efficient parameter reduction without sacrificing performance in the quantum parameter generation framework. This work showcases the potential of quantum-enhanced parameter reduction, offering a scalable quantum-classical solution for fine-tuning LLMs while preserving the feasibility of inference on classical hardware.",ICLR.cc/2025/Conference,6.0,True,0.8327,modeling high dimensional dependencies between many variables remains fundamental challenge deep learning due the exponential growth parameters quantum inspired tensor train architectures qitta that adapts quantum many body physics techniques create exponentially more efficient neural representations our parameterizes high dimensional functions tensor networks matrix product state structures enabling compact representation complex correlations between exponentially many variable configurations experiments high dimensional density estimation quantum many body simulation and combinatorial optimization that qitta achieves comparable superior standard architectures while exponentially fewer parameters nd² qitta represents fundamental advancement neural for high dimensional problems adapting quantum inspired tensor methods create tractable models for many body dependencies,although quantum machine learning qml offers theoretical benefits various applications challenges such large size data encoding the input stage and the reliance quantum resources the inference stage limit its practicality for tasks like fine tuning large language models llms quantum parameter generation qml addresses these limitations quantum neural networks qnns generate classical weights parameters exclusively during training thereby decoupling inference from quantum hardware this quantum parameter adaptation qpa the quantum parameter generation which integrates qnns classical multi layer perceptron mapping generate parameters for fine tuning methods gemma and gpt case studies qpa demonstrates significant parameter reduction for parameter efficient fine tuning methods such low rank adaptation lora while maintaining comparable improved text generation tasks these highlight qpa ability achieve efficient parameter reduction sacrificing the quantum parameter generation,2025-08-26T01:56:21.867482
45,**Progressive Pruning-and-Growing: Neural Architecture Optimization with Structural Exploration**,"Neural architecture search has enabled significant advances but typically explores fixed search spaces with limited structural flexibility. We introduce Progressive Pruning-and-Growing (PPG), a novel optimization framework that dynamically explores architectural structures by iteratively removing underperforming components while introducing new structural variations. Our approach combines principled pruning based on contribution analysis with guided structural exploration using a learned meta-controller. The key innovation is a bi-level optimization scheme that jointly optimizes architecture topology and parameters while maintaining differentiability throughout the search process. By formulating architecture evolution as a differentiable Markov decision process, PPG efficiently navigates the vast design space without requiring separate search and training phases. Extensive evaluations on image classification, natural language processing, and reinforcement learning demonstrate that PPG discovers architectures that outperform both human-designed networks and previous NAS methods by 2.5-4.1% while reducing parameter counts by 30-45%. Notably, our method discovers unconventional architectural motifs, such as heterogeneous skip connections and adaptive pathway selection, that are rarely explored in manual design. PPG establishes a new paradigm for neural architecture optimization that transcends fixed search spaces through dynamic structural exploration.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1316,POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator,"Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.
Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.
This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.
Results were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.",ICLR.cc/2025/Conference,4.4,False,0.8530,neural search has enabled significant advances but explores fixed search spaces limited structural flexibility progressive pruning and growing ppg optimization that dynamically explores architectural structures iteratively removing underperforming components while introducing structural variations the key innovation level optimization scheme that jointly optimizes topology and parameters while maintaining differentiability throughout the search process extensive evaluations image classification natural language processing and reinforcement learning that ppg discovers architectures that outperform both human designed networks and previous nas methods while reducing parameter counts ppg establishes paradigm for neural optimization that transcends fixed search spaces dynamic structural exploration,neural search nas automates the neural network architectures minimising dependence human expertise and iterative experimentation recently transferable neural search transferable nas has emerged generalising the search process from being dataset dependent task dependent this diffusion based streamlines computation generating architectures optimised for unseen datasets the need for further adaptation this introduces the pareto optimal many objective neural generator pomonag extending diffusionnag many objective diffusion process pomonag optimisation enhanced expanding its training meta dataset applying pareto front filtering generated architectures and refining embeddings for conditional generation were validated two distinct search spaces nasbench201 and mobilenetv3 and evaluated across image classification datasets,2025-08-26T01:56:21.867484
46,**Emergent Communication Transformers: Learning Latent Communication Channels Between Network Components**,"Traditional neural network architectures employ fixed communication pathways between components, limiting their ability to dynamically route information based on input characteristics. We present Emergent Communication Transformers (ECTs), a novel architecture where neural modules learn to communicate through latent communication channels that emerge during training. Our approach introduces learnable communication protocols between network components, enabling adaptive information exchange through a specialized attention mechanism. The key innovation is a differentiable communication layer that allows modules to selectively transmit information to relevant counterparts while suppressing irrelevant signals. By implementing a competitive bottleneck on communication bandwidth, ECTs are incentivized to develop efficient, specialized protocols for different input types. Experiments across vision, language, and multi-modal tasks demonstrate that ECTs consistently outperform standard transformers by 5-8% in sample efficiency and 2-4% in final performance, with particularly strong improvements on compositional generalization tasks. Analysis of the emergent communication patterns reveals interpretable specialization of different channels for specific semantic concepts. This work establishes a new direction in neural architecture design where inter-component communication emerges as a learned behavior rather than being fixed by architecture design.",ICLR,neural architectures,claude-3-7-sonnet-latest,False,,A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language,"Increase in data, size, or compute can lead to sudden learning of specific capabilities by a neural network---a phenomenon often called ""emergence"". Beyond scientific understanding, establishing the causal factors underlying such emergent capabilities is crucial to enable risk regulation frameworks for AI. In this work, we seek inspiration from study of emergent properties in other fields and propose a phenomenological definition for the concept in the context of neural networks. Our definition implicates the acquisition of general regularities underlying the data-generating process as a cause of sudden performance growth for specific, narrower tasks. We empirically investigate this definition by proposing an experimental system grounded in a context-sensitive formal language, and find that Transformers trained to perform tasks on top of strings from this language indeed exhibit emergent capabilities. Specifically, we show that once the language's underlying grammar and context-sensitivity inducing regularities are learned by the model, performance on narrower tasks suddenly begins to improve. We then analogize our network's learning dynamics with the process of percolation on a bipartite graph, establishing a formal phase transition model that predicts the shift in the point of emergence observed in our experiments when intervening on the data regularities. Overall, our experimental and theoretical frameworks yield a step towards better defining, characterizing, and predicting emergence in neural networks.",ICLR.cc/2025/Conference,7.0,True,0.7993,traditional neural network architectures employ fixed communication pathways between components limiting their ability dynamically route information input characteristics emergent communication transformers ects where neural modules learn communicate latent communication channels that emerge during training our introduces learnable communication protocols between network components enabling adaptive information exchange specialized attention mechanism analysis the emergent communication patterns reveals interpretable specialization different channels for specific semantic concepts this establishes direction neural where inter component communication emerges learned behavior rather than being fixed,increase data size compute can lead sudden learning specific capabilities neural network phenomenon often called emergence this seek inspiration from emergent properties other fields and phenomenological definition for the concept the context neural networks empirically this definition proposing experimental grounded context sensitive formal language and find that transformers trained perform tasks top strings from this language indeed exhibit emergent capabilities then analogize our network learning dynamics the process percolation bipartite graph establishing formal phase transition that predicts the shift the point emergence observed our experiments when intervening the data regularities overall our experimental and theoretical frameworks yield step towards better defining characterizing and predicting emergence neural networks,2025-08-26T01:56:21.867488
47,**Geometric Deep Learning with Fiber Bundles: Equivariant Networks for Non-Euclidean Domains**,"Incorporating geometric structure into neural architectures remains challenging for non-Euclidean domains with complex symmetries. We introduce Fiber Bundle Networks (FBNets), a principled geometric deep learning framework that generalizes equivariant architectures to arbitrary manifolds and symmetry groups. Our approach represents features as sections of fiber bundles over the input manifold, with learnable operations that respect the underlying geometric structure through covariant derivatives and group convolutions. The key innovation is a general mathematical formulation that unifies previously disparate approaches to geometric deep learning within a single cohesive framework based on differential geometry. By developing specialized numerical techniques for parallel transport and connection forms, FBNets maintain computational tractability despite their mathematical sophistication. Experiments on spherical image recognition, molecular property prediction, and geometric shape analysis demonstrate that FBNets outperform existing approaches by 10-18% while providing theoretical guarantees on equivariance properties. Importantly, our method generalizes naturally to manifolds and symmetry groups not seen during training. FBNets establish a comprehensive mathematical foundation for designing neural architectures that inherently respect geometric structure, enabling more sample-efficient learning for problems with known symmetries and invariances.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3885,Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing,"In numerous applications of geometric deep learning, the studied systems exhibit spatial symmetries and it is desirable to enforce these. For the symmetry of global rotations and reflections, this means that the model should be equivariant with respect to the transformations that form the group of $\mathrm O(d)$.
While many approaches for equivariant message passing require specialized architectures, including non-standard normalization layers or non-linearities, we here present a framework based on local reference frames (""local canonicalization"") which can be integrated with any architecture without restrictions.
We enhance equivariant message passing based on local canonicalization by introducing tensorial messages to communicate geometric information consistently between different local coordinate frames.
Our framework applies to message passing on geometric data in Euclidean spaces of arbitrary dimension.
We explicitly show how our approach can be adapted to make a popular existing point cloud architecture equivariant. We demonstrate the superiority of tensorial messages and achieve state-of-the-art results on normal vector regression and competitive results on other standard 3D point cloud tasks.",ICLR.cc/2025/Conference,6.333333333333333,True,0.8416,incorporating geometric structure into neural architectures remains challenging for non euclidean domains complex symmetries fiber bundle networks fbnets principled geometric deep learning that generalizes equivariant architectures arbitrary manifolds and symmetry groups the key innovation general mathematical formulation that unifies previously disparate approaches geometric deep learning within single cohesive differential geometry fbnets establish comprehensive mathematical foundation for designing neural architectures that inherently respect geometric structure enabling more sample efficient learning for problems known symmetries and invariances,numerous applications geometric deep learning the studied systems exhibit spatial symmetries and desirable enforce these,2025-08-26T01:56:21.867490
48,**Neural State Machines with Learnable Transition Dynamics: Bridging Symbolic and Distributed Representations**,"Neural networks excel at pattern recognition but struggle with explicit reasoning and symbolic manipulation. We introduce Neural State Machines (NSMs), a novel architecture that combines distributed neural representations with discrete state-transition dynamics inspired by finite automata. Our approach represents computation as transitions between learnable neural states governed by differentiable transition functions, enabling both pattern recognition and rule-based processing within a unified framework. The key innovation is a temperature-controlled discretization mechanism that allows backpropagation through state transitions while encouraging convergence to discrete attractor states. By incorporating inductive biases from automata theory within a fully differentiable architecture, NSMs learn explicit computational rules while maintaining the flexibility of neural networks. Experiments on algorithmic reasoning, program induction, and compositional generalization demonstrate that NSMs outperform standard architectures by 15-25% on tasks requiring systematic generalization, while maintaining competitive performance on pattern recognition. Analysis reveals that NSMs discover interpretable state spaces with clear functional specialization. This work establishes a new hybrid architecture paradigm that bridges connectionist and symbolic approaches through differentiable state machines with learnable transition dynamics.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3053,Discrete Neural Algorithmic Reasoning,"Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classic algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classic computations are not affected by distributional shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. To achieve that, we separate discrete and continuous data flows and describe the interaction between them. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on multiple algorithmic problems and get perfect test scores both in single-task and multitask setups. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.",ICLR.cc/2025/Conference,5.4,False,0.8375,neural networks excel pattern recognition but struggle explicit reasoning and symbolic manipulation neural state machines nsms that combines distributed neural representations discrete state transition dynamics inspired finite automata our represents computation transitions between learnable neural states governed differentiable transition functions enabling both pattern recognition and rule based processing within unified incorporating inductive biases from automata theory within fully differentiable nsms learn explicit computational rules while maintaining the flexibility neural networks experiments algorithmic reasoning program induction and compositional generalization that nsms outperform standard architectures tasks requiring systematic generalization while maintaining competitive pattern recognition,neural algorithmic reasoning aims capture computations neural networks learning the models imitate the execution classic algorithms while common architectures are expressive enough contain the correct the weights space current neural reasoners are struggling generalize well out distribution data this force neural reasoners maintain the execution trajectory combination finite predefined states,2025-08-26T01:56:21.867493
49,**Hyperbolic Attention Networks: Learning Hierarchical Representations in Non-Euclidean Space**,"Traditional attention mechanisms operate in Euclidean space, limiting their ability to efficiently model hierarchical relationships. We present Hyperbolic Attention Networks (HANs), a novel architecture that reformulates attention mechanisms in hyperbolic space to better capture hierarchical structures inherent in many datasets. Our approach represents tokens and their relationships in negatively curved Poincaré or Lorentz models, enabling exponentially more efficient embedding of tree-like structures. The key innovation is a mathematically rigorous formulation of attention operations that respect hyperbolic geometry, including hyperbolic distance-based compatibility functions and Möbius transformations for value aggregation. By carefully designing manifold-aware operations and developing stable optimization techniques for Riemannian manifolds, HANs overcome the numerical challenges typically associated with hyperbolic deep learning. Experiments on hierarchical text classification, knowledge graph reasoning, and parse tree modeling demonstrate that HANs consistently outperform Euclidean attention by 7-12% while using substantially lower embedding dimensions. Visualization of the learned attention patterns reveals that HANs naturally discover hierarchical structures without explicit supervision. This work establishes a principled framework for attention mechanisms in hyperbolic space, enabling more parameter-efficient modeling of data with inherent hierarchical organization.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3887,Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces,"Learning in hyperbolic spaces has gained increasing attention due to the superior capability of modeling hierarchical structures. Existing hyperbolic learning methods use a fixed distance measure that assumes a uniform hierarchical structure across all data points. However, this assumption does not always hold in real-world scenarios, considering the diversity of the hierarchical structures of data. This work proposes to learn geometry aware distance measures that dynamically adjust to accommodate diverse hierarchical structures in hyperbolic spaces. We derive geometry aware distance measures by generating projections and curvatures for each pair of samples, which maps each pair to a suitable hyperbolic space. We introduce a revised low-rank decomposition scheme and a hard-pair mining mechanism to reduce the computational cost incurred by the pairwise generation without compromising accuracy. Moreover, we derive an upper bound of the low-rank approximation error via Talagrand concentration inequality to guarantee the effectiveness of our low-rank decomposition scheme. Theoretical analysis and experiments on standard image classification and few-shot learning tasks affirm the effectiveness of our method in refining hyperbolic learning through our geometry aware distance measures.",ICLR.cc/2025/Conference,4.75,nan,0.8589,traditional attention mechanisms operate euclidean space limiting their ability hierarchical relationships hyperbolic attention networks hans that reformulates attention mechanisms hyperbolic space better capture hierarchical structures inherent many datasets our represents tokens and their relationships negatively curved poincaré lorentz models enabling exponentially more efficient embedding tree like structures the key innovation mathematically rigorous formulation attention operations that respect hyperbolic geometry including hyperbolic distance based compatibility functions and möbius transformations for value aggregation carefully designing manifold aware operations and developing stable optimization techniques for riemannian manifolds hans overcome the numerical challenges associated hyperbolic deep learning experiments hierarchical text classification knowledge graph reasoning and parse tree modeling that hans consistently outperform euclidean attention while lower embedding dimensions visualization the learned attention patterns reveals that hans naturally discover hierarchical structures explicit supervision this establishes principled for attention mechanisms hyperbolic space enabling more parameter efficient modeling data inherent hierarchical organization,learning hyperbolic spaces has gained increasing attention due the superior capability modeling hierarchical structures existing hyperbolic learning methods use fixed distance measure that assumes uniform hierarchical structure across all data points revised low rank decomposition scheme and hard pair mining mechanism reduce the computational cost incurred the pairwise generation compromising theoretical analysis and experiments standard image classification and few shot learning tasks affirm the effectiveness our refining hyperbolic learning our geometry aware distance measures,2025-08-26T01:56:21.867495
50,**Spatio-Temporal Graph Transformers: Unified Message Passing with Global Attention**,"Modeling complex systems with both spatial and temporal dynamics remains challenging for existing architectures. We introduce Spatio-Temporal Graph Transformers (STGTs), a unified architecture that seamlessly integrates local graph message passing with global attention mechanisms across both spatial and temporal dimensions. Our approach represents dynamic systems as evolving graphs where nodes exchange information through a novel dual-attention mechanism that captures both spatial locality and temporal dependencies. The key innovation is a hierarchical attention scheme that dynamically determines the relative importance of spatial versus temporal information for each node at different time scales. By developing specialized positional encodings for spatio-temporal graphs and incorporating physically-motivated inductive biases, STGTs effectively balance computational efficiency with expressive power. Experiments on traffic forecasting, physical simulation, and multi-agent systems demonstrate that STGTs outperform both pure GNNs and Transformers by 12-20% in prediction accuracy while scaling more favorably to large systems. Analysis reveals that STGTs automatically learn to focus on spatially distant but temporally correlated events crucial for accurate prediction. This work establishes a new architectural paradigm for modeling complex spatio-temporal systems through the integration of graph-structured inductive biases with global attention mechanisms.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6270,Spatial-temporal Graph Attention Network for Forex Forecasting with Hierarchical Transformer,"The foreign exchange market, with its daily trading volume reaching nearly trillions of dollars, presents significant opportunities for the application of advanced predictive analytics. Traditional exchange rate forecasting methods often overlook the interdependencies between currencies and struggle with long-range data dependencies, leading to challenges in capturing the true market dynamics. To overcome these limitations, this paper introduces a novel Spatial-Temporal Graph Attention Network with Hierarchical Transformer (STGAT). Our model innovatively combines spatial graph convolutions with a dual-view temporal transformer-based mechanism, utilizing a Temporal Linearity Graph Attention Network (TLGAT) to account for currency relations in a time-sensitive manner. By integrating a linear attention mechanism for enhanced efficiency and capturing both local and global sequential data embeddings, STGAT provides a framework based on a hierarchical transformer for predicting exchange rates. We validate our approach on exchange rates of seventeen currencies over 2,092 trading days, demonstrating superior performance compared to state-of-the-art models.",ICLR.cc/2025/Conference,3.0,False,0.9001,spatio temporal graph transformers stgts unified that seamlessly integrates local graph message passing global attention mechanisms across both spatial and temporal dimensions the key innovation hierarchical attention scheme that dynamically determines the relative importance spatial versus temporal information for each node different time scales experiments traffic forecasting physical simulation and multi agent systems that stgts outperform both pure gnns and transformers prediction while scaling more favorably large systems analysis reveals that stgts automatically learn focus spatially distant but temporally correlated events crucial for accurate prediction this establishes architectural paradigm for modeling complex spatio temporal systems the integration graph structured inductive biases global attention mechanisms,overcome these limitations this introduces spatial temporal graph attention network hierarchical transformer stgat our innovatively combines spatial graph convolutions dual view temporal transformer based mechanism utilizing temporal linearity graph attention network tlgat account for currency relations time sensitive manner integrating linear attention mechanism for enhanced efficiency and capturing both local and global sequential data embeddings stgat provides hierarchical transformer for predicting exchange rates,2025-08-26T01:56:21.867501
51,**Neural Population Coding Networks: Distributed Representation Learning with Functional Specialization**,"Current neural architectures typically rely on homogeneous computational units, contrasting with the functional specialization observed in biological neural networks. We introduce Neural Population Coding Networks (NPCNs), a novel architecture inspired by neuroscience principles of distributed, heterogeneous population coding. Our approach organizes computation into functionally specialized neural populations that collectively represent information through distributed activation patterns rather than individual neurons. The key innovation is a differentiable population-level activation mechanism where information is encoded in the statistical moments of activation distributions rather than individual unit responses. By implementing lateral competition and homeostatic plasticity mechanisms, NPCNs naturally develop specialized feature detectors that form the basis for robust distributed representations. Experiments on sensory processing, adversarial robustness, and transfer learning demonstrate that NPCNs outperform standard architectures by 8-15% in robustness to noise and adversarial attacks while maintaining competitive accuracy on clean data. Notably, our population-based approach enables graceful degradation under partial network damage rather than catastrophic failure. This work establishes a new biologically-inspired paradigm for neural architecture design based on heterogeneous population coding rather than homogeneous layers, offering insights into more robust and adaptive computational systems.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2259,Beyond single neurons: population response geometry in digital twins of mouse visual cortex,"Hierarchical visual processing  is essential for cognitive functions like object recognition and spatial localization. Traditional studies of the neural basis of these computations have focused on single-neuron activity, but recent advances in large-scale neural recordings emphasize the growing need to understand computations at the population level. Digital twins-computational models trained on neural data-have successfully replicated single-neuron behavior, but their effectiveness in capturing the joint activity of neurons remains unclear. In this study, we investigate how well digital twins describe population responses in  mouse visual cortex. We show that these models fail to accurately represent the geometry of  population activity, particularly its differentiability and how this geometry evolves across the visual hierarchy. To address this, we explore how dataset, network architecture, loss function, and training method affect the ability of digital twins to recapitulate population properties. We demonstrate that improving model alignment with experiments requires training strategies that enhance robustness and generalization, reflecting principles observed in biological systems. These findings underscore the need to evaluate digital twins from multiple perspectives, identify key areas for refinement, and establish a foundation for using these models to explore neural computations at the population level.",ICLR.cc/2025/Conference,6.333333333333333,True,0.8232,current neural architectures rely homogeneous computational units contrasting the functional specialization observed biological neural networks neural population coding networks npcns inspired neuroscience principles distributed heterogeneous population coding our organizes computation into functionally specialized neural populations that collectively represent information distributed activation patterns rather than individual neurons implementing lateral competition and homeostatic plasticity mechanisms npcns naturally specialized feature detectors that form the basis for robust distributed representations experiments sensory processing adversarial robustness and transfer learning that npcns outperform standard architectures robustness noise and adversarial attacks while maintaining competitive clean data notably our population based enables graceful degradation under partial network damage rather than catastrophic failure this establishes biologically inspired paradigm for neural heterogeneous population coding rather than homogeneous layers offering insights into more robust and adaptive computational systems,hierarchical visual processing essential for cognitive functions like object recognition and spatial localization traditional studies the neural basis these computations have focused single neuron activity but recent advances large scale neural recordings emphasize the growing need understand computations the population level digital twins computational models trained neural data have replicated single neuron behavior but their effectiveness capturing the joint activity neurons remains unclear address this how network loss function and training affect the ability digital twins recapitulate population properties that improving alignment experiments requires training strategies that enhance robustness and generalization reflecting principles observed biological systems these underscore the need digital twins from multiple perspectives identify key areas for refinement and establish foundation for these models neural computations the population level,2025-08-26T01:56:21.867503
52,**Learnable Activation Functions through Functional Optimization in Reproducing Kernel Hilbert Spaces**,"Activation function design significantly impacts neural network performance, yet most approaches rely on fixed functions or simple parameterizations. We introduce Kernel-based Learnable Activation Functions (KLAFs), a novel framework that learns optimal activation functions directly from data through functional optimization in reproducing kernel Hilbert spaces. Our approach represents activation functions as expansions in a carefully chosen kernel basis, enabling the discovery of complex, non-parametric functions while maintaining efficient optimization. The key innovation is a regularized functional optimization procedure that balances expressivity with generalization through appropriate RKHS norm constraints. By developing layer-specific functional gradient descent algorithms and a specialized initialization scheme, KLAFs overcome the challenges of joint optimization of network weights and activation functions. Experiments across image classification, reinforcement learning, and signal processing demonstrate that KLAFs consistently outperform fixed and parametric activation functions by 3-7% while providing insights into task-specific nonlinearities. Analysis of the learned functions reveals that KLAFs automatically discover complex activation patterns, including multi-modal and context-dependent behaviors that aren't captured by standard functions. This work establishes a principled, kernel-based framework for learning optimal activation functions through functional optimization rather than restricted parameterizations.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10619,Hadamard Representations: Augmenting Hyperbolic Tangents in RL,"Activation functions are one of the key components of a deep neural network. The most commonly used activation functions can be classed into the category of continuously differentiable (e.g. tanh) and piece-wise linear functions (e.g. ReLU), both having their own strengths and drawbacks with respect to downstream performance and representation capacity through learning (e.g. measured by the number of dead neurons and the effective rank). In reinforcement learning, the performance of continuously differentiable activations often falls short as compared to piece-wise linear functions. We provide insights into the vanishing gradients associated with the former, and show that the dying neuron problem is not exclusive to ReLU's. To alleviate vanishing gradients and the resulting dying neuron problem occurring with continuously differentiable activations, we propose a Hadamard representation. Using deep Q-networks, proximal policy optimization and parallelized Q-networks in the Atari domain, we show faster learning, a reduction in dead neurons and increased effective rank.",ICLR.cc/2025/Conference,4.75,False,0.8529,activation function impacts neural network yet most approaches rely fixed functions simple parameterizations kernel based learnable activation functions klafs that learns optimal activation functions directly from data functional optimization reproducing kernel hilbert spaces our represents activation functions expansions carefully chosen kernel basis enabling the discovery complex non parametric functions while maintaining efficient optimization the key innovation regularized functional optimization procedure that balances expressivity generalization appropriate rkhs norm constraints developing layer specific functional gradient descent algorithms and specialized initialization scheme klafs overcome the challenges joint optimization network weights and activation functions experiments across image classification reinforcement learning and signal processing that klafs consistently outperform fixed and parametric activation functions while providing insights into task specific nonlinearities this establishes principled kernel based for learning optimal activation functions functional optimization rather than restricted parameterizations,activation functions are one the key components deep neural network relu both having their own strengths and drawbacks respect downstream and representation capacity learning reinforcement learning the continuously differentiable activations often falls short compared piece wise linear functions alleviate vanishing gradients and the resulting dying neuron problem occurring continuously differentiable activations hadamard representation deep networks proximal policy optimization and parallelized networks the atari domain faster learning reduction dead neurons and increased effective rank,2025-08-26T01:56:21.867506
53,**Neural Spectral Methods: Learning PDEs through Differentiable Spectral Solvers**,"Deep learning approaches for physical systems typically rely on spatial discretization, introducing resolution limitations and failing to respect underlying mathematical structures. We present Neural Spectral Methods (NSMs), a novel architecture that incorporates differentiable spectral solvers as architectural components for modeling physical phenomena governed by partial differential equations. Our approach represents solutions in the spectral domain using learned basis functions, enabling resolution-independent processing and naturally incorporating physical constraints. The key innovation is a differentiable spectral projection layer that maps between spatial and spectral representations while preserving the mathematical structure of the underlying PDEs. By developing specialized techniques for differentiating through spectral solvers and incorporating physics-informed regularization, NSMs effectively combine data-driven learning with mathematical rigor. Experiments on fluid dynamics, electromagnetic field prediction, and quantum mechanics demonstrate that NSMs achieve 20-35% lower prediction error than spatial discretization approaches while generalizing better to unseen boundary conditions and domain geometries. Notably, our method maintains consistent accuracy across different resolution scales due to its spectral formulation. This work establishes a new paradigm for scientific machine learning that bridges deep learning and classical spectral methods through differentiable spectral solvers embedded within neural architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10003,P-Align: Self-Alignment in Physical Dynamical System Modeling,"Deep learning has emerged as the new paradigm in modeling complex physical dynamical systems. Nevertheless, data-driven methods learn patterns by optimizing statistical metrics, tend to overlook the adherence to physical laws. Previous work have attempted to incorporate physical constraints into neural networks, but they often face limitations due to lack of flexibility or optimization challenges. In this paper, we propose a novel framework, Physics-aware Self-Alignment (P-Align), to enhance the physical consistency of dynamical systems modeling.  P-Align enables dynamical system models to provides physics-aware rewards, which makes self-alignment of dynamical system models possible. Comprehensive experiments show that \method{} not only gave an average statistical skill score boost of more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics. All of our source codes will be released via GitHub.",ICLR.cc/2025/Conference,4.25,nan,0.8307,deep learning approaches for physical systems rely spatial discretization introducing resolution limitations and failing respect underlying mathematical structures neural spectral methods nsms that incorporates differentiable spectral solvers architectural components for modeling physical phenomena governed partial differential equations our represents solutions the spectral domain learned basis functions enabling resolution independent processing and naturally incorporating physical constraints developing specialized techniques for differentiating spectral solvers and incorporating physics informed regularization nsms combine data driven learning mathematical rigor experiments fluid dynamics electromagnetic field prediction and quantum mechanics that nsms achieve lower prediction error than spatial discretization approaches while generalizing better unseen boundary conditions and domain geometries this establishes paradigm for scientific machine learning that bridges deep learning and classical spectral methods differentiable spectral solvers embedded within neural architectures,deep learning has emerged the paradigm modeling complex physical dynamical systems previous have attempted incorporate physical constraints into neural networks but they often face limitations due lack flexibility optimization challenges,2025-08-26T01:56:21.867513
54,**Dynamic Neural Fields: Continuous-Space Architectures with Emergent Attractor Dynamics**,"Existing neural networks struggle to model continuous dynamical systems and emergent pattern formation. We introduce Dynamic Neural Fields (DNFs), a novel architecture inspired by neural field theory that represents computation as continuous spatiotemporal fields rather than discrete layers. Our approach implements neural computation through learnable integro-differential equations that govern the evolution of activation fields, enabling the emergence of complex dynamical behaviors such as traveling waves, attractor states, and pattern formation. The key innovation is a differentiable neural field layer that approximates the continuous field equations while maintaining tractable computation through adaptive discretization. By incorporating biologically-motivated lateral interaction kernels and carefully designed stability constraints, DNFs learn stable dynamical systems with controllable attractor properties. Experiments on spatiotemporal sequence prediction, stable memory formation, and continuous control demonstrate that DNFs outperform RNNs and Neural ODEs by 15-25% on tasks requiring complex attractor dynamics or pattern formation. Theoretical analysis establishes connections to dynamical systems theory and provides guarantees on the existence and stability of learned attractors. This work introduces a fundamentally new architecture paradigm based on continuous neural fields rather than discrete units, enabling more natural modeling of systems with spatiotemporal dynamics and emergent pattern formation.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,59,Neural ODE with Differentiable Hidden State for Irregular Time Series,"Capturing the continuous underlying dynamics of irregular time series is essential for accurately reflecting the ongoing evolution and intricate correlations within the data. The discrete nature of current models, including RNN-based models and transformer variants, poses challenges when it comes to generalizing to the continuous-time data paradigms, which is necessary for capturing ongoing dynamics of irregular time series. 
Neural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis. However, integrating new information while maintaining the continuity of latent dynamics remains challenging. 
To tackle this problem, we introduce Differentiable Hidden State (DHS) enhanced neural ODE, a data-dependent framework that is capable of effectively capturing temporal dependencies and ensuring the continuity of the hidden process. We leverage the theory of generalized inverses to innovatively compute attention mechanism in reverse and obtain a continuous representation. To capture more accurate temporal relationships, we introduce Hoyer metric and maximize the sparsity of it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our model.",ICLR.cc/2025/Conference,2.0,nan,0.8367,existing neural networks struggle continuous dynamical systems and emergent pattern formation dynamic neural fields dnfs inspired neural field theory that represents computation continuous spatiotemporal fields rather than discrete layers our implements neural computation learnable integro differential equations that govern the evolution activation fields enabling the emergence complex dynamical behaviors such traveling waves attractor states and pattern formation the key innovation differentiable neural field layer that approximates the continuous field equations while maintaining tractable computation adaptive discretization experiments spatiotemporal sequence prediction stable memory formation and continuous control that dnfs outperform rnns and neural odes tasks requiring complex attractor dynamics pattern formation this introduces fundamentally paradigm continuous neural fields rather than discrete units enabling more natural modeling systems spatiotemporal dynamics and emergent pattern formation,the discrete nature current models including rnn based models and transformer variants poses challenges when comes generalizing the continuous time data paradigms which necessary for capturing ongoing dynamics irregular time series neural ordinary differential equations nodes assume continuous latent dynamic and provide elegant for irregular time series analysis tackle this problem differentiable hidden state dhs enhanced neural ode data dependent that capable capturing temporal dependencies and ensuring the continuity the hidden process leverage the theory generalized inverses innovatively compute attention mechanism reverse and obtain continuous representation,2025-08-26T01:56:21.867514
55,**Tensor Eigendecomposition Networks: Learning Higher-Order Feature Interactions through Spectral Methods**,"Standard neural networks primarily capture pairwise feature interactions, limiting their ability to model complex higher-order relationships. We introduce Tensor Eigendecomposition Networks (TENets), a novel architecture that explicitly models higher-order feature interactions through learnable tensor spectral decompositions. Our approach represents high-order relationships using CP and Tucker decompositions of interaction tensors, enabling efficient parameterization of complex cross-feature dependencies. The key innovation is a differentiable tensor eigendecomposition layer that projects input features onto learned eigenspaces corresponding to significant interaction modes. By carefully designing the rank allocation across different interaction orders and developing specialized initialization schemes, TENets overcome the computational and optimization challenges typically associated with high-order tensor methods. Experiments on recommendation systems, polynomial function approximation, and scientific computing demonstrate that TENets outperform standard architectures by 10-18% on tasks with inherent higher-order relationships while using fewer parameters. Analysis of the learned decompositions provides interpretable insights into the discovered feature interaction patterns. This work establishes a new tensor-spectral approach to neural architecture design that explicitly captures higher-order feature relationships through principled tensor decompositions rather than relying on deep compositions of pairwise interactions.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3717,E(n) Equivariant Topological Neural Networks,"Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features. Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue. TDL enables the principled modeling of arbitrary multi-way, hierarchical higher-order interactions by operating on combinatorial topological spaces, such as simplicial or cell complexes, instead of graphs. However, little is known about how to leverage geometric features such as positions and velocities for TDL. This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial complexes, formal objects unifying graphs, hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric node features while respecting rotation, reflection, and translation equivariance. Moreover, being TDL models, ETNNs are natively ready for settings with heterogeneous interactions.  We provide a theoretical analysis to show the improved expressiveness of ETNNs over architectures for geometric graphs. We also show how E(n)-equivariant variants of TDL models can be directly derived from our framework. The broad applicability of ETNNs is demonstrated through two tasks of vastly different scales: i) molecular property prediction on the QM9 benchmark and ii) land-use regression for hyper-local estimation of air pollution with multi-resolution irregular geospatial data.  The results indicate that ETNNs are an effective tool for learning from diverse types of richly structured data, as they match or surpass SotA equivariant TDL models with a significantly smaller computational burden, thus highlighting the benefits of a principled geometric inductive bias. Our implementation of ETNNs can be found at https://github.com/NSAPH-Projects/topological-equivariant-networks.",ICLR.cc/2025/Conference,6.0,True,0.8178,standard neural networks primarily capture pairwise feature interactions limiting their ability complex higher order relationships tensor eigendecomposition networks tenets that explicitly models higher order feature interactions learnable tensor spectral decompositions carefully designing the rank allocation across different interaction orders and developing specialized initialization schemes tenets overcome the computational and optimization challenges associated high order tensor methods analysis the learned decompositions provides interpretable insights into the discovered feature interaction patterns this establishes tensor spectral neural that explicitly captures higher order feature relationships principled tensor decompositions rather than relying deep compositions pairwise interactions,graph neural networks excel modeling pairwise interactions but they cannot flexibly accommodate higher order interactions and features topological deep learning tdl has emerged recently promising tool for addressing this issue this introduces equivariant topological neural networks etnns which are equivariant message passing networks operating combinatorial complexes formal objects unifying graphs hypergraphs simplicial path and cell complexes the broad applicability etnns demonstrated two tasks vastly different scales molecular property prediction the qm9 and land use regression for hyper local estimation air pollution multi resolution irregular geospatial data the indicate that etnns are effective tool for learning from diverse types richly structured data they match surpass sota equivariant tdl models smaller computational burden thus highlighting the benefits principled geometric inductive bias,2025-08-26T01:56:21.867520
56,**Causal Discovery Networks: Learning Directed Graphical Models with Interventional Training**,"Standard neural networks excel at prediction but struggle to capture causal relationships crucial for robust generalization and intervention. We present Causal Discovery Networks (CDNs), a novel architecture that explicitly learns causal graphical models from observational and interventional data. Our approach embeds a differentiable directed acyclic graph (DAG) structure within the computational graph, enabling joint optimization of causal structure and functional relationships. The key innovation is an interventional training framework that alternates between observational updates and targeted interventional queries to disambiguate correlation from causation. By incorporating specialized regularizers that enforce acyclicity and sparsity constraints, CDNs learn compact causal models that support counterfactual reasoning. Experiments on synthetic causal systems, gene regulatory networks, and economic time-series demonstrate that CDNs outperform both standard predictive models and classical causal discovery algorithms by 15-25% in identifying true causal relationships. Importantly, our architecture demonstrates superior generalization under distribution shifts that preserve causal mechanisms. This work establishes a new approach to neural architecture design focused on causal representation learning rather than purely predictive modeling, enabling more robust generalization and supporting interventional queries beyond the training distribution.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7158,Gradient based Causal Discovery with Diffusion Model,"Causal discovery from observational data is an important problem in many applied sciences. Incorporating a recently proposed smooth characterization of acyclicity, gradient-based causal discovery approaches search for a Directed Acyclic Graph (DAG) by optimizing various neural models. Although they show some inspiring results given certain assumptions satisfied, their capability of modeling complex nonlinear causal generative functions is still unsatisfactory. Motivated by recent advances in deep generative models, we propose to use diffusion models for causal discovery,  and search for the DAG under continuous optimization frameworks. The underlying nonlinear causal generative process is modeled with diffusion process, and with flexible parameter configurations, it has the ability to represent various functions, and the proposed causal discovery approach are able to generate graphs with satisfactory accuracy on  observational data generated by either linear or nonlinear causal models. This is evidenced by empirical results on both synthetic and real data.",ICLR.cc/2025/Conference,5.0,False,0.8631,standard neural networks excel prediction but struggle capture causal relationships crucial for robust generalization and intervention our embeds differentiable directed acyclic graph dag structure within the computational graph enabling joint optimization causal structure and functional relationships incorporating specialized regularizers that enforce acyclicity and sparsity constraints cdns learn compact causal models that support counterfactual reasoning this establishes neural focused causal representation learning rather than purely predictive modeling enabling more robust generalization and supporting interventional queries beyond the training distribution,incorporating recently proposed smooth characterization acyclicity gradient based causal discovery approaches search for directed acyclic graph dag optimizing various neural models motivated recent advances deep generative models use diffusion models for causal discovery and search for the dag under continuous optimization frameworks,2025-08-26T01:56:21.867524
57,**Multi-Resolution Hash Encoding Networks: Memory-Efficient Representation Learning with Perfect Spatial Hashing**,"Neural networks for high-dimensional continuous signals often require prohibitive memory for fine-grained representations. We introduce Multi-Resolution Hash Encoding Networks (MRHENs), a novel architecture that enables memory-efficient learning of high-resolution representations through perfect spatial hashing. Our approach represents continuous functions using a hierarchy of learned hash grids that map spatial coordinates to feature vectors with near-constant memory scaling regardless of resolution. The key innovation is a differentiable perfect spatial hash that enables constant-time queries with minimal hash collisions, allowing efficient gradient-based optimization of both hash parameters and encoded features. By developing specialized gradient update rules that account for hash collisions and implementing a progressive multi-resolution strategy, MRHENs overcome the challenges of learning hash-based representations. Experiments on neural radiance fields, physics-based simulation, and 3D shape representation demonstrate that MRHENs achieve comparable quality to explicit representations while reducing memory requirements by 10-20× and enabling previously impossible resolutions. Theoretical analysis provides bounds on hash collision rates and representational capacity. This work establishes a new paradigm for memory-efficient neural representations through learnable perfect spatial hashing, enabling fine-grained modeling of complex signals with constrained memory budgets.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7440,Reshaping Reservoirs: Hebbian Plasticity for Improved Data Separability,"This paper introduces Hebbian Architecture Generation (HAG), a method grounded in Hebbian plasticity principles, designed to optimize the structure of Reservoir Computing networks. HAG adapts the synaptic weights in Recurrent Neural Networks by dynamically forming connections between neurons that exhibit high Pearson correlation. Unlike conventional reservoir computing models that rely on static, randomly initialized connectivity matrices, HAG tailors the reservoir architecture to specific tasks by autonomously optimizing network properties such as signal decorrelation and singular value spread. This task-specific adaptability enhances the linear separability of input data, as supported by Cover’s theorem, which posits that increasing the dimensionality of the feature space improves pattern recognition. Experimental results show that HAG outperforms traditional Echo State Networks across various predictive modeling and pattern recognition benchmarks. By aligning with biological principles of structural plasticity, HAG addresses limitations of static reservoir architectures, offering a biologically plausible and highly adaptable alternative for improved performance in dynamic learning environments.",ICLR.cc/2025/Conference,5.25,False,0.8110,neural networks for high dimensional continuous signals often require prohibitive memory for fine grained representations multi resolution hash encoding networks mrhens that enables memory efficient learning high resolution representations perfect spatial hashing our represents continuous functions hierarchy learned hash grids that map spatial coordinates feature vectors near constant memory scaling regardless resolution the key innovation differentiable perfect spatial hash that enables constant time queries minimal hash collisions allowing efficient gradient based optimization both hash parameters and encoded features developing specialized gradient update rules that account for hash collisions and implementing progressive multi resolution strategy mrhens overcome the challenges learning hash based representations experiments neural radiance fields physics based simulation and shape representation that mrhens achieve comparable quality explicit representations while reducing memory requirements and enabling previously impossible resolutions this establishes paradigm for memory efficient neural representations learnable perfect spatial hashing enabling fine grained modeling complex signals constrained memory budgets,this introduces hebbian generation hag grounded hebbian plasticity principles designed optimize the structure reservoir computing networks hag adapts the synaptic weights recurrent neural networks dynamically forming connections between neurons that exhibit high pearson correlation unlike conventional reservoir computing models that rely static randomly initialized connectivity matrices hag tailors the reservoir specific tasks autonomously optimizing network properties such signal decorrelation and singular value spread this task specific adaptability enhances the linear separability input data supported cover theorem which posits that increasing the dimensionality the feature space improves pattern recognition experimental that hag outperforms traditional echo state networks across various predictive modeling and pattern recognition benchmarks aligning biological principles structural plasticity hag addresses limitations static reservoir architectures offering biologically plausible and highly adaptable alternative for improved dynamic learning environments,2025-08-26T01:56:21.867530
58,**Modular Meta-Architecture Search: Learning to Compose Task-Specific Architectures from Reusable Components**,"Current neural architecture search approaches discover monolithic architectures optimized for specific tasks, limiting knowledge transfer and reusability. We introduce Modular Meta-Architecture Search (MMAS), a novel framework that learns a library of reusable architectural components alongside composition rules for assembling task-specific architectures. Our approach formulates architecture design as a hierarchical composition of learned modules with compatible interfaces, enabling rapid adaptation to new tasks through recombination rather than search from scratch. The key innovation is a meta-learning procedure that jointly optimizes the internal structure of modules and the composition policy across a distribution of tasks, encouraging the emergence of specialized, reusable components. By implementing a differentiable module selection mechanism and incorporating interface compatibility constraints, MMAS maintains end-to-end trainability while ensuring valid compositions. Experiments across computer vision, natural language processing, and reinforcement learning demonstrate that MMAS reduces architecture search time by 8-15× while achieving performance comparable to or better than task-specific search. Analysis reveals that the learned module library contains interpretable components with clear functional specialization. This work establishes a new paradigm for architecture search based on compositional modularity rather than monolithic design, enabling more efficient knowledge transfer across tasks.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1821,When and how are modular networks better?,"Many real-world learning tasks have an underlying hierarchical modular structure, composed of smaller sub-functions. Traditional neural networks (NNs), however, often ignore this structure, leading to inefficiencies in learning and generalization. Leveraging known structural information can enhance performance by aligning the network architecture with the task’s inherent modularity. In this work, we investigate how modular NNs can outperform traditional dense networks by systematically varying the degree of structural knowledge incorporated. We compare architectures ranging from monolithic dense NNs, which assume no prior knowledge, to hierarchically modular NNs with shared modules, which leverage sparsity, modularity, and module reusability. Our experiments demonstrate that incorporating structural knowledge, particularly through module reuse and fixed connectivity, significantly improves learning efficiency and generalization. Hierarchically modular NNs excel in data-scarce scenarios by promoting functional specialization within the modules and reducing redundancy. These findings suggest that task-specific architectural biases can lead to more efficient, interpretable, and effective learning systems.",ICLR.cc/2025/Conference,3.75,False,0.8399,current neural search approaches discover monolithic architectures optimized for specific tasks limiting knowledge transfer and reusability our formulates hierarchical composition learned modules compatible interfaces enabling rapid adaptation tasks recombination rather than search from scratch experiments across computer vision natural language processing and reinforcement learning that mmas reduces search time while achieving comparable better than task specific search this establishes paradigm for search compositional modularity rather than monolithic enabling more efficient knowledge transfer across tasks,many real world learning tasks have underlying hierarchical modular structure composed smaller sub functions traditional neural networks nns however often ignore this structure leading inefficiencies learning and generalization leveraging known structural information can enhance aligning the network the task inherent modularity this how modular nns can outperform traditional dense networks systematically varying the degree structural knowledge incorporated our experiments that incorporating structural knowledge module reuse and fixed connectivity improves learning efficiency and generalization these suggest that task specific architectural biases can lead more efficient interpretable and effective learning systems,2025-08-26T01:56:21.867531
59,**Neuromorphic Computing Architectures: Event-Based Neural Networks with Time-to-First-Spike Coding**,"Traditional deep learning architectures are incompatible with neuromorphic hardware that promises orders-of-magnitude improvements in energy efficiency. We present Time-to-First-Spike Networks (TFSNets), a novel neuromorphic architecture that processes information through precise spike timing rather than continuous activations. Our approach encodes information in the relative timing of the first spike of each neuron, enabling ultra-low latency and energy-efficient computation compatible with neuromorphic hardware. The key innovation is a differentiable time-to-first-spike coding scheme where earlier spikes carry greater significance, creating an implicit prioritization mechanism that focuses computational resources on the most important features. By developing specialized surrogate gradient methods and temporal encoding techniques, TFSNets overcome the non-differentiability challenges inherent in spiking neural networks. Experiments on object recognition, sensory processing, and real-time control demonstrate that TFSNets achieve accuracy within 2-5% of conventional networks while reducing energy consumption by 95-98% and latency by 75-85% when implemented on neuromorphic hardware. Theoretical analysis establishes formal connections between time-to-first-spike coding and rate-based approximations. This work establishes a new paradigm for neuromorphic architecture design specifically optimized for sparse, event-driven computation with minimal energy consumption through precise temporal coding.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8222,Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness,"Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the brain, allow for energy-efficient implementation on neuromorphic hardware. However, SNNs trained with current direct training approaches are constrained to a specific time step. This ""temporal inflexibility"" 1) hinders SNNs' deployment on time-step-free fully event-driven chips and 2) prevents energy-performance balance based on dynamic inference time steps. In this study, we first explore the feasibility of training SNNs that generalize across different time steps. We then introduce Mixed Time-step Training (MTT), a novel method that improves the temporal flexibility of SNNs, making SNNs adaptive to diverse temporal structures. During each iteration of MTT, random time steps are assigned to different SNN stages, with spikes transmitted between stages via communication modules. After training, the weights are deployed and evaluated on both time-stepped and fully event-driven platforms. Experimental results show that models trained by MTT gain remarkable temporal flexibility, friendliness for both event-driven and clock-driven deployment (nearly lossless on N-MNIST and 10.1\% higher than standard methods on CIFAR10-DVS), enhanced network generalization, and near SOTA performance. To the best of our knowledge, this is the first work to report the results of large-scale SNN deployment on fully event-driven scenarios.",ICLR.cc/2025/Conference,6.2,True,0.8729,traditional deep learning architectures are incompatible neuromorphic hardware that promises orders magnitude improvements energy efficiency developing specialized surrogate gradient methods and temporal encoding techniques tfsnets overcome the non differentiability challenges inherent spiking neural networks,spiking neural networks snns models inspired neural mechanisms the brain allow for energy efficient implementation neuromorphic hardware experimental that models trained mtt gain remarkable temporal flexibility friendliness for both event driven and clock driven deployment nearly lossless mnist and higher than standard methods cifar10 dvs enhanced network generalization and near sota,2025-08-26T01:56:21.867534
60,**Elastic Neural Networks: Dynamic Architecture Adaptation through Differential Topology Optimization**,"Fixed neural architectures impose significant constraints on model flexibility, leading to either overparameterization or underfitting across different tasks. We introduce Elastic Neural Networks (ENNs), a novel framework that dynamically adapts network topology during training through differentiable structure optimization. Our approach represents architecture as a continuous relaxation of the discrete topology space, enabling joint optimization of structure and weights via gradient descent. The key innovation is a topology optimization procedure inspired by structural mechanics that iteratively redistributes computational resources based on information flow patterns. By formulating architectural constraints as differentiable penalties and developing specialized gradient estimators for structural updates, ENNs maintain stable training despite their dynamic nature. Experiments across vision, language, and reinforcement learning domains demonstrate that ENNs autonomously discover efficient architectures tailored to specific tasks, achieving performance comparable to manually optimized networks while reducing parameters by 30-45%. Analysis reveals that ENNs naturally discover well-known architectural motifs like skip connections and bottlenecks without explicit priors. This work establishes a new paradigm for neural architecture design where structural adaptation becomes an integral part of the learning process rather than a separate optimization phase.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,407,Real-time design of architectural structures with differentiable mechanics and neural networks,"Designing mechanically efficient geometry for architectural structures like shells, towers, and bridges, is an expensive iterative process.
Existing techniques for solving such inverse problems rely on traditional optimization methods, which are slow and computationally expensive, limiting iteration speed and design exploration.
Neural networks would seem to offer a solution via data-driven amortized optimization, but they often require extensive fine-tuning and cannot ensure that important design criteria, such as mechanical integrity, are met.
In this work, we combine neural networks with a differentiable mechanics simulator to develop a model that accelerates the solution of shape approximation problems for architectural structures represented as bar systems.
This model explicitly guarantees compliance with mechanical constraints while generating designs that closely match target geometries.
We validate our approach in two tasks, the design of masonry shells and cable-net towers.
Our model achieves better accuracy and generalization than fully neural alternatives, and comparable accuracy to direct optimization but in real time, enabling fast and reliable design exploration.
We further demonstrate its advantages by integrating it into 3D modeling software and fabricating a physical prototype.
Our work opens up new opportunities for accelerated mechanical design enhanced by neural networks for the built environment.",ICLR.cc/2025/Conference,6.5,True,0.8414,fixed neural architectures impose significant constraints flexibility leading either overparameterization underfitting across different tasks elastic neural networks enns that dynamically adapts network topology during training differentiable structure optimization our represents continuous relaxation the discrete topology space enabling joint optimization structure and weights gradient descent the key innovation topology optimization procedure inspired structural mechanics that iteratively redistributes computational resources information flow patterns experiments across vision language and reinforcement learning domains that enns autonomously discover efficient architectures tailored specific tasks achieving comparable manually optimized networks while reducing parameters this establishes paradigm for neural where structural adaptation becomes integral part the learning process rather than separate optimization phase,existing techniques for solving such inverse problems rely traditional optimization methods which are slow and computationally expensive limiting iteration speed and exploration neural networks would seem offer solution data driven amortized optimization but they often require extensive fine tuning and cannot ensure that important criteria such mechanical integrity are met this combine neural networks differentiable mechanics simulator that accelerates the solution shape approximation problems for architectural structures represented bar systems our achieves better and generalization than fully neural alternatives and comparable direct optimization but real time enabling fast and reliable exploration our opens opportunities for accelerated mechanical enhanced neural networks for the built environment,2025-08-26T01:56:21.867539
61,**Memory-Augmented Recurrent Transformers with Neural Episodic Control**,"Transformers excel at modeling sequential dependencies but struggle with efficient long-term memory retrieval and rapid adaptation to new patterns. We present Memory-Augmented Recurrent Transformers (MART), a novel architecture that integrates external episodic memory with transformer components through a differentiable neural controller. Our approach combines the parallel processing advantages of transformers with the adaptive memory operations of recurrent models, enabling more efficient information storage and retrieval. The key innovation is a semi-parametric episodic memory system that learns to store, update, and access experiences through content-based addressing and relational reasoning over memory entries. By implementing a learnable write-protection mechanism and a specialized memory consolidation process, MART mitigates catastrophic forgetting while maintaining memory coherence. Experiments on long-context language modeling, multi-step reasoning, and continual learning demonstrate that MART outperforms vanilla transformers by 15-25% in sample efficiency and 8-12% in final performance, with particularly dramatic improvements on tasks requiring adaptation to novel patterns. Qualitative analysis reveals interpretable memory access patterns that provide insights into the model's reasoning process. This work establishes a new architectural paradigm that enhances transformers with explicit episodic memory, bridging the gap between attention-based and memory-based neural architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6378,Selective Induction Heads: How Transformers Select Causal Structures in Context,"Transformers have exhibited exceptional capabilities in sequence modelling tasks, leveraging self-attention and in-context learning. Critical to this success are induction heads, attention circuits that enable copying tokens based on their previous occurrences. In this work, we introduce a novel synthetic framework designed to enable the theoretical analysis of transformers’ ability to dynamically handle causal structures. Existing works rely on Markov Chains to study the formation of induction heads, revealing how transformers capture causal dependencies and learn transition probabilities in-context. However, they rely on a fixed causal structure that fails to capture the complexity of natural languages, where the relationship between tokens dynamically changes with context.  To this end, our framework varies the causal structure through interleaved Markov chains with different lags while keeping the transition probabilities fixed. This setting unveils the formation of Selective Induction Heads, a new circuit that endows transformers with the ability to select the correct causal structure in-context. We empirically demonstrate that attention-only transformers learn this mechanism to predict the next token by identifying the correct lag and copying the corresponding token from the past. We provide a detailed construction of a 3-layer transformer to implement the selective induction head, and a theoretical analysis proving that this mechanism asymptotically converges to the maximum likelihood solution. Our findings advance the theoretical understanding of how transformers select causal structures, providing new insights into their functioning and interpretability.",ICLR.cc/2025/Conference,6.2,True,0.8552,transformers excel modeling sequential dependencies but struggle efficient long term memory retrieval and rapid adaptation patterns memory augmented recurrent transformers mart that integrates external episodic memory transformer components differentiable neural controller our combines the parallel processing advantages transformers the adaptive memory operations recurrent models enabling more efficient information storage and retrieval the key innovation semi parametric episodic memory that learns store update and access experiences content based addressing and relational reasoning over memory entries experiments long context language modeling multi step reasoning and continual learning that mart outperforms vanilla transformers sample efficiency and final dramatic improvements tasks requiring adaptation patterns qualitative analysis reveals interpretable memory access patterns that provide insights into the model reasoning process this establishes architectural paradigm that enhances transformers explicit episodic memory bridging the gap between attention based and memory based neural architectures,transformers have exhibited exceptional capabilities sequence modelling tasks leveraging self attention and context learning critical this success are induction heads attention circuits that enable copying tokens their previous occurrences provide detailed construction layer transformer the selective induction head and theoretical analysis proving that this mechanism asymptotically converges the maximum likelihood solution our advance the theoretical understanding how transformers select causal structures providing insights into their functioning and interpretability,2025-08-26T01:56:21.867542
62,**Multi-Scale Equivariant Graph Neural Networks through Wavelet Scattering Transforms**,"Graph neural networks face fundamental limitations in capturing multi-scale structural patterns while maintaining invariance to geometric transformations. We introduce Wavelet Scattering Graph Networks (WSGNs), a novel architecture that leverages wavelet scattering transforms to extract hierarchical, equivariant features from graph-structured data. Our approach represents graphs in a multi-scale wavelet basis that systematically captures structural information at different resolutions while preserving equivariance to permutations and other geometric transformations. The key innovation is a learnable wavelet scattering module that computes invariant graph signatures through cascaded wavelet transforms and nonlinearities, providing theoretically guaranteed stability to deformations. By incorporating learnable parameters within the scattering framework and developing specialized pooling operations for graph wavelets, WSGNs combine the theoretical advantages of scattering transforms with the adaptability of neural networks. Experiments on molecular property prediction, social network analysis, and quantum chemistry demonstrate that WSGNs outperform state-of-the-art GNNs by 8-14% in accuracy while providing provable invariance guarantees and improved robustness to structural noise. Theoretical analysis establishes connections to the Weisfeiler-Lehman hierarchy and characterizes the discriminative power of the learned representations. This work establishes a new mathematically principled approach to graph neural network design through the integration of wavelet scattering theory.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,10855,Graph Distributional Analytics: Enhancing GNN Explainability through Scalable Embedding and Distribution Analysis,"Graph Neural Networks (GNNs) have achieved significant success in processing graph-structured data but often lack interpretability, limiting their practical applicability. We introduce the Graph Distributional Analytics (GDA) framework, leveraging novel combinations of scalable techniques to enhance GNN explainability. The integration of Weisfeiler-Leman (WL) graph kernels with distributional distance analysis enables GDA to efficiently quantify graph data distributions, while capturing global structural complexities without significant computational costs. GDA creates high-dimensional embeddings employing WL kernels, measures the distribution of distances from measures of categorical central tendency, and assigns distribution scores to quantify each graph's deviation from this vector We evaluate GDA on the ENZYMES, ogbg-ppa, and MalNet-Tiny datasets. Our experiments demonstrate GDA not only accurately characterizes graph distributions but also outperforms baseline methods in identifying specific structural features responsible for misclassifications. This comprehensive analysis provides deeper insights into how training data distributions affect model performance, particularly with out-of-distribution (OOD) data. By revealing the underlying structural causes of GNN predictions through a novel synergy of established techniques, GDA enhances transparency and offers a practical tool for practitioners to build more interpretable and robust graph-based models. Our framework's scalability, efficiency, and ability to integrate with various embedding methods make it a valuable addition to the suite of tools available for GNN analysis.",ICLR.cc/2025/Conference,3.4,nan,0.8488,graph neural networks face fundamental limitations capturing multi scale structural patterns while maintaining invariance geometric transformations incorporating learnable parameters within the scattering and developing specialized pooling operations for graph wavelets wsgns combine the theoretical advantages scattering transforms the adaptability neural networks experiments molecular property prediction social network analysis and quantum chemistry that wsgns outperform state the art gnns while providing provable invariance guarantees and improved robustness structural noise this establishes mathematically principled graph neural network the integration wavelet scattering theory,graph neural networks gnns have achieved significant success processing graph structured data but often lack interpretability limiting their practical applicability our framework scalability efficiency and ability integrate various embedding methods make valuable addition the suite tools available for gnn analysis,2025-08-26T01:56:21.867545
63,**Neural Diffusion Architectures: Modeling Complex Systems with Stochastic Differential Equations**,"Traditional neural networks struggle to model stochastic dynamical systems and complex diffusion processes prevalent in many scientific domains. We present Neural Diffusion Architectures (NDAs), a novel framework that embeds stochastic differential equations (SDEs) as fundamental building blocks within neural networks. Our approach represents hidden states as probability distributions that evolve according to learnable drift and diffusion terms, enabling direct modeling of uncertainty propagation through the network. The key innovation is a differentiable SDE solver that allows end-to-end training of both the drift and diffusion components via pathwise derivatives, addressing the challenges of backpropagating through stochastic processes. By developing specialized parameterizations that ensure well-posedness and implementing variance reduction techniques for gradient estimation, NDAs maintain stable training despite their stochastic nature. Experiments on probabilistic time series forecasting, stochastic control, and molecular dynamics demonstrate that NDAs outperform deterministic architectures and variational approaches by 15-25% in calibration metrics and predictive accuracy. Theoretical analysis establishes connections to uncertainty quantification and provides guarantees on the well-posedness of the learned models. This work introduces a fundamental advancement in neural architecture design for stochastic systems by directly incorporating SDEs as computational primitives.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3102,Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions,"Neural Stochastic Differential Equations (Neural SDEs) have emerged as powerful mesh-free generative models for continuous stochastic processes, with critical applications in fields such as finance, physics, and biology. Previous state-of-the-art methods have relied on adversarial training, such as GANs, or on minimizing distance measures between processes using signature kernels. However, GANs suffer from issues like instability, mode collapse, and the need for specialized training techniques, while signature kernel-based methods require solving linear PDEs and backpropagating gradients through the solver, whose computational complexity scales quadratically with the discretization steps. In this paper, we identify a novel class of strictly proper scoring rules for comparing continuous Markov processes. This theoretical finding naturally leads to a novel approach called Finite Dimensional Matching (FDM) for training Neural SDEs. Our method leverages the Markov property of SDEs to provide a computationally efficient training objective. This scoring rule allows us to bypass the computational overhead associated with signature kernels and reduces the training complexity from $O(D^2)$ to $O(D)$ per epoch, where $D$ represents the number of discretization steps of the process. We demonstrate that FDM achieves superior performance, consistently outperforming existing methods in terms of both computational efficiency and generative quality.",ICLR.cc/2025/Conference,6.0,True,0.8507,traditional neural networks struggle stochastic dynamical systems and complex diffusion processes prevalent many scientific domains neural diffusion architectures ndas that embeds stochastic differential equations sdes fundamental building blocks within neural networks our represents hidden states probability distributions that evolve according learnable drift and diffusion terms enabling direct modeling uncertainty propagation the network this introduces fundamental advancement neural for stochastic systems directly incorporating sdes computational primitives,neural stochastic differential equations neural sdes have emerged powerful mesh free generative models for continuous stochastic processes critical applications fields such finance physics and biology this theoretical naturally leads called finite dimensional matching fdm for training neural sdes,2025-08-26T01:56:21.867548
64,**Topological Deep Learning: Persistent Homology Networks for Structure-Aware Representation**,"Standard neural architectures rely on local patterns and struggle to capture global topological structures crucial for many complex systems. We introduce Topological Neural Networks (TNNs), a novel architecture that integrates computational topology with deep learning to explicitly model structural features at different scales. Our approach implements differentiable persistent homology computations as neural layers, enabling direct optimization of topological features through gradient descent. The key innovation is a differentiable filtration layer that computes persistence diagrams and transforms them into learnable topological signatures while maintaining end-to-end differentiability. By developing specialized backpropagation algorithms for simplicial complexes and implementing efficient approximations of persistent homology, TNNs overcome the computational challenges typically associated with topological data analysis. Experiments on graph classification, molecular property prediction, and scientific imaging demonstrate that TNNs outperform conventional architectures by 10-18% on tasks where global structure determines functionality. Analysis reveals that TNNs learn filtration functions that highlight task-relevant topological features, providing interpretable insights into the structural patterns driving predictions. This work establishes a new paradigm for neural architecture design that explicitly incorporates topological information processing, enabling more effective learning from structurally complex data.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3717,E(n) Equivariant Topological Neural Networks,"Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features. Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue. TDL enables the principled modeling of arbitrary multi-way, hierarchical higher-order interactions by operating on combinatorial topological spaces, such as simplicial or cell complexes, instead of graphs. However, little is known about how to leverage geometric features such as positions and velocities for TDL. This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial complexes, formal objects unifying graphs, hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric node features while respecting rotation, reflection, and translation equivariance. Moreover, being TDL models, ETNNs are natively ready for settings with heterogeneous interactions.  We provide a theoretical analysis to show the improved expressiveness of ETNNs over architectures for geometric graphs. We also show how E(n)-equivariant variants of TDL models can be directly derived from our framework. The broad applicability of ETNNs is demonstrated through two tasks of vastly different scales: i) molecular property prediction on the QM9 benchmark and ii) land-use regression for hyper-local estimation of air pollution with multi-resolution irregular geospatial data.  The results indicate that ETNNs are an effective tool for learning from diverse types of richly structured data, as they match or surpass SotA equivariant TDL models with a significantly smaller computational burden, thus highlighting the benefits of a principled geometric inductive bias. Our implementation of ETNNs can be found at https://github.com/NSAPH-Projects/topological-equivariant-networks.",ICLR.cc/2025/Conference,6.0,True,0.8521,standard neural architectures rely local patterns and struggle capture global topological structures crucial for many complex systems topological neural networks tnns that integrates computational topology deep learning explicitly structural features different scales our implements differentiable persistent homology computations neural layers enabling direct optimization topological features gradient descent this establishes paradigm for neural that explicitly incorporates topological information processing enabling more effective learning from structurally complex data,graph neural networks excel modeling pairwise interactions but they cannot flexibly accommodate higher order interactions and features topological deep learning tdl has emerged recently promising tool for addressing this issue this introduces equivariant topological neural networks etnns which are equivariant message passing networks operating combinatorial complexes formal objects unifying graphs hypergraphs simplicial path and cell complexes the broad applicability etnns demonstrated two tasks vastly different scales molecular property prediction the qm9 and land use regression for hyper local estimation air pollution multi resolution irregular geospatial data the indicate that etnns are effective tool for learning from diverse types richly structured data they match surpass sota equivariant tdl models smaller computational burden thus highlighting the benefits principled geometric inductive bias,2025-08-26T01:56:21.867550
65,**Invertible Neural Architectures: Bijective Models with Exact Likelihood Estimation**,"Deep generative models typically rely on approximate inference or implicit density estimation, limiting their application in scenarios requiring exact probabilities. We present Invertible Neural Architectures (INAs), a comprehensive framework for designing exactly invertible networks with tractable likelihood computation for both discriminative and generative tasks. Our approach implements a systematic construction of invertible building blocks that guarantee bijectivity while maintaining expressivity comparable to standard architectures. The key innovation is a composition scheme for conditional invertible transformations that enables exact likelihood computation without restrictive architectural constraints. By developing specialized weight parameterizations and normalization techniques that preserve invertibility, INAs overcome the expressivity limitations of previous normalizing flow approaches. Experiments on density estimation, classification with uncertainty quantification, and hybrid generative-discriminative tasks demonstrate that INAs achieve performance comparable to conventional architectures while providing exact likelihood evaluation and invertibility guarantees. Theoretical analysis establishes universal approximation properties under appropriate conditions and characterizes the relationship between architectural depth and expressivity. This work establishes a unified framework for neural architecture design based on exact invertibility principles, enabling new applications requiring bijective mappings between input and representation spaces.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8708,IMPLICIT VARIATIONAL REJECTION SAMPLING,"Variational Inference (VI) is a cornerstone technique in Bayesian machine learning, employed to approximate complex posterior distributions. However, traditional VI methods often rely on mean-field assumptions, which may inadequately capture the true posterior's complexity. To address this limitation, recent advancements have utilized neural networks to model implicit distributions, thereby offering increased flexibility. Despite this, the practical constraints of neural network architectures can still result in inaccuracies in posterior approximations. In this work, we introduce a novel method called Implicit Variational Rejection Sampling (IVRS), which integrates implicit distributions with rejection sampling to enhance the approximation of the posterior distribution. Our method employs neural networks to construct implicit proposal distributions and utilizes rejection sampling with a meticulously designed acceptance probability function. A discriminator network is employed to estimate the density ratio between the implicit proposal and the true posterior, thereby refining the approximation. We propose the Implicit Resampling Evidence Lower Bound (IR-ELBO) as a metric to characterize the quality of the resampled distribution, enabling the derivation of a tighter variational lower bound. Experimental results demonstrate that our method outperforms traditional variational inference techniques in terms of both accuracy and efficiency, leading to significant improvements in inference performance. This work not only showcases the effective combination of implicit distributions and rejection sampling but also offers a novel perspective and methodology for advancing variational inference.",ICLR.cc/2025/Conference,5.25,False,0.8353,deep generative models rely approximate inference implicit density estimation limiting their application scenarios requiring exact probabilities invertible neural architectures inas comprehensive for designing exactly invertible networks tractable likelihood computation for both discriminative and generative tasks experiments density estimation classification uncertainty quantification and hybrid generative discriminative tasks that inas achieve comparable conventional architectures while providing exact likelihood evaluation and invertibility guarantees this establishes unified for neural exact invertibility principles enabling applications requiring bijective mappings between input and representation spaces,variational inference cornerstone bayesian machine learning employed approximate complex posterior distributions address this limitation recent advancements have utilized neural networks implicit distributions thereby offering increased flexibility despite this the practical constraints neural network architectures can still inaccuracies posterior approximations our employs neural networks construct implicit proposal distributions and utilizes rejection sampling meticulously designed acceptance probability function discriminator network employed estimate the density ratio between the implicit proposal and the true posterior thereby refining the approximation,2025-08-26T01:56:21.867553
66,**Symmetry-Aware Transformer Networks: Group-Equivariant Attention for Geometric Deep Learning**,"Traditional transformers lack equivariance guarantees, limiting their sample efficiency on problems with inherent symmetries. We introduce Symmetry-Aware Transformer Networks (SATNs), a mathematically rigorous framework that extends self-attention mechanisms to respect arbitrary group symmetries. Our approach reformulates attention operations within the framework of group theory, enabling equivariance to translations, rotations, and more general transformations. The key innovation is a steerable attention mechanism where queries, keys, and values transform according to group representations, guaranteeing that attention operations commute with group actions. By developing specialized positional encodings for different symmetry groups and implementing efficient algorithms for group-theoretic operations, SATNs maintain computational tractability while providing theoretical guarantees on equivariance properties. Experiments on molecular modeling, crystallographic structure prediction, and physical simulation demonstrate that SATNs achieve 30-50% sample efficiency improvements over standard transformers, with particularly dramatic gains on tasks with clear symmetry structure. Theoretical analysis establishes formal equivariance guarantees and characterizes the expressive power of the architecture relative to invariant counterparts. This work establishes a comprehensive framework for incorporating symmetry constraints into transformer architectures, enabling more efficient learning in domains with known invariances.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8261,symmetry aware transformer networks satns mathematically rigorous that extends self attention mechanisms respect arbitrary group symmetries our reformulates attention operations within the group theory enabling equivariance translations rotations and more general transformations the key innovation steerable attention mechanism where queries keys and values transform according group representations guaranteeing that attention operations commute group actions this establishes comprehensive for incorporating symmetry constraints into transformer architectures enabling more efficient learning domains known invariances,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T01:56:21.867555
67,**Progressive Growing Transformers: Scale-Invariant Architectures for Arbitrary Sequence Lengths**,"Transformer models are typically trained on fixed context lengths, causing performance degradation when deployed on sequences of different lengths. We present Progressive Growing Transformers (PGTs), a novel architecture that enables seamless generalization across sequence lengths through an innovative training regime and architectural modifications. Our approach implements a curriculum that gradually increases sequence length during training while maintaining scale-invariant representations through carefully designed normalization and positional encoding schemes. The key innovation is a hierarchical attention mechanism that processes information at multiple temporal resolutions, allowing the model to capture both local patterns and global dependencies regardless of sequence length. By developing specialized initialization techniques and implementing progressive positional encodings, PGTs overcome the instabilities typically associated with variable-length training. Experiments on language modeling, time series forecasting, and long-range reasoning demonstrate that PGTs maintain consistent performance across sequence lengths from 100 to 100,000 tokens without requiring retraining or fine-tuning. Analysis reveals that PGTs develop scale-invariant attention patterns that adapt automatically to input length. This work establishes a new paradigm for transformer architecture design focused on length generalization, enabling deployment in scenarios where input lengths may vary dramatically from training conditions.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7041,"Can Mamba Always Enjoy the ""Free Lunch""?","Transformers have been the cornerstone of current Large Language Models (LLMs); however, its linear growth in overhead during inference with respect to sequence length poses challenges for modeling long sequences. In this context, Mamba has gradually attracted attention due to its constant-level size during inference and existing empirical results have shown that it can perform comparably to Transformers in sequence modeling while offering significant savings. However, one may ask that, can Mamba always enjoy the ``free lunch""? In this paper, we focus on analyzing the expressive ability of Mamba from a theoretical standpoint. First, inspired by the connection between Mamba and linear attention, we investigate potential shortcomings of the Mamba when performing the COPY operation. Our results indicate that Mamba with constant size may encounter bottlenecks when handling COPY, while it can achieve perfect performance when the size scales linearly with sequence length. Based on this observation, we analyze Mamba's ability to tackle DP problems when equipped with Chain of Thought (CoT). Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is comparable to standard and efficient Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our results contribute to a deeper understanding of Mamba.",ICLR.cc/2025/Conference,4.2,False,0.8656,transformer models are trained fixed context lengths causing degradation when deployed sequences different lengths the key innovation hierarchical attention mechanism that processes information multiple temporal resolutions allowing the capture both local patterns and global dependencies regardless sequence length experiments language modeling time series forecasting and long range reasoning that pgts maintain consistent across sequence lengths from tokens requiring retraining fine tuning analysis reveals that pgts scale invariant attention patterns that adapt automatically input length this establishes paradigm for transformer focused length generalization enabling deployment scenarios where input lengths may vary dramatically from training conditions,transformers have been the cornerstone current large language models llms however its linear growth overhead during inference respect sequence length poses challenges for modeling long sequences this context mamba has gradually attracted attention due its constant level size during inference and existing empirical have shown that can perform comparably transformers sequence modeling while offering significant savings,2025-08-26T01:56:21.867561
68,**Neural Architecture Distillation: Compressing Architectural Knowledge Through Functional Extraction**,"Knowledge distillation typically transfers information from parameters to parameters, failing to capture architectural innovations that drive performance in larger models. We introduce Neural Architecture Distillation (NAD), a novel framework that transfers architectural knowledge rather than just parameter values from complex teacher networks to compact student models. Our approach extracts functional components from pre-trained networks and reformulates them as modular, reusable operations that can be integrated into smaller architectures. The key innovation is a functional extraction algorithm that identifies and isolates computational motifs within neural networks, distilling architectural patterns rather than just weights. By implementing a graph isomorphism search to identify recurring computational subgraphs and developing specialized techniques for modularizing these components, NAD enables efficient transfer of architectural innovations across model scales. Experiments on computer vision, natural language processing, and reinforcement learning demonstrate that NAD-distilled models outperform traditional knowledge distillation by 5-8% while using equivalent parameter counts. Analysis reveals that NAD successfully transfers complex architectural motifs such as gating mechanisms and attention patterns to smaller models. This work establishes a new approach to knowledge transfer focused on architectural innovations rather than parameter values, enabling more effective compression of state-of-the-art models.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1659,AugKD: Ingenious Augmentations Empower Knowledge Distillation for Image Super-Resolution,"Knowledge distillation (KD) compresses deep neural networks by transferring task-related knowledge from cumbersome pre-trained teacher models to more compact student models. However, vanilla KD for image super-resolution (SR) networks yields only limited improvements due to the inherent nature of SR tasks, where the outputs of teacher models are noisy approximations of high-quality label images. In this work, we show that the potential of vanilla KD has been underestimated and demonstrate that the ingenious application of data augmentation methods can close the gap between it and more complex, well-designed methods. Unlike conventional training processes typically applying image augmentations simultaneously to both low-quality inputs and high-quality labels, we propose AugKD utilizing unpaired data augmentations to 1) generate auxiliary distillation samples and 2) impose label consistency regularization. Comprehensive experiments show that the AugKD significantly outperforms existing state-of-the-art KD methods across a range of SR tasks.",ICLR.cc/2025/Conference,6.0,True,0.8367,knowledge distillation transfers information from parameters parameters failing capture architectural innovations that drive larger models neural distillation nad that transfers architectural knowledge rather than just parameter values from complex teacher networks compact student models the key innovation functional extraction that identifies and isolates computational motifs within neural networks distilling architectural patterns rather than just weights implementing graph isomorphism search identify recurring computational subgraphs and developing specialized techniques for modularizing these components nad enables efficient transfer architectural innovations across scales experiments computer vision natural language processing and reinforcement learning that nad distilled models outperform traditional knowledge distillation while equivalent parameter counts analysis reveals that nad transfers complex architectural motifs such gating mechanisms and attention patterns smaller models this establishes knowledge transfer focused architectural innovations rather than parameter values enabling more effective compression state the art models,knowledge distillation compresses deep neural networks transferring task related knowledge from cumbersome pre trained teacher models more compact student models,2025-08-26T01:56:21.867563
69,**Neural Sheaves: Topological Deep Learning with Fiber Bundle Representations**,"Graph neural networks face fundamental limitations in representing higher-order relationships and hierarchical structures. We introduce Neural Sheaves, a novel topological deep learning framework that represents data and features as sheaves—mathematical objects that systematically track information across different scales and localities. Our approach implements message passing on cellular sheaf structures, enabling consistent information flow between local and global representations through restriction and extension maps. The key innovation is a learnable sheaf Laplacian that generalizes graph convolutions to operate on fiber bundles over simplicial complexes, providing a unified framework for processing hierarchical, multi-scale data. By developing specialized initialization schemes and implementing efficient sparse representations of sheaf structures, Neural Sheaves maintain computational tractability despite their mathematical sophistication. Experiments on hierarchical classification, multi-resolution image segmentation, and scientific simulation demonstrate that Neural Sheaves outperform traditional graph neural networks by 12-20% on tasks involving complex topological structures. Theoretical analysis establishes connections to spectral graph theory and provides expressivity guarantees beyond message-passing GNNs. This work introduces a fundamentally new approach to neural architecture design based on sheaf theory, enabling principled processing of multi-scale, hierarchical data with formal mathematical guarantees.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,5228,TopoTune: A Framework for Generalized Combinatorial Complex Neural Networks,"Graph Neural Networks (GNNs) excel in learning from relational datasets, processing node and edge features in a way that preserves the symmetries of the graph domain. However, many complex systems---such as biological or social networks---involve multiway complex interactions that are more naturally represented by higher-order topological domains. The emerging field of Topological Deep Learning (TDL) aims to accommodate and leverage these higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly general TDL models, have been shown to be more expressive and better performing than GNNs. However, differently from the graph deep learning ecosystem, TDL lacks a principled and standardized framework for easily defining new architectures, restricting its accessibility and applicability. To address this issue, we introduce Generalized CCNNs (GCCNs), a novel simple yet powerful family of TDL models that can be used to systematically transform any (graph) neural network into its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show that these architectures consistently match or outperform CCNNs, often with less model complexity. In an effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight software for defining, building, and training GCCNs with unprecedented flexibility and ease.",ICLR.cc/2025/Conference,6.25,False,0.8327,graph neural networks face fundamental limitations representing higher order relationships and hierarchical structures neural sheaves topological deep learning that represents data and features sheaves mathematical objects that systematically track information across different scales and localities the key innovation learnable sheaf laplacian that generalizes graph convolutions operate fiber bundles over simplicial complexes providing unified for processing hierarchical multi scale data developing specialized initialization schemes and implementing efficient sparse representations sheaf structures neural sheaves maintain computational tractability despite their mathematical sophistication experiments hierarchical classification multi resolution image segmentation and scientific simulation that neural sheaves outperform traditional graph neural networks tasks involving complex topological structures this introduces fundamentally neural sheaf theory enabling principled processing multi scale hierarchical data formal mathematical guarantees,graph neural networks gnns excel learning from relational datasets processing node and edge features way that preserves the symmetries the graph domain the emerging field topological deep learning tdl aims accommodate and leverage these higher order structures combinatorial complex neural networks ccnns fairly general tdl models have been shown more expressive and better performing than gnns however differently from the graph deep learning ecosystem tdl lacks principled and standardized for easily defining architectures restricting its accessibility and applicability address this issue generalized ccnns gccns simple yet powerful family tdl models that can used systematically transform any graph neural network into its tdl counterpart,2025-08-26T01:56:21.867568
70,**Self-Organizing Neural Architectures: Competitive Hebbian Learning with Emergent Specialization**,"Current neural networks rely on backpropagation, which bears little resemblance to learning in biological systems and requires global coordination of updates. We present Self-Organizing Neural Architectures (SONAs), a biologically plausible framework that enables end-to-end training through local learning rules without backpropagation. Our approach implements competitive Hebbian learning with lateral inhibition, allowing networks to develop specialized feature detectors through self-organization rather than gradient descent. The key innovation is a differentiable formulation of neural competition that enables specialized units to emerge while maintaining end-to-end trainability with theoretical guarantees on convergence. By carefully designing local plasticity rules and implementing a homeostatic regulation mechanism, SONAs overcome the instability challenges typically associated with Hebbian learning. Experiments on image classification, unsupervised representation learning, and continual learning demonstrate that SONAs achieve performance within 3-5% of backpropagation-trained networks while showing superior resilience to catastrophic forgetting and adversarial attacks. Analysis reveals interpretable specialization patterns where individual units respond to specific input features in a manner resembling cortical organization. This work establishes a new biologically-inspired paradigm for neural architecture design based on self-organization principles, providing an alternative to backpropagation with competitive advantages in specific domains.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7780,What should a neuron aim for? Designing local objective functions based on information theory,"In modern deep neural networks, the learning dynamics of individual neurons are often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. Here, we show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e., feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.",ICLR.cc/2025/Conference,7.5,True,0.8700,current neural networks rely backpropagation which bears little resemblance learning biological systems and requires global coordination updates self organizing neural architectures sonas biologically plausible that enables end end training local learning rules backpropagation our implements competitive hebbian learning lateral inhibition allowing networks specialized feature detectors self organization rather than gradient descent the key innovation differentiable formulation neural competition that enables specialized units emerge while maintaining end end trainability theoretical guarantees convergence carefully designing local plasticity rules and implementing homeostatic regulation mechanism sonas overcome the instability challenges associated hebbian learning experiments image classification unsupervised representation learning and continual learning that sonas achieve within backpropagation trained networks while showing superior resilience catastrophic forgetting and adversarial attacks this establishes biologically inspired paradigm for neural self organization principles providing alternative backpropagation competitive advantages specific domains,modern deep neural networks the learning dynamics individual neurons are often obscure the networks are trained global optimization conversely biological systems build self organized local learning achieving robustness and efficiency limited global information here how self organization between individual artificial neurons can achieved designing bio inspired local learning goals this selection expressed weighted sum pid terms which for given problem can directly derived from intuitive reasoning numerical optimization offering window into understanding task relevant local information processing achieving neuron level interpretability while enabling strong local learning our advances principled information theoretic foundation for local learning strategies,2025-08-26T01:56:21.867575
71,**Hyperdimensional Computing Networks: Neuromorphic Architectures with Vector Symbolic Operations**,"Deep learning models struggle with symbolic reasoning and compositional generalization despite their success in pattern recognition. We introduce Hyperdimensional Computing Networks (HCNs), a novel neuromorphic architecture that implements vector symbolic operations as differentiable neural components. Our approach represents concepts as high-dimensional vectors (hypervectors) and implements symbolic operations such as binding, bundling, and permutation as specialized neural layers. The key innovation is a differentiable framework for manipulating distributed symbolic representations while maintaining compatibility with gradient-based learning. By developing specialized initialization techniques for hypervectors and implementing efficient approximations of vector symbolic algebra operations, HCNs bridge the gap between connectionist and symbolic approaches. Experiments on compositional generalization, relational reasoning, and structured prediction demonstrate that HCNs outperform standard neural networks by 15-25% on tasks requiring systematic generalization to novel combinations. Analysis reveals that HCNs naturally develop compositional representations where complex concepts are encoded as structured combinations of simpler elements. This work establishes a new approach to neural architecture design inspired by hyperdimensional computing theory, enabling more efficient handling of symbolic and compositional structures within the neural network paradigm.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6601,Unsupervised Learning of Categorical Structure,"Humans are known to reason using logic and abstract categories, and yet most state of the art neural models use continuous distributed representations. These representations offer impressive gradient-based learning capabilities, but it is often difficult to know what symbolic algorithm the network might implicitly be implementing, if any. We find that there are representational geometries that naturally suggest a symbolic structure, which can be expressed in terms of binary components. We show that we can recover this structure by fitting the geometry of this binary embedding to the representational geometry of the original objects. After establishing general facts and providing some intuitions, we present two algorithms that work on low-rank or full-rank data, respectively. We assess their reliability on simulated data, and then use them to interpret neural word embeddings, in which we expect a compositional structure.",ICLR.cc/2025/Conference,5.6,False,0.8542,deep learning models struggle symbolic reasoning and compositional generalization despite their success pattern recognition hyperdimensional computing networks hcns neuromorphic that implements vector symbolic operations differentiable neural components our represents concepts high dimensional vectors hypervectors and implements symbolic operations such binding bundling and permutation specialized neural layers the key innovation differentiable for manipulating distributed symbolic representations while maintaining compatibility gradient based learning experiments compositional generalization relational reasoning and structured prediction that hcns outperform standard neural networks tasks requiring systematic generalization combinations this establishes neural inspired hyperdimensional computing theory enabling more efficient handling symbolic and compositional structures within the neural network paradigm,humans are known reason logic and categories and yet most state the art neural models use continuous distributed representations these representations offer impressive gradient based learning capabilities but often difficult know what symbolic the network might implicitly implementing any that can recover this structure fitting the geometry this binary embedding the representational geometry the original objects assess their reliability simulated data and then use them interpret neural word embeddings which expect compositional structure,2025-08-26T01:56:21.867579
72,**Quasi-Recurrent Mixture of Experts: Conditional Computation with Dynamic Routing**,"Transformer architectures excel at parallel processing but face fundamental efficiency challenges for long sequences due to their quadratic attention complexity. We introduce Quasi-Recurrent Mixture of Experts (QR-MoE), a novel architecture that combines the parallelism of convolutions with the conditional computation benefits of mixture-of-experts models. Our approach implements a hierarchical routing mechanism that dynamically selects specialized quasi-recurrent experts based on input characteristics, enabling efficient processing of long sequences with linear scaling properties. The key innovation is a gated quasi-recurrent layer that combines convolution and recurrence through specialized experts, allowing adaptive computation depth based on sequence complexity. By developing a load-balancing mechanism that ensures optimal expert utilization and implementing specialized batch processing techniques, QR-MoE overcomes the training instabilities typically associated with conditional computation. Experiments on language modeling, time series forecasting, and document processing demonstrate that QR-MoE achieves accuracy comparable to transformers while reducing computation by 65-80% and enabling processing of sequences 5-10× longer without quadratic scaling. Analysis reveals that the routing mechanism automatically specializes experts for different linguistic or structural patterns. This work establishes a new direction for efficient sequence processing that combines the advantages of recurrent, convolutional, and mixture-of-experts architectures.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6320,NIMBA : Towards Robust and Principled Processing of Point Clouds With SSMs,"Transformers have become dominant in large-scale deep learning tasks across various domains, including text, 2D and 3D vision. However, the quadratic complexity of their attention mechanism limits their efficiency as the sequence length increases, particularly in high-resolution 3D data such as point clouds. Recently, state space models (SSMs) like Mamba have emerged as promising alternatives, offering linear complexity, scalability, and high performance in long-sequence tasks. The key challenge in the application of SSMs in this domain lies in reconciling the non-sequential structure of point clouds with the inherently directional (or bi-directional) order-dependent processing of recurrent models like Mamba. To achieve this, previous research proposed reorganizing point clouds along multiple directions or predetermined paths in 3D space, concatenating the results to produce a single 1D sequence capturing different views. In our work we introduce a method to convert point clouds into 1D sequences that maintains 3D spatial structure with no need for data replication, allowing Mamba’s sequential processing to be applied effectively in an almost permutation-invariant manner. In contrast to other works, we found that our method does not require positional embeddings, and allows for shorter sequence lengths while still achieving state-of-the-art results in ModelNet40 and ScanObjectNN datasets and surpassing Transformer-based models in both accuracy and efficiency.",ICLR.cc/2025/Conference,4.0,False,0.8561,transformer architectures excel parallel processing but face fundamental efficiency challenges for long sequences due their quadratic attention complexity our implements hierarchical routing mechanism that dynamically selects specialized quasi recurrent experts input characteristics enabling efficient processing long sequences linear scaling properties the key innovation gated quasi recurrent layer that combines convolution and recurrence specialized experts allowing adaptive computation depth sequence complexity developing load balancing mechanism that ensures optimal expert utilization and implementing specialized batch processing techniques moe overcomes the training instabilities associated conditional computation experiments language modeling time series forecasting and document processing that moe achieves comparable transformers while reducing computation and enabling processing sequences longer quadratic scaling this establishes direction for efficient sequence processing that combines the advantages recurrent convolutional and mixture experts architectures,transformers have become dominant large scale deep learning tasks across various domains including text and vision however the quadratic complexity their attention mechanism limits their efficiency the sequence length increases high resolution data such point clouds the key challenge the application ssms this domain lies reconciling the non sequential structure point clouds the inherently directional directional order dependent processing recurrent models like mamba our convert point clouds into sequences that maintains spatial structure need for data replication allowing mamba sequential processing applied almost permutation invariant manner,2025-08-26T01:56:21.867583
73,**Neural Partial Differential Equation Solvers with Physical Inductive Biases**,"Deep learning approaches to scientific computing often lack physical consistency guarantees critical for reliable simulation. We introduce Neural PDE Solvers (NeuralPDEs), a specialized architecture for solving partial differential equations with built-in physical inductive biases. Our approach embeds physical constraints directly into the computational graph through differentiable PDE operators that enforce conservation laws, boundary conditions, and other domain-specific requirements. The key innovation is a constraint satisfaction layer that projects neural outputs onto the manifold of physically valid solutions, ensuring that predictions automatically satisfy necessary physical properties. By implementing specialized finite element basis functions and developing efficient backpropagation through implicit PDE solvers, NeuralPDEs maintain computational efficiency while providing physical consistency guarantees. Experiments on fluid dynamics, electromagnetic simulation, and structural mechanics demonstrate that NeuralPDEs achieve 30-50% lower prediction error than unconstrained neural networks while guaranteeing conservation of fundamental quantities like mass, momentum, and energy. Theoretical analysis provides bounds on approximation errors and characterizes the trade-off between expressivity and physical consistency. This work establishes a new paradigm for scientific machine learning that incorporates physical knowledge as architectural constraints rather than soft regularization terms.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2419,Text2PDE: Latent Diffusion Models for Accessible Physics Simulation,"Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training cost, numerical accuracy, and ease of applicability to different problem setups. To address these limitations, we introduce several methods to apply latent diffusion models to physics simulation. Firstly, we introduce a mesh autoencoder to compress arbitrarily discretized PDE data, allowing for efficient diffusion training across various physics. Furthermore, we investigate full spatiotemporal solution generation to mitigate autoregressive error accumulation. Lastly, we investigate conditioning on initial physical quantities, as well as conditioning solely on a text prompt to introduce text2PDE generation. We show that language can be a compact, interpretable, and accurate modality for generating physics simulations, paving the way for more usable and accessible PDE solvers. Through experiments on both uniform and structured grids, we show that the proposed approach is competitive with current neural PDE solvers in both accuracy and efficiency, with promising scaling behavior up to $\sim$3 billion parameters. By introducing a scalable, accurate, and usable physics simulator, we hope to bring neural PDE solvers closer to practical use.",ICLR.cc/2025/Conference,5.333333333333333,True,0.8672,deep learning approaches scientific computing often lack physical consistency guarantees critical for reliable simulation neural pde solvers neuralpdes specialized for solving partial differential equations built physical inductive biases the key innovation constraint satisfaction layer that projects neural outputs onto the manifold physically valid solutions ensuring that predictions automatically satisfy necessary physical properties experiments fluid dynamics electromagnetic simulation and structural mechanics that neuralpdes achieve lower prediction error than unconstrained neural networks while guaranteeing conservation fundamental quantities like mass momentum and energy this establishes paradigm for scientific machine learning that incorporates physical knowledge architectural constraints rather than soft regularization terms,recent advances deep learning have inspired numerous works data driven solutions partial differential equation pde problems these neural pde solvers can often much faster than their numerical counterparts however each presents its unique limitations and balances training cost numerical and ease applicability different problem setups furthermore full spatiotemporal solution generation mitigate autoregressive error accumulation lastly conditioning initial physical quantities well conditioning solely text prompt text2pde generation that language can compact interpretable and accurate modality for generating physics simulations paving the way for more usable and accessible pde solvers experiments both uniform and structured grids that the proposed competitive current neural pde solvers both and efficiency promising scaling behavior sim billion parameters introducing scalable accurate and usable physics simulator hope bring neural pde solvers closer practical use,2025-08-26T01:56:21.867588
74,**Liquid Time-Constant Networks: Adaptive Dynamics with Learned Timescales**,"Current recurrent architectures use fixed timescales for temporal information processing, limiting their ability to model phenomena with multiple characteristic frequencies. We introduce Liquid Time-Constant Networks (LTCNs), a novel continuous-time architecture with learnable, input-dependent timescales that adapt to the temporal dynamics of the data. Our approach implements a system of neural ordinary differential equations where time constants are themselves functions of the input and hidden state, enabling the network to automatically adjust its processing speed for different temporal patterns. The key innovation is a timescale regulation mechanism that learns to allocate computational resources across different temporal frequencies based on input characteristics. By developing specialized numerical integration schemes and implementing stability-preserving parameterizations, LTCNs overcome the challenges of training adaptive continuous-time models. Experiments on speech recognition, physiological signal analysis, and chaotic systems demonstrate that LTCNs outperform fixed-timescale recurrent models by 15-22% while requiring fewer parameters and exhibiting superior generalization to novel timescales. Analysis reveals that LTCNs automatically discover meaningful timescale hierarchies aligned with the natural frequencies in the data. This work establishes a new paradigm for temporal neural architecture design based on adaptive dynamics rather than fixed computation steps, enabling more efficient modeling of multi-scale temporal phenomena.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,59,Neural ODE with Differentiable Hidden State for Irregular Time Series,"Capturing the continuous underlying dynamics of irregular time series is essential for accurately reflecting the ongoing evolution and intricate correlations within the data. The discrete nature of current models, including RNN-based models and transformer variants, poses challenges when it comes to generalizing to the continuous-time data paradigms, which is necessary for capturing ongoing dynamics of irregular time series. 
Neural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis. However, integrating new information while maintaining the continuity of latent dynamics remains challenging. 
To tackle this problem, we introduce Differentiable Hidden State (DHS) enhanced neural ODE, a data-dependent framework that is capable of effectively capturing temporal dependencies and ensuring the continuity of the hidden process. We leverage the theory of generalized inverses to innovatively compute attention mechanism in reverse and obtain a continuous representation. To capture more accurate temporal relationships, we introduce Hoyer metric and maximize the sparsity of it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our model.",ICLR.cc/2025/Conference,2.0,nan,0.8431,our implements neural ordinary differential equations where time constants are themselves functions the input and hidden state enabling the network automatically adjust its processing speed for different temporal patterns this establishes paradigm for temporal neural adaptive dynamics rather than fixed computation steps enabling more efficient modeling multi scale temporal phenomena,the discrete nature current models including rnn based models and transformer variants poses challenges when comes generalizing the continuous time data paradigms which necessary for capturing ongoing dynamics irregular time series neural ordinary differential equations nodes assume continuous latent dynamic and provide elegant for irregular time series analysis tackle this problem differentiable hidden state dhs enhanced neural ode data dependent that capable capturing temporal dependencies and ensuring the continuity the hidden process leverage the theory generalized inverses innovatively compute attention mechanism reverse and obtain continuous representation,2025-08-26T01:56:21.867590
75,**Tensor Product Transformers: Higher-Order Interactions with Multiplicative Attention**,"Standard Transformer architectures rely on additive attention mechanisms that struggle to model higher-order interactions between tokens. We introduce Tensor Product Transformers (TPTs), a novel architecture that represents token relationships through explicit tensor products, enabling multiplicative interactions that capture complex dependencies. Our approach reformulates self-attention using tensor contractions between query, key, and value representations, creating a natural framework for modeling higher-order token relationships. The key innovation is a factorized parameterization scheme that makes higher-order attention computationally tractable while preserving expressivity. By developing specialized initialization techniques and implementing efficient tensor contraction algorithms, TPTs overcome the computational challenges typically associated with higher-order models. Extensive experiments across language modeling, multi-relational graph learning, and compositional reasoning tasks demonstrate that TPTs outperform standard Transformers by 8-15% on complex compositional tasks while using comparable parameter counts. Analysis reveals that TPTs naturally discover interpretable compositional patterns without explicit supervision. This work establishes a principled extension to the Transformer architecture that significantly enhances its ability to model complex compositional relationships, opening new possibilities for tasks requiring sophisticated relational reasoning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2287,Learning positional encodings in transformers depends on initialization,"The attention mechanism is central to the transformer's ability to capture complex dependencies between tokens of an input sequence.
Key to the successful application of the attention mechanism in transformers is its choice of positional encoding (PE).
The PE provides essential information that distinguishes the position and order amongst tokens in a sequence.
Most prior investigations of PE effects on generalization were tailored to 1D input sequences, such as those presented in natural language, where adjacent tokens (e.g., words) are highly related.
In contrast, many real world tasks involve datasets with highly non-trivial positional arrangements, such as datasets organized in multiple spatial dimensions, or datasets for which ground truth positions are not known, such as in biological data.
Here we study the importance of learning accurate PE for problems which rely on a non-trivial arrangement of input tokens. 
Critically, we find that the choice of initialization of a learnable PE greatly influences its ability to learn accurate PEs that lead to enhanced generalization.
We empirically demonstrate our findings in a 2D relational reasoning task and a real world 3D neuroscience dataset, applying interpretability analyses to verify the learning of accurate PEs.
Overall, we find that a learned PE initialized from a small-norm distribution can 1) uncover interpretable PEs that mirror ground truth positions, 2) learn non-trivial and modular PEs in a real-world neuroscience dataset, and 3) lead to improved downstream generalization in both datasets.
Importantly, choosing an ill-suited PE can be detrimental to both model interpretability and generalization.
Together, our results illustrate the feasibility of learning identifiable and interpretable PEs for enhanced generalization.",ICLR.cc/2025/Conference,5.25,False,0.8636,standard transformer architectures rely additive attention mechanisms that struggle higher order interactions between tokens the key innovation factorized parameterization scheme that makes higher order attention computationally tractable while preserving expressivity extensive experiments across language modeling multi relational graph learning and compositional reasoning tasks that tpts outperform standard transformers complex compositional tasks while comparable parameter counts this establishes principled extension the transformer that enhances its ability complex compositional relationships opening possibilities for tasks requiring sophisticated relational reasoning,the attention mechanism central the transformer ability capture complex dependencies between tokens input sequence key the successful application the attention mechanism transformers its choice positional encoding here the importance learning accurate for problems which rely non trivial arrangement input tokens empirically our relational reasoning task and real world neuroscience applying interpretability analyses verify the learning accurate pes importantly choosing ill suited can detrimental both interpretability and generalization together our illustrate the feasibility learning identifiable and interpretable pes for enhanced generalization,2025-08-26T01:56:21.867592
76,**Differentiable Program Induction: Learning Architectural Patterns as Executable Code**,"Neural networks excel at pattern recognition but struggle with explicit algorithmic reasoning required for many tasks. We present Differentiable Program Induction (DPI), a novel framework that learns to discover and implement algorithmic patterns as interpretable program structures. Our approach bridges neural and symbolic computation by representing network components as differentiable program primitives that can be composed into executable algorithmic modules. The key innovation is a neurosymbolic architecture that learns to induce explicit programs from examples while maintaining end-to-end differentiability through carefully designed relaxations of discrete program constructs. By incorporating domain-specific languages and implementing specialized gradient estimators for program induction, DPI enables discovery of interpretable algorithmic patterns from data. Experiments on sequence manipulation, algorithmic reasoning, and structured prediction demonstrate that DPI achieves performance comparable to black-box neural networks while producing explicit programs that generalize perfectly to unseen inputs following the same pattern. Notably, the induced programs can be extracted, verified, and integrated into traditional software systems. This work establishes a new paradigm for neural architecture design where learned computational patterns are represented as explicit algorithmic structures rather than implicit weight matrices, enabling better interpretability and systematic generalization.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3053,Discrete Neural Algorithmic Reasoning,"Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classic algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classic computations are not affected by distributional shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. To achieve that, we separate discrete and continuous data flows and describe the interaction between them. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on multiple algorithmic problems and get perfect test scores both in single-task and multitask setups. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.",ICLR.cc/2025/Conference,5.4,False,0.8380,neural networks excel pattern recognition but struggle explicit algorithmic reasoning required for many tasks our bridges neural and symbolic computation representing network components differentiable program primitives that can composed into executable algorithmic modules experiments sequence manipulation algorithmic reasoning and structured prediction that dpi achieves comparable black box neural networks while producing explicit programs that generalize perfectly unseen inputs following the same pattern this establishes paradigm for neural where learned computational patterns are represented explicit algorithmic structures rather than implicit weight matrices enabling better interpretability and systematic generalization,neural algorithmic reasoning aims capture computations neural networks learning the models imitate the execution classic algorithms while common architectures are expressive enough contain the correct the weights space current neural reasoners are struggling generalize well out distribution data this force neural reasoners maintain the execution trajectory combination finite predefined states,2025-08-26T01:56:21.867594
77,**Hierarchical Latent Trees: Neural Architectures with Explicit Probabilistic Structure**,"Deep neural networks typically employ flat, unstructured latent representations that fail to capture hierarchical relationships inherent in many domains. We introduce Hierarchical Latent Tree Networks (HLTNs), a novel architecture that organizes latent variables in tree-structured probabilistic graphical models learned jointly with neural feature extractors. Our approach combines the representational power of deep networks with the interpretable structure and statistical properties of hierarchical latent variable models. The key innovation is a differentiable tree structure learning algorithm that discovers optimal hierarchical decompositions while maintaining tractable inference through specialized message-passing operations. By implementing stochastic variational inference techniques tailored for tree structures and developing regularization methods that encourage meaningful hierarchies, HLTNs overcome the challenges of jointly learning neural representations and structured probabilistic models. Experiments on image classification, hierarchical clustering, and multi-label prediction demonstrate that HLTNs outperform both standard deep networks and traditional probabilistic models by 5-12% while providing interpretable hierarchical explanations for predictions. Analysis reveals that HLTNs discover meaningful conceptual hierarchies aligned with human intuition without explicit supervision. This work establishes a new approach to neural architecture design that explicitly incorporates hierarchical probabilistic structure, enabling more interpretable and statistically principled representation learning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,7862,From Logits to Hierarchies: Hierarchical Clustering made Simple,"The structure of many real-world datasets is intrinsically hierarchical, making the modeling of such hierarchies a critical objective in both unsupervised and supervised machine learning. Recently, novel approaches for hierarchical clustering with deep architectures have been proposed. In this work, we take a critical perspective on this line of research and demonstrate that many approaches exhibit major limitations when applied to realistic datasets, partly due to their high computational complexity. In particular, we show that a lightweight procedure implemented on top of pre-trained non-hierarchical clustering models outperforms models designed specifically for hierarchical clustering. Our proposed approach is computationally efficient and applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our findings, we illustrate how our method can also be applied in a supervised setup, recovering meaningful hierarchies from a pre-trained ImageNet classifier.",ICLR.cc/2025/Conference,3.8,False,0.8345,deep neural networks employ flat unstructured latent representations that fail capture hierarchical relationships inherent many domains hierarchical latent tree networks hltns that organizes latent variables tree structured probabilistic graphical models learned jointly neural feature extractors our combines the representational power deep networks the interpretable structure and statistical properties hierarchical latent variable models the key innovation differentiable tree structure learning that discovers optimal hierarchical decompositions while maintaining tractable inference specialized message passing operations implementing stochastic variational inference techniques tailored for tree structures and developing regularization methods that encourage meaningful hierarchies hltns overcome the challenges jointly learning neural representations and structured probabilistic models experiments image classification hierarchical clustering and multi label prediction that hltns outperform both standard deep networks and traditional probabilistic models while providing interpretable hierarchical explanations for predictions this establishes neural that explicitly incorporates hierarchical probabilistic structure enabling more interpretable and statistically principled representation learning,the structure many real world datasets intrinsically hierarchical making the modeling such hierarchies critical objective both unsupervised and supervised machine learning recently approaches for hierarchical clustering deep architectures have been proposed particular that lightweight procedure implemented top pre trained non hierarchical clustering models outperforms models designed for hierarchical clustering our proposed computationally efficient and applicable any pre trained clustering that outputs logits requiring any fine tuning highlight the generality our illustrate how our can also applied supervised setup recovering meaningful hierarchies from pre trained imagenet classifier,2025-08-26T01:56:21.867597
78,**Dynamic Sparse Training: Neural Architecture Evolution Through Gradient-Guided Connectivity**,"Traditional neural networks use fixed architectural connectivity patterns throughout training, leading to inefficient parameter utilization and suboptimal inductive biases. We introduce Dynamic Sparse Training (DST), a novel approach that evolves network connectivity during training through gradient-guided topology optimization. Our method maintains extreme sparsity (95-99%) while dynamically reallocating connections to form optimal sparse architectures tailored to specific tasks. The key innovation is a connectivity exploration algorithm that iteratively prunes low-impact connections and grows new ones in promising directions identified through gradient information. By developing specialized weight initialization schemes for sparse networks and implementing efficient sparse operations, DST overcomes the training instabilities typically associated with sparse models. Comprehensive experiments across image classification, natural language processing, and reinforcement learning demonstrate that DST consistently outperforms both dense models and static sparse networks by 3-7% in accuracy while using only 1-5% of parameters. Analysis of the evolved connectivity patterns reveals interpretable, task-specific architectures with meaningful structural motifs. This work establishes a new paradigm for neural architecture design where connectivity patterns emerge through dynamic training processes rather than being fixed a priori, enabling discovery of highly efficient and specialized network topologies.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8949,Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency,"The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.
In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. 
The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.
Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.",ICLR.cc/2025/Conference,5.75,True,0.8366,traditional neural networks use fixed architectural connectivity patterns throughout training leading inefficient parameter utilization and suboptimal inductive biases dynamic sparse training dst that evolves network connectivity during training gradient guided topology optimization comprehensive experiments across image classification natural language processing and reinforcement learning that dst consistently outperforms both dense models and static sparse networks while only parameters this establishes paradigm for neural where connectivity patterns emerge dynamic training processes rather than being fixed priori enabling discovery highly efficient and specialized network topologies,the human brain utilizes spikes for information transmission and dynamically reorganizes its network structure boost energy efficiency and cognitive capabilities throughout its lifespan drawing inspiration from this spike based computation spiking neural networks snns have been developed construct event driven models that emulate this efficiency despite these advances deep snns continue suffer from over parameterization during training and inference stark contrast the brain ability self organize this two stage dynamic structure learning for deep snns aimed maintaining effective sparse training from scratch while optimizing compression efficiency this improves the exploration sparse structures training deep snns adapting sparsity dynamically from the point view compression efficiency our experiments that this sparse training not only aligns the current deep snns models but also improves the efficiency compressing sparse snns,2025-08-26T01:56:21.867599
79,**Adaptive Computation Graphs: Conditional Execution Pathways with Learned Decision Boundaries**,"Standard neural networks apply the same computation to all inputs regardless of complexity, leading to inefficient resource utilization. We present Adaptive Computation Graphs (ACGs), a novel architecture that dynamically determines which computational pathways to execute based on input characteristics. Our approach implements a hierarchical decision structure that routes inputs through specialized computational branches, enabling more efficient processing by matching computation to input complexity. The key innovation is a differentiable routing mechanism that learns decision boundaries through end-to-end training, allowing joint optimization of both the routing policy and the specialized computational pathways. By formulating routing decisions as soft, probabilistic choices and implementing a specialized gradient estimation technique, ACGs maintain stable training despite their conditional execution structure. Experiments on image classification, language modeling, and multi-modal tasks demonstrate that ACGs reduce computation by 45-70% compared to static models while maintaining or slightly improving accuracy. Analysis reveals that ACGs learn meaningful decision boundaries that effectively separate inputs based on complexity and required computation. This work establishes a new direction for neural architecture design based on conditional computation and learned execution pathways, enabling more efficient models that adaptively allocate computational resources according to input requirements.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,9734,Kolmogorov-Arnold Networks with Variable Function Basis,"\begin{abstract}
Neural networks exhibit exceptional performance in processing complex data, yet their internal structures remain largely unexplored. The emergence of Kolmogorov-Arnold Networks (KANs) represents a significant departure from traditional Multi-Layer Perceptrons (MLPs). In contrast to MLPs, KANs replace fixed activation functions at nodes (``neurons"") with learnable activation functions on edges (``weights""), enhancing both accuracy and interpretability.
As data evolves, the demand for models that are both flexible and robust minimizing the influence of input data variability continues to grow. Addressing this need, we propose a general framework for KANs utilizing a \underline{\textbf{V}}ariety \underline{\textbf{B}}ernstei\underline{\textbf{n}} Polynomial Function Basis for \underline{\textbf{K}}olmogorov-\underline{\textbf{A}}rnold \underline{\textbf{N}}etworks (VBn-KAN). This framework leverages the Weierstrass approximation theorem to extend function basis within KANs in theory, specifically selecting Bernstein polynomials ($B_n$) for their robustness, assured by the uniform convergence proposition. Additionally, to enhance flexibility, we implement techniques to vary the function basis $B_n$ when handling diverse datasets. Comprehensive experiments across three fields: multivariate time series forecasting, computer vision, and function approximation—demonstrate that our method outperforms conventional approaches and other variants of KANs.
\end{abstract}",ICLR.cc/2025/Conference,2.5,nan,0.8287,standard neural networks apply the same computation all inputs regardless complexity leading inefficient resource utilization our implements hierarchical decision structure that routes inputs specialized computational branches enabling more efficient processing matching computation input complexity the key innovation differentiable routing mechanism that learns decision boundaries end end training allowing joint optimization both the routing policy and the specialized computational pathways experiments image classification language modeling and multi modal tasks that acgs reduce computation compared static models while maintaining slightly improving this establishes direction for neural conditional computation and learned execution pathways enabling more efficient models that adaptively allocate computational resources according input requirements,begin neural networks exhibit exceptional processing complex data yet their internal structures remain largely unexplored contrast mlps kans replace fixed activation functions nodes neurons learnable activation functions edges weights enhancing both and interpretability,2025-08-26T01:56:21.867604
80,**Lie Group Equivariant Message Passing Networks for Geometric Deep Learning**,"Standard graph neural networks struggle to respect continuous symmetries fundamental to physical systems and geometric data. We introduce Lie Group Equivariant Message Passing Networks (LieGMPNs), a mathematically rigorous framework for building neural architectures that respect symmetries defined by arbitrary Lie groups. Our approach reformulates message passing operations within the framework of differential geometry, enabling equivariance to continuous transformations such as rotations, scaling, and more general symmetry groups. The key innovation is a steerable message passing mechanism where features transform according to group representations, guaranteeing that network operations commute with group actions. By developing specialized numerical techniques for computing parallel transport on manifolds and implementing efficient group-theoretic operations, LieGMPNs maintain computational tractability while providing theoretical guarantees on equivariance properties. Experiments on molecular dynamics, physical simulation, and geometric data processing demonstrate that LieGMPNs achieve 20-35% higher sample efficiency than non-equivariant models, with particularly striking improvements on out-of-distribution generalization to unseen transformations. Theoretical analysis establishes universal approximation results for equivariant functions under specific conditions. This work establishes a comprehensive mathematical framework for building neural architectures with guaranteed equivariance to continuous symmetry groups, enabling more efficient learning for problems with known invariances.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8735,Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups,"The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models.",ICLR.cc/2025/Conference,6.5,True,0.8554,standard graph neural networks struggle respect continuous symmetries fundamental physical systems and geometric data lie group equivariant message passing networks liegmpns mathematically rigorous for building neural architectures that respect symmetries defined arbitrary lie groups the key innovation steerable message passing mechanism where features transform according group representations guaranteeing that network operations commute group actions experiments molecular dynamics physical simulation and geometric data processing that liegmpns achieve higher sample efficiency than non equivariant models striking improvements out distribution generalization unseen transformations this establishes comprehensive mathematical for building neural architectures guaranteed equivariance continuous symmetry groups enabling more efficient learning for problems known invariances,the quest for robust and generalizable machine learning models has driven recent interest exploiting symmetries equivariant neural networks the context pde solvers recent works have shown that lie point symmetries can useful inductive bias for physics informed neural networks pinns data and loss augmentation this lie algebra canonicalization lielac that exploits only the action infinitesimal generators the symmetry group circumventing the need for knowledge the full group structure finally showcase lielac efficacy tasks invariant image classification and lie point symmetry equivariant neural pde solvers pre trained models,2025-08-26T01:56:21.867606
81,**Meta-Learned Configuration Spaces: Discovering Optimal Architectural Subspaces Through Gradient-Based Hypernetworks**,"Neural architecture search typically explores vast design spaces with limited structure, leading to inefficient exploration and suboptimal results. We introduce Meta-Learned Configuration Spaces (MLCS), a novel framework that automatically discovers low-dimensional architectural subspaces containing high-performing models. Our approach learns a mapping from a compact latent space to full architectural configurations using a hypernetwork trained through a bi-level optimization procedure. The key innovation is a differentiable architecture parameterization that enables gradient-based discovery of architectural subspaces with theoretically optimal properties. By formulating architecture search as meta-learning of configuration spaces rather than individual architectures, MLCS enables rapid adaptation to new tasks through efficient exploration of the learned subspace. Experiments across computer vision, natural language processing, and reinforcement learning demonstrate that MLCS reduces search time by two orders of magnitude compared to standard methods while consistently finding architectures that outperform manually designed networks by 4-7%. Analysis reveals that the discovered configuration spaces capture meaningful architectural trade-offs and transferable design principles. This work establishes a new paradigm for neural architecture search based on learning structured configuration spaces rather than exhaustive exploration, enabling more efficient discovery of optimal architectures through meta-learning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3148,Powering Neural Architecture Search with Robust Masked Autoencoders,"Neural Architecture Search (NAS) relies heavily on labeled data, which is labor-intensive and time-consuming to obtain. In this paper, we propose a novel NAS method based on an unsupervised paradigm, specifically Masked Autoencoders (MAE), thereby eliminating the need for labeled data during the searching process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) in the unsupervised setting by designing a hierarchical decoder. Through extensive experiments conducted across various search spaces and datasets, we demonstrate the effectiveness and robustness of our method, offering empirical evidence of its superiority over baseline approaches.",ICLR.cc/2025/Conference,4.0,False,0.8359,neural search explores vast spaces limited structure leading inefficient exploration and suboptimal our learns mapping from compact latent space full architectural configurations hypernetwork trained level optimization procedure formulating search meta learning configuration spaces rather than individual architectures mlcs enables rapid adaptation tasks efficient exploration the learned subspace experiments across computer vision natural language processing and reinforcement learning that mlcs reduces search time two orders magnitude compared standard methods while consistently architectures that outperform manually designed networks this establishes paradigm for neural search learning structured configuration spaces rather than exhaustive exploration enabling more efficient discovery optimal architectures meta learning,neural search nas relies heavily labeled data which labor intensive and time consuming obtain this nas unsupervised paradigm masked autoencoders mae thereby eliminating the need for labeled data during the searching process replacing the supervised learning objective image reconstruction task our enables the robust discovery network architectures compromising and generalization ability additionally address the problem collapse encountered the widely used differentiable search darts the unsupervised setting designing hierarchical decoder extensive experiments conducted across various search spaces and datasets the effectiveness and robustness our offering empirical evidence its superiority over approaches,2025-08-26T01:56:21.867610
82,**Graph Neural Networks with Adaptive Frequency Response: Learning in the Spectral Domain**,"Standard graph neural networks operate primarily in the spatial domain, limiting their ability to capture complex spectral properties crucial for many graph-structured problems. We introduce Adaptive Frequency Response Graph Networks (AFR-GNNs), a novel architecture that explicitly models and adapts its response across different graph frequencies. Our approach implements learnable spectral filters that automatically identify and extract relevant frequency components from graph signals, enabling more effective processing of complex structural patterns. The key innovation is a parameterization scheme that allows precise control over the network's frequency response while maintaining spatial localization through specialized spectral convolutional operations. By developing efficient numerical techniques for spectral filtering and implementing stabilization methods for eigendecomposition, AFR-GNNs overcome the computational challenges typically associated with spectral graph approaches. Experiments on graph classification, community detection, and graph signal processing demonstrate that AFR-GNNs outperform spatial GNNs by 10-18% on tasks with specific spectral characteristics. Analysis of the learned filters reveals interpretable patterns corresponding to structural properties like clustering coefficients and path lengths. This work establishes a new approach to graph neural network design focused on adaptive frequency response, enabling more effective modeling of complex spectral properties in graph-structured data.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8770,Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective,"Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based learning tasks. While their performance is often attributed to the powerful neighborhood aggregation mechanism, recent studies suggest that other components such as non-linear layers may also significantly affecting how GNNs process the input graph data in the spectral domain. Such evidence challenges the prevalent opinion that neighborhood aggregation mechanisms dominate the behavioral characteristics of GNNs in the spectral domain. To demystify such a conflict, this paper introduces a comprehensive benchmark to measure and evaluate GNNs' capability in capturing and leveraging the information encoded in different frequency components of the input graph data. Specifically, we first conduct an exploratory study demonstrating that GNNs can flexibly yield outputs with diverse frequency components even when certain frequencies are absent or filtered out from the input graph data. We then formulate a novel research problem of measuring and benchmarking the performance of GNNs from a spectral perspective. To take an initial step towards a comprehensive benchmark, we design an evaluation protocol supported by comprehensive theoretical analysis. Finally, we introduce a comprehensive benchmark on real-world datasets, revealing insights that challenge prevalent opinions from a spectral perspective. We believe that our findings will open new avenues for future advancements in this area. Our implementations can be found at: https://github.com/yushundong/Spectral-benchmark.",ICLR.cc/2025/Conference,5.25,True,0.8863,standard graph neural networks operate primarily the spatial domain limiting their ability capture complex spectral properties crucial for many graph structured problems our implements learnable spectral filters that automatically identify and extract relevant frequency components from graph signals enabling more effective processing complex structural patterns experiments graph classification community detection and graph signal processing that afr gnns outperform spatial gnns tasks specific spectral characteristics analysis the learned filters reveals interpretable patterns corresponding structural properties like clustering coefficients and path lengths this establishes graph neural network focused adaptive frequency response enabling more effective modeling complex spectral properties graph structured data,graph neural networks gnns have achieved remarkable success various graph based learning tasks while their often attributed the powerful neighborhood aggregation mechanism recent studies suggest that other components such non linear layers may also affecting how gnns process the input graph data the spectral domain such evidence challenges the prevalent opinion that neighborhood aggregation mechanisms dominate the behavioral characteristics gnns the spectral domain,2025-08-26T01:56:21.867613
83,**Neural Circuit Architectures: Biologically Plausible Computation with Dendritic Processing**,"Standard neural networks employ simplified neuron models that ignore the computational capabilities of biological neural circuits. We present Neural Circuit Architectures (NCAs), a biologically plausible framework that incorporates dendritic computation and circuit motifs inspired by cortical microcircuits. Our approach models neurons as multi-compartment units with nonlinear dendritic processing, enabling complex computations within individual cells rather than just between them. The key innovation is a differentiable dendritic compartment model that supports various forms of coincidence detection, sequence sensitivity, and logical operations while maintaining end-to-end trainability. By implementing specialized circuit motifs including lateral inhibition, feedforward inhibition, and disinhibitory circuits, NCAs capture computational principles observed in biological neural systems. Experiments on perceptual tasks, temporal sequence learning, and logical reasoning demonstrate that NCAs achieve comparable performance to standard architectures with 30-50% fewer parameters and improved robustness to noise. Analysis reveals that NCAs develop specialized dendritic compartments that implement computational primitives like coincidence detection and feature binding, similar to those observed in biological neurons. This work establishes a new biologically inspired paradigm for neural architecture design that incorporates dendritic computation and circuit-level interactions, bridging the gap between artificial neural networks and biological neural circuits.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1749,Unraveling Neural Cellular Automata for Lightweight Image Compression,"Neural Cellular Automata (NCA) are computational models inspired by cellular growth, capable of learning complex behaviors through local interactions. While NCAs have been applied to various tasks like image restoration and synthesis, their potential for image compression remains largely unexplored. This paper aims to unravel the capabilities of NCAs for lightweight image compression by introducing a Grid Neural Cellular Automata (GNCA) training strategy. Unlike traditional methods that depend on large deep learning models, NCAs offer a low-cost compact and highly parallelizable alternative with intrinsic robustness to noise. Through experiments on the COCO 2017 dataset, we compare the compression performance of NCAs against JPEG, JPEG-2000 and WebP, using the metrics  PSNR, SSIM, and MSE and Compression Rate. Our results demonstrate that NCAs achieve competitive compression rates and image quality reconstruction, highlighting their potential as a lightweight solution for efficient image compression. The code will be available upon acceptance.",ICLR.cc/2025/Conference,3.4,False,0.8277,standard neural networks employ simplified neuron models that ignore the computational capabilities biological neural circuits neural circuit architectures ncas biologically plausible that incorporates dendritic computation and circuit motifs inspired cortical microcircuits implementing specialized circuit motifs including lateral inhibition feedforward inhibition and disinhibitory circuits ncas capture computational principles observed biological neural systems experiments perceptual tasks temporal sequence learning and logical reasoning that ncas achieve comparable standard architectures fewer parameters and improved robustness noise analysis reveals that ncas specialized dendritic compartments that computational primitives like coincidence detection and feature binding similar those observed biological neurons this establishes biologically inspired paradigm for neural that incorporates dendritic computation and circuit level interactions bridging the gap between artificial neural networks and biological neural circuits,neural cellular automata nca are computational models inspired cellular growth capable learning complex behaviors local interactions this aims unravel the capabilities ncas for lightweight image compression introducing grid neural cellular automata gnca training strategy unlike traditional methods that depend large deep learning models ncas offer low cost compact and highly parallelizable alternative intrinsic robustness noise,2025-08-26T01:56:21.867616
84,**Generalized Polytope Attention: Geometry-Aware Attention Mechanisms with Linear Complexity**,"Traditional attention mechanisms scale quadratically with sequence length, creating a fundamental bottleneck for long sequences. We introduce Generalized Polytope Attention (GPA), a novel attention framework that achieves linear complexity while preserving expressivity through a geometric reformulation of attention operations. Our approach represents attention patterns as high-dimensional polytopes with learnable vertices, enabling compact parameterization of complex attention matrices through geometric operations. The key innovation is a differentiable polytope interpolation mechanism that computes attention weights through barycentric coordinates relative to learned polytope vertices, reducing complexity from O(n²) to O(n) without sacrificing representational power. By developing specialized initialization schemes for polytope vertices and implementing efficient algorithms for polytope operations, GPA overcomes the computational challenges typically associated with geometric approaches. Extensive experiments on language modeling, document classification, and long-sequence understanding demonstrate that GPA achieves performance comparable to standard attention while scaling efficiently to sequences 10-100× longer. Theoretical analysis establishes universal approximation guarantees for attention matrices under specific conditions. This work introduces a fundamentally new geometric perspective on attention mechanisms that enables linear scaling without the limitations of previous efficient attention approaches, opening new possibilities for processing extremely long sequences.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,9893,PolaFormer: Polarity-aware Linear Attention for Vision Transformers,"Linear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation lead to significant information loss compared to the original query-key dot products, resulting in less discriminative attention maps with higher entropy. To address the missing interactions driven by negative values in query-key pairs, we propose a polarity-aware linear attention mechanism that explicitly models both same-signed and opposite-signed query-key interactions, ensuring comprehensive coverage of relational information. Furthermore, to restore the spiky properties of attention maps, we provide a theoretical analysis proving the existence of a class of element-wise functions (with positive first and second derivatives) that can reduce entropy in the attention distribution. For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated. Extensive experiments demonstrate that the proposed PolaFormer improves performance on various vision tasks, enhancing both expressiveness and efficiency by up to 4.6%.",ICLR.cc/2025/Conference,7.0,True,0.8455,traditional attention mechanisms scale quadratically sequence length creating fundamental bottleneck for long sequences generalized polytope attention gpa attention that achieves linear complexity while preserving expressivity geometric reformulation attention operations our represents attention patterns high dimensional polytopes learnable vertices enabling compact parameterization complex attention matrices geometric operations the key innovation differentiable polytope interpolation mechanism that computes attention weights barycentric coordinates relative learned polytope vertices reducing complexity from sacrificing representational power extensive experiments language modeling document classification and long sequence understanding that gpa achieves comparable standard attention while scaling sequences longer theoretical analysis establishes universal approximation guarantees for attention matrices under specific conditions this introduces fundamentally geometric perspective attention mechanisms that enables linear scaling the limitations previous efficient attention approaches opening possibilities for processing extremely long sequences,linear attention has emerged promising alternative softmax based attention leveraging kernelized feature maps reduce complexity from quadratic linear sequence length however the non negative constraint feature maps and the relaxed exponential function used approximation lead significant information loss compared the original query key dot products resulting less discriminative attention maps higher entropy address the missing interactions driven negative values query key pairs polarity aware linear attention mechanism that explicitly models both same signed and opposite signed query key interactions ensuring comprehensive coverage relational information furthermore restore the spiky properties attention maps provide theoretical analysis proving the existence class element wise functions positive first and second derivatives that can reduce entropy the attention distribution for simplicity and recognizing the distinct contributions each dimension employ learnable power function for rescaling allowing strong and weak attention signals separated extensive experiments that the proposed polaformer improves various vision tasks enhancing both expressiveness and efficiency,2025-08-26T01:56:21.867620
85,**Mutual Information Neural Estimation: Architecture-Based Bounds for Representation Learning**,"Deep representation learning lacks principled objectives that quantify the information captured in learned features. We introduce Mutual Information Neural Estimation (MINE) architectures, a theoretically grounded framework for building neural networks that optimize information-theoretic objectives with provable guarantees. Our approach implements variational estimators of mutual information as core architectural components, enabling networks to explicitly maximize the relevant information extracted from inputs. The key innovation is a specialized neural architecture that provides provable lower bounds on mutual information while remaining fully differentiable, allowing end-to-end training with theoretical guarantees. By developing variance reduction techniques for mutual information estimation and implementing specialized training procedures that ensure bound validity, MINE architectures overcome the estimation challenges that have hindered previous information-theoretic approaches. Experiments on representation learning, disentanglement, and self-supervised learning demonstrate that MINE architectures consistently outperform heuristic objectives by 12-20% while providing theoretical guarantees on the information content of learned representations. Analysis reveals that maximizing provable information bounds leads to qualitatively different representations than standard approaches. This work establishes a rigorous information-theoretic foundation for neural architecture design, enabling representation learning with provable guarantees on the information captured in network features.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,3958,Latent Point Collapse Induces an Information Bottleneck in Deep Neural Network Classifiers,"The information-bottleneck principle suggests that the foundation of learning lies in the ability to create compact representations. In machine learning, this goal can be formulated as a Lagrangian optimization problem, where the mutual information between the input and latent representations must be minimized without compromising the correctness of the model's predictions.
Unfortunately, mutual information is difficult to compute in deterministic deep neural network classifiers, which greatly limits the application of this approach to challenging scenarios. In this paper, we tackle this problem from a different perspective that does not involve direct computation of the mutual information. We develop a method that induces the collapse of latent representations belonging to the same class into a single point. 
This point collapse not only significantly reduces the entropy of the latent distribution, thereby creating an information bottleneck that correlates with improved generalization, but also makes the network Lipschitz, offering guarantees for enhanced robustness.
Our method is straightforward to implement. We demonstrate that it substantially improves the network's robustness, provides a small yet statistically significant increase in generalization, and enhances the network's ability to detect misclassifications.",ICLR.cc/2025/Conference,4.25,False,0.8292,deep representation learning lacks principled objectives that quantify the information captured learned features mutual information neural estimation mine architectures theoretically grounded for building neural networks that optimize information theoretic objectives provable guarantees the key innovation specialized neural that provides provable lower bounds mutual information while remaining fully differentiable allowing end end training theoretical guarantees experiments representation learning disentanglement and self supervised learning that mine architectures consistently outperform heuristic objectives while providing theoretical guarantees the information content learned representations this establishes rigorous information theoretic foundation for neural enabling representation learning provable guarantees the information captured network features,the information bottleneck principle suggests that the foundation learning lies the ability create compact representations machine learning this goal can formulated lagrangian optimization problem where the mutual information between the input and latent representations must minimized compromising the correctness the model predictions unfortunately mutual information difficult compute deterministic deep neural network classifiers which greatly limits the application this challenging scenarios this point collapse not only reduces the entropy the latent distribution thereby creating information bottleneck that correlates improved generalization but also makes the network lipschitz offering guarantees for enhanced robustness,2025-08-26T01:56:21.867623
86,**Probabilistic Circuit Transformers: Tractable Uncertainty Estimation with Attention Mechanisms**,"Transformer architectures provide point estimates without principled uncertainty quantification, limiting their reliability in critical applications. We introduce Probabilistic Circuit Transformers (PCTs), a novel architecture that combines the expressivity of transformer attention with the tractable uncertainty estimation of probabilistic circuits. Our approach implements attention mechanisms within the framework of sum-product networks, enabling exact computation of probabilities and moments without sacrificing the modeling power of self-attention. The key innovation is a specialized parameterization that ensures the tractability of probabilistic operations while maintaining the flexibility of attention-based architectures. By developing techniques for conditioning probabilistic circuits on attention outputs and implementing efficient inference algorithms, PCTs overcome the challenges of integrating these disparate computational paradigms. Experiments on classification with rejection, out-of-distribution detection, and probabilistic sequence modeling demonstrate that PCTs provide well-calibrated uncertainty estimates that outperform both standard transformers and Bayesian deep learning approaches by 15-25% in calibration metrics. Notably, PCTs support exact marginalization and conditioning operations crucial for probabilistic reasoning. This work establishes a new direction for transformer architecture design that provides rigorous uncertainty quantification through tractable probabilistic modeling, enabling more reliable deployment in safety-critical applications.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6378,Selective Induction Heads: How Transformers Select Causal Structures in Context,"Transformers have exhibited exceptional capabilities in sequence modelling tasks, leveraging self-attention and in-context learning. Critical to this success are induction heads, attention circuits that enable copying tokens based on their previous occurrences. In this work, we introduce a novel synthetic framework designed to enable the theoretical analysis of transformers’ ability to dynamically handle causal structures. Existing works rely on Markov Chains to study the formation of induction heads, revealing how transformers capture causal dependencies and learn transition probabilities in-context. However, they rely on a fixed causal structure that fails to capture the complexity of natural languages, where the relationship between tokens dynamically changes with context.  To this end, our framework varies the causal structure through interleaved Markov chains with different lags while keeping the transition probabilities fixed. This setting unveils the formation of Selective Induction Heads, a new circuit that endows transformers with the ability to select the correct causal structure in-context. We empirically demonstrate that attention-only transformers learn this mechanism to predict the next token by identifying the correct lag and copying the corresponding token from the past. We provide a detailed construction of a 3-layer transformer to implement the selective induction head, and a theoretical analysis proving that this mechanism asymptotically converges to the maximum likelihood solution. Our findings advance the theoretical understanding of how transformers select causal structures, providing new insights into their functioning and interpretability.",ICLR.cc/2025/Conference,6.2,True,0.8328,transformer architectures provide point estimates principled uncertainty quantification limiting their reliability critical applications probabilistic circuit transformers pcts that combines the expressivity transformer attention the tractable uncertainty estimation probabilistic circuits our implements attention mechanisms within the sum product networks enabling exact computation probabilities and moments sacrificing the modeling power self attention developing techniques for conditioning probabilistic circuits attention outputs and implementing efficient inference algorithms pcts overcome the challenges integrating these disparate computational paradigms experiments classification rejection out distribution detection and probabilistic sequence modeling that pcts provide well calibrated uncertainty estimates that outperform both standard transformers and bayesian deep learning approaches calibration metrics notably pcts support exact marginalization and conditioning operations crucial for probabilistic reasoning this establishes direction for transformer that provides rigorous uncertainty quantification tractable probabilistic modeling enabling more reliable deployment safety critical applications,transformers have exhibited exceptional capabilities sequence modelling tasks leveraging self attention and context learning critical this success are induction heads attention circuits that enable copying tokens their previous occurrences provide detailed construction layer transformer the selective induction head and theoretical analysis proving that this mechanism asymptotically converges the maximum likelihood solution our advance the theoretical understanding how transformers select causal structures providing insights into their functioning and interpretability,2025-08-26T01:56:21.867625
87,**Functional Neural Architecture Search: Learning Architectural Priors through Function Space Optimization**,"Current neural architecture search methods focus on structural configuration without explicitly considering the functional properties of the resulting networks. We introduce Functional Neural Architecture Search (FNAS), a novel framework that explores architecture space by directly optimizing the functional properties of candidate networks. Our approach represents architectures as points in a function space and guides search using functional priors such as smoothness, stability, and complexity. The key innovation is a differentiable mapping between architectural choices and functional properties that enables direct optimization of architectures based on their behavior rather than just structure. By implementing a Wasserstein gradient flow in function space and developing efficient functional property estimators, FNAS enables principled exploration guided by functional considerations. Experiments across computer vision, time series analysis, and control tasks demonstrate that FNAS discovers architectures that outperform structure-based search methods by 5-9% while showing dramatically improved robustness and generalization properties. Analysis reveals that functionally-optimized architectures exhibit systematically different inductive biases aligned with task requirements. This work establishes a fundamentally new perspective on neural architecture search based on functional optimization rather than structural exploration, enabling the discovery of architectures with specific behavioral properties required for reliable deployment.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1316,POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator,"Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.
Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.
This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.
Results were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.",ICLR.cc/2025/Conference,4.4,False,0.8305,current neural search methods focus structural configuration explicitly considering the functional properties the resulting networks functional neural search fnas that explores space directly optimizing the functional properties candidate networks the key innovation differentiable mapping between architectural choices and functional properties that enables direct optimization architectures their behavior rather than just structure experiments across computer vision time series analysis and control tasks that fnas discovers architectures that outperform structure based search methods while showing dramatically improved robustness and generalization properties this establishes fundamentally perspective neural search functional optimization rather than structural exploration enabling the discovery architectures specific behavioral properties required for reliable deployment,neural search nas automates the neural network architectures minimising dependence human expertise and iterative experimentation recently transferable neural search transferable nas has emerged generalising the search process from being dataset dependent task dependent this diffusion based streamlines computation generating architectures optimised for unseen datasets the need for further adaptation this introduces the pareto optimal many objective neural generator pomonag extending diffusionnag many objective diffusion process pomonag optimisation enhanced expanding its training meta dataset applying pareto front filtering generated architectures and refining embeddings for conditional generation were validated two distinct search spaces nasbench201 and mobilenetv3 and evaluated across image classification datasets,2025-08-26T01:56:21.867626
88,**Compositional Program Synthesis Networks: Learning to Assemble Neural Modules**,"Modular neural networks promise compositional generalization but rely on predefined decompositions that limit their flexibility. We present Compositional Program Synthesis Networks (CPSNs), a novel architecture that learns to synthesize neural programs by dynamically composing primitive modules into complex computational graphs. Our approach represents computation as programs in a differentiable domain-specific language, enabling learned composition of neural components guided by input characteristics. The key innovation is a program synthesis mechanism that generates computational graphs through hierarchical, recursive application of learned composition rules while maintaining end-to-end differentiability. By implementing specialized gradient estimation techniques for discrete program construction and developing regularization methods that encourage reusable components, CPSNs overcome the challenges of learning compositional structure from data. Experiments on visual reasoning, algorithmic tasks, and multi-step problem solving demonstrate that CPSNs outperform monolithic architectures by 15-30% on compositional generalization benchmarks while providing interpretable execution traces. Analysis reveals that CPSNs discover meaningful abstractions that enable systematic recombination of learned skills. This work establishes a new paradigm for neural architecture design based on learned program synthesis rather than fixed computational graphs, enabling more flexible compositional generalization through dynamic assembly of neural modules.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,6149,COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural Networks Feedback Control for Program Synthesis,"Program synthesis methods, whether formal or neural-based, lack fine-grained control and flexible modularity, which limits their adaptation to complex software development. These limitations stem from rigid Domain-Specific Language (DSL) frameworks and neural network incorrect predictions. To this end, we propose the \textbf{Chain of Logic (CoL)}, which organizes synthesis stages into a chain and provides precise heuristic control to guide the synthesis process. Furthermore, by integrating neural networks with libraries and introducing a \textbf{Neural Network Feedback Control (NNFC)} mechanism, our approach modularizes synthesis and mitigates the impact of neural network mispredictions. Experiments on relational and symbolic synthesis tasks show that CoL significantly enhances the efficiency and reliability of DSL program synthesis across multiple metrics. Specifically, CoL improves accuracy by 70\% while reducing tree operations by 91\% and time by 95\%. Additionally, NNFC further boosts accuracy by 6\%, with a 64\% reduction in tree operations under challenging conditions such as insufficient training
data, increased difficulty, and multidomain synthesis. These improvements confirm COOL as a highly efficient and reliable program synthesis framework.",ICLR.cc/2025/Conference,2.5,nan,0.8416,modular neural networks promise compositional generalization but rely predefined decompositions that limit their flexibility compositional program synthesis networks cpsns that learns synthesize neural programs dynamically composing primitive modules into complex computational graphs our represents computation programs differentiable domain specific language enabling learned composition neural components guided input characteristics implementing specialized gradient estimation techniques for discrete program construction and developing regularization methods that encourage reusable components cpsns overcome the challenges learning compositional structure from data this establishes paradigm for neural learned program synthesis rather than fixed computational graphs enabling more flexible compositional generalization dynamic assembly neural modules,program synthesis methods whether formal neural based lack fine grained control and flexible modularity which limits their adaptation complex software development these limitations stem from rigid domain specific language dsl frameworks and neural network incorrect predictions furthermore integrating neural networks libraries and introducing textbf neural network feedback control nnfc mechanism our modularizes synthesis and mitigates the impact neural network mispredictions,2025-08-26T01:56:21.867633
89,**Energy-Based Neural Architectures: Implicit Layer Models with Stability Guarantees**,"Conventional neural networks use explicit layer transformations that can limit their expressivity and stability. We introduce Energy-Based Neural Architectures (EBNAs), a novel framework that defines layers implicitly through energy minimization rather than explicit transformations. Our approach represents each layer as the solution to an optimization problem parameterized by learnable energy functions, enabling more flexible computation with theoretical stability guarantees. The key innovation is a differentiable fixed-point layer that efficiently solves the energy minimization problem during both forward and backward passes, maintaining end-to-end trainability despite the implicit definition. By implementing specialized energy parameterizations and developing efficient optimization algorithms with provable convergence, EBNAs overcome the computational challenges typically associated with implicit layers. Experiments across computer vision, adversarial robustness, and inverse problems demonstrate that EBNAs outperform explicit architectures by 8-15% on complex tasks while providing formal guarantees on stability and well-posedness. Theoretical analysis establishes connections to contractive mappings and provides bounds on Lipschitz constants. This work introduces a fundamentally new perspective on neural architecture design based on energy minimization rather than explicit transformations, enabling networks with provable stability properties and enhanced expressivity through implicit computation.",ICLR,neural architectures,claude-3-7-sonnet-latest,False,,Equivariant Polynomial Functional Networks,"Neural Functional Networks (NFNs) have gained increasing interest due to their wide range of applications, including extracting information from implicit representations of data, editing network weights, and evaluating policies. A key design principle of NFNs is their adherence to the permutation and scaling symmetries inherent in the connectionist structure of the input neural networks.  Recent NFNs have been proposed with permutation and scaling equivariance based on either graph-based message-passing mechanisms or parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from high memory consumption and long running times. On the other hand, parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower memory consumption and faster running time, yet their expressivity is limited due to the large size of the symmetric group of the input neural networks. The challenge of designing a permutation and scaling equivariant NFN that maintains low memory consumption and running time while preserving expressivity remains unresolved. In this paper, we propose a novel solution with the development of MAGEP-NFN (**M**onomial m**A**trix **G**roup **E**quivariant **P**olynomial **NFN**). Our approach follows the parameter-sharing mechanism but differs from previous works by constructing a nonlinear equivariant layer represented as a polynomial in the input weights. This polynomial formulation enables us to incorporate additional relationships between weights from different input hidden layers, enhancing the model's expressivity while keeping memory consumption and running time low, thereby addressing the aforementioned challenge. We provide empirical evidence demonstrating that MAGEP-NFN achieves competitive performance and efficiency compared to existing baselines.",ICLR.cc/2025/Conference,5.5,False,0.7961,conventional neural networks use explicit layer transformations that can limit their expressivity and stability energy based neural architectures ebnas that defines layers implicitly energy minimization rather than explicit transformations our represents each layer the solution optimization problem parameterized learnable energy functions enabling more flexible computation theoretical stability guarantees implementing specialized energy parameterizations and developing efficient optimization algorithms provable convergence ebnas overcome the computational challenges associated implicit layers this introduces fundamentally perspective neural energy minimization rather than explicit transformations enabling networks provable stability properties and enhanced expressivity implicit computation,neural functional networks nfns have gained increasing interest due their wide range applications including extracting information from implicit representations data editing network weights and evaluating policies key principle nfns their adherence the permutation and scaling symmetries inherent the connectionist structure the input neural networks the other hand parameter sharing based nfns built upon equivariant linear layers exhibit lower memory consumption and faster running time yet their expressivity limited due the large size the symmetric group the input neural networks,2025-08-26T01:56:21.867639
90,**Kronecker-Factored Lattice Transformers: Logarithmic Attention with Provable Quadratic Expressivity**,"The quadratic complexity of attention mechanisms remains a fundamental bottleneck for scaling transformer models to longer sequences. We introduce Kronecker-Factored Lattice Transformers (KFLTs), a novel architecture that reduces attention complexity from O(n²) to O(n log n) while preserving expressivity through a mathematically principled factorization approach. Our method represents attention matrices as Kronecker products of smaller attention lattices operating on structured decompositions of the input sequence. The key innovation is a hierarchical lattice structure that enables efficient computation of global dependencies through logarithmic-depth interaction paths, maintaining the expressivity of full attention through carefully designed cross-lattice connections. By developing specialized initialization schemes and implementing numerically stable factorization algorithms, KFLTs overcome the training instabilities typically associated with factorized approaches. Extensive experiments on language modeling, long document classification, and multimodal sequence tasks demonstrate that KFLTs match or exceed the performance of standard transformers while handling sequences 5-10× longer with the same computational budget. Theoretical analysis proves that KFLTs maintain quadratic expressivity despite their logarithmic computational scaling. This work establishes a new approach to efficient transformer design based on structured Kronecker factorizations, enabling significant advances in sequence length capabilities without sacrificing model expressiveness.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,942,Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data,"Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism.  In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. 
To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation.
We validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.",ICLR.cc/2025/Conference,3.75,False,0.8739,the quadratic complexity attention mechanisms remains fundamental bottleneck for scaling transformer models longer sequences kronecker factored lattice transformers kflts that reduces attention complexity from log while preserving expressivity mathematically principled factorization our represents attention matrices kronecker products smaller attention lattices operating structured decompositions the input sequence the key innovation hierarchical lattice structure that enables efficient computation global dependencies logarithmic depth interaction paths maintaining the expressivity full attention carefully designed cross lattice connections extensive experiments language modeling long document classification and multimodal sequence tasks that kflts match exceed the standard transformers while handling sequences longer the same computational budget this establishes efficient transformer structured kronecker factorizations enabling significant advances sequence length capabilities sacrificing expressiveness,transformers are now ubiquitous for sequence modeling tasks but their extension multi dimensional data remains challenge due the quadratic cost the attention mechanism address the computational challenges associated high order tensor attention kronecker factorized attention mechanism that reduces the attention cost quadratic each axis dimension rather than quadratic the total size the input tensor this strategy maintains the model expressiveness while enabling scalable attention computation the effectiveness hot two high dimensional tasks including multivariate time series forecasting and medical image classification,2025-08-26T01:56:21.867641
91,**Neural Field Hypernetworks: Continuous Representation Spaces for Adaptive Computation**,"Standard neural architectures rely on discrete parameterizations that limit their ability to adapt computation across input distributions. We present Neural Field Hypernetworks (NFHs), a novel architecture that represents entire networks as continuous functions parameterized by input-dependent neural fields. Our approach implements hypernetworks as continuous mappings from compact latent spaces to infinite-dimensional function spaces, enabling more flexible adaptation to input characteristics than discrete architectures. The key innovation is a coordinate-based neural field parameterization that represents weights, activation functions, and architectural connectivity through implicit neural representations with theoretical guarantees on functional capacity. By developing specialized gradient estimators for function-space optimization and implementing efficient neural field parameterizations, NFHs overcome the challenges of optimizing continuous network representations. Experiments on multi-task learning, few-shot adaptation, and transfer learning demonstrate that NFHs achieve 15-25% better sample efficiency than discrete architectures while requiring significantly fewer task-specific parameters. Analysis reveals that NFHs learn smooth, interpretable transformations between related tasks by traversing the continuous architecture space. This work establishes a new paradigm for neural architecture design based on continuous function spaces rather than discrete parameter tensors, enabling more efficient adaptation and transfer across tasks through learned functional transformations.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,4222,Equivariant Polynomial Functional Networks,"Neural Functional Networks (NFNs) have gained increasing interest due to their wide range of applications, including extracting information from implicit representations of data, editing network weights, and evaluating policies. A key design principle of NFNs is their adherence to the permutation and scaling symmetries inherent in the connectionist structure of the input neural networks.  Recent NFNs have been proposed with permutation and scaling equivariance based on either graph-based message-passing mechanisms or parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from high memory consumption and long running times. On the other hand, parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower memory consumption and faster running time, yet their expressivity is limited due to the large size of the symmetric group of the input neural networks. The challenge of designing a permutation and scaling equivariant NFN that maintains low memory consumption and running time while preserving expressivity remains unresolved. In this paper, we propose a novel solution with the development of MAGEP-NFN (**M**onomial m**A**trix **G**roup **E**quivariant **P**olynomial **NFN**). Our approach follows the parameter-sharing mechanism but differs from previous works by constructing a nonlinear equivariant layer represented as a polynomial in the input weights. This polynomial formulation enables us to incorporate additional relationships between weights from different input hidden layers, enhancing the model's expressivity while keeping memory consumption and running time low, thereby addressing the aforementioned challenge. We provide empirical evidence demonstrating that MAGEP-NFN achieves competitive performance and efficiency compared to existing baselines.",ICLR.cc/2025/Conference,5.5,False,0.8499,standard neural architectures rely discrete parameterizations that limit their ability adapt computation across input distributions neural field hypernetworks nfhs that represents entire networks continuous functions parameterized input dependent neural fields our implements hypernetworks continuous mappings from compact latent spaces infinite dimensional function spaces enabling more flexible adaptation input characteristics than discrete architectures the key innovation coordinate based neural field parameterization that represents weights activation functions and architectural connectivity implicit neural representations theoretical guarantees functional capacity developing specialized gradient estimators for function space optimization and implementing efficient neural field parameterizations nfhs overcome the challenges optimizing continuous network representations experiments multi task learning few shot adaptation and transfer learning that nfhs achieve better sample efficiency than discrete architectures while requiring fewer task specific parameters this establishes paradigm for neural continuous function spaces rather than discrete parameter tensors enabling more efficient adaptation and transfer across tasks learned functional transformations,neural functional networks nfns have gained increasing interest due their wide range applications including extracting information from implicit representations data editing network weights and evaluating policies key principle nfns their adherence the permutation and scaling symmetries inherent the connectionist structure the input neural networks the other hand parameter sharing based nfns built upon equivariant linear layers exhibit lower memory consumption and faster running time yet their expressivity limited due the large size the symmetric group the input neural networks,2025-08-26T01:56:21.867643
92,**Differentiable Architecture Compression: Joint Optimization of Computation and Memory Through Structure Distillation**,"Neural architecture deployment on resource-constrained devices requires balancing computational efficiency with memory footprint, yet existing methods optimize these objectives separately. We introduce Differentiable Architecture Compression (DAC), a unified framework that jointly optimizes computational structure and parameter compression through end-to-end differentiable structure distillation. Our approach formulates architecture compression as a constrained optimization problem over both discrete architectural choices and continuous parameter values. The key innovation is a structure distillation mechanism that transfers knowledge from complex teacher architectures to compressed student networks through learned structural transformations rather than just matching outputs. By implementing relaxed architectural choices with straight-through estimators and developing specialized regularization techniques, DAC enables joint optimization of network structure and parameters despite the mixed discrete-continuous nature of the problem. Experiments across computer vision, edge deployment, and mobile applications demonstrate that DAC consistently outperforms traditional compression techniques by 15-30% in the computation-memory tradeoff space. Analysis reveals that DAC discovers non-intuitive architectural patterns specialized for target hardware constraints. This work establishes a new approach to neural architecture optimization that treats computation and memory as jointly optimizable aspects of a unified compression problem, enabling more efficient deployment across diverse hardware platforms.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,2617,A Multi-Decomposition Method for Compressing Larger AI Models Based on Reinforcement Learning,"With the development of modern deep neural network (DNN), the scale of parameters is increasing, making it difficult to deploy models for use on resource-constrained edge devices. To address this issue, model compression is necessary, and using low-rank matrix decomposition to compress DNN models is an effective research approach. However, traditional studies on low-rank decomposition compression typically apply a single matrix decomposition method to each parameter matrix in the neural network, without considering the structural characteristics of each layer in AI models, thus failing to achieve the optimal compression effect. Therefore, this paper proposes, for the first time, a scheme for model compression using multiple decomposition methods, selecting the most suitable decomposition method for each layer in the model. However, to truly implement this approach, it is essential to balance model accuracy and compression cost. To address this, we propose a joint optimization paradigm that simultaneously optimizes model accuracy and compression rate. We also introduce a framework LMFBRL based on reinforcement learning that jointly selects the optimal decomposition method and rank. Tests were conducted on five models such as LeNet-300, ResNet-20, and Vgg-16. Compared to singly using the MF method for compressing the LeNet300 model, our approach has shown an improvement of 3.6% in compression rate and a 1.8% increase in accuracy. The test results validate the effectiveness of the algorithm proposed in this paper.",ICLR.cc/2025/Conference,2.0,nan,0.8517,neural deployment resource constrained devices requires balancing computational efficiency memory footprint yet existing methods optimize these objectives separately our formulates compression constrained optimization problem over both discrete architectural choices and continuous parameter values the key innovation structure distillation mechanism that transfers knowledge from complex teacher architectures compressed student networks learned structural transformations rather than just matching outputs implementing relaxed architectural choices straight through estimators and developing specialized regularization techniques dac enables joint optimization network structure and parameters despite the mixed discrete continuous nature the problem this establishes neural optimization that treats computation and memory jointly optimizable aspects unified compression problem enabling more efficient deployment across diverse hardware platforms,the development modern deep neural network dnn the scale parameters increasing making difficult deploy models for use resource constrained edge devices however traditional studies low rank decomposition compression apply single matrix decomposition each parameter matrix the neural network considering the structural characteristics each layer models thus failing achieve the optimal compression effect address this joint optimization paradigm that simultaneously optimizes and compression rate also lmfbrl reinforcement learning that jointly selects the optimal decomposition and rank,2025-08-26T01:56:21.867644
93,**Cellular Neural Architectures: Topology-Aware Networks with Neighborhood Computing**,"Standard neural networks rely on fixed connectivity patterns that limit their ability to model complex structural relationships in data. We introduce Cellular Neural Architectures (CNAs), a novel framework inspired by cellular automata that implements computation through local neighborhood interactions governed by learnable rules. Our approach represents computation as dynamic processes evolving on grid-like topologies, where each cell updates its state based on learned interaction rules with neighboring cells. The key innovation is a differentiable cellular update mechanism that enables end-to-end training of neighborhood interaction rules while preserving the spatial locality inherent in cellular systems. By developing specialized synchronization mechanisms and implementing efficient parallel update schemes, CNAs overcome the computational challenges typically associated with cellular computation. Experiments on image processing, physical simulation, and emergent pattern modeling demonstrate that CNAs outperform traditional convolutional and graph-based architectures by 10-18% on tasks requiring complex spatial pattern recognition or physical dynamics modeling. Analysis reveals that CNAs discover interpretable local rules that generate complex global behaviors, similar to emergent phenomena in natural systems. This work establishes a new paradigm for neural architecture design based on local interaction rules rather than fixed computational graphs, enabling more effective modeling of complex spatial and temporal patterns.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,1749,Unraveling Neural Cellular Automata for Lightweight Image Compression,"Neural Cellular Automata (NCA) are computational models inspired by cellular growth, capable of learning complex behaviors through local interactions. While NCAs have been applied to various tasks like image restoration and synthesis, their potential for image compression remains largely unexplored. This paper aims to unravel the capabilities of NCAs for lightweight image compression by introducing a Grid Neural Cellular Automata (GNCA) training strategy. Unlike traditional methods that depend on large deep learning models, NCAs offer a low-cost compact and highly parallelizable alternative with intrinsic robustness to noise. Through experiments on the COCO 2017 dataset, we compare the compression performance of NCAs against JPEG, JPEG-2000 and WebP, using the metrics  PSNR, SSIM, and MSE and Compression Rate. Our results demonstrate that NCAs achieve competitive compression rates and image quality reconstruction, highlighting their potential as a lightweight solution for efficient image compression. The code will be available upon acceptance.",ICLR.cc/2025/Conference,3.4,False,0.8833,standard neural networks rely fixed connectivity patterns that limit their ability complex structural relationships data cellular neural architectures cnas inspired cellular automata that implements computation local neighborhood interactions governed learnable rules experiments image processing physical simulation and emergent pattern modeling that cnas outperform traditional convolutional and graph based architectures tasks requiring complex spatial pattern recognition physical dynamics modeling this establishes paradigm for neural local interaction rules rather than fixed computational graphs enabling more effective modeling complex spatial and temporal patterns,neural cellular automata nca are computational models inspired cellular growth capable learning complex behaviors local interactions this aims unravel the capabilities ncas for lightweight image compression introducing grid neural cellular automata gnca training strategy unlike traditional methods that depend large deep learning models ncas offer low cost compact and highly parallelizable alternative intrinsic robustness noise,2025-08-26T01:56:21.867646
94,**Mixture-of-Experts Gating with Meta-Learned Routers: Dynamic Specialization with Optimal Expert Allocation**,"Mixture-of-Experts models face critical challenges with load balancing, expert specialization, and routing collapse that limit their practical effectiveness. We introduce Meta-Learned Router Networks (MLRNs), a novel architecture that addresses fundamental MoE limitations through principled meta-learning of gating functions. Our approach implements a hierarchical routing mechanism where meta-routers learn optimal expert allocation strategies that dynamically adapt to input distributions and task requirements. The key innovation is a bi-level optimization procedure that trains routers to optimize both task performance and information-theoretic expert utilization through specialized meta-objectives. By implementing gradient isolation between experts and routers and developing specialized regularization techniques, MLRNs overcome the instabilities typically associated with expert-based architectures. Experiments on language modeling, multi-task learning, and continual learning demonstrate that MLRNs outperform standard MoE approaches by 8-15% while achieving significantly better expert specialization and load balancing. Analysis reveals that meta-learned routers discover interpretable specialization patterns that align experts with semantically meaningful input categories. This work establishes a principled meta-learning framework for mixture-of-experts architectures that addresses their core limitations through specialized router optimization, enabling more effective conditional computation with stable training dynamics.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8308,Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model,"The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLM encourage different experts to specialize in different tokens, and they usually employ a router to predict the routing of each token. However, the router is not optimized concerning distinct parameter optimization directions generated from tokens within an expert. This may lead to severe interference between tokens within an expert. To address this problem, we propose to use the token-level gradient analysis to Solving Token Gradient Conflict (STGC) in this paper. Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a regularization loss tailored to encourage conflicting tokens routing from their current experts to other experts, for reducing interference between tokens within an expert. Our method can serve as a plug-in for diverse LVLM methods, and extensive experimental results demonstrate its effectiveness. demonstrate its effectiveness. 
The code will be publicly available at https://github.com/longrongyang/STGC.",ICLR.cc/2025/Conference,6.4,True,0.8498,the key innovation level optimization procedure that trains routers optimize both task and information theoretic expert utilization specialized meta objectives experiments language modeling multi task learning and continual learning that mlrns outperform standard moe approaches while achieving better expert specialization and load balancing,the mixture experts moe has gained increasing attention studying large vision language models lvlms however the router not optimized concerning distinct parameter optimization directions generated from tokens within expert,2025-08-26T01:56:21.867650
95,**Tensor Ring Architectures: Circular Decomposition Networks with Global Receptive Fields**,"Capturing long-range dependencies while maintaining parameter efficiency remains a fundamental challenge in deep learning. We introduce Tensor Ring Networks (TRNs), a novel architecture that parameterizes layers through circular tensor decompositions, enabling exponentially more efficient modeling of global interactions. Our approach implements neural operations as tensor ring contractions with circular parameter sharing, creating an implicit receptive field that spans the entire input regardless of layer depth. The key innovation is a stable parameterization of tensor ring weights that ensures well-conditioned training despite the high-order tensor operations involved. By developing specialized initialization schemes and implementing efficient tensor contraction algorithms, TRNs overcome the computational and numerical challenges typically associated with tensor networks. Experiments on sequence modeling, image classification, and relational reasoning demonstrate that TRNs achieve comparable or superior performance to attention-based architectures while using 70-85% fewer parameters and requiring no explicit attention mechanism. Theoretical analysis establishes connections to recurrent architectures and provides expressivity guarantees based on tensor rank properties. This work introduces a fundamentally new approach to architecture design based on circular tensor decompositions, enabling parameter-efficient models with implicit global receptive fields through the mathematical properties of tensor ring contractions.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,11640,Learning Parameter Sharing with Tensor Decompositions and Sparsity,"Large neural networks achieve remarkable performance, but their size hinders deployment on resource-constrained devices. While various compression techniques exist, parameter sharing remains relatively unexplored. This paper introduces Fine-grained Parameter Sharing (FiPS), a novel algorithm that leverages the relationship between parameter sharing, tensor decomposition, and sparsity to efficiently compress large vision transformer models. FiPS employs a shared base and sparse factors to represent shared neurons across multi-layer perception (MLP) modules. Shared parameterization is initialized via Singular Value Decomposition (SVD) and optimized by minimizing block-wise reconstruction error. Experiments demonstrate that FiPS compresses DeiT-B and Swin-L MLPs to 25-40%  of their original parameter count while maintaining accuracy within one percentage point of the original models.",ICLR.cc/2025/Conference,4.75,False,0.8222,capturing long range dependencies while maintaining parameter efficiency remains fundamental challenge deep learning our implements neural operations tensor ring contractions circular parameter sharing creating implicit receptive field that spans the entire input regardless layer depth experiments sequence modeling image classification and relational reasoning that trns achieve comparable superior attention based architectures while fewer parameters and requiring explicit attention mechanism,large neural networks achieve remarkable but their size hinders deployment resource constrained devices this introduces fine grained parameter sharing fips that leverages the relationship between parameter sharing tensor decomposition and sparsity compress large vision transformer models,2025-08-26T01:56:21.867653
96,**Persistent Homology Networks: Topological Feature Extraction with Learnable Filtrations**,"Deep neural networks struggle to identify topological features crucial for understanding the global structure of complex data. We present Persistent Homology Networks (PHNets), a novel architecture that integrates computational topology with deep learning through differentiable persistent homology layers. Our approach implements topological feature extraction as an integral part of the network architecture, enabling direct optimization of topological descriptors through gradient descent. The key innovation is a learnable filtration mechanism that automatically discovers optimal filtration functions for extracting task-relevant topological features from data. By developing efficient differentiable approximations of persistence diagrams and implementing specialized backpropagation algorithms, PHNets overcome the computational challenges of integrating topological data analysis with neural networks. Experiments on graph classification, materials science, and medical imaging demonstrate that PHNets outperform conventional architectures by 12-20% on tasks where global structural properties are decisive. Analysis reveals that learned filtrations automatically identify meaningful structural features without explicit supervision. This work establishes a new paradigm for neural architecture design that directly incorporates topological feature extraction, enabling more effective learning from structurally complex data by capturing global topological invariants that complement traditional local feature processing.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,642,Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity,"Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information.",ICLR.cc/2025/Conference,8.0,True,0.8361,deep neural networks struggle identify topological features crucial for understanding the global structure complex data persistent homology networks phnets that integrates computational topology deep learning differentiable persistent homology layers our implements topological feature extraction integral part the network enabling direct optimization topological descriptors gradient descent developing efficient differentiable approximations persistence diagrams and implementing specialized backpropagation algorithms phnets overcome the computational challenges integrating topological data analysis neural networks this establishes paradigm for neural that directly incorporates topological feature extraction enabling more effective learning from structurally complex data capturing global topological invariants that complement traditional local feature processing,topological deep learning tdl rapidly growing field that seeks leverage topological structure data and facilitate learning from data supported topological objects ranging from molecules shapes,2025-08-26T01:56:21.867658
97,**Intrinsically-Motivated Architectural Growth: Neural Networks that Adaptively Expand their Capacity**,"Fixed neural architectures impose fundamental constraints on model capacity that must be determined before training begins. We introduce Intrinsically-Motivated Architectural Growth (IMAG), a novel framework where networks autonomously expand their capacity based on learning progress and complexity requirements. Our approach implements a principled growth mechanism that incrementally adds architectural capacity guided by information-theoretic complexity measures rather than validation performance. The key innovation is a self-supervised architectural expansion policy that identifies when and where to add capacity based on measures of representation compression, learning progress, and expected information gain. By formulating architectural growth as an intrinsic motivation problem and developing specialized consolidation techniques for new capacity, IMAG maintains stable training despite its dynamically evolving structure. Experiments on supervised learning, reinforcement learning, and continual learning demonstrate that IMAG achieves 10-18% better performance-efficiency tradeoffs than fixed architectures while automatically discovering appropriate model scales for different problems. Theoretical analysis establishes connections to minimum description length principles and provides bounds on growth dynamics. This work establishes a new paradigm for neural architecture design based on intrinsically-motivated capacity expansion, enabling models that automatically grow to appropriate complexity based on problem requirements rather than requiring manual capacity tuning.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,4305,Just How Flexible are Neural Networks in Practice?,"Although overparameterization theory suggests that neural networks can fit any dataset with up to as many samples as they have parameters, practical limitations often prevent them from reaching this capacity. In this study, we empirically investigate the practical flexibility of neural networks and uncover several surprising findings. Firstly, we observe that standard optimizers, such as stochastic gradient descent (SGD), often converge to solutions that fit significantly fewer samples than the model's parameter count, highlighting a gap between theoretical and practical capacity. Secondly, we find that convolutional neural networks (CNNs) are substantially more parameter-efficient than multi-layer perceptrons (MLPs) and Vision Transformers (ViTs), even when trained on randomly labeled data, emphasizing the role of architectural inductive biases. Thirdly, we demonstrate that the difference in a network's ability to fit correctly labeled data versus incorrectly labeled data is a strong predictor of generalization performance, offering a novel metric for predicting generalization. Lastly, we show that stochastic training methods like SGD enable networks to fit more data than full-batch gradient descent, suggesting that stochasticity enhances flexibility beyond regularization effects. These findings highlight the importance of understanding practical capacity limits and their implications for model generalization, providing new insights into neural network training and architectural design.",ICLR.cc/2025/Conference,4.8,False,0.8230,fixed neural architectures impose fundamental constraints capacity that must determined before training begins intrinsically motivated architectural growth imag where networks autonomously expand their capacity learning progress and complexity requirements the key innovation self supervised architectural expansion policy that identifies when and where add capacity measures representation compression learning progress and expected information gain experiments supervised learning reinforcement learning and continual learning that imag achieves better performance efficiency tradeoffs than fixed architectures while automatically discovering appropriate scales for different problems this establishes paradigm for neural intrinsically motivated capacity expansion enabling models that automatically grow appropriate complexity problem requirements rather than requiring manual capacity tuning,although overparameterization theory suggests that neural networks can fit any many samples they have parameters practical limitations often prevent them from reaching this capacity this empirically the practical flexibility neural networks and uncover several surprising secondly find that convolutional neural networks cnns are more parameter efficient than multi layer perceptrons mlps and vision transformers vits even when trained randomly labeled data emphasizing the role architectural inductive biases these highlight the importance understanding practical capacity limits and their implications for generalization providing insights into neural network training and architectural,2025-08-26T01:56:21.867665
98,**Fourier Neural Operators with Spectral Upsampling: Resolution-Invariant Architectures for Continuous Signals**,"Traditional neural networks for continuous physical signals rely on spatial discretization, introducing fundamental limitations in resolution and generalization. We present Fourier Neural Operators with Spectral Upsampling (FNO-SU), a novel architecture that operates directly in the spectral domain with theoretically guaranteed resolution invariance. Our approach implements neural operators through learnable spectral filters with continuous parameterizations, enabling processing of continuous signals independent of discretization resolution. The key innovation is a spectral upsampling mechanism that allows training on low-resolution data while inferring high-resolution outputs through principled spectral completion rather than spatial interpolation. By developing specialized parameterizations for bandlimited functions and implementing efficient Fourier-domain operations, FNO-SU overcomes the challenges of learning resolution-invariant operators. Experiments on partial differential equation solving, physical simulation, and super-resolution demonstrate that FNO-SU achieves 25-40% lower error than spatial discretization approaches while generalizing seamlessly across resolutions not seen during training. Theoretical analysis establishes error bounds on spectral approximation and provides guarantees on resolution invariance properties. This work introduces a fundamentally new approach to neural architecture design for continuous signals based on spectral domain processing, enabling truly resolution-invariant models that can train on limited-resolution data while generalizing to arbitrary resolutions.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,4413,Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators,"In recent years, neural operators have emerged as a prominent approach for learning mappings between function spaces, such as the solution operators of parametric PDEs. A notable example is the Fourier Neural Operator (FNO), which models the integral kernel as a convolution operator and uses the Convolution Theorem to learn the kernel directly in the frequency domain. The parameters are decoupled from the resolution of the data, allowing the FNO to take inputs of different resolutions.
However, training at a lower resolution and inferring at a finer resolution does not guarantee consistent performance, nor can fine details, present only in fine-scale data, be learned solely from coarse data. In this work, we address this misconception by defining and examining the discretization mismatch error: the discrepancy between the outputs of the neural operator when using different discretizations of the input data. We demonstrate that neural operators may suffer from discretization mismatch errors that hinder their effectiveness when inferred on data with resolutions different from that of the training data or when trained on data with varying resolutions. As neural operators underpin many critical cross-resolution scientific tasks, such as climate modeling and fluid dynamics, understanding discretization mismatch errors is essential. Based on our findings, we propose a Cross-Resolution Operator-learning Pipeline that is free of aliasing and discretization mismatch errors, enabling efficient cross-resolution and multi-spatial-scale learning, and resulting in superior performance.",ICLR.cc/2025/Conference,6.5,True,0.8706,traditional neural networks for continuous physical signals rely spatial discretization introducing fundamental limitations resolution and generalization fourier neural operators spectral upsampling fno that operates directly the spectral domain theoretically guaranteed resolution invariance our implements neural operators learnable spectral filters continuous parameterizations enabling processing continuous signals independent discretization resolution developing specialized parameterizations for bandlimited functions and implementing efficient fourier domain operations fno overcomes the challenges learning resolution invariant operators this introduces fundamentally neural for continuous signals spectral domain processing enabling truly resolution invariant models that can train limited resolution data while generalizing arbitrary resolutions,recent years neural operators have emerged prominent for learning mappings between function spaces such the solution operators parametric pdes notable example the fourier neural operator fno which models the integral kernel convolution operator and uses the convolution theorem learn the kernel directly the frequency domain this address this misconception defining and examining the discretization mismatch error the discrepancy between the outputs the neural operator when different discretizations the input data that neural operators may suffer from discretization mismatch errors that hinder their effectiveness when inferred data resolutions different from that the training data when trained data varying resolutions neural operators underpin many critical cross resolution scientific tasks such climate modeling and fluid dynamics understanding discretization mismatch errors essential,2025-08-26T01:56:21.867670
99,**Gumbel-Softmax Graph Networks: Differentiable Architecture Induction through Discrete Structure Learning**,"Neural architecture design typically relies on fixed connectivity patterns, limiting adaptation to complex and diverse data structures. We introduce Gumbel-Softmax Graph Networks (GSGNs), a novel framework that learns the optimal architecture connectivity directly from data through differentiable structure induction. Our approach represents neural architectures as learnable computation graphs where both operations and connections are jointly optimized using a specialized Gumbel-Softmax relaxation. The key innovation is a differentiable discrete structure learning mechanism that enables end-to-end optimization of architecture topology while converging to sparse, discrete connectivity patterns. By implementing a temperature-controlled relaxation scheme and developing specialized gradient estimators for graph structure learning, GSGNs overcome the challenges of joint optimization of discrete structure and continuous parameters. Experiments on relational reasoning, algorithmic tasks, and neural-symbolic integration demonstrate that GSGNs outperform fixed architectures by 15-25% while discovering interpretable computational graphs tailored to specific problem structures. Analysis reveals that the learned architectures discover meaningful computational abstractions that align with the underlying structure of the problem domain. This work establishes a new paradigm for neural architecture design where network topology emerges through differentiable structure learning rather than being manually specified, enabling the discovery of optimal computational graphs directly from data.",ICLR,neural architectures,claude-3-7-sonnet-latest,True,8013,Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks,"Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks.",ICLR.cc/2025/Conference,7.6,True,0.8350,neural relies fixed connectivity patterns limiting adaptation complex and diverse data structures our represents neural architectures learnable computation graphs where both operations and connections are jointly optimized specialized gumbel softmax relaxation the key innovation differentiable discrete structure learning mechanism that enables end end optimization topology while converging sparse discrete connectivity patterns implementing temperature controlled relaxation scheme and developing specialized gradient estimators for graph structure learning gsgns overcome the challenges joint optimization discrete structure and continuous parameters analysis reveals that the learned architectures discover meaningful computational ions that align the underlying structure the problem domain this establishes paradigm for neural where network topology emerges differentiable structure learning rather than being manually specified enabling the discovery optimal computational graphs directly from data,the graph based structure enables operations unstructured meshes which critical for representing complex geometries spatially localized high gradients while latent space diffusion modeling multi scale gnn allows for efficient learning and inference entire distributions solutions,2025-08-26T01:56:21.867673
