ai_index,ai_title,ai_abstract,ai_conference,ai_domain,ai_model,match_found,human_index,human_title,human_abstract,human_venue,human_overall_score,human_accepted,similarity_score,ai_processed_text,human_processed_text,generated_at
0,EQUI-Score: Learning Group-Equivariant Latent Score Fields for Unsupervised Representation Learning,"Many domains exhibit symmetries that should be preserved by learned representations, yet existing self-supervised methods rarely encode exact equivariances, limiting out-of-distribution generalization. We introduce EQUI-Score, a latent score-matching framework that learns group-equivariant representations by coupling a diffusion-based latent prior with a steerable encoder and equivariant score field. Our approach minimizes a denoising score-matching objective in latent space while enforcing equivariance via irreducible representation (irrep) constraints and a group-aligned Cholesky parameterization of the latent covariance. We prove that, under mild conditions on the data distribution and group action, EQUI-Score identifies irrep subspaces up to orthogonal ambiguity and yields invariants for classification. Algorithmically, we develop an efficient amortized encoder with equivariant normalization layers and a fast group-sampled training scheme. Empirically, EQUI-Score achieves state-of-the-art linear-probe and few-shot transfer under unseen transformations on Rotated-ImageNet, smallNORB, and synthetic SE(2) benchmarks, improving accuracy by 3–7% over competitive equivariant contrastive methods. On 3D point clouds with SO(3) actions, it reduces OOD error by 12% and improves sample efficiency in pose-robust recognition. Ablations confirm the necessity of equivariant latent score modeling and irrep regularization. EQUI-Score bridges generative diffusion and equivariant representation learning, enabling symmetry-aware features without labels. Our results suggest that embedding symmetry into latent stochastic dynamics yields robust, data-efficient representations for downstream tasks in vision, robotics, and scientific imaging.",ICLR,representation learning,gpt-5,True,10314,Improving Equivariant Networks with Probabilistic Symmetry Breaking,"Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannot *break* symmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as its input. This poses an important problem, both (1) for prediction tasks on domains where self-symmetries are common, and (2) for generative models, which must break symmetries in order to reconstruct from highly symmetric latent spaces. This fundamental limitation can in fact be addressed by considering *equivariant conditional distributions*, instead of equivariant functions. We therefore present novel theoretical results that establish necessary and sufficient conditions for representing such distributions. Concretely, this representation provides a practical framework for breaking symmetries in any equivariant network via randomized canonicalization. Our method, SymPE (Symmetry-breaking Positional Encodings), admits a simple interpretation in terms of positional encodings. This approach expands the representational power of equivariant networks while retaining the inductive bias of symmetry, which we justify through generalization bounds. Experimental results demonstrate that SymPE significantly improves performance of group-equivariant and graph neural networks across diffusion models for graphs, graph autoencoders, and lattice spin system modeling.",ICLR.cc/2025/Conference,7.0,True,0.8147,our minimizes denoising score matching objective latent space while enforcing equivariance irreducible representation irrep constraints and group aligned cholesky parameterization the latent covariance that under mild conditions the data distribution and group action equi score identifies irrep subspaces orthogonal ambiguity and yields invariants for classification empirically equi score achieves state the art linear probe and few shot transfer under unseen transformations rotated imagenet smallnorb and synthetic benchmarks improving over competitive equivariant contrastive methods point clouds actions reduces ood error and improves sample efficiency pose robust recognition equi score bridges generative diffusion and equivariant representation learning enabling symmetry aware features labels our suggest that embedding symmetry into latent stochastic dynamics yields robust data efficient representations for downstream tasks vision robotics and scientific imaging,equivariance encodes known symmetries into neural networks often enhancing generalization however equivariant networks cannot break symmetries the output equivariant network must definition have least the same self symmetries its input this poses important problem both for prediction tasks domains where self symmetries are common and for generative models which must break symmetries order reconstruct from highly symmetric latent spaces concretely this representation provides practical for breaking symmetries any equivariant network randomized canonicalization experimental that sympe improves group equivariant and graph neural networks across diffusion models for graphs graph autoencoders and lattice spin modeling,2025-08-26T01:35:51.567417
1,CoTransport: Causal Multi-View Representation Learning via Conditional Optimal Transport,"Multi-view data often blend causal content with view-specific nuisances, undermining generalization under interventions or shifts. We propose CoTransport, a causal representation learning framework that isolates intervention-stable factors by aligning conditional transport plans across views. Our key insight is that, under a broad class of mixing mechanisms, causal content induces an invariant conditional coupling between views given interventions, whereas spurious factors perturb the coupling. CoTransport learns encoders whose joint latent distribution minimizes a conditional Sinkhorn divergence across intervention strata and maximizes informativeness about each view via a masked-prediction auxiliary task. We prove that, under identifiability conditions on sparsity and mixing, the learned latents recover causal variables up to invertible transformations. Practically, we introduce an efficient minibatch estimator for conditional transport and an intervention-balancing reweighting scheme. On synthetic structural equation benchmarks with complex confounding, CoTransport outperforms state-of-the-art invariant risk and multi-view contrastive baselines by 6–15% in OOD accuracy. On image–text pairs with spurious backgrounds and style shifts, it reduces worst-group error by 11% and improves zero-shot retrieval robustness. Ablations reveal the impact of conditional alignment versus unconditional transport. By targeting intervention-stable couplings, CoTransport yields representations that transfer robustly across environments, providing a principled path toward causally grounded multi-view learning in vision–language, genomics, and sensor fusion.",ICLR,representation learning,gpt-5,True,976,Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning,"Directed acyclic graphs (DAGs) are often assumed in causal discovery, however, accurately identifying these DAGs necessitates various assumptions, particularly in latent causal models, which can be challenging to validate in real-world applications. This raises a critical question: Are DAG assumptions truly necessary for certain applications? In this work, we introduce a novel latent partial causal model for multimodal data, which features two latent coupled variables, connected by an undirected edge, effectively representing transferable knowledge across different modalities. We focus on a prominent learning framework, e.g., multimodal contrastive learning, and demonstrate that, with certain statistical assumptions, multimodal contrastive learning successfully identifies the latent coupled variables up to trivial transformation. This finding enhances our understanding of the mechanisms driving the success of multimodal contrastive learning. Furthermore, this finding reveals a unique potential for disentanglement in multimodal contrastive representation learning, improving the utility of pre-trained models like CLIP that are trained using this approach. Through experiments with synthetic data, we demonstrate the robustness of our findings, even in the presence of violated assumptions. In addition, we validate the disentanglement capabilities of pre-trained CLIP in learning disentangled representations, facilitating few-shot learning and improving domain generalization across a diverse range of real-world datasets.",ICLR.cc/2025/Conference,5.75,False,0.8144,cotransport causal representation learning that isolates intervention stable factors aligning conditional transport plans across views image text pairs spurious backgrounds and style shifts reduces worst group error and improves zero shot retrieval robustness targeting intervention stable couplings cotransport yields representations that transfer robustly across environments providing principled path toward causally grounded multi view learning vision language genomics and sensor fusion,this latent partial causal for multimodal data which features two latent coupled variables connected undirected edge representing transferable knowledge across different modalities focus prominent learning multimodal contrastive learning and that certain statistical assumptions multimodal contrastive learning identifies the latent coupled variables trivial transformation this enhances our understanding the mechanisms driving the success multimodal contrastive learning furthermore this reveals unique potential for disentanglement multimodal contrastive representation learning improving the utility pre trained models like clip that are trained this experiments synthetic data the robustness our even the presence violated assumptions addition the disentanglement capabilities pre trained clip learning disentangled representations facilitating few shot learning and improving domain generalization across diverse range real world datasets,2025-08-26T01:35:51.567446
2,SCTTA: Subspace-Consistent Test-Time Adaptation of Representations for Non-Stationary Streams,"Test-time adaptation promises robustness in deployment, but current methods often overfit or drift under non-stationary streams. We introduce SCTTA, a parameter-efficient framework that adapts representations online by preserving a stable subspace while correcting stream-specific distortions. Our approach maintains a low-dimensional “core” subspace identified via online spectral estimation of feature covariance and adapts only orthogonal complement components using lightweight adapters. We regularize updates with a Jacobian spectral penalty to stabilize local geometry and a bilevel objective that aligns pseudo-label confidence with subspace consistency. We prove a dynamic regret bound under piecewise-stationary shifts and show that subspace preservation mitigates catastrophic forgetting. SCTTA is architecture-agnostic and memory-bounded via sketching for covariance estimation. On CIFAR-10C/100C, ImageNet-C/A, and WILDS streams with recurring and abrupt shifts, SCTTA improves average and worst-case accuracy by 2.8–6.4% over Tent, EATA, and CoTTA with 10× fewer adapted parameters and <1% additional latency. Under oscillatory shifts, SCTTA maintains stability where existing methods collapse, evidenced by 40–60% lower feature drift. Ablations support the role of the spectral penalty and subspace updates. Our results establish subspace consistency as a principled inductive bias for test-time representation learning in non-stationary environments, enabling robust deployment in edge and streaming applications.",ICLR,representation learning,gpt-5,True,9281,Decentralizing Test-time Adaptation under Heterogeneous Data Streams,"While Test-Time Adaptation (TTA) has shown promise in addressing distribution shifts between training and testing data, its effectiveness diminishes with heterogenous data streams due to uniform target estimation. As previous attempts merely stabilize model fine-tuning over time to handle continually changing environments, they fundamentally assume a homogeneous target domain at any moment, leaving the intrinsic real-world data heterogeneity unresolved. This paper delves into TTA under heterogeneous data streams, moving beyond current model-centric limitations. By revisiting TTA from a data-centric perspective, we discover that decomposing samples into Fourier space facilitates an accurate data separation across different frequency levels. Drawing from this insight, we propose a novel Frequency-based Decentralized Adaptation framework, which transitions data from globally heterogeneous to locally homogeneous in Fourier space and employs decentralized adaptation to manage diverse distribution shifts.
Particularly, multiple local models are allowed to independently adjust to their specific data segments while periodically exchanging knowledge to form a cohesive global model. As such, not only can data diversity be captured, but also the overall model generalization can be enhanced across multiple distribution shifts. Importantly, we devise a novel Fourier-based augmentation strategy to assist in decentralizing adaptation, which selectively augments samples for each type of distribution shift and further enhances model robustness in complex real-world environments. Extensive experiments across various settings (corrupted, natural, and medical) demonstrate the superiority of our proposed framework over the state-of-the-arts.",ICLR.cc/2025/Conference,5.25,nan,0.8682,test time adaptation promises robustness deployment but current methods often overfit drift under non stationary streams our maintains low dimensional core subspace identified online spectral estimation feature covariance and adapts only orthogonal complement components lightweight adapters under oscillatory shifts sctta maintains stability where existing methods collapse evidenced lower feature drift our establish subspace consistency principled inductive bias for test time representation learning non stationary environments enabling robust deployment edge and streaming applications,while test time adaptation tta has shown promise addressing distribution shifts between training and testing data its effectiveness diminishes heterogenous data streams due uniform target estimation previous attempts merely stabilize fine tuning over time handle continually changing environments they fundamentally assume homogeneous target domain any moment leaving the intrinsic real world data heterogeneity unresolved drawing from this insight frequency based decentralized adaptation which transitions data from globally heterogeneous locally homogeneous fourier space and employs decentralized adaptation manage diverse distribution shifts multiple local models are allowed independently adjust their specific data segments while periodically exchanging knowledge form cohesive global importantly devise fourier based augmentation strategy assist decentralizing adaptation which selectively augments samples for each type distribution shift and further enhances robustness complex real world environments,2025-08-26T01:35:51.567466
3,SPOT: Spectral Optimal Transport Pretraining for Transferable Graph Representations,"Pretraining on graphs remains challenging due to heterogeneous structures and limited label availability. We present SPOT, a self-supervised graph pretraining method that aligns diffusion spectra across graphs via optimal transport in the spectral domain. SPOT learns encoders that map graphs to node and graph-level embeddings whose induced graph Laplacian spectra match those of reference diffusion processes, minimizing a Sinkhorn divergence between pushforward spectral measures. We show that SPOT preserves diffusion distances and commute-time metrics in latent space, yielding transferability across graph families. The training objective combines spectral transport with masked edge/node prediction for local consistency. We provide sample complexity bounds for spectral estimation under subsampling and establish stability to graph perturbations. On OGB (ogbn-arxiv, ogbg-molhiv, ogbl-ppa) and cross-domain transfer (protein–social), SPOT outperforms contrastive, context-prediction, and graph masking baselines by 3–9% in linear-probe accuracy and ROC-AUC, with robust gains under distribution shifts (degree, sparsity, and motif edits). SPOT’s benefits persist across GCN, GIN, and Transformer backbones, and ablations confirm the necessity of spectral transport. By aligning graphs in a geometry dictated by diffusion, SPOT produces structure-aware representations that generalize across domains, offering a principled and scalable pretraining primitive for graph learning in molecules, networks, and scientific discovery.",ICLR,representation learning,gpt-5,True,10186,Global Context-aware Representation Learning for Spatially Resolved Transcriptomics,"Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that captures the spatial context of cells within tissues, enabling the study of complex biological networks. Recently, graph-based deep learning has been utilized in identifying meaningful spatial domains by leveraging both gene expression and spatial information. However, these approaches fall short in obtaining qualified spot representations, particularly for those located around the boundary of cell type clusters, as they heavily emphasize spatially local spots that have minimal feature differences from an anchor node. To address this limitation, we propose a novel framework, Spotscape, which introduces the Similarity Telescope module designed to learn spot representations by capturing the global relationships among multiple spots. Additionally, to address the challenges that arise when integrating multiple slices from heterogeneous sources, we propose a similarity scaling strategy that explicitly regulates the distances between intra- and inter-slice spots to ensure they remain nearly the same. Extensive experiments demonstrate the superiority of Spotscape in various downstream tasks, including spatial domain identification, multi-slice integration, and alignment tasks, compared to baseline methods. Our code is available at the following link: https://anonymous.4open.science/r/Spotscape-E312/",ICLR.cc/2025/Conference,5.75,False,0.8235,spot self supervised graph pretraining that aligns diffusion spectra across graphs optimal transport the spectral domain the training objective combines spectral transport masked edge node prediction for local consistency ogb ogbn arxiv ogbg molhiv ogbl ppa and cross domain transfer protein social spot outperforms contrastive context prediction and graph masking baselines linear probe and roc auc robust gains under distribution shifts degree sparsity and motif edits spot benefits persist across gcn gin and transformer backbones and ablations confirm the necessity spectral transport aligning graphs geometry dictated diffusion spot produces structure aware representations that generalize across domains offering principled and scalable pretraining primitive for graph learning molecules networks and scientific discovery,recently graph based deep learning has been utilized identifying meaningful spatial domains leveraging both gene expression and spatial information however these approaches fall short obtaining qualified spot representations for those located around the boundary cell type clusters they heavily emphasize spatially local spots that have minimal feature differences from anchor node extensive experiments the superiority spotscape various downstream tasks including spatial domain identification multi slice integration and alignment tasks compared methods,2025-08-26T01:35:51.567473
4,Coded Neural Fields: A Universal Representation Substrate for Multimodal Self-Supervision,"Discrete tokenization can fragment continuous structure in images, audio, and 3D signals, while modality-specific pipelines hinder cross-modal transfer. We propose Coded Neural Fields (CNF), a modality-agnostic representation that encodes data as continuous fields parameterized by a shared neural atlas and compact latent codes. CNF jointly learns: (i) a universal coordinate encoder with band-limited Fourier features, (ii) modality-specific decoders for reconstruction, and (iii) a cross-modal alignment head that enforces code agreement across paired modalities. The training objective combines masked field reconstruction, cross-modal code contrast, and a curvature penalty to regularize field smoothness. Theoretically, we show that CNF codes approximate sufficient statistics for a broad class of stationary processes and bound code complexity via a rate–distortion analysis. Practically, CNF yields a single code space for images, audio, and radiance fields, enabling lightweight downstream heads. On ImageNet-1K, AudioSet, and NeRF-Synthetic, CNF achieves competitive linear-probe and retrieval performance, while a single code supports editing and cross-modal retrieval. With 8–16× smaller codes than VQ-based tokenizers, CNF improves sample efficiency and maintains fidelity under heavy masking. Ablations highlight the role of curvature regularization and alignment. CNF provides a unifying substrate for multimodal representation learning, simplifying pipelines and enabling transfer across heterogeneous signals.",ICLR,representation learning,gpt-5,True,9130,Advancing Multimodal Unified Discrete Representations,"To enhance the interpretability of multimodal unified representations, many studies have focused on discrete unified representations. These efforts typically start with contrastive learning and gradually extend to the disentanglement of modal information, achieving solid multimodal discrete unified representations. However, existing research often overlooks two critical issues: 1) Different modalities have unique characteristics, and a uniform alignment approach does not fully exploit these traits; 2) The use of Euclidean distance for quantization in discrete representations often overlooks the important distinctions among different dimensions of features, resulting in redundant representations after quantization. To address these issues, we propose Fine and Coarse Cross-modal Information Disentangling (FCCID) and Training-Free Optimization of Codebook (TOC). These methods respectively perform fine and coarse disentanglement of information based on the specific characteristics of different modalities and refine the unified discrete representations obtained from pretraining. Compared to the previous state-of-the-art, our model demonstrates significant performance improvements. The code is provided in the supplementary materials.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8282,discrete tokenization can fragment continuous structure images audio and signals while modality specific pipelines hinder cross modal transfer coded neural fields cnf modality agnostic representation that encodes data continuous fields parameterized shared neural atlas and compact latent codes cnf provides unifying substrate for multimodal representation learning simplifying pipelines and enabling transfer across heterogeneous signals,enhance the interpretability multimodal unified representations many studies have focused discrete unified representations these efforts start contrastive learning and gradually extend the disentanglement modal information achieving solid multimodal discrete unified representations address these issues fine and coarse cross modal information disentangling fccid and training free optimization codebook toc,2025-08-26T01:35:51.567480
5,Composable Sparse Autoencoders Expose and Control Latent Circuits in Foundation Models,"Interpretable representations promise safer, more steerable foundation models, yet existing sparse autoencoders (SAEs) often discover fragmented features with limited compositionality. We introduce Composable Sparse Autoencoders (CSAE), a training framework that induces dictionaries aligned with functional circuits and supports controllable composition. CSAE augments SAE objectives with: (i) submodular coverage regularization to encourage diverse, non-redundant features, (ii) group-structured sparsity that captures coherent motifs, and (iii) a composition consistency loss aligning linear combinations of features with known functional directions. We provide identifiability results showing CSAE recovers sparse generators of piecewise-linear regions in ReLU networks under mild incoherence. Applied to ViT and LLM activations, CSAE yields higher sparsity, stability across seeds, and disentanglement than recent SAE variants. On ImageNet, CSAE features support causal editing (e.g., texture, shape, and background control) with minimal collateral changes, improving targeted manipulation success by 9–14%. On Llama-2, CSAE discovers sentiment, numeracy, and factuality circuits; feature clamping enables controlled generation and reduces toxic content without prompt engineering, while preserving perplexity. User studies and automated metrics validate improved interpretability and compositional control. CSAE advances representation learning for interpretability by discovering actionable, composable features, offering a practical pathway for safe model steering and mechanistic insights.",ICLR,representation learning,gpt-5,False,,The Computational Complexity of Circuit Discovery for Inner Interpretability,"Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries for mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons. Our findings reveal a challenging complexity landscape. Many queries are intractable, remain fixed-parameter intractable relative to model/circuit features, and inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems with better-understood heuristics, and prove the tractability or fixed-parameter tractability of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands on existing and future architectures.",ICLR.cc/2025/Conference,7.5,True,0.7925,llama csae discovers sentiment numeracy and factuality circuits feature clamping enables controlled generation and reduces toxic content prompt engineering while preserving perplexity user studies and automated metrics improved interpretability and compositional control csae advances representation learning for interpretability discovering actionable composable features offering practical pathway for safe steering and mechanistic insights,many proposed applications neural networks machine learning cognitive brain science and society hinge the feasibility inner interpretability circuit discovery address this circuit discovery classical and parameterized computational complexity theory describe conceptual scaffolding reason about circuit queries terms affordances for description explanation prediction and control formalize comprehensive set queries for mechanistic explanation and formal for their analysis use settle the complexity many query variants and relaxations practical interest multi layer perceptrons this allows understand the scope and limits interpretability queries viable options and their resource demands existing and future architectures,2025-08-26T01:35:51.567491
6,PAC-FaRe: Flatness-Regularized Representation Compression with PAC-Bayesian Guarantees,"Learning compact representations that generalize remains a central challenge, especially under resource constraints. We present PAC-FaRe, a representation learning framework that couples flatness regularization with quantization-aware noise to yield compressible, robust embeddings with PAC-Bayesian guarantees. Our objective penalizes local curvature of the representation-induced loss via a tractable Hutchinson estimator of the Jacobian–Hessian norm and injects structured noise aligned with a target quantizer. We derive a PAC-Bayes bound linking representation flatness, mutual information with inputs, and post-quantization generalization error, motivating our training criterion. PAC-FaRe is architecture-agnostic and complements contrastive or masked modeling losses. On image and text benchmarks (CIFAR, ImageNet-100, GLUE probes), PAC-FaRe reduces bitwidth to 4–6 bits with <1% accuracy drop and improves OOD robustness (ImageNet-C/A) by 2–4% over quantization-aware and spectral norm baselines. Under severe memory budgets, it outperforms pruning and low-rank adapters, and yields 1.6–2.3× faster inference on edge devices. Ablations show flatness regularization synergizes with quantization noise to widen basins and stabilize representations. Our results establish a principled bridge between flat minima and compressible features, offering an efficient route to deployable, robust representation learning with theoretical end-to-end guarantees.",ICLR,representation learning,gpt-5,True,8084,Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric,"In typical multimodal contrastive learning, such as CLIP, encoders produce one
point in the latent representation space for each input. However, one-point representation
has difficulty in capturing the relationship and the similarity structure of a
huge amount of instances in the real world. For richer classes of the similarity, we
propose the use of weighted point sets, namely, sets of pairs of weight and vector,
as representations of instances. In this work, we theoretically show the benefit
of our proposed method through a new understanding of the contrastive loss of
CLIP, which we call symmetric InfoNCE. We clarify that the optimal similarity
that minimizes symmetric InfoNCE is the pointwise mutual information, and show
an upper bound of excess risk on downstream classification tasks of representations
that achieve the optimal similarity. In addition, we show that our proposed
similarity based on weighted point sets consistently achieves the optimal similarity.
To verify the effectiveness of our proposed method, we demonstrate pretraining of
text-image representation models and classification tasks on common benchmarks.",ICLR.cc/2025/Conference,7.333333333333333,True,0.8080,learning compact representations that generalize remains central challenge under resource constraints pac fare representation learning that couples flatness regularization quantization aware noise yield compressible robust embeddings pac bayesian guarantees derive pac bayes bound linking representation flatness mutual information inputs and post quantization generalization error motivating our training criterion image and text benchmarks cifar imagenet glue probes pac fare reduces bitwidth bits drop and improves ood robustness imagenet over quantization aware and spectral norm baselines our establish principled bridge between flat minima and compressible features offering efficient route deployable robust representation learning theoretical end end guarantees,typical multimodal contrastive learning such clip encoders produce one point the latent representation space for each input however one point representation has difficulty capturing the relationship and the similarity structure huge amount instances the real world clarify that the optimal similarity that minimizes symmetric infonce the pointwise mutual information and upper bound excess risk downstream classification tasks representations that achieve the optimal similarity verify the effectiveness our proposed pretraining text image representation models and classification tasks common benchmarks,2025-08-26T01:35:51.567498
7,IDA-Rep: Inverse-Dynamics-Aligned Representations for Sample-Efficient Reinforcement Learning,"Representation learning in RL must support control, yet purely reconstruction- or contrastive-based methods can misalign features with decision-relevant dynamics. We propose IDA-Rep, a unified objective that aligns representations with both forward and inverse dynamics while preserving bisimulation structure. IDA-Rep trains an encoder with three coupled terms: (i) inverse-dynamics discrimination that separates action-conditioned transitions, (ii) forward predictive coding with multi-step consistency, and (iii) a bisimulation regularizer that contracts latent distances for behaviorally equivalent states. We prove that, under stochastic dynamics, IDA-Rep preserves a task-agnostic bisimulation metric and supports linear value approximation for a class of policies. Implemented atop pixel-based agents with lightweight decoders, IDA-Rep improves sample efficiency on DMControl (pixels) by 25–45% over DrQv2 and CURL and accelerates Atari learning (100k/500k steps) with consistent gains. In offline RL, IDA-Rep enhances policy improvement on D4RL by yielding value-aware latents, outperforming representation baselines under dataset shift. Ablations confirm the necessity of inverse-dynamics alignment and the bisimulation term. By targeting control-relevant invariances, IDA-Rep delivers robust, data-efficient representations for both online and offline RL, providing a principled foundation for scaling pixel-based control and transfer across tasks and embodiments.",ICLR,representation learning,gpt-5,True,3675,Online Laplacian-Based Representation Learning in Reinforcement Learning,"Representation learning plays a crucial role in reinforcement learning, especially in complex environments with high-dimensional and unstructured states. Effective representations can enhance the efficiency of learning algorithms by improving sample efficiency and generalization across tasks. This paper considers the Laplacian-based framework for representation learning, where the eigenvectors of the Laplacian matrix of the underlying transition graph are leveraged to encode meaningful features from raw sensory observations of the states. Despite the promising algorithmic advances in this framework, it remains an open question whether the Laplacian-based representations can be learned online and with theoretical guarantees along with policy learning. To answer this question, we study online Laplacian-based representation learning, where the graph-based representation is updated simultaneously while the policy is updated by the reinforcement learning algorithm. We design an online optimization formulation by introducing the Asymmetric Graph Drawing Objective (AGDO) and provide a theoretical analysis of the convergence of running online projected gradient descent on AGDO under mild assumptions. Specifically, we show that if the policy learning algorithm induces a bounded drift on the policy, running online projected gradient descent on AGDO exhibits ergodic convergence. Our extensive simulation studies empirically validate the guarantees of convergence to the true Laplacian representation. Furthermore, we provide insights into the compatibility of different reinforcement learning algorithms with online representation learning.",ICLR.cc/2025/Conference,4.5,False,0.8386,representation learning must support control yet purely reconstruction contrastive based methods can misalign features decision relevant dynamics implemented atop pixel based agents lightweight decoders ida rep improves sample efficiency dmcontrol pixels over drqv2 and curl and accelerates atari learning 100k 500k steps consistent gains offline ida rep enhances policy improvement d4rl yielding value aware latents outperforming representation baselines under shift targeting control relevant invariances ida rep delivers robust data efficient representations for both online and offline providing principled foundation for scaling pixel based control and transfer across tasks and embodiments,representation learning plays crucial role reinforcement learning complex environments high dimensional and unstructured states effective representations can enhance the efficiency learning algorithms improving sample efficiency and generalization across tasks this considers the laplacian based for representation learning where the eigenvectors the laplacian matrix the underlying transition graph are leveraged encode meaningful features from raw sensory observations the states despite the promising algorithmic advances this remains open question whether the laplacian based representations can learned online and theoretical guarantees along policy learning answer this question online laplacian based representation learning where the graph based representation updated simultaneously while the policy updated the reinforcement learning online optimization formulation introducing the asymmetric graph drawing objective agdo and provide theoretical analysis the convergence running online projected gradient descent agdo under mild assumptions that the policy learning induces bounded drift the policy running online projected gradient descent agdo exhibits ergodic convergence our extensive simulation studies empirically the guarantees convergence the true laplacian representation furthermore provide insights into the compatibility different reinforcement learning algorithms online representation learning,2025-08-26T01:35:51.567510
8,GeoMask: Curvature-Aware Masked Autoencoding via Learnable Riemannian Metrics,"Masked autoencoders learn powerful features, yet standard objectives are agnostic to the intrinsic geometry of data manifolds, leading to brittle representations under distribution shift. We introduce GeoMask, a geometry-aware self-supervised framework that equips encoders with an explicit Riemannian structure and exploits curvature to guide masking and reconstruction. Our approach learns a pullback metric induced by the encoder and regularizes its Ricci curvature to discourage pathological warping while preserving discriminative directions. A curvature-aware scheduler prioritizes masking high-curvature regions that concentrate semantic variation, and a geodesic neighborhood contrastive term aligns features along shortest-path neighborhoods rather than Euclidean distances. We prove that GeoMask controls the encoder’s local Lipschitz constant and bounds linear probe risk via curvature-dependent generalization terms. Practically, we develop efficient estimators for sectional curvature using Jacobian-vector products and implement geodesic k-NN using implicit differentiation. On ImageNet-100, LibriSpeech, and ModelNet40, GeoMask improves linear-probe and few-shot transfer by 2.3–5.1% over MAE/iBOT while reducing worst-case corruption error (ImageNet-C/A) by 3–6%. Under synthetic manifold shifts and domain transfer (Sketch/Photo), it retains 8–12% more accuracy than baselines with matched FLOPs. Ablations validate the contribution of curvature-aware masking and geodesic contrast. By marrying masked modeling with learnable Riemannian geometry, GeoMask yields robust, semantically aligned representations that better respect data manifolds, improving transfer in vision, audio, and 3D tasks.",ICLR,representation learning,gpt-5,True,6237,MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations,"We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that strong representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple instance discrimination (ID) heads that are connected to different intermediate layers. In each head, a nearest neighbor ID objective constructs clusters that capture semantic information which improves performance on downstream tasks, including off-the-shelf and fine-tuning settings.

The refinement process is short and simple -  yet highly effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing (84.7\%) and low-shot classification among models that are pre-trained on ImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID objectives, enabling scaling ID objectives to billion parameter models using relatively little compute. MIM-Refiner compares favorably against previous state-of-the-art SSL models on various benchmarks such as low-shot classification, long-tailed classification and semantic segmentation.",ICLR.cc/2025/Conference,6.5,True,0.8101,curvature aware scheduler prioritizes masking high curvature regions that concentrate semantic variation and geodesic neighborhood contrastive term aligns features along shortest path neighborhoods rather than euclidean distances imagenet librispeech and modelnet40 geomask improves linear probe and few shot transfer over mae ibot while reducing worst case corruption error imagenet under synthetic manifold shifts and domain transfer sketch photo retains more than baselines matched flops marrying masked modeling learnable riemannian geometry geomask yields robust semantically aligned representations that better respect data manifolds improving transfer vision audio and tasks,mim masked image modeling refiner contrastive learning boost for pre trained mim models each head nearest neighbor objective constructs clusters that capture semantic information which improves downstream tasks including off the shelf and fine tuning settings refining vit pre trained data2vec imagenet sets state the art linear probing and low shot classification among models that are pre trained imagenet mim refiner compares favorably against previous state the art ssl models various benchmarks such low shot classification long tailed classification and semantic segmentation,2025-08-26T01:35:51.567519
9,Meta-Anchors: Continual Representation Consolidation with Prototype Subspaces and Regret Guarantees,"Catastrophic forgetting plagues representation learning in non-stationary streams, where gradients entangle past and new information. We propose Meta-Anchors, a continual self-supervised method that stabilizes features by consolidating prototype subspaces while adapting rapidly to new tasks. Our approach maintains a small set of anchor prototypes selected by a determinantal point process over the representation manifold and learns lightweight adapters that evolve only in the orthogonal complement. A bilevel meta-objective aligns gradient steps to preserve anchor geometry, regularized by a Fisher-based consolidation term derived from contrastive curvature. We derive a dynamic regret bound under piecewise-stationary assumptions, showing that prototype subspace preservation controls forgetting without freezing the encoder. Algorithmically, we introduce rehearsal-free, memory-bounded updates via low-rank sketches and an anchor-aware masked prediction loss that ties anchors to local invariances. On Split CIFAR-100, Tiny-ImageNet, CORe50-NC/NI, and DomainNet streams, Meta-Anchors improves average and worst-task accuracy by 4–9% over EWC, DER, CaSSLe, and online CLIP-adapters, with 10× smaller memory than exemplar replay. Under long horizons with order and domain shifts, it reduces representation drift by 35–60%. Ablations confirm the synergy between DPP selection and Fisher consolidation. Meta-Anchors provides a principled route to lifelong representation learning, combining strong theoretical stability with practical scalability for on-device and streaming deployments.",ICLR,representation learning,gpt-5,True,9850,Stabilize continual learning with hyperspherical replay,"Neural networks face catastrophic forgetting of previously learned knowledge
when training on new task data. While the field of continual learning has made
promising progress in reducing this forgetting, recent work has uncovered an
interesting phenomenon: existing techniques often exhibit a sharp performance
drop on prior tasks during the initial stages of new task training, a phenomenon
known as the ”stability gap.” This phenomenon not only raises safety concerns
but also challenges the current understanding of neural network behavior in continual learning scenarios. Inspired by this discovery, we revisit two fundamental
questions in continual learning: 1) Is the past learned knowledge within deep
networks lost abruptly or gradually? and 2) Is past learned knowledge ever completely erased? Our analysis reveals that abrupt forgetting occurs not only in the
final fully connected layer but also permeates the feature space and most layers,
sparing only the earliest layers. Alarmingly, a single gradient update can severely
disrupt the learned class structure. We identify degenerate solutions in the softmax
cross-entropy loss as a major contributing factor, with memory samples exhibiting
higher feature norms compared to new samples. To address these issues, we pro-
pose Adaptive Angular Replay (AAR), a simple yet effective approach that learns
features in hyperspherical space using feature and weight normalization. Angular
ER demonstrates a strong ability to preserve class structure during task transitions. Additionally, we introduce an adaptive scaling strategy to further mitigate
the stability gap and improve overall accuracy.",ICLR.cc/2025/Conference,3.0,nan,0.8284,catastrophic forgetting plagues representation learning non stationary streams where gradients entangle past and information our maintains small set anchor prototypes selected determinantal point process over the representation manifold and learns lightweight adapters that evolve only the orthogonal complement algorithmically rehearsal free memory bounded updates low rank sketches and anchor aware masked prediction loss that ties anchors local invariances under long horizons order and domain shifts reduces representation drift meta anchors provides principled route lifelong representation learning combining strong theoretical stability practical scalability for device and streaming deployments,neural networks face catastrophic forgetting previously learned knowledge when training task data while the field continual learning has made promising progress reducing this forgetting recent has uncovered interesting phenomenon existing techniques often exhibit sharp drop prior tasks during the initial stages task training phenomenon known the stability gap this phenomenon not only raises safety concerns but also challenges the current understanding neural network behavior continual learning scenarios inspired this discovery revisit two fundamental questions continual learning the past learned knowledge within deep networks lost abruptly gradually and past learned knowledge ever completely erased our analysis reveals that abrupt forgetting occurs not only the final fully connected layer but also permeates the feature space and most layers sparing only the earliest layers identify degenerate solutions the softmax cross entropy loss major contributing factor memory samples exhibiting higher feature norms compared samples address these issues pro pose adaptive angular replay aar simple yet effective that learns features hyperspherical space feature and weight normalization,2025-08-26T01:35:51.567526
10,Scale-Consistent Energy Encoders: Multi-Resolution EBMs for Self-Supervised Representation Learning,"Self-supervised learning benefits from multi-scale context, yet most objectives treat scales independently or rely on architectural heuristics. We introduce Scale-Consistent Energy Encoders (SCEE), a pretraining paradigm that learns a family of tied energy-based models across resolutions and enforces probabilistic consistency between them. SCEE parameterizes energies E_s over latents of images downsampled to scale s, coupled by a probability flow that marginalizes fine-scale energies to coarser ones. Training minimizes a denoising score-matching loss at each scale and a cross-scale path-consistency penalty derived from the Fokker–Planck equation, while an encoder amortizes sampling and shares features across scales. We prove that SCEE enforces scale-equivariant representations and bounds Fisher information distortion across resolutions, yielding latents that preserve task-relevant structure under zoom and aliasing. Efficient sliced score estimators and multigrid Langevin updates make training practical. On ImageNet-100 and COCO, SCEE surpasses MoCo-v3 and iBOT under multi-scale linear probes and semantic segmentation transfer, improving mIoU by 2.6 points with matched compute. Under synthetic scale shifts and heavy downsampling, it retains up to 9% more accuracy and improves robustness on ImageNet-Sketch/Style. Ablations show the necessity of cross-scale energy tying over naive multi-scale contrast. SCEE establishes a principled multi-resolution EBM framework that turns consistency into a strong inductive bias for scale-robust representation learning.",ICLR,representation learning,gpt-5,True,3796,PooDLe🐩: Pooled and dense self-supervised learning from naturalistic videos,"Self-supervised learning has driven significant progress in learning from single-subject, _iconic_ images.
However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain _dense_ scenes with many independent objects, imbalanced class distributions, and varying object sizes.
In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping.
Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos.
We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.",ICLR.cc/2025/Conference,6.0,True,0.8239,self supervised learning benefits from multi scale context yet most objectives treat scales independently rely architectural heuristics imagenet and coco scee surpasses moco and ibot under multi scale linear probes and semantic segmentation transfer improving miou points matched compute under synthetic scale shifts and heavy downsampling retains more and improves robustness imagenet sketch style scee establishes principled multi resolution ebm that turns consistency into strong inductive bias for scale robust representation learning,self supervised learning has driven significant progress learning from single subject _iconic_ images this poodle self supervised learning that combines invariance based objective pooled representations dense ssl objective that enforces equivariance optical flow warping our that unified objective applied multiple feature scales essential for learning effective image representations from naturalistic videos our experiments the bdd100k driving video and the walking tours first person video demonstrating its ability capture spatial understanding from dense objective and semantic understanding pooled representation objective,2025-08-26T01:35:51.567532
11,DynRF-SSL: Self-Supervised 3D-Aware Video Representations via Differentiable Rendering and Motion Fields,"Video encoders often entangle appearance and viewpoint, hampering transfer to tasks requiring 3D understanding. We propose DynRF-SSL, a self-supervised framework that learns 3D-aware video representations by coupling dynamic radiance fields with differentiable rendering and motion fields. A coordinate MLP models scene appearance and density, while a low-rank, SE(3)-aware motion field captures per-frame deformations. Training uses photometric cycle consistency across time, epipolar reprojection geometry, and a novel occlusion-aware masking scheme. We show identifiability of latent 3D structure up to gauge under mild parallax and texture conditions, and derive bounds linking reprojection error to feature consistency. A lightweight encoder distills geometric features from the radiance field for downstream tasks. On UCF101 and Kinetics-200, DynRF-SSL improves linear-probe action recognition by 3–6% over TimeSformer and video-MAE pretraining at similar compute. On synthetic multi-view datasets, it reconstructs geometry with 20–30% lower Chamfer distance and yields viewpoint-invariant retrieval. Transfer to visuomotor imitation shows 18% better success under camera shifts. Ablations highlight the role of motion fields and occlusion-aware masking. By embedding differentiable 3D reasoning into pretraining, DynRF-SSL produces geometry-consistent video representations that advance robustness and sample efficiency in perception and control.",ICLR,representation learning,gpt-5,True,10937,VideoLights: A Cross-Modal Cross-Task Transformer Model for Joint Video Highlight Detection and Moment Retrieval,"Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook cross-task  dynamics and video-text alignment. We propose VideoLights, a novel HD/MR framework addressing these limitations through: (i) Convolutional Projection and Feature Refinement modules with an intermodal alignment loss for better video-text feature alignment. (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations. (iii) Uni-Directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, we introduce  hard positive/negative losses for adaptive error penalization and improved learning. Our approach includes intelligent pretraining and finetuning using synthetic  data and features from various encoders. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance.",ICLR.cc/2025/Conference,3.5,nan,0.8317,video encoders often entangle appearance and viewpoint hampering transfer tasks requiring understanding identifiability latent structure gauge under mild parallax and texture conditions and derive bounds linking reprojection error feature consistency ucf101 and kinetics dynrf ssl improves linear probe action recognition over timesformer and video mae pretraining similar compute transfer visuomotor imitation shows better success under camera shifts embedding differentiable reasoning into pretraining dynrf ssl produces geometry consistent video representations that advance robustness and sample efficiency perception and control,video highlight detection and moment retrieval are essential video analysis recent joint prediction transformer models often overlook cross task dynamics and video text alignment videolights addressing these limitations convolutional projection and feature refinement modules intermodal alignment loss for better video text feature alignment directional cross modal fusion network for strongly coupled query aware clip representations addition hard positive negative losses for adaptive error penalization and improved learning,2025-08-26T01:35:51.567543
12,FedInv: Personalized Federated Invariant Representations with Divergence-Aware Aggregation and PAC-Bayes Guarantees,"Federated learning faces client heterogeneity that undermines global models and brittle self-supervision. We introduce FedInv, a federated representation learning framework that disentangles invariant factors shared across clients from idiosyncratic variations, enabling personalization without sacrificing transfer. FedInv trains encoders with a variational decomposition z = (z_inv, z_id) and penalizes mutual information between z_inv and client identity while maximizing utility via masked prediction. A divergence-aware server aggregates z_inv by solving a Fisher–Rao barycenter over client posteriors, while z_id remains local. We derive a PAC-Bayesian generalization bound under client-shift characterized by f-divergences, motivating the invariant aggregation rule and a reweighting scheme for skewed participation. Communication-efficient updates use low-rank adapters and quantized gradients. On FEMNIST, SpeechCommands, and WILDS-Camelyon, FedInv improves average client accuracy by 3–8% and worst-client accuracy by 6–12% over FedSimCLR, FedProx, MOON, and Ditto, with 30–50% fewer rounds to target accuracy. Under extreme label and feature skew, it preserves invariants and reduces negative transfer. Ablations confirm the benefits of Fisher–Rao aggregation and identity disentanglement. FedInv provides a principled pathway to federated self-supervision that is robust to heterogeneity, offering theory-backed, communication-efficient invariants for edge and privacy-preserving applications.",ICLR,representation learning,gpt-5,True,1668,pMixFed: Mixing up model coefficients for Efficient Personalized Federated Learning,"Federated Learning  enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy  and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses   data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges,  we propose ``pMixFed'', a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model,  gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate   its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of  heterogeneity when using pMixFed as compared with the state-of-the-art PFL models.",ICLR.cc/2025/Conference,2.0,nan,0.8808,federated learning faces client heterogeneity that undermines global models and brittle self supervision fedinv federated representation learning that disentangles invariant factors shared across clients from idiosyncratic variations enabling personalization sacrificing transfer fedinv trains encoders variational decomposition z_inv z_id and penalizes mutual information between z_inv and client identity while maximizing utility masked prediction under extreme label and feature skew preserves invariants and reduces negative transfer,federated learning enables decentralized collaborative learning machine learning models which presents challenges such data privacy and client drift for heterogeneous data personalized federated learning pfl addresses data heterogeneity tackling these issues balancing generalization and personalization level adaptive partitioning between shared and personalized layers the gradual transition personalization allow seamless adaptation local clients improved generalization across clients and mitigation catastrophic forgetting,2025-08-26T01:35:51.567549
13,DistEmbed: Distributional Contrastive Learning with Heteroscedastic Representations,"Most contrastive methods embed inputs as deterministic vectors, ignoring uncertainty that arises from occlusion, noise, or ambiguity, which harms calibration and robustness. We present DistEmbed, a distributional contrastive learning framework where each input maps to a heteroscedastic distribution in latent space. Encoders output mean and covariance, and training minimizes an expected InfoNCE objective under the induced distributions, using closed-form divergences (2-Wasserstein and KL) as similarity. A temperature-as-variance prior aligns scale with epistemic uncertainty, and a Jacobian penalty stabilizes covariance estimation. We prove that DistEmbed optimizes a lower bound on mutual information under uncertainty and show robustness guarantees to additive perturbations proportional to predicted variance. Efficient reparameterization and covariance factorization keep compute modest. On ImageNet-100, STL-10, and AudioSet, DistEmbed matches or exceeds strong baselines (SimCLR, VICReg) on linear probes while delivering calibrated confidence: ECE reduces by 25–40% and OOD detection AUROC improves by 5–9%. In retrieval and few-shot tasks with label noise, uncertainty-aware matching yields consistent gains. Under distribution shift (ImageNet-C/A), performance degrades gracefully with predictive variance tracking error. Ablations validate the role of distributional similarity and heteroscedastic modeling. DistEmbed advances representation learning by natively encoding uncertainty, improving robustness, calibration, and downstream decision quality.",ICLR,representation learning,gpt-5,True,5805,Probabilistic Contrastive Learning with Explicit Concentration on the Hypersphere,"Contrastive learning is predominantly deterministic, limiting its effectiveness in noisy and uncertain environments. We propose a probabilistic approach inspired by the von Mises-Fisher (vMF) distribution, embedding representations on a hyperspherical space. To address numerical instability, we introduce an unnormalized and regularized vMF distribution, preserving essential properties with theoretical guarantees. 
The concentration parameter, $\kappa$, serves as an interpretable measure of aleatoric uncertainty.
Empirical evaluations show a strong correlation between estimated $\kappa$ and unseen data corruption severity, enabling effective failure analysis and enhancing out-of-distribution detection without modeling epistemic uncertainty. From a fresh perspective, our approach introduces a flexible alignment mechanism for improved uncertainty estimation in high-dimensional spaces while remaining compatible with existing contrastive learning frameworks.",ICLR.cc/2025/Conference,2.0,nan,0.8640,most contrastive methods embed inputs deterministic vectors ignoring uncertainty that arises from occlusion noise ambiguity which harms calibration and robustness distembed distributional contrastive learning where each input maps heteroscedastic distribution latent space that distembed optimizes lower bound mutual information under uncertainty and robustness guarantees additive perturbations proportional predicted variance imagenet stl and audioset distembed matches exceeds strong baselines simclr vicreg linear probes while delivering calibrated confidence ece reduces and ood detection auroc improves retrieval and few shot tasks label noise uncertainty aware matching yields consistent gains distembed advances representation learning natively encoding uncertainty improving robustness calibration and downstream decision quality,contrastive learning predominantly deterministic limiting its effectiveness noisy and uncertain environments probabilistic inspired the von mises fisher vmf distribution embedding representations hyperspherical space empirical evaluations strong correlation between estimated kappa and unseen data corruption severity enabling effective failure analysis and enhancing out distribution detection modeling epistemic uncertainty from fresh perspective our introduces flexible alignment mechanism for improved uncertainty estimation high dimensional spaces while remaining compatible existing contrastive learning frameworks,2025-08-26T01:35:51.567555
14,TabCopula-SSL: Self-Supervised Copula Representations for Mixed-Type Tabular Data with Missingness,"Tabular domains mix discrete, continuous, and ordinal variables, often with non-random missingness, challenging standard self-supervision. We propose TabCopula-SSL, a representation learning framework that models dependencies via a latent copula while handling mixed types and missing data. Encoders learn monotone transports that map marginals to uniform variables, and a vine-copula Transformer captures high-order dependencies. Training combines masked-cell imputation with a contrastive objective over conditional copula samples, and a missingness-aware mechanism explicitly models selection bias (MNAR) using a shared instrument. We show identifiability up to monotone transforms under mild sparsity and provide generalization bounds for masked imputation with dependent missingness. Efficient sampling and likelihood estimators enable scalable training. On healthcare (MIMIC-III), finance (Credit, Churn), and UCI benchmarks, TabCopula-SSL improves linear-probe AUC by 2–6% over SAINT, VIME, and Masked TabTransformer, reduces imputation RMSE by 10–18%, and maintains performance under heavy missingness and column shifts. It supports zero-shot column expansion via transport reuse and yields calibrated uncertainty for decision support. Ablations confirm the value of vine structure and MNAR modeling. TabCopula-SSL delivers principled, scalable representations for real-world tabular data, bridging statistical copulas and modern self-supervision.",ICLR,representation learning,gpt-5,True,7866,Cross-Modal Alignment via Variational Copula Modelling,"Various data modalities are common in real-world applications. In healthcare, for example, electronic health records, medical images, and clinical notes provide comprehensive information for diagnosis and treatment.
    Thus, it is essential to develop multimodal learning methods that aggregate information from multiple modalities to generate meaningful representations for downstream tasks.
    The key challenge here is how to appropriately align the representations of the respective modalities and fuse them into a joint distribution.
    Existing methods mainly focus on fusing the representations via concatenation or the Kronecker product, which oversimplifies the interaction structure between modalities, prompting the need to model more complex interactions.
    Moreover, the notion of joint distribution of the latent representation that incorporates higher-order interactions between modalities is also underexplored.
    Copula is a powerful statistical structure in modelling the interactions between variables, as it bridges the joint distribution and marginal distributions of multiple variables.
    In this paper, we propose a novel copula modelling-driven multimodal learning framework, which focuses on learning the joint distribution of various modalities to capture the complex interaction among them.
    The key idea is interpreting the copula model as a tool to align the marginal distributions of the modalities efficiently. 
    By assuming a Gaussian mixture distribution for each modality and a copula model on the joint distribution, our model can also generate accurate representations for missing modalities.
    Extensive experiments on public MIMIC datasets demonstrate the superior performance of our model over other competitors.
    Ablation studies also validate the effectiveness of the copula alignment strategy and the robustness of our model over different choices of the copula family. 
    Code is anonymously available at https://anonymous.4open.science/r/CM2-C1FD/README.md.",ICLR.cc/2025/Conference,5.75,nan,0.8173,tabcopula ssl representation learning that models dependencies latent copula while handling mixed types and missing data encoders learn monotone transports that map marginals uniform variables and vine copula transformer captures high order dependencies supports zero shot column expansion transport reuse and yields calibrated uncertainty for decision support,thus essential multimodal learning methods that aggregate information from multiple modalities generate meaningful representations for downstream tasks moreover the notion joint distribution the latent representation that incorporates higher order interactions between modalities also underexplored this copula modelling driven multimodal learning which focuses learning the joint distribution various modalities capture the complex interaction among them ablation studies also the effectiveness the copula alignment strategy and the robustness our over different choices the copula family,2025-08-26T01:35:51.567561
15,MotifToken: Hierarchical Subgraph Tokenization and Masked Modeling for Transferable Graph Representations,"Graph pretraining struggles to capture meso-scale structure that governs function in molecules and networks. We introduce MotifToken, a self-supervised method that learns a discrete, hierarchical vocabulary of subgraphs and builds representations via masked motif modeling. A differentiable tokenizer assigns nodes to motif tokens using Gumbelized assignments over Weisfeiler–Lehman features and spectral signatures, while a hierarchy module composes motifs into superstructures. Pretraining alternates between (i) masked motif reconstruction with cross-level consistency and (ii) graph–motif contrast that aligns global context with local tokens. We provide sample complexity bounds for recovering frequent motifs and show robustness to edge noise via stability of the tokenization map. MotifToken is backbone-agnostic and integrates with GIN and Graph Transformers. On OGB (ogbg-molpcba, ogbg-molesol, ogbl-ppa) and cross-domain transfer (protein–social), it improves linear-probe and fine-tune performance by 3–8% over GraphCL, GraphMAE, and GPT-GNN, with stronger gains on scaffold and topology shifts. Visualizations confirm coherent motif vocabularies (rings, functional groups, communities), and ablations show the necessity of hierarchical composition. By turning subgraphs into tokens and pretraining with masked modeling at the right granularity, MotifToken yields structure-aware, transferable graph representations for chemistry, biology, and network science.",ICLR,representation learning,gpt-5,True,1059,Towards Fine-grained Molecular Graph-Text Pre-training,"Understanding molecular structure and related knowledge is crucial for scientific research. Recent studies integrate molecular graphs with their textual descriptions to enhance molecular representation learning. However, they focus on the whole molecular graph and neglect frequently occurring subgraphs, known as motifs, which are essential for determining molecular properties. Without such fine-grained knowledge, these models struggle to generalize to unseen molecules and tasks that require motif-level insights. To bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular graph-Text pre-training framework to jointly learn coarse-grained molecule-level knowledge and fine-grained motif-level knowledge. Specifically, FineMolTex consists of two pre-training tasks: a contrastive alignment task for coarse-grained matching and a masked multi-modal modeling task for fine-grained matching. In particular, the latter predicts the labels of masked motifs and words, leveraging insights from each other, thereby enabling FineMolTex to understand the fine-grained matching between motifs and words. Finally, we conduct extensive experiments across three downstream tasks, achieving up to 230% improvement in the text-based molecule editing task. Additionally, our case studies reveal that FineMolTex successfully captures fine-grained knowledge, potentially offering valuable insights for drug discovery and catalyst design.",ICLR.cc/2025/Conference,4.0,nan,0.8027,provide sample complexity bounds for recovering frequent motifs and robustness edge noise stability the tokenization map ogb ogbg molpcba ogbg molesol ogbl ppa and cross domain transfer protein social improves linear probe and fine tune over graphcl graphmae and gpt gnn stronger gains scaffold and topology shifts turning subgraphs into tokens and pretraining masked modeling the right granularity motiftoken yields structure aware transferable graph representations for chemistry biology and network science,understanding molecular structure and related knowledge crucial for scientific recent studies integrate molecular graphs their textual descriptions enhance molecular representation learning bridge this gap finemoltex fine grained molecular graph text pre training jointly learn coarse grained molecule level knowledge and fine grained motif level knowledge,2025-08-26T01:35:51.567571
16,BridgeRep: Schrödinger Bridge Pretraining for Manifold-Aligned Representations,"Self-supervised learning with augmentations implicitly assumes a smooth manifold linking views of the same instance, yet standard objectives do not model this geometry and can distort semantic structure. We introduce BridgeRep, a pretraining framework that learns representations by solving a Schrödinger bridge between augmented views, constructing minimal-energy stochastic flows in latent space that align data manifolds. Our approach couples an encoder with a pair of time-indexed drift fields that transport the latent distribution from one augmentation to another under entropic regularization. Training minimizes a path-space KL divergence and a cross-time consistency loss derived from probability flow ODEs, while a contrastive boundary term preserves instance discrimination. We prove that BridgeRep induces geodesic alignment with respect to a learned Riemannian metric on latents, controls Fisher information distortion, and reduces curvature-induced bias in linear probes. Efficient implementation uses stochastic interpolants, sliced score estimation, and a shared amortized encoder across time. On ImageNet-100, STL-10, and ShapeNet10 point clouds, BridgeRep improves linear-probe accuracy by 2.1–4.8% over MoCo-v3 and MAE with matched compute, and reduces worst-group error under augmentation-violating shifts by 7–10%. Robustness on ImageNet-C/A improves by 2–5% without adversarial training. Ablations confirm the value of path consistency and entropic regularization. By explicitly learning minimal stochastic transports between views, BridgeRep yields manifold-aligned features that transfer more reliably across tasks, corruptions, and modalities.",ICLR,representation learning,gpt-5,True,10433,Annotation Bootstrapping: Reinforcing Visual Pre-Training using Unlabelled Images,"A common approach to learning from unlabeled images is to train models to satisfy invariances on these images, such as consistency under augmentations or crops. Despite successes on Imagenet, these approaches struggle to learn from larger uncurated datasets like web crawls or video, where such inductive biases only weakly hold. How can we more effectively learn from broader datasets? Instead of training models to be invariant across views, we study an alternative approach encouraging model representations to be \textit{predictive} of important semantics of adjacent views of an image. We concurrently train a model to predict semantic annotations from images (generated either self-supervised, or from auxiliary datasets); and bootstrap the model's semantics by predicting, given a cropped view of an image and the coordinates for a nearby crop, the model's annotation distribution for the neighboring view.  A core strength of this approach is the ability to extract information universally from both unlabelled and labelled image data, incorporating captions, bounding boxes, and other annotations when they are present. Our experiments show that annotation propagation improves pre-training on unlabelled datasets in the wild, including video datasets like EpicKitchens, scene datasets like COCO, and uncurated web-scale image datasets like CC12M.",ICLR.cc/2025/Conference,4.25,False,0.8200,self supervised learning augmentations implicitly assumes smooth manifold linking views the same instance yet standard objectives not this geometry and can distort semantic structure robustness imagenet improves adversarial training explicitly learning minimal stochastic transports between views bridgerep yields manifold aligned features that transfer more reliably across tasks corruptions and modalities,common learning from unlabeled images train models satisfy invariances these images such consistency under augmentations crops concurrently train predict semantic annotations from images generated either self supervised from auxiliary datasets and bootstrap the model semantics predicting given cropped view image and the coordinates for nearby crop the model annotation distribution for the neighboring view,2025-08-26T01:35:51.567575
17,Certify-NCE: Randomized Smoothing for Certified Robust Contrastive Representations,"Contrastive pretraining yields strong features but offers no formal guarantees against perturbations, limiting reliability in safety-critical settings. We present Certify-NCE, a certified-robust self-supervised framework that integrates randomized smoothing into contrastive learning to obtain instance-level robustness certificates. Each input is mapped to a smoothed latent distribution via input-space or feature-space Gaussian noise, and a closed-form smoothed cosine similarity defines the InfoNCE objective. We derive certificates on nearest-neighbor consistency and linear-probe predictions under ℓ2 perturbations using the Neyman–Pearson lemma, and extend to ℓ∞ via tailored noise distributions. A variance-adaptive temperature and Jacobian-norm penalty stabilize training and minimize certification–utility trade-offs. We prove that Certify-NCE maximizes a lower bound on mutual information of smoothed variables and show bounds relating certified radii to local curvature of the encoder. Practically, we introduce efficient Monte Carlo estimators with control variates and mixed-precision smoothing. On CIFAR-10/100 and ImageNet-100, Certify-NCE matches or surpasses SimCLR/MoCo-v3 in linear probes while achieving certified top-1 agreement radii 1.5–2× larger than robust-contrastive baselines. Against PGD and AutoAttack, smoothed k-NN and linear probes show 3–7% higher robust accuracy. Under ImageNet-C/A, performance degrades gracefully with calibrated uncertainty. Ablations highlight the synergy of variance-adaptive temperature and feature-space smoothing. Certify-NCE provides the first practical route to certified robust feature learning at scale, bridging representation quality and provable reliability.",ICLR,representation learning,gpt-5,True,8070,Robust Representation Consistency Model via Contrastive Denoising,"Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3\% on average, with up to 11.6\% at larger radii, while reducing inference costs by 85x on average.",ICLR.cc/2025/Conference,6.75,True,0.8280,certify nce certified robust self supervised that integrates randomized smoothing into contrastive learning obtain instance level robustness certificates certify nce provides the first practical route certified robust feature learning scale bridging representation quality and provable reliability,robustness essential for deep neural networks security sensitive applications this end randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations,2025-08-26T01:35:51.567579
18,TimeWarp: Self-Supervised Temporal Reparameterization-Invariant Representations for Irregular Time Series,"Real-world time series are irregularly sampled and subject to unknown temporal warps, yet most self-supervised objectives assume uniform clocks, leading to brittle features under resampling or latency. We introduce TimeWarp, a self-supervised continuous-time framework that learns representations invariant to monotone time reparameterizations while preserving dynamics. TimeWarp employs a neural controlled differential equation encoder driven by learned control paths; invariance is enforced by aligning latent trajectories under optimal transport over warp distributions and penalizing deviations from a common arc-length parameterization. A temporal cycle-consistency loss couples forward and backward flows, and a local stability regularizer bounds sensitivity to sampling jitter via the encoder’s log-Lipschitz constant. We prove invariance to strictly increasing reparameterizations and establish generalization bounds linking alignment cost to downstream risk under clock shifts. Efficient training uses truncated signatures, Sinkhorn-regularized temporal OT, and event batching. On PhysioNet 2019, MIMIC-III vitals, and ETTh1 electricity, TimeWarp improves linear-probe AUROC by 2.8–6.3% over TS2Vec, T-Loss, and MaskedCSD, and lowers forecasting MAE by 7–12% under irregular subsampling. Robustness to variable sampling rates and latency spikes improves markedly, with worst-case performance gaps halved relative to baselines. Ablations confirm the necessity of warp-invariant alignment and arc-length normalization. TimeWarp advances continuous-time representation learning, delivering deployment-ready features for healthcare, sensor networks, and finance.",ICLR,representation learning,gpt-5,False,,An Exploration with Entropy Constrained 3D Gaussians for 2D Video Compression,"3D Gaussian Splatting (3DGS) has witnessed its rapid development in novel view synthesis, which attains high quality reconstruction and real-time rendering. At the same time, there is still a gap before implicit neural representation (INR)  can become a practical compressor due to the lack of stream decoding and real-time frame reconstruction on consumer-grade hardware. It remains a question whether the fast rendering and partial parameter decoding characteristics of 3DGS are applicable to video compression. To address these challenges, we propose a Toast-like Sliding Window (TSW) orthographic projection for converting any 3D Gaussian model into a video representation model. This method efficiently represents video by leveraging temporal redundancy through a sliding window approach. Additionally, the converted model is inherently stream-decodable and offers a higher rendering frame rate compared to INR methods. Building on TSW, we introduce an end-to-end trainable video compression method, GSVC, which employs deformable Gaussian representation and optical flow guidance to capture dynamic content in videos. Experimental results demonstrate that our method effectively transforms a 3D Gaussian model into a practical video compressor.  GSVC further achieves better rate-distortion performance than NeRV on the UVG dataset, while achieving higher frame reconstruction speed (+30%~40% fps) and stream decoding. Code is available at [Github](https://github.com/actcwlf/GSVC)",ICLR.cc/2025/Conference,6.25,True,0.7852,timewarp employs neural controlled differential equation encoder driven learned control paths invariance enforced aligning latent trajectories under optimal transport over warp distributions and penalizing deviations from common arc length parameterization robustness variable sampling rates and latency spikes improves markedly worst case gaps halved relative baselines timewarp advances continuous time representation learning delivering deployment ready features for healthcare sensor networks and finance,the same time there still gap before implicit neural representation inr can become practical compressor due the lack stream decoding and real time frame reconstruction consumer grade hardware address these challenges toast like sliding window tsw orthographic projection for converting any gaussian into video representation building tsw end end trainable video compression gsvc which employs deformable gaussian representation and optical flow guidance capture dynamic content videos,2025-08-26T01:35:51.567588
19,GaugeFormer: Gauge-Equivariant Self-Supervision for Physical Field Representations,"Many scientific domains exhibit gauge symmetries, where observations depend on arbitrary local reference choices, yet existing representation learners enforce only global group equivariances. We propose GaugeFormer, a self-supervised architecture and objective that encode local gauge symmetry for vector and tensor fields. GaugeFormer lifts inputs to fiber bundles and performs attention via parallel transport along learned connections, ensuring equivariance to local U(1)/SO(2) and SU(2) gauge transformations. A masked field modeling objective predicts missing fibers, while a Noether-consistency loss penalizes violations of conservation laws derived from learned Lagrangians. We prove that GaugeFormer’s fiber attention implements gauge-covariant message passing and show identifiability of gauge-invariant content up to bundle isomorphism under mild regularity. Efficient implementations leverage Lie-algebra parameterizations and fast geodesic transports. On magnetostatics, cosmology lensing maps, and turbulent fluid simulations, GaugeFormer improves reconstruction and downstream regression (e.g., vorticity, energy spectra) by 3–9% over equivariant CNNs and physics-informed Transformers, and maintains performance across arbitrary gauge choices. Under distribution shifts in discretization and boundary conditions, it preserves invariants and reduces error by 15–25%. Visualizations reveal physically meaningful, gauge-invariant latents. By respecting local symmetry at the representational level, GaugeFormer produces faithful, transferable features for scientific ML, reducing data needs and improving out-of-distribution reliability.",ICLR,representation learning,gpt-5,True,10430,Incorporating gauge-invariance in equivariant networks,"Gauge theories, which describe fundamental forces in nature, arise from the principle of locality in physical interactions. These theories are characterized by their invariance under local symmetry transformations and the presence of a gauge field that mediates interactions. While recent works have introduced gauge equivariant neural networks, these models often focus on specific cases like tangent bundles or quotient spaces, limiting their applicability to the diverse gauge theories in physics. We propose a novel architecture for learning general gauge invariant quantities by explicitly modeling the gauge field in the context of graph neural networks. Our framework fills a critical gap in the existing literature by providing a general recipe for gauge invariance without restrictions on the fiber spaces. This approach allows for the modeling of more complex gauge theories, such as those with $SU(N)$ gauge groups, which are prevalent in particle physics. We evaluate our method on classical physical systems, including the XY model on various curved geometries, demonstrating its ability to capture gauge invariant properties in settings where existing equivariant architectures fall short. Our work takes a significant step towards bridging the gap between gauge theories in physics and equivariant neural network architectures, opening new avenues for applying machine learning to fundamental physical problems.",ICLR.cc/2025/Conference,4.25,False,0.8348,many scientific domains exhibit gauge symmetries where observations depend arbitrary local reference choices yet existing representation learners enforce only global group equivariances gaugeformer lifts inputs fiber bundles and performs attention parallel transport along learned connections ensuring equivariance local and gauge transformations that gaugeformer fiber attention implements gauge covariant message passing and identifiability gauge invariant content bundle isomorphism under mild regularity,while recent works have introduced gauge equivariant neural networks these models often focus specific cases like tangent bundles quotient spaces limiting their applicability the diverse gauge theories physics for learning general gauge invariant quantities explicitly modeling the gauge field the context graph neural networks our takes significant step towards bridging the gap between gauge theories physics and equivariant neural network architectures opening avenues for applying machine learning fundamental physical problems,2025-08-26T01:35:51.567592
20,Latent Compiler: Program-Supervised Representation Learning with Differentiable Assertions,"Many domains admit declarative knowledge—safety rules, invariants, and physical constraints—that standard self-supervision fails to exploit. We introduce Latent Compiler, a framework that supervises representations with programmatic specifications encoded as differentiable assertions. Specifications written in a fragment of first-order logic with arithmetic and temporal operators are compiled into relaxation networks that compute violation measures; these serve as losses alongside masked prediction or contrast. A dual decomposition bridges logical structure and deep features, yielding certificates that bound constraint violation in terms of latent margin. We prove convergence of the Lagrangian relaxation and establish sample-complexity benefits when constraints reduce hypothesis space capacity. Practically, we provide a compiler from Signal Temporal Logic and domain-specific constraint libraries (rigid-body, monotonicity, resource bounds). On embodied perception (physical plausibility of video), tabular decision support (monotone credit risk), and robotics state estimation (contact and kinematics), Latent Compiler improves downstream accuracy by 2–6% over strong self-supervised baselines and reduces hard-violation rates by 40–70% without labeled data. The approach remains robust under noisy or partial specifications via learned slack variables. Ablations show gains from dual certificates and temporal operators. Latent Compiler turns weak programmatic structure into a powerful supervisory signal, producing controllable, verifiable representations suitable for safety-critical deployment.",ICLR,representation learning,gpt-5,False,,DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL,"Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Büchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Code available at: https://deep-ltl.github.io/",ICLR.cc/2025/Conference,8.0,True,0.7878,specifications written fragment first order logic arithmetic and temporal operators are compiled into relaxation networks that compute violation measures these serve losses alongside masked prediction contrast dual decomposition bridges logical structure and deep features yielding certificates that bound constraint violation terms latent margin,linear temporal logic ltl has recently been adopted powerful formalism for specifying complex temporally extended tasks multi task reinforcement learning however learning policies that satisfy arbitrary specifications not observed during training remains challenging problem this learning address these concerns experiments variety discrete and continuous domains that our able zero shot satisfy wide range finite and infinite horizon specifications and outperforms existing methods terms both satisfaction probability and efficiency,2025-08-26T01:35:51.567599
21,RouteMix: Optimal-Transport-Guided Modular Routing for Disentangled Representation Pretraining,"Mixture-of-experts (MoE) backbones promise scalable capacity, but naive gating yields entangled experts and brittle transfer. We present RouteMix, a modular representation pretraining framework that induces specialized, complementary experts via optimal-transport-guided routing. Inputs are softly assigned to experts by a differentiable router trained to minimize a Sinkhorn divergence between the empirical task mixture and the expert usage distribution, with an information bottleneck that penalizes redundant processing. Cross-expert agreement regularizers encourage consistent global semantics while allowing local specialization, and an expert diversity term maximizes spectral separation of Jacobians. We prove that RouteMix increases subspace diversity and reduces gradient interference, yielding tighter bounds on linear probe risk under task mixtures. Training integrates masked modeling or contrastive losses per expert with shared low-level stems. On multi-domain benchmarks (DomainNet, ImageNet-Sketch/Style, and VTAB), RouteMix improves average linear-probe accuracy by 2.9–6.1% over dense backbones and MoE baselines at matched FLOPs, and boosts worst-domain performance by up to 8%. Experts exhibit interpretable specialization (e.g., edges vs. textures, sketches vs. photos), and adapters fine-tune efficiently per domain. Under few-shot transfer, RouteMix reduces sample complexity by 20–30%. By aligning routing with an optimal transport objective, RouteMix yields modular, disentangled features that scale capacity without sacrificing generalization.",ICLR,representation learning,gpt-5,True,3840,Stealing User Prompts from Mixture-of-Experts Models,"Mixture of Expert (MoE) models improve the efficiency and scalability of dense language models by \emph{routing} each token to a small number of experts in each layer of the model. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit expert-choice routing to the full disclosure of a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layered Mixtral model. Our results show that we can extract the entire prompt using $\mathcal{O}(\text{Vocabulary size} \times \text{prompt length}^2)$ queries or a maximum of 100 queries per token in the setting we consider. Our work is the first of its kind data reconstruction attack that originates from in a flaw in the model architecture, as opposed to the model parameterization.",ICLR.cc/2025/Conference,4.333333333333333,False,0.8818,mixture experts moe backbones promise scalable capacity but naive gating yields entangled experts and brittle transfer routemix modular representation pretraining that induces specialized complementary experts optimal transport guided routing inputs are softly assigned experts differentiable router trained minimize sinkhorn divergence between the empirical task mixture and the expert usage distribution information bottleneck that penalizes redundant processing photos and adapters fine tune per domain under few shot transfer routemix reduces sample complexity,mixture expert moe models improve the efficiency and scalability dense language models emph routing each token small number experts each layer the,2025-08-26T01:35:51.567605
22,HyperMAE: Hierarchical Hyperbolic Masked Autoencoding for Long-Tailed and Hierarchical Structure,"Natural data often follow hierarchical, tree-like structure (e.g., taxonomies, scenes), yet Euclidean representation spaces struggle to embed such geometry efficiently. We propose HyperMAE, a masked autoencoding framework that learns hierarchical representations in a learned-curvature hyperbolic space. An encoder maps inputs into the Poincaré ball via gyrovector operations, while a curvature-adaptive decoder reconstructs masked patches or tokens using Möbius linear layers. A hierarchy alignment loss encourages latent distances to match known or discovered taxonomies via hierarchical clustering, and a negative curvature regularizer prevents local distortion. We derive generalization bounds showing that hyperbolic curvature reduces distortion for tree metrics and improves linear separability for long-tailed categories. Training remains efficient via Riemannian reparameterization and mixed-curvature layers that interleave Euclidean and hyperbolic components. On ImageNet-LT, iNaturalist, and WordNet-aligned label spaces, HyperMAE improves head-to-tail balanced accuracy by 3–7% over MAE/iBOT and boosts zero-shot hierarchical retrieval by 6–10% mAP. Under cross-domain transfer (Sketch/Photo, CIFAR→CIFAR-LT), it preserves fine-grained distinctions while respecting global hierarchy. Ablations validate curvature learning and hierarchy alignment. By marrying masked modeling with hyperbolic geometry, HyperMAE yields compact, hierarchy-aware features that excel on long-tailed recognition and taxonomy-sensitive retrieval.",ICLR,representation learning,gpt-5,True,199,HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean Space,"Data representation in non-Euclidean spaces has proven effective for capturing hierarchical and complex relationships in real-world datasets. Hyperbolic spaces, in particular, provide efficient embeddings for hierarchical structures. This paper introduces the Hyperbolic Vision Transformer (HVT), a novel extension of the Vision Transformer (ViT) that integrates hyperbolic geometry. While traditional ViTs operate in Euclidean space, our method enhances the self-attention mechanism by leveraging hyperbolic distance and Möbius transformations. This enables more effective modeling of hierarchical and relational dependencies in image data. We present rigorous mathematical formulations, showing how hyperbolic geometry can be incorporated into attention layers, feed-forward networks, and optimization. We offer improved performance for image classification using the ImageNet dataset.",ICLR.cc/2025/Conference,3.4,nan,0.8298,taxonomies scenes yet euclidean representation spaces struggle embed such geometry imagenet inaturalist and wordnet aligned label spaces hypermae improves head tail balanced over mae ibot and boosts zero shot hierarchical retrieval map under cross domain transfer sketch photo cifar cifar preserves fine grained distinctions while respecting global hierarchy ablations curvature learning and hierarchy alignment marrying masked modeling hyperbolic geometry hypermae yields compact hierarchy aware features that excel long tailed recognition and taxonomy sensitive retrieval,data representation non euclidean spaces has proven effective for capturing hierarchical and complex relationships real world datasets this introduces the hyperbolic vision transformer hvt extension the vision transformer vit that integrates hyperbolic geometry rigorous mathematical formulations showing how hyperbolic geometry can incorporated into attention layers feed forward networks and optimization offer improved for image classification the imagenet,2025-08-26T01:35:51.567610
23,PolyFactor: Multiplicative Disentanglement via Segre-Embedded Tensor Representations,"Many nuisance variations (style, lighting, background) interact multiplicatively with content, but additive latent models often fail to separate them, hindering robustness to domain shifts. We introduce PolyFactor, a self-supervised framework that learns multiplicatively disentangled representations using Segre embeddings and low-rank tensor factorization. Inputs under a set of augmentations are mapped to order-k latent factors; their outer product forms a Segre tensor constrained to be low-rank via nuclear norm on matricizations. A factor swap-and-recombine objective enforces that content remains invariant while style-like factors interchange predictably, and a cycle-consistency loss prevents degenerate factor leakage. We prove identifiability of factors up to scaling and permutation under mild diversity in augmentation groups and provide bounds linking factor separability to domain generalization error. Efficient training uses randomized SVD on sketches and factorized decoders. On DomainBed (PACS, VLCS, OfficeHome) and Stylized-ImageNet, PolyFactor improves worst-domain accuracy by 4–9% over invariant risk and contrastive baselines, and reduces sensitivity to texture/style by 30–40% measured via controlled perturbations. Linear probes on content factors are robust under heavy style corruptions, while style factors enable targeted editing and augmentation synthesis. Ablations confirm the necessity of Segre construction and low-rank constraints. PolyFactor offers a principled route to multiplicative disentanglement, delivering robust, controllable features across domains.",ICLR,representation learning,gpt-5,True,6452,Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions,"Understanding identifiability of latent content and style variables from unaligned multi-domain data is essential for tasks such as
domain translation and data generation. Existing works on content-style identification were often developed under somewhat stringent conditions, e.g., that all latent components are mutually independent and that the dimensions of the content and style variables are known. We introduce a new analytical framework via cross-domain *latent distribution matching* (LDM), which establishes content-style identifiability under substantially more relaxed conditions. Specifically, we show that restrictive assumptions such as component-wise independence of the latent variables can be removed. Most notably, we prove that prior knowledge of the content and style dimensions is not necessary for ensuring identifiability, if sparsity constraints are properly imposed onto the learned latent representations. Bypassing the knowledge of the exact latent dimension has been a longstanding aspiration in unsupervised representation learning---our analysis is the first to underpin its theoretical and practical viability. On the implementation side, we recast the LDM formulation into a regularized multi-domain GAN loss with coupled latent variables. We show that the reformulation is equivalent to LDM under mild conditions---yet requiring considerably less computational resource. Experiments corroborate with our theoretical claims.",ICLR.cc/2025/Conference,6.6,True,0.8045,many nuisance variations style lighting background interact multiplicatively content but additive latent models often fail separate them hindering robustness domain shifts identifiability factors scaling and permutation under mild diversity augmentation groups and provide bounds linking factor separability domain generalization error,understanding identifiability latent content and style variables from unaligned multi domain data essential for tasks such domain translation and data generation most notably that prior knowledge the content and style dimensions not necessary for ensuring identifiability sparsity constraints are properly imposed onto the learned latent representations bypassing the knowledge the exact latent dimension has been longstanding aspiration unsupervised representation learning our analysis the first underpin its theoretical and practical viability,2025-08-26T01:35:51.567614
24,PrivSSL: Calibrated Differentially Private Self-Supervision with Representation Stability,"Self-supervised pretraining has become a de facto foundation, yet deploying it on sensitive data demands differential privacy (DP), which often cripples utility. We present PrivSSL, a DP self-supervision framework that preserves representation quality by coupling privacy-preserving gradient updates with a stability-inducing latent regularizer. PrivSSL integrates per-sample clipping and adaptive noise scheduling into contrastive or masked objectives, but crucially adds a representation stability penalty that controls local Lipschitzness via Jacobian spectrum smoothing and penalizes variance amplification induced by DP noise. Theoretically, we derive a mutual information upper bound between representations and inputs as a function of the privacy budget (ε, δ) and stability terms, yielding a non-vacuous generalization bound for linear probes under DP-SGD. We further show that PrivSSL admits a privacy amplification by view-composition when using independent augmentations. Algorithmically, we introduce an efficient per-layer clipping scheme with automatic sensitivity calibration and a variance-aware temperature that compensates for DP-induced noise. On ImageNet-100, CIFAR-10/100, SpeechCommands, and MIMIC-III tabular tasks, PrivSSL matches or surpasses non-private baselines at moderate privacy (ε≈3–5), and retains strong utility under tight privacy (ε≈1), improving linear-probe accuracy by 3–8% over DP-SimCLR/MAE and DP-BYOL. Robustness to corruptions and label-scarce transfer also improves. PrivSSL demonstrates that carefully stabilizing the geometry of representations can reconcile stringent privacy guarantees with high-fidelity features, enabling practical, privacy-compliant pretraining in vision, audio, and healthcare.",ICLR,representation learning,gpt-5,True,62,PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse,"Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.",ICLR.cc/2025/Conference,3.0,nan,0.8192,privssl self supervision that preserves representation quality coupling privacy preserving gradient updates stability inducing latent regularizer privssl integrates per sample clipping and adaptive noise scheduling into contrastive masked objectives but crucially adds representation stability penalty that controls local lipschitzness jacobian spectrum smoothing and penalizes variance amplification induced noise robustness corruptions and label scarce transfer also improves,self supervised learning ssl data driven learning that utilizes the innate structure the data guide the learning process contrast supervised learning which depends external labels ssl utilizes the inherent characteristics the data produce its own supervisory signal however one frequent issue ssl methods representation collapse where the outputs constant input invariant feature representation this issue hinders the potential application ssl methods data modalities trying avoid representation collapse wastes researchers time and effort this introduces ssl for time series data called prediction functionals from masked latents pfml the designed avoid representation collapse rendering straightforwardly applicable different time series data domains such sensor modalities clinical data the effectiveness pfml complex real life classification tasks across three different data modalities infant posture and movement classification from multi sensor inertial measurement unit data emotion recognition from speech data and sleep stage classification from eeg data the that pfml superior conceptually similar pre existing ssl and competitive against the current state the art ssl while also being conceptually simpler and suffering from representation collapse,2025-08-26T01:35:51.567617
25,TopoSRL: Persistent Homology Regularization for Topology-Preserving Self-Supervised Representations,"Many downstream tasks depend on global topology—holes, clusters, and connectivity—yet common self-supervised objectives focus on local invariances, distorting global structure. We propose TopoSRL, a topology-aware self-supervised learning framework that regularizes latent geometry using persistent homology. TopoSRL computes barcodes on mini-batch point clouds in latent space via differentiable Vietoris–Rips filtrations and penalizes deviations from topology derived from input-space witnesses, aligning prominent homological features across augmentations. A topology contrast term matches persistence diagrams using entropic regularized Wasserstein distances, while a curvature control stabilizes local neighborhoods. We prove that TopoSRL bounds topological distortion under bi-Lipschitz assumptions and show that preserving long bars induces robustness to spurious shortcuts that commonly degrade linear probe performance. Efficient approximations leverage subsampled landmarks, fast union-find filtrations, and straight-through diagram matching. On ImageNet-100, FashionMNIST-TopoShapes, 3D point clouds (ModelNet40), and retinal vessel datasets, TopoSRL improves linear-probe accuracy by 2–6% over SimCLR, BYOL, and MAE with matched compute, while significantly enhancing shape classification and topology-sensitive segmentation (Dice +3.2). Under distribution shifts that alter local texture but preserve global connectivity, TopoSRL retains up to 10% more accuracy. Ablations validate the contributions of diagram matching and curvature control. By injecting persistent homology into pretraining, TopoSRL learns topology-preserving representations that better capture global structure, benefiting shape understanding, medical imaging, and 3D perception.",ICLR,representation learning,gpt-5,True,4452,ToRL: Topology-preserving Representation Learning Of Object Deformations From Images,"Representation learning of object deformations from images has been a long-standing challenge in various image or video analysis tasks. Existing deep neural networks typically focus on visual features (e.g., intensity and texture), but they often fail to capture the underlying geometric and topological structures of objects. This limitation becomes especially critical in areas, such as medical imaging and 3D modeling, where maintaining the structural integrity of objects is essential for accuracy and generalization across diverse datasets. In this paper, we introduce ToRL, a novel *Topology-preserving Representation Learning* model that, for the first time, offers an explicit mechanism for modeling intricate object topology in the latent feature space. We develop a comprehensive learning framework that captures object deformations via learned transformation groups in the latent space. Each layer of our network's decoder is carefully designed with an integrated smooth composition module, ensuring that topological properties are preserved throughout the learning process. Moreover, in contrast to a few related works that rely on a reference image to predict object deformations during inference, our approach eliminates this impractical requirement. To validate ToRL's effectiveness, we conduct extensive multi-class classification experiments across a wide range of datasets, including synthetic 2D images, real 3D brain magnetic resonance imaging (MRI) scans, real 3D adrenal computed tomography (CT) shapes, and \textcolor{blue}{real 2D facial expression images}. Experimental results demonstrate that ToRL outperforms state-of-the-art methods, setting a new way to enforce topological consistency in representation learning. Our code is available at - https://anonymous.4open.science/r/ToRL-44BF/",ICLR.cc/2025/Conference,5.5,False,0.8523,toposrl topology aware self supervised learning that regularizes latent geometry persistent homology that toposrl bounds topological distortion under lipschitz assumptions and that preserving long bars induces robustness spurious shortcuts that degrade linear probe imagenet fashionmnist toposhapes point clouds modelnet40 and retinal vessel datasets toposrl improves linear probe over simclr byol and mae matched compute while enhancing shape classification and topology sensitive segmentation dice,representation learning object deformations from images has been long standing challenge various image video analysis tasks existing deep neural networks focus visual features this torl topology preserving representation learning that for the first time offers explicit mechanism for modeling intricate object topology the latent feature space comprehensive learning that captures object deformations learned transformation groups the latent space each layer our network decoder carefully designed integrated smooth composition module ensuring that topological properties are preserved throughout the learning process torl effectiveness conduct extensive multi class classification experiments across wide range datasets including synthetic images real brain magnetic resonance imaging mri scans real adrenal computed tomography shapes and textcolor blue real facial expression images experimental that torl outperforms state the art methods setting way enforce topological consistency representation learning,2025-08-26T01:35:51.567627
26,SympMAE: Symplectic Masked Modeling for Dynamics-Aware Representation Learning,"Representations for physical and robotic systems should encode latent dynamics, yet conventional self-supervision ignores structure from Hamiltonian mechanics, limiting predictive fidelity and transfer. We introduce SympMAE, a masked autoencoding framework that enforces symplectic consistency in latent space to learn dynamics-aware features. SympMAE maps observations to latent coordinates and momenta, and trains a symplectic integrator in latent space to predict masked trajectories; a Hamiltonian network parameterizes energy with separable inductive bias. A Poisson bracket regularizer preserves canonical structure, and an invariance term conserves integrals of motion identified by Noether-like symmetries. We show that SympMAE yields a bounded error growth under long-horizon rollouts and preserves bisimulation metrics for control-affine systems. Efficient training uses variational integrators, adjoint-based backpropagation, and masked trajectory reconstruction without labels. On synthetic Hamiltonian systems, MuJoCo pixel tasks, and molecular trajectory snippets (MD17), SympMAE improves long-horizon prediction (N-step MSE −18–35%) and boosts downstream control sample efficiency by 20–30% over video MAE and contrastive baselines. In molecular dynamics, learned latents capture slow collective variables and enhance metastable state discovery. Ablations confirm the importance of symplectic regularization and energy parameterization. SympMAE shows that enforcing canonical structure during pretraining produces faithful, transferable representations for physics, robotics, and molecular simulation.",ICLR,representation learning,gpt-5,True,4422,Identifying latent state transitions in non-linear dynamical systems,"This work aims to recover the underlying states and their time evolution in a latent dynamical system from high-dimensional sensory measurements. Previous works on identifiable representation learning in dynamical systems focused on identifying the latent states, often with linear transition approximations. As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior. Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present. Our identifiability theory relies on two key assumptions: (i) sufficient variability in the latent noise, and (ii) the bijectivity of the augmented transition function. Drawing from this theory, we introduce a practical algorithm based on variational auto-encoders. We empirically demonstrate that it improves generalization and interpretability of target dynamical systems by (i) recovering latent state dynamics with high accuracy, (ii) correspondingly achieving high future prediction accuracy, and (iii) adapting fast to new environments. Additionally, for complex real-world dynamics, (iv) it produces state-of the-art future prediction results for long horizons, highlighting its usefulness for practical scenarios.",ICLR.cc/2025/Conference,6.5,True,0.8345,representations for physical and robotic systems should encode latent dynamics yet conventional self supervision ignores structure from hamiltonian mechanics limiting predictive fidelity and transfer sympmae maps observations latent coordinates and momenta and trains symplectic integrator latent space predict masked trajectories hamiltonian network parameterizes energy separable inductive bias synthetic hamiltonian systems mujoco pixel tasks and molecular trajectory snippets md17 sympmae improves long horizon prediction step mse and boosts downstream control sample efficiency over video mae and contrastive baselines,previous works identifiable representation learning dynamical systems focused identifying the latent states often linear transition approximations empirically that improves generalization and interpretability target dynamical systems recovering latent state dynamics high correspondingly achieving high future prediction and iii adapting fast environments additionally for complex real world dynamics produces state the art future prediction for long horizons highlighting its usefulness for practical scenarios,2025-08-26T01:35:51.567630
27,OPAL-Rep: Operator-Equivariant Self-Supervision for Inverse Imaging Representations,"Imaging systems vary widely in physics (blur, sampling, noise), making representations brittle across scanners and reconstruction pipelines. We present OPAL-Rep, a self-supervised framework that learns features equivariant to a family of linear forward operators, enabling robust transfer across inverse problems. OPAL-Rep augments masked modeling with paired views generated by sampled operators (e.g., MRI undersampling masks, PSFs, CT projections) and trains an encoder–decoder to satisfy operator consistency: encoding either raw measurements or reconstructed images yields latents aligned up to a learned linear action representing the operator’s effect. A commutator penalty enforces approximate representation of the operator algebra, while a data-consistency projection limits hallucinations. We prove identifiability of a canonical latent up to a unitary transform when operators span a stable frame, and show generalization to unseen operators via interpolation in the operator manifold. On fastMRI, GoPro deblurring, and LoDoPaB CT, OPAL-Rep improves linear probes for segmentation and detection by 3–7% over MAE/iGPT pretraining on images alone, and reduces performance drop under operator shifts by 30–50%. For zero-shot adaptation to new sampling masks, OPAL-Rep preserves downstream accuracy without fine-tuning. Ablations highlight the role of commutator regularization. OPAL-Rep enables operator-aware features that generalize across acquisition physics, streamlining deployment in clinical and scientific imaging.",ICLR,representation learning,gpt-5,False,,Learning General-purpose Biomedical Volume Representations using Randomized Synthesis,"Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network's features can be used as robust representations of input images for downstream tasks and its weights provide a strong, _dataset-agnostic_ initialization for finetuning on new datasets. As a result, we set new standards across _both_ multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.",ICLR.cc/2025/Conference,7.0,True,0.7929,opal rep self supervised that learns features equivariant family linear forward operators enabling robust transfer across inverse problems commutator penalty enforces approximate representation the operator algebra while data consistency projection limits hallucinations fastmri gopro deblurring and lodopab opal rep improves linear probes for segmentation and detection over mae igpt pretraining images alone and reduces drop under operator shifts for zero shot adaptation sampling masks opal rep preserves downstream fine tuning,address this creating representation learning that instead anticipates strong domain shifts training time itself then train single network for any voxel level task contrastive learning that pretrains the network stable against nuisance imaging variation simulated the data engine key inductive bias for generalization set standards across _both_ multimodality registration and few shot segmentation first for any biomedical vision all pre training any existing real images,2025-08-26T01:35:51.567636
28,FairCal: Multi-Calibrated Fair Representation Learning under Distribution Shift,"Fair representation learning often trades accuracy for equity and fails to generalize when demographic mixtures shift. We propose FairCal, a self-supervised representation framework that enforces multi-calibration across intersectional subgroups while preserving utility and robustness. FairCal learns an encoder with two coupled objectives: a masked prediction loss and a multi-calibration constraint implemented via differentiable isotonic regression on group-wise score distributions. An adversarial reweighting game dynamically upweights miscalibrated groups, while a distributionally robust risk (f-divergence ball) mitigates shift. We provide generalization guarantees showing that multi-calibration in latent space controls downstream calibration and subgroup error for any Lipschitz classifier, and derive rates under covariate shift. A projection algorithm enforces approximate multi-calibration with provable convergence. On CelebA, FairFace, Adult, and CivilComments, FairCal reduces worst-group error by 6–12% and ECE by 30–45% compared to adversarially fair, invariant risk, and DRO baselines with similar average accuracy. Under simulated demographic shifts and label imbalance, FairCal maintains calibration and narrows disparities without oracle group labels at test time. Ablations support the necessity of intersectional calibration and robust weighting. FairCal provides a principled pathway to fair, reliable representations that remain equitable under deployment shifts, enabling safer use in socially sensitive applications.",ICLR,representation learning,gpt-5,True,11416,Learning Disentangled Representations for Fairness with Limited Demographics,"Fair representation learning is a promising way to mitigate discrimination in downstream tasks. Many existing fair representation learning methods require access to sensitive information, but the collection of sensitive information is often difficult and even involves privacy issues.  Additionally, a model trained to be fair with respect to one sensitive attribute may not ensure fairness for other sensitive groups. Thus, how to flexibly address fairness issues when we have limited access to sensitive information is a challenging problem. In this work, we answer this question: ``given limited sensitive information, can we learn a representation to be fair w.r.t. varying sensitive groups?'' To achieve this, we propose a novel two-step framework. We first learn a disentangled representation by employing Non-linear Independent Component Analysis (Nonlinear ICA). Second, we remove sensitive information in the latent space to obtain fair representation. The learned representation can be easily adapted to be fair w.r.t different sensitive groups and to be used for different downstream tasks without re-training. Among the entire process, only a small portion of sensitive information is required in the second step to learn a fair representation. We compare with methods that require different amounts of sensitive information on real-world images and tabular datasets. We empirically demonstrate the utility and flexibility of our approach, and our method is capable of achieving improved fairness results in various tasks.",ICLR.cc/2025/Conference,4.75,nan,0.8543,fair representation learning often trades for equity and fails generalize when demographic mixtures shift faircal self supervised representation that enforces multi calibration across intersectional subgroups while preserving utility and robustness faircal learns encoder two coupled objectives masked prediction loss and multi calibration constraint implemented differentiable isotonic regression group wise distributions,fair representation learning promising way mitigate discrimination downstream tasks many existing fair representation learning methods require access sensitive information but the collection sensitive information often difficult and even involves privacy issues additionally trained fair respect one sensitive attribute may not ensure fairness for other sensitive groups thus how flexibly address fairness issues when have limited access sensitive information challenging problem this answer this question given limited sensitive information can learn representation fair first learn disentangled representation employing non linear independent component analysis nonlinear ica second remove sensitive information the latent space obtain fair representation the learned representation can easily adapted fair among the entire process only small portion sensitive information required the second step learn fair representation empirically the utility and flexibility our and our capable achieving improved fairness various tasks,2025-08-26T01:35:51.567644
29,NeuroBridge: Weakly-Supervised Brain–Model Alignment for Semantically Grounded Representations,"Foundation models excel on benchmarks but often misalign with human perception. We introduce NeuroBridge, a representation learning framework that weakly aligns model features to human neural responses, producing semantically grounded embeddings. NeuroBridge trains an encoder on standard self-supervision augmented with a brain-alignment term: linear readouts from intermediate features predict fMRI/MEG responses to naturalistic stimuli, and a CCA-based alignment encourages shared subspaces across subjects. A hierarchical temporal prior accounts for response dynamics, and a sparsity-inducing constraint yields localized, interpretable mappings. We prove that adding a small alignment penalty preserves task-optimal invariances while shrinking the hypothesis space toward brain-aligned functions, yielding tighter generalization bounds. Efficient training uses subject-agnostic adapters and multi-task batching across stimuli and modalities. On Natural Scenes Dataset (fMRI), Algonauts, and MEG-Objects, NeuroBridge improves brain-score metrics by 8–15% over strong baselines, while maintaining or slightly improving linear-probe accuracy on ImageNet-100 and zero-shot retrieval. Under distribution shift (sketches, line drawings), aligned features demonstrate better semantic consistency and human similarity judgments. Ablations confirm the value of temporal priors and cross-subject CCA. NeuroBridge offers a practical route to human-aligned representation learning, advancing interpretability and robustness without sacrificing task performance.",ICLR,representation learning,gpt-5,True,7749,LinBridge: A Learnable Framework for Interpreting Nonlinear Neural Encoding Models,"Neural encoding of artificial neural networks (ANNs) aligns the computational representations of ANNs with brain responses, providing profound insights into the neural basis underpinning information processing in the human brain. Current neural encoding studies primarily employ linear encoding models for interpretability, despite the prevalence of nonlinear neural responses. This leads to a growing interest in developing nonlinear encoding models that retain interpretability. To address this problem, we propose LinBridge, a learnable and flexible framework based on Jacobian analysis for interpreting nonlinear encoding models. LinBridge posits that the nonlinear mapping between ANN representations and neural responses can be factorized into a linear inherent component that approximates the complex nonlinear relationship, and a mapping bias that captures sample-selective nonlinearity. The Jacobian matrix, which reflects output change rates relative to input, enables the analysis of sample-selective mapping in nonlinear models. LinBridge employs a self-supervised learning strategy to extract both the linear inherent component and nonlinear mapping biases from the Jacobian matrices of the test set, allowing it to adapt effectively to various nonlinear encoding models. We validate the LinBridge framework in the scenario of neural visual encoding, using computational visual representations from CLIP-ViT to predict brain activity recorded via functional magnetic resonance imaging (fMRI). Our experimental results demonstrate that: 1) the linear inherent component extracted by LinBridge accurately reflects the complex mappings of nonlinear neural encoding models; 2) the sample-selective mapping bias elucidates the variability of nonlinearity across different levels of the visual processing hierarchy. This study not only introduces a novel tool for interpreting nonlinear neural encoding models but also provides novel evidence regarding the distribution of hierarchical nonlinearity within the visual cortex.",ICLR.cc/2025/Conference,4.4,False,0.8206,neurobridge representation learning that weakly aligns features human neural responses producing semantically grounded embeddings efficient training uses subject agnostic adapters and multi task batching across stimuli and modalities natural scenes fmri algonauts and meg objects neurobridge improves brain score metrics over strong baselines while maintaining slightly improving linear probe imagenet and zero shot retrieval under distribution shift sketches line drawings aligned features better semantic consistency and human similarity judgments neurobridge offers practical route human aligned representation learning advancing interpretability and robustness sacrificing task,neural encoding artificial neural networks anns aligns the computational representations anns brain responses providing profound insights into the neural basis underpinning information processing the human brain current neural encoding studies primarily employ linear encoding models for interpretability despite the prevalence nonlinear neural responses this leads growing interest developing nonlinear encoding models that retain interpretability linbridge posits that the nonlinear mapping between ann representations and neural responses can factorized into linear inherent component that approximates the complex nonlinear relationship and mapping bias that captures sample selective nonlinearity linbridge employs self supervised learning strategy extract both the linear inherent component and nonlinear mapping biases from the jacobian matrices the set allowing adapt various nonlinear encoding models the linbridge the scenario neural visual encoding computational visual representations from clip vit predict brain activity recorded functional magnetic resonance imaging fmri our experimental that the linear inherent component extracted linbridge accurately reflects the complex mappings nonlinear neural encoding models the sample selective mapping bias elucidates the variability nonlinearity across different levels the visual processing hierarchy this not only introduces tool for interpreting nonlinear neural encoding models but also provides evidence regarding the distribution hierarchical nonlinearity within the visual cortex,2025-08-26T01:35:51.567650
30,BioFusion: Cross-Species Self-Supervision for Unified Biological Sequence Representations,"Biological sequences across species share latent biochemical principles, yet most representation learners are siloed by modality (DNA, RNA, protein) or species, limiting transfer. We propose BioFusion, a cross-species, cross-modality self-supervised framework that learns unified embeddings for genomics and proteomics. BioFusion combines masked token modeling with structure-aware auxiliary tasks: a thermodynamic consistency loss for RNA secondary structure, a codon–amino acid coupling constraint for coding regions, and a contact-map prediction head for proteins. A shared encoder with adapter routes supports modality-specific nuances while enforcing a common latent alphabet via contrast across orthologous pairs and syntenic blocks. We prove that incorporating orthology-based constraints identifies conserved functional features up to permutation and derive sample-complexity benefits from multi-species co-training. Training scales via blockwise masking, sparse attention on long sequences, and homology-aware negative sampling. On DeepSEA variants, EternaFold RNA, and protein function prediction (Swiss-Prot, GO), BioFusion outperforms modality-specific pretraining by 3–7% AUROC and improves zero-shot transfer across species. It enhances variant effect prediction on ClinVar and enables cross-modal retrieval between coding DNA and protein domains. Ablations show each auxiliary task’s contribution and the importance of orthology contrast. BioFusion delivers biologically grounded, transferable representations, accelerating discovery in genomics and proteomics.",ICLR,representation learning,gpt-5,True,5331,Hyperbolic Genome Embeddings,"Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets---the Transposable Elements Benchmark---which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.",ICLR.cc/2025/Conference,6.5,True,0.8241,biological sequences across species share latent biochemical principles yet most representation learners are siloed modality dna rna protein species limiting transfer biofusion combines masked token modeling structure aware auxiliary tasks thermodynamic consistency loss for rna secondary structure codon amino acid coupling constraint for coding regions and contact map prediction head for proteins training scales blockwise masking sparse attention long sequences and homology aware negative sampling deepsea variants eternafold rna and protein function prediction swiss prot biofusion outperforms modality specific pretraining auroc and improves zero shot transfer across species enhances variant effect prediction clinvar and enables cross modal retrieval between coding dna and protein domains,current approaches genomic sequence modeling often struggle align the inductive biases machine learning models the evolutionarily informed structure biological systems notably our even surpasses state the art seven gue datasets consistently outperforming many dna language models while orders magnitude fewer parameters and avoiding pretraining our include set datasets the transposable elements benchmark which explores major but understudied component the genome deep evolutionary significance throughout these assessments find persistent evidence highlighting the potential our hyperbolic robust paradigm for genome representation learning,2025-08-26T01:35:51.567656
31,Graphon-CL: Graphon-Consistent Contrastive Learning for Subsampled and Evolving Graphs,"Real-world graphs are often observed as subsamples of larger, evolving networks, making representations sensitive to sampling artifacts and size. We present Graphon-CL, a self-supervised method that learns graph representations consistent with an underlying graphon, the limit object of dense graphs. Graphon-CL generates positive pairs by sampling multiple induced subgraphs via different kernels from the same estimated graphon and enforces contrastive agreement after size-normalized pooling. A stochastic graphon estimator with spectral regularization learns a low-rank kernel, and a consistency loss penalizes deviations between encoder outputs and graphon feature maps. We show that Graphon-CL yields size- and subsampling-invariant embeddings and prove concentration bounds on representation drift under node sampling. An efficient training scheme interleaves Nyström approximations and neighborhood sampling, scaling to large graphs. On OGB datasets (ogbn-arxiv, ogbg-ppa), temporal graphs (Reddit, MOOC), and synthetic graphon benchmarks, Graphon-CL improves linear and transfer performance by 2–6% over GraphCL, BGRL, and masked graph modeling, and substantially stabilizes features across varying sample sizes. Under temporal evolution, it preserves community-level semantics and predicts future link patterns more accurately. Ablations confirm the necessity of graphon consistency and spectral regularization. Graphon-CL grounds graph representation learning in limit theory, enabling robust, size-invariant features for dynamic networks.",ICLR,representation learning,gpt-5,True,1270,SAGMAN: Stability Analysis  of Graph Neural Networks  on the Manifolds,"Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability.  We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems. Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks.",ICLR.cc/2025/Conference,4.25,nan,0.8409,stochastic graphon estimator spectral regularization learns low rank kernel and consistency loss penalizes deviations between encoder outputs and graphon feature maps that graphon yields size and subsampling invariant embeddings and concentration bounds representation drift under node sampling ogb datasets ogbn arxiv ogbg ppa temporal graphs reddit mooc and synthetic graphon benchmarks graphon improves linear and transfer over graphcl bgrl and masked graph modeling and stabilizes features across varying sample sizes graphon grounds graph representation learning limit theory enabling robust size invariant features for dynamic networks,modern graph neural networks gnns can sensitive changes the input graph structure and node features potentially resulting unpredictable behavior and degraded distance preserving graph dimension reduction gdr that utilizes spectral graph embedding and probabilistic graphical models pgms create low dimensional input output graph based manifolds for meaningful stability analysis our empirical evaluations that sagman assesses the stability each node when subjected various edge feature perturbations offering scalable for evaluating the stability gnns extending applications within recommendation systems,2025-08-26T01:35:51.567658
32,ActiveAug: Learning Compute-Optimal Augmentation Policies for Self-Supervised Transfer,"Strong self-supervised learners rely on handcrafted augmentation pipelines whose effects vary across domains and budgets, leading to suboptimal transfer. We propose ActiveAug, a bilevel framework that learns augmentation policies end-to-end to maximize downstream utility under explicit compute constraints. A reinforcement learning controller parameterizes a stochastic view policy over a rich augmentation space (appearance, geometry, mixed modalities); it is trained via meta-gradients from a proxy transfer objective that estimates linear-probe risk using a small held-out buffer and influence-function correction. To enforce efficiency, we introduce a differentiable cost regularizer that prices augmentation latency and memory, yielding Pareto-efficient policies. Theoretically, we prove stability of the bilevel estimator and show ActiveAug converges to a stationary point of a compute-constrained mutual information surrogate. Practically, we develop a low-variance credit assignment scheme using augmentation-specific control variates and a two-timescale optimizer that alternates representation updates with policy refinement. On ImageNet-100, DomainNet, and VTAB-1k, ActiveAug applied to SimCLR, BYOL, and MAE improves linear-probe accuracy by 2.3–5.7% at matched FLOPs and reduces worst-domain error by up to 8%. Under tight budgets, it discovers non-intuitive policies (e.g., sparse geometry with selective color jitter) that outperform expert designs. Ablations confirm the value of influence-corrected meta-gradients and cost regularization. ActiveAug turns augmentation design into a learnable component, delivering compute-aware, transfer-optimized self-supervision.",ICLR,representation learning,gpt-5,True,10038,Maximum Total Correlation Reinforcement Learning,"Simplicity is a powerful inductive bias. In reinforcement learning, regularization is used for simpler policies, data augmentation for simpler representations, and sparse reward functions for simpler objectives, all that, with the underlying motivation to increase generalizability and robustness by focusing on the essentials. Supplementary to these techniques, we investigate how to promote simple behavior throughout the duration of the episode. To that end, we introduce a modification of the reinforcement learning problem, that additionally maximizes the total correlation within the induced trajectories. We propose a practical algorithm that optimizes all models, including policy and state representation, based on a lower bound approximation. In simulated robot locomotion environments, our method naturally generates policies that induce periodic and compressible trajectories, and that exhibit superior robustness to noise and changes in dynamics compared to baseline methods, while also improving performance in the original tasks.",ICLR.cc/2025/Conference,4.5,False,0.8210,strong self supervised learners rely handcrafted augmentation pipelines whose effects vary across domains and budgets leading suboptimal transfer reinforcement learning controller parameterizes stochastic view policy over rich augmentation space appearance geometry mixed modalities trained meta gradients from proxy transfer objective that estimates linear probe risk small held out buffer and influence function correction practically low variance credit assignment scheme augmentation specific control variates and two timescale optimizer that alternates representation updates policy refinement,reinforcement learning regularization used for simpler policies data augmentation for simpler representations and sparse reward functions for simpler objectives all that the underlying motivation increase generalizability and robustness focusing the essentials that end modification the reinforcement learning problem that additionally maximizes the total correlation within the induced trajectories simulated robot locomotion environments our naturally generates policies that induce periodic and compressible trajectories and that exhibit superior robustness noise and changes dynamics compared methods while also improving the original tasks,2025-08-26T01:35:51.567664
33,Stream-SSM: Token-Free Masked Pretraining with State Space Models for Ultra-Long Sequences,"Transformer-based masked modeling struggles with ultra-long sequences due to quadratic cost and fragmented context. We introduce Stream-SSM, a token-free self-supervised framework that replaces discrete tokens with continuous state trajectories governed by linear state space models (SSMs) augmented with learned nonlinear readouts. Inputs (text bytes, audio waveforms, DNA bases) are encoded by a shared front-end into continuous signals; a multi-scale SSM processes streams in O(T) time with memory O(1), and masked pretraining reconstructs missing segments via conditional state smoothing. We regularize with a controllability–observability penalty to stabilize long-horizon information flow and add a cross-scale consistency loss to tie coarse and fine states. We prove that Stream-SSM bounds approximation error for band-limited processes and derive a generalization bound that scales with effective Hankel rank, not sequence length. Efficient training uses reversible state updates, parallel scan kernels, and randomized masking spanning thousands of steps. On LibriSpeech-960 audio, The Pile chunks (1M tokens), and human genome windows (100k bp), Stream-SSM surpasses masked Transformer baselines in linear probes by 2–5% while using 3–6× less memory and enabling sequences 10–50× longer. It improves long-context retrieval and variant effect prediction and exhibits strong few-shot adaptation with lightweight adapters. Stream-SSM provides a scalable, modality-agnostic pretraining recipe for ultra-long sequence representation learning.",ICLR,representation learning,gpt-5,True,7928,DC-Spin: A Speaker-invariant Speech Tokenizer For Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",ICLR.cc/2025/Conference,4.75,False,0.8391,librispeech audio the pile chunks tokens and human genome windows 100k stream ssm surpasses masked transformer baselines linear probes while less memory and enabling sequences longer improves long context retrieval and variant effect prediction and exhibits strong few shot adaptation lightweight adapters stream ssm provides scalable modality agnostic pretraining recipe for ultra long sequence representation learning,spoken language models slms have gained increasing attention advancements text based decoder only language models slms process text and speech enabling simultaneous speech understanding and generation this presents double codebook speaker invariant clustering spin which aims improve speech tokenization bridging audio signals and slm tokens spin extracts speaker invariant tokens rich phonetic information and resilient input variations enhancing zero shot slm tasks and speech resynthesis comparisons tokenization methods self supervised and neural audio codecs scalability and downstream task proxies that tokens easily modeled gram aligned phonemes offer strong providing insights for designing speech tokenizers for slms,2025-08-26T01:35:51.567671
34,RAW2Rep: ISP-Invariant Representation Learning from Raw Sensor Data,"Visual pipelines vary widely across cameras and in-camera image signal processors (ISPs), causing models pretrained on RGB images to fail under sensor shifts. We present RAW2Rep, a self-supervised framework that learns camera- and ISP-invariant representations directly from raw mosaics. RAW2Rep employs a physics-aware encoder with learnable demosaicing layers and spectral sensitivity embeddings, paired with an invertible normalizing flow that maps raw measurements to a canonical color space. Training combines masked raw reconstruction with an ISP-consistency objective: features extracted from raw and their rendered counterparts are aligned using a learned family of invertible operators that approximate unknown ISP mappings. A radiometric monotonicity regularizer enforces order preservation, while cross-sensor spectral augmentation improves robustness. We prove identifiability of a canonical latent up to channel-wise affine transforms under mild assumptions on the ISP class and sensor spectra, and show generalization to unseen cameras via interpolating spectral embeddings. On the HDR+ and MIT-5k RAW datasets, RAW2Rep improves linear-probe accuracy for detection and segmentation on rendered images by 3–6% over MAE/SimCLR trained on sRGB. Cross-camera transfer (Sony→Huawei, Canon→Pixel) reduces performance drop by 25–45%, and low-light robustness improves substantially. Ablations confirm the necessity of invertible ISP alignment. RAW2Rep grounds representation learning in sensor physics, enabling durable features across camera pipelines.",ICLR,representation learning,gpt-5,False,,CamI2V: Camera-Controlled Image-to-Video Diffusion Model,"Recent advancements have integrated camera pose as a user-friendly and physics-informed condition in video diffusion models, enabling precise camera control. In this paper, we identify one of the key challenges as effectively modeling noisy cross-frame interactions to enhance geometry consistency and camera controllability. We innovatively associate the quality of a condition with its ability to reduce uncertainty and interpret noisy cross-frame features as a form of noisy condition. Recognizing that noisy conditions provide deterministic information while also introducing randomness and potential misguidance due to added noise, we propose applying epipolar attention to only aggregate features along corresponding epipolar lines, thereby accessing an optimal amount of noisy conditions. Additionally, we address scenarios where epipolar lines disappear, commonly caused by rapid camera movements, dynamic objects, or occlusions, ensuring robust performance in diverse environments.
Furthermore, we develop a more robust and reproducible evaluation pipeline to address the inaccuracies and instabilities of existing camera control metrics. Our method achieves a 25.64\% improvement in camera controllability on the RealEstate10K dataset without compromising dynamics and generation quality, also demonstrating strong generalization to out-of-domain images. Training and inference require only 24GB and 12GB of memory, respectively, for 16-frame sequences at 256×256 resolution. We will release all checkpoints, along with training and evaluation code. Dynamic videos are available for viewing on our supplementary anonymous web page.",ICLR.cc/2025/Conference,5.5,False,0.7885,radiometric monotonicity regularizer enforces order preservation while cross sensor spectral augmentation improves robustness the hdr and mit raw datasets raw2rep improves linear probe for detection and segmentation rendered images over mae simclr trained srgb cross camera transfer sony huawei canon pixel reduces drop and low light robustness improves raw2rep grounds representation learning sensor physics enabling durable features across camera pipelines,recognizing that noisy conditions provide deterministic information while also introducing randomness and potential misguidance due added noise applying epipolar attention only aggregate features along corresponding epipolar lines thereby accessing optimal amount noisy conditions our achieves improvement camera controllability the realestate10k compromising dynamics and generation quality also demonstrating strong generalization out domain images,2025-08-26T01:35:51.567674
35,SpikeSSL: Event-Centric Self-Supervised Learning for Neuromorphic Vision,"Event cameras provide microsecond latency and high dynamic range but yield asynchronous spike streams that break standard image-based pretraining. We introduce SpikeSSL, a self-supervised framework that learns representations directly from event streams without rasterization. SpikeSSL processes events with spatiotemporal graph neural networks over voxelized windows and continuous-time kernels, and optimizes three complementary objectives: (i) polarity-consistent masked event prediction via thinning superposition, (ii) motion-invariant contrast using equivariant warping under SE(2) optical flow, and (iii) a Poisson rate-matching loss that aligns predicted event rates with observed counts. We derive a probabilistic model linking latent features to a conditional Hawkes process, yielding a tractable evidence lower bound and stability via Lipschitz control of intensity functions. Efficient training uses event batching, neighbor hashing, and differentiable time warping. On DSEC and Prophesee EV-IMO, SpikeSSL improves linear-probe accuracy for object classification and optical flow by 4–9% over rasterized MAE and event-contrastive baselines, and reduces latency and power on neuromorphic hardware (Loihi-2) by 1.7× with sparse inference. Under extreme illumination changes and motion blur where frames fail, SpikeSSL maintains robust performance. Ablations highlight the impact of Hawkes modeling and equivariant warping. SpikeSSL unlocks principled self-supervision for neuromorphic sensors, advancing low-power, high-speed perception.",ICLR,representation learning,gpt-5,True,237,SPLR: A Spiking Neural Network for Long-Range Temporal Dependency Learning,"Spiking Neural Networks (SNNs) offer an efficient framework for processing event-driven data due to their sparse, spike-based communication, making them ideal for real-time tasks. However, their inability to capture long-range dependencies limits their effectiveness in complex temporal modeling. To address this challenge, we present a **SPLR (SPiking Network for Learning Long-range Relations)**, a novel architecture designed to overcome these limitations. The core contribution of SPLR is the **Spike-Aware HiPPO (SA-HiPPO)** mechanism, which adapts the HiPPO framework for discrete, spike-driven inputs, enabling efficient long-range memory retention in event-driven systems. Additionally, SPLR includes a convolutional layer that integrates state-space dynamics to enhance feature extraction while preserving the efficiency of sparse, asynchronous processing. Together, these innovations enable SPLR to model both short- and long-term dependencies effectively, outperforming prior methods on various event-based datasets. Experimental results demonstrate that SPLR achieves superior performance in tasks requiring fine-grained temporal dynamics and long-range memory, establishing it as a scalable and efficient solution for real-time applications such as event-based vision and sensor fusion in neuromorphic computing.",ICLR.cc/2025/Conference,5.2,False,0.8332,spikessl processes events spatiotemporal graph neural networks over voxelized windows and continuous time kernels and optimizes three complementary objectives polarity consistent masked event prediction thinning superposition motion invariant contrast equivariant warping under optical flow and iii poisson rate matching loss that aligns predicted event rates observed counts dsec and prophesee imo spikessl improves linear probe for object classification and optical flow over rasterized mae and event contrastive baselines and reduces latency and power neuromorphic hardware loihi sparse inference,spiking neural networks snns offer efficient for processing event driven data due their sparse spike based communication making them ideal for real time tasks address this challenge splr spiking network for learning long range relations designed overcome these limitations additionally splr includes convolutional layer that integrates state space dynamics enhance feature extraction while preserving the efficiency sparse asynchronous processing experimental that splr achieves superior tasks requiring fine grained temporal dynamics and long range memory establishing scalable and efficient solution for real time applications such event based vision and sensor fusion neuromorphic computing,2025-08-26T01:35:51.567677
36,SeedAlign: Reproducible and Transferable Representations via Optimal Transport Alignment Across Random Seeds,"Stochastic training yields representation drift across random seeds, harming reproducibility and downstream consistency. We propose SeedAlign, a post-hoc and online method that aligns representations learned under different seeds into a common canonical space using optimal transport (OT). SeedAlign estimates sparse OT maps between mini-batch embeddings from multiple seeds, regularized by isometry and class-agnostic neighborhood preservation, and composes them into a global alignment via barycentric averaging on the Stiefel manifold. We show that OT alignment minimizes a bound on downstream linear-probe disagreement and prove stability under subspace perturbations. An online variant interleaves alignment with self-supervised training, adding a small alignment loss that penalizes deviations from the evolving barycenter without sharing data or labels. On ImageNet-100, CIFAR-100, and DomainNet, SeedAlign reduces cross-seed k-NN and linear-probe variance by 40–65% and improves average accuracy by 1–2% by mitigating seed-specific artifacts. In federated and multi-view settings where independent models are trained on disjoint data, SeedAlign enables feature fusion, boosting transfer by 2–4%. We demonstrate consistent saliency and probing results across seeds, aiding interpretability. SeedAlign offers a practical path to reproducible, stable representation learning without constraining base training, improving reliability and enabling collaborative model ecosystems.",ICLR,representation learning,gpt-5,True,6696,Heterogeneous Federated Learning: A Dual Matching Dataset Distillation Approach,"Federated Learning (FL) often struggles with error accumulation during local training, particularly on heterogeneous data, which hampers overall performance and convergence. While dataset distillation is commonly introduced to FL to enhance efficiency, our work finds that communicating distilled data instead of models can completely get rid of the error accumulation issue, albeit at the cost of exacerbating data heterogeneity across clients. To address the amplified heterogeneity due to distilled data, we propose a novel FL algorithm termed \textit{FedDualMatch}, which performs dual matching in the way that local distribution matching captures client data distributions while global gradient matching aligns gradients on the server. This dual approach enriches feature representations and enhances convergence stability. It proves effective for FL due to a bounded difference in the testing loss between optimal models trained on the aggregation of either distilled or original data across clients. At the same time, it can converge to within a bounded constant of the optimal model loss. Experiments on controlled heterogeneous dataset MNIST/CIFAR10 and naturally heterogeneous dataset Digital-Five/Office-Home demonstrate its advantages over the state-of-the-art methods that communicate either model or distilled data, in terms of accuracy and convergence. Notably, it maintains accuracy even when data heterogeneity significantly increases, underscoring its potential for practical applications.",ICLR.cc/2025/Conference,3.8,False,0.8174,stochastic training yields representation drift across random seeds harming reproducibility and downstream consistency federated and multi view settings where independent models are trained disjoint data seedalign enables feature fusion boosting transfer consistent saliency and probing across seeds aiding interpretability seedalign offers practical path reproducible stable representation learning constraining base training improving reliability and enabling collaborative ecosystems,federated learning often struggles error accumulation during local training heterogeneous data which hampers overall and convergence this dual enriches feature representations and enhances convergence stability,2025-08-26T01:35:51.567683
37,MixDiff-Rep: Mixture-of-Diffusions Teacher–Student Pretraining for Heterogeneous Data,"Real-world corpora are mixtures of subpopulations (styles, domains, structures), yet standard pretraining collapses them into a single embedding geometry, hurting worst-group performance. We propose MixDiff-Rep, a teacher–student framework where a mixture of latent diffusion models defines subpopulation-aware target fields for representation learning. A teacher learns K diffusion priors over latent codes with Dirichlet mixing; its score fields guide a student encoder via a cross-field regression loss that aligns student features with mixture-weighted scores at multiple noise levels. A sparsity-promoting gating network predicts soft subpopulation memberships per instance; an entropy regularizer prevents collapse. We prove that, under mild separability, the student approximates the Fisher score of the underlying mixture, reducing Bayes error for mixture-aware linear probes. Training scales via sliced score matching and shared denoisers. On DomainBed (PACS, VLCS, OfficeHome) and multi-style speech (VCTK), MixDiff-Rep improves worst-domain accuracy by 4–8% over contrastive and invariant risk baselines, and maintains average accuracy. On heterogeneous medical images (Camelyon, ISIC), it reduces negative transfer and enhances OOD detection (AUROC +5–9%). Ablations confirm the roles of multi-noise guidance and gating sparsity. MixDiff-Rep leverages generative mixtures to sculpt subpopulation-sensitive features, improving robustness in heterogeneous data regimes.",ICLR,representation learning,gpt-5,True,2464,DEPT: Decoupled Embeddings for Pre-training Language Models,"Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",ICLR.cc/2025/Conference,8.0,True,0.8073,real world corpora are mixtures subpopulations styles domains structures yet standard pretraining collapses them into single embedding geometry hurting worst group mixdiff rep teacher student where mixture latent diffusion models defines subpopulation aware target fields for representation learning sparsity promoting gating network predicts soft subpopulation memberships per instance entropy regularizer prevents collapse heterogeneous medical images camelyon isic reduces negative transfer and enhances ood detection auroc mixdiff rep leverages generative mixtures sculpt subpopulation sensitive features improving robustness heterogeneous data regimes,language pre training uses broad data mixtures enhance across domains and languages since these data sources vary lexical syntactic and semantic aspects they cause negative interference the curse multilinguality our decouples embeddings from the transformer body while simultaneously training the latter multiple data sources requiring shared vocabulary dept can train robustly and under significant data heterogeneity minimize token embedding parameters only what the data source vocabulary requires while cutting communication costs direct proportion both the communication frequency and the reduction parameters enhance transformer body plasticity and generalization improving both average perplexity and downstream task and enable training custom optimized vocabularies per data source dept potential the first vocabulary agnostic federated pre training billion scale models reducing communication costs orders magnitude and embedding memory,2025-08-26T01:35:51.567693
38,Spectral Options: Self-Supervised Subgoal Representations for Sample-Efficient Control,"Reinforcement learning from pixels remains sample-inefficient due to entangled features that ignore controllability structure. We introduce Spectral Options, a representation learning framework that discovers subgoal-aligned features via graph Laplacian eigenfunctions and controllability measures, enabling option discovery and efficient control. From agent experience, we build a kNN transition graph in latent space; a spectral module learns eigenfunctions with Dirichlet boundary conditions on discovered bottlenecks, while a controllability penalty aligns features with state-action influence. A masked predictive objective ties features to multi-step dynamics, and a bisimulation regularizer contracts behaviorally equivalent states. We prove that spectral features minimize commute times and bound value approximation error for options constructed from level sets. Implemented with lightweight decoders atop DrQv2, Spectral Options improves sample efficiency on DMControl (pixels) by 30–55%, achieves competitive Atari-100k performance, and transfers options across task variants with minimal fine-tuning. In sparse-reward mazes and manipulation, discovered subgoals coincide with chokepoints and grasp poses, accelerating learning. Ablations validate the interplay between spectral structure and controllability. Spectral Options elevates representation learning for control by exposing actionable subgoals, narrowing the gap between self-supervision and model-based exploration.",ICLR,representation learning,gpt-5,True,9775,Objects matter: object-centric world models improve reinforcement learning in visually complex environments,"Deep reinforcement learning has achieved remarkable success in learning control policies from pixels across a wide range of tasks, yet its application remains hindered by low sample efficiency, requiring significantly more environment interactions than humans to reach comparable performance.
Model-based reinforcement learning (MBRL) offers a solution by leveraging learnt world models to generate simulated experience, thereby improving sample efficiency.
However, in visually complex environments, small or dynamic elements can be critical for decision-making.
Yet, traditional MBRL methods in pixel-based environments typically rely on auto-encoding with an $L_2$ loss, which is dominated by large areas and often fails to capture decision-relevant details.
To address these limitations, we propose an **object-centric MBRL pipeline**, which integrates recent advances in computer vision to allow agents to focus on key decision-related elements.
Our approach consists of four main steps: (1) annotating key objects related to rewards and goals with segmentation masks, (2) extracting object features using a pre-trained, frozen foundation vision model, (3) incorporating these object features with the raw observations to predict environmental dynamics, and (4) training the policy using imagined trajectories generated by this object-centric world model.
Building on the efficient MBRL algorithm STORM, we call this pipeline **OC-STORM**.
We demonstrate OC-STORM's practical value in overcoming the limitations of conventional MBRL approaches on both Atari games and the visually complex game Hollow Knight.
Code and videos are available in the supplementary materials.",ICLR.cc/2025/Conference,6.0,False,0.8326,reinforcement learning from pixels remains sample inefficient due entangled features that ignore controllability structure spectral options representation learning that discovers subgoal aligned features graph laplacian eigenfunctions and controllability measures enabling option discovery and efficient control sparse reward mazes and manipulation discovered subgoals coincide chokepoints and grasp poses accelerating learning spectral options elevates representation learning for control exposing actionable subgoals narrowing the gap between self supervision and model based exploration,deep reinforcement learning has achieved remarkable success learning control policies from pixels across wide range tasks yet its application remains hindered low sample efficiency requiring more environment interactions than humans reach comparable model based reinforcement learning mbrl offers solution leveraging learnt world models generate simulated experience thereby improving sample efficiency address these limitations object centric mbrl pipeline which integrates recent advances computer vision allow agents focus key decision related elements our consists four main steps annotating key objects related rewards and goals segmentation masks extracting object features pre trained frozen foundation vision incorporating these object features the raw observations predict environmental dynamics and training the policy imagined trajectories generated this object centric world,2025-08-26T01:35:51.567697
39,ExecGraph-SSL: Execution-Guided Graph Self-Supervision for Code Representations,"Learning code representations solely from syntax overlooks dynamic semantics critical for program understanding and verification. We present ExecGraph-SSL, a self-supervised framework that unifies static graphs with lightweight execution traces to learn semantically faithful embeddings. We construct hybrid program graphs combining AST, control/data flow, and a trace DAG from partial execution on sampled inputs in a sandbox. A graph encoder with typed attention processes the hybrid graph, trained with (i) masked node/edge prediction, (ii) execution consistency—predicting trace states and variable lifetimes—and (iii) an alias/taint contrast to separate independent flows. We establish that execution consistency reduces spurious equivalences in the latent space and prove generalization benefits for downstream relational queries under a PAC-Bayesian analysis. Efficient tracing uses symbolic execution fallbacks and caching. On CodeSearchNet, Devign (vulnerability detection), and BigCloneBench, ExecGraph-SSL improves MRR/F1 by 3–7% over code-only pretraining (CodeBERT, GraphCodeBERT) with similar compute and enhances cross-language transfer (Java↔Python). It identifies security sinks with higher precision and supports zero-shot clone detection via flow-aware similarity. Ablations show the necessity of trace DAGs and alias contrast. ExecGraph-SSL advances representation learning for code by injecting execution semantics, enabling safer, more accurate program analysis and retrieval.",ICLR,representation learning,gpt-5,True,11327,VisualCoder: Guiding Vision Language Models in Code Execution with Fine-grained Chain-of-Thought Reasoning,"Predicting program behavior and reasoning about code execution remain significant challenges in software engineering, particularly for large language models (LLMs) designed for code analysis. While these models excel at understanding static syntax, they often struggle with dynamic reasoning tasks. We introduce VisualCoder, a novel approach that enhances code reasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with visual Control Flow Graphs (CFGs). By aligning code snippets with their corresponding CFGs, VisualCoder provides deeper insights into execution flow, enabling more accurate predictions of code behavior. Our experiments demonstrate that augmenting LLMs with visual CFGs significantly outperforms text-based CFG descriptions in code reasoning tasks. We address challenges in multimodal CoT integration through a reference mechanism, ensuring consistency between code and its execution path, thereby improving performance in program behavior prediction, error detection, and output generation.",ICLR.cc/2025/Conference,2.0,nan,0.8328,learning code representations solely from syntax overlooks dynamic semantics critical for program understanding and verification graph encoder typed attention processes the hybrid graph trained masked node edge prediction execution consistency predicting trace states and variable lifetimes and iii alias taint contrast separate independent flows codesearchnet devign vulnerability detection and bigclonebench execgraph ssl improves mrr over code only pretraining codebert graphcodebert similar compute and enhances cross language transfer java python identifies security sinks higher and supports zero shot clone detection flow aware similarity execgraph ssl advances representation learning for code injecting execution semantics enabling safer more accurate program analysis and retrieval,predicting program behavior and reasoning about code execution remain significant challenges software engineering for large language models llms designed for code analysis while these models excel understanding static syntax they often struggle dynamic reasoning tasks visualcoder that enhances code reasoning integrating multimodal chain thought cot reasoning visual control flow graphs cfgs our experiments that augmenting llms visual cfgs outperforms text based cfg descriptions code reasoning tasks address challenges multimodal cot integration reference mechanism ensuring consistency between code and its execution path thereby improving program behavior prediction error detection and output generation,2025-08-26T01:35:51.567701
40,FunctorRep: Compositional Representation Learning with Category-Theoretic Consistency,"Many real-world tasks require compositional generalization—reasoning about novel combinations of known parts—yet standard self-supervised objectives lack explicit structure for composition. We introduce FunctorRep, a category-theoretic framework that enforces compositionality by aligning transformations in input space with composition in latent space. We model data augmentations, editing operations, and multi-modal bindings as morphisms in a small monoidal category, and learn an encoder that is a lax monoidal functor: compositions of operations map to corresponding compositions of latent maps. Concretely, we instantiate learned natural transformations that tie per-operation adapters across layers and train with a functorial consistency loss, a monoidal coherence penalty, and a masked reconstruction or contrastive base objective. We prove that functorial consistency bounds the error of zero-shot composition and yields improved sample complexity for downstream tasks whose labels are definable by finite compositions. Efficient implementations use low-rank adapters, operator caching, and amortized naturality checks over random diagrams. On CLEVR and CATER (visual reasoning), TextCaps (vision–language composition), and COCO-panoptic, FunctorRep improves compositional generalization by 4–9% over strong baselines (iBOT, CLIP-initialized MAE), while preserving or improving average accuracy. It reliably extrapolates to unseen composition depths and novel operation orders. Ablations confirm the necessity of monoidal coherence and layerwise naturality. FunctorRep makes compositional structure a first-class citizen in representation learning, enabling robust zero-shot composition, data-efficient transfer, and principled multi-operation reasoning.",ICLR,representation learning,gpt-5,False,,Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning,"Compositional zero-shot learning (CZSL) aims to recognize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle attribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are hampered by three limitations: (1) the efficacy of disentanglement is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to capture complex multimodal semantic information. (3) overconfidence exhibited by existing models in seen compositions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and attribute smoothing guided disentanglement (TRIDENT) for CZSL. First, we leverage feature adaptive aggregation (FAA) modules to mitigate the impact of background, and utilize learnable condition masks to capture multi-granularity features for subsequent disentanglement. Then, the last hidden states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing through leveraging auxiliary attributes generated by Large Language Model (LLM) for each seen composition, addressing the issue of overconfidence by encouraging the model to learn more attributes in one given composition instead of just fitting a fixed attribute-object combination. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three challenging datasets: MIT-States, C-GQA, and VAW-CZSL, respectively.",ICLR.cc/2025/Conference,4.6,nan,0.7925,that functorial consistency bounds the error zero shot composition and yields improved sample complexity for downstream tasks whose labels are definable finite compositions functorrep makes compositional structure first class citizen representation learning enabling robust zero shot composition data efficient transfer and principled multi operation reasoning,compositional zero shot learning czsl aims recognize compositions attributes and objects learned from seen compositions previous works disentangle attribute and object extracting shared and exclusive parts between image pairs sharing the same attribute object well aligning them pretrained word embeddings improve unseen attribute object recognition existing word embeddings fail capture complex multimodal semantic information being aware these named multimodal large language mllm embeddings and attribute smoothing guided disentanglement trident for czsl first leverage feature adaptive aggregation faa modules mitigate the impact background and utilize learnable condition masks capture multi granularity features for subsequent disentanglement then the last hidden states mllm are employed word embeddings for their superior representation capabilities moreover attribute smoothing leveraging auxiliary attributes generated large language llm for each seen composition addressing the issue overconfidence encouraging the learn more attributes one given composition instead just fitting fixed attribute object combination,2025-08-26T01:35:51.567704
41,KoopFlow: Self-Supervised Representations via Koopman-Consistent Linearization of Dynamics,"Representations for dynamical data should linearize evolution to support forecasting and control, yet standard self-supervision ignores operator structure. We propose KoopFlow, a self-supervised framework that learns features in which nonlinear dynamics evolve approximately linearly under a shared Koopman operator. An encoder maps observations to observables; a low-rank operator advances latents over time, trained jointly with the encoder via a contrastive predictive loss with multi-step consistency and a spectral regularizer enforcing a stable eigenstructure. A residual nonlinear head captures irreducible dynamics while a commutator penalty encourages time-shift equivariance. We prove that KoopFlow minimizes a bound on forecast risk proportional to the mismatch between true and learned Koopman subspaces and show identifiability up to similarity for systems with sparse spectral measures. Training is efficient via block Krylov updates, truncated backpropagation, and curriculum on prediction horizons. On video physics (KTH-Physics, BAIR), robotics proprioception (DMControl pixels), and climate reanalysis fields, KoopFlow improves long-horizon prediction (MSE −15–32%) and control sample efficiency by 18–35% over contrastive and masked modeling baselines. Latents exhibit linear phase portraits and interpretable modes (e.g., oscillatory vs. dissipative). Under distribution shift (changed damping/forcing), KoopFlow retains superior transfer. By aligning features with Koopman structure, KoopFlow yields dynamics-aware representations that scale across domains, bridging forecasting, control, and self-supervision.",ICLR,representation learning,gpt-5,True,4103,Predictive Differential Training Guided by Training Dynamics,"This paper centers around a novel concept proposed recently by researchers from the control community where the training process of a deep neural network can be considered a nonlinear dynamical system acting upon the high-dimensional weight space. Koopman operator theory, a data-driven dynamical system analysis framework, can then be deployed to discover the otherwise non-intuitive training dynamics. Taking advantage of the predictive power of the Koopman operator theory, the time-consuming Stochastic Gradient Descent ( SGD) iterations can be bypassed by directly predicting network weights a few epochs later. This novel predictive training framework, however, often suffers from gradient explosion especially for more extensive and complex models. In this paper, we incorporate the idea of differential learning, where different parts of the network can undergo different learning rates during training, into the predictive training framework and propose the so-called ""predictive differential training'' (PDT) to sustain robust performance for accelerated learning even for complex network structures. The key contribution is the design of an effective masking strategy based on Koopman analysis of training dynamics of each parameter in order to select the subset of parameters that exhibits ""good'' prediction performance. PDT also includes the design of an acceleration scheduler to keep track of the prediction error so that the training process can roll back to the traditional GD-based approaches to ""correct'' deviations  from off-predictions. We demonstrate that PDT can be seamlessly integrated as a plug-in with existing optimizers, including, for example, SGD, momentum, and Adam. The experimental results have shown consistent performance improvement in terms of faster convergence, lower training/testing loss, and fewer number of epochs to achieve the best loss of Baseline.",ICLR.cc/2025/Conference,5.5,False,0.8011,training efficient block krylov updates truncated backpropagation and curriculum prediction horizons video physics kth physics bair robotics proprioception dmcontrol pixels and climate reanalysis fields koopflow improves long horizon prediction mse and control sample efficiency over contrastive and masked modeling baselines under distribution shift changed damping forcing koopflow retains superior transfer,this centers around concept proposed recently researchers from the control community where the training process deep neural network can considered nonlinear dynamical acting upon the high dimensional weight space taking advantage the predictive power the koopman operator theory the time consuming stochastic gradient descent sgd iterations can bypassed directly predicting network weights few epochs later this incorporate the idea differential learning where different parts the network can undergo different learning rates during training into the predictive training and the called predictive differential training pdt sustain robust for accelerated learning even for complex network structures the key the effective masking strategy koopman analysis training dynamics each parameter order select the subset parameters that exhibits good prediction pdt also includes the acceleration scheduler keep track the prediction error that the training process can roll back the traditional based approaches correct deviations from off predictions,2025-08-26T01:35:51.567709
42,MDL-MAE: Minimum Description Length Regularization for Generalizable Masked Autoencoders,"Masked autoencoders excel at reconstruction yet may overfit nuisance detail, harming transfer. We introduce MDL-MAE, a self-supervised objective that augments masked modeling with a principled minimum description length (MDL) regularizer to encourage compressive, task-relevant features. MDL-MAE replaces standard reconstruction loss with normalized maximum likelihood (NML) code length for masked tokens/patches, approximated by a tractable stochastic complexity surrogate with bits-back amortization. A dual penalty constrains decoder capacity and encourages predictive sufficiency of the encoder, while a curvature-aware Jacobian term smooths local geometry. We provide generalization bounds linking downstream linear-probe risk to latent codelength and derive conditions under which MDL regularization filters high-frequency nuisances while preserving label-sufficient statistics. Efficient training employs variance-reduced NML estimators, partial parameter tying in the decoder, and dynamic mask schedules. On ImageNet-100, ImageNet-LT, and LibriSpeech, MDL-MAE improves linear-probe accuracy by 2.0–4.3% over MAE/iBOT with matched compute and reduces worst-group error under texture/style shifts by 6–9%. Under heavy masking, it maintains fidelity with 20–30% fewer decoder parameters, and exhibits stronger robustness on ImageNet-C/A. Ablations confirm the contributions of NML codelength and bits-back amortization. MDL-MAE grounds masked modeling in information-theoretic principles, producing compact, generalizable representations that transfer across domains and budgets.",ICLR,representation learning,gpt-5,True,8904,Point Cloud Self-supervised Learning via 3D to Multi-view Masked Leaner,"Recently, multi-modal masked autoencoders (MAE) has been introduced in 3D self-supervised learning, offering enhanced feature learning by leveraging both 2D and 3D data to capture richer cross-modal representations. However, these approaches have two limitations: (1) they inefficiently require both 2D and 3D modalities as inputs, even though the inherent multi-view properties of 3D point clouds already contain 2D modality.
(2) input 2D modality causes the reconstruction learning to unnecessarily rely on visible 2D information, hindering 3D geometric representation learning.
To address these challenges, we propose a 3D to Multi-View Learner (Multi-View ML) that only utilizes 3D modalities as inputs and effectively capture rich spatial information in 3D point clouds. 
Specifically, we first project 3D point clouds to multi-view 2D images at the feature level based on 3D-based pose.
Then, we introduce two components: (1) a 3D to multi-view autoencoder that reconstructs point clouds and multi-view images from 3D and projected 2D features; 
(2) a multi-scale multi-head (MSMH) attention mechanism that facilitates local-global information interactions in each decoder transformer block through attention heads at various scales. 
Additionally, a novel two-stage self-training strategy is proposed to align 2D and 3D representations.
Empirically, our method significantly outperforms state-of-the-art counterparts across various downstream tasks, including 3D classification, part segmentation, and object detection.
Such performance superiority showcases that Multi-View ML enriches the model's comprehension of geometric structures and inherent multi-modal properties of point clouds.",ICLR.cc/2025/Conference,6.0,False,0.8638,masked autoencoders excel reconstruction yet may overfit nuisance detail harming transfer under heavy masking maintains fidelity fewer decoder parameters and exhibits stronger robustness imagenet mdl mae grounds masked modeling information theoretic principles producing compact generalizable representations that transfer across domains and budgets,recently multi modal masked autoencoders mae has been introduced self supervised learning offering enhanced feature learning leveraging both and data capture richer cross modal representations input modality causes the reconstruction learning unnecessarily rely visible information hindering geometric representation learning first project point clouds multi view images the feature level based pose then two components multi view autoencoder that reconstructs point clouds and multi view images from and projected features multi scale multi head msmh attention mechanism that facilitates local global information interactions each decoder transformer block attention heads various scales empirically our outperforms state the art counterparts across various downstream tasks including classification part segmentation and object detection,2025-08-26T01:35:51.567715
43,ConformalRep: Distribution-Free Risk Control for Representation Learning,"Representation learning typically optimizes surrogate losses without explicit guarantees on downstream decisions such as k-NN or retrieval. We propose ConformalRep, a self-supervised framework that calibrates latent spaces with conformal prediction, providing finite-sample, distribution-free guarantees on neighborhood decisions. ConformalRep trains standard encoders (contrastive or masked) augmented with a conformity head that outputs uncertainty-aware radii. Using a held-out calibration set, we compute nonparametric quantiles of conformity scores, yielding prediction sets in latent space that provably control miscoverage for nearest-neighbor classification, retrieval, and clustering assignments. A differentiable surrogate of set size encourages compact, well-calibrated neighborhoods; a Lipschitz penalty stabilizes distances. We derive guarantees for adaptive-radii k-NN and retrieval precision under exchangeability and show robustness under covariate shift with weighted conformal scores. Efficient algorithms use batched quantile sketches and memory banks for calibration. On CIFAR-10/100, ImageNet-100, and Fashion-MNIST retrieval, ConformalRep matches or exceeds SimCLR/MoCo in linear probes while providing calibrated coverage: empirical miscoverage aligns with target levels across datasets. Under distribution shift (ImageNet-C/A and Sketch), calibrated k-NN degrades gracefully and retains 4–7% higher controlled recall. Ablations highlight the roles of uncertainty heads and weighted conformal scores. ConformalRep couples strong unsupervised features with actionable, distribution-free risk control, improving reliability in open-world retrieval and decision-making.",ICLR,representation learning,gpt-5,True,1402,A Generic Framework for Conformal Fairness,"Conformal Prediction (CP) is a popular method for uncertainty quantification with machine learning models. While conformal prediction provides probabilistic guarantees regarding the coverage of the true label, these guarantees are agnostic to the presence of sensitive attributes within the dataset. In this work, we formalize \textit{Conformal Fairness}, a notion of fairness using conformal predictors, and provide a theoretically well-founded algorithm and associated framework to control for the gaps in coverage between different sensitive groups. Our framework leverages the exchangeability assumption (implicit to CP) rather than the typical IID assumption, allowing us to apply the notion of Conformal Fairness to data types and tasks that are not IID, such as graph data. Experiments were conducted on graph and tabular datasets to demonstrate that the algorithm can control fairness-related gaps in addition to coverage aligned with theoretical expectations.",ICLR.cc/2025/Conference,6.0,True,0.8309,representation learning optimizes surrogate losses explicit guarantees downstream decisions such retrieval held out calibration set compute nonparametric quantiles conformity scores yielding prediction sets latent space that provably control miscoverage for nearest neighbor classification retrieval and clustering assignments derive guarantees for adaptive radii and retrieval under exchangeability and robustness under covariate shift weighted conformal scores conformalrep couples strong unsupervised features actionable distribution free risk control improving reliability open world retrieval and decision making,conformal prediction popular for uncertainty quantification machine learning models while conformal prediction provides probabilistic guarantees regarding the coverage the true label these guarantees are agnostic the presence sensitive attributes within the this formalize textit conformal fairness notion fairness conformal predictors and provide theoretically well founded and associated control for the gaps coverage between different sensitive groups our leverages the exchangeability assumption implicit rather than the typical iid assumption allowing apply the notion conformal fairness data types and tasks that are not iid such graph data,2025-08-26T01:35:51.567719
44,HeteroSign: Polarity-Aware Self-Supervision for Heterophilous and Signed Graphs,"Many real-world graphs exhibit heterophily and antagonistic relations (e.g., signed social networks), where homophily-based pretraining fails. We propose HeteroSign, a self-supervised method tailored to signed and heterophilous graphs that learns polarity-aware, structure-sensitive representations. HeteroSign combines three components: (i) sign-consistent message passing using filters derived from the signed Laplacian and balance theory, (ii) masked subgraph prediction that reconstructs both edge existence and sign under random node/edge removal, and (iii) a polarity contrastive loss aligning embeddings across balanced cycles while separating embeddings participating in frustration. We establish sample complexity bounds for sign recovery and show that our filters approximate the optimal spectral response for heterophily under degree-corrected models. Implementation leverages polynomial Chebyshev filters and localized signed diffusion. On signed networks (Bitcoin-Alpha/OTC, Epinions) and heterophilous benchmarks (Roman-empire, Chameleon, Squirrel), HeteroSign improves linear-probe and node classification performance by 3–8% over graph masking, GraphCL, and signed-GCN baselines. It better predicts controversy, reduces sensitivity to adversarial edge flips, and transfers across sparsity levels. Ablations confirm the necessity of signed spectral filters and frustration-aware contrast. HeteroSign expands self-supervision beyond homophily, delivering representations that respect polarity and structural antagonism, with impact on moderation, recommendation under disagreement, and robustness in noisy relational data.",ICLR,representation learning,gpt-5,False,,Exploring and Unleashing the Power of Message Passing on Heterophilous Graphs,"Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNN).
Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. 
This raises the question: why does message passing remain effective on heterophilous graphs?
To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism.
Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes.
Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs.
To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix.
A thorough evaluation involving 10 benchmark datasets and comparative analysis against 17 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.",ICLR.cc/2025/Conference,3.5,False,0.7911,heterosign combines three components sign consistent message passing filters derived from the signed laplacian and balance theory masked subgraph prediction that reconstructs both edge existence and sign under random node edge removal and iii polarity contrastive loss aligning embeddings across balanced cycles while separating embeddings participating frustration signed networks bitcoin alpha otc epinions and heterophilous benchmarks roman empire chameleon squirrel heterosign improves linear probe and node classification over graph masking graphcl and signed gcn baselines heterosign expands self supervision beyond homophily delivering representations that respect polarity and structural antagonism impact moderation recommendation under disagreement and robustness noisy relational data,graph neural networks gnns have demonstrated strong graph mining tasks due their message passing mechanism which aligned the homophily assumption that adjacent nodes exhibit similar behaviors answer this question this revisit the message passing mechanisms heterophilous graph neural networks and reformulate them into unified heterophilious message passing htmp mechanism moreover argue that the full potential the compatibility matrix not completely achieved due the existence incomplete and noisy semantic neighborhoods real world heterophilous graphs,2025-08-26T01:35:51.567723
45,CoresetSSL: Streaming Coreset Selection for Scalable Self-Supervised Pretraining with Regret Guarantees,"Web-scale pretraining is bottlenecked by data and compute, yet not all samples contribute equally to representation quality. We introduce CoresetSSL, a streaming coreset selection algorithm for self-supervised learning that retains a small, high-value subset with provable regret bounds. CoresetSSL optimizes a submodular surrogate of downstream linear-probe utility estimated from Jacobian sketches and influence functions; it admits a single-pass, memory-bounded greedy with periodic reweighting. A diversity term based on kernel herding prevents redundancy, and a hardness-aware masking policy prioritizes informative regions. We provide theoretical guarantees: a (1−1/e)-approximation to the optimal subset under submodularity and dynamic regret bounds under drifting data streams. Implementation uses low-cost gradient sketches, reservoir sampling warm starts, and distributed selection. Applied to SimCLR, BYOL, and MAE backbones on ImageNet-1K, LAION subsets, and AudioSet, CoresetSSL achieves within 0.5–1.5% of full-data linear-probe accuracy using 20–40% of the data and matches full-data performance with 60% compute. It improves worst-group accuracy on DomainNet by 3–5% through balanced selection and accelerates convergence by 1.6–2.1×. Ablations validate influence estimates and diversity. CoresetSSL offers a practical, theoretically grounded route to efficient pretraining, enabling resource-aware representation learning without sacrificing quality.",ICLR,representation learning,gpt-5,True,2052,ViDROP: Video Dense Representation through Spatio-Temporal Sparsity,"Self-supervised learning (SSL) has revolutionized image processing, but extending its success to video understanding presents unique challenges due to increased data complexity and computational demands. We introduce ViDROP (Video Dense Representation thrOugh spatio-temporal sParsity), a novel SSL architecture for video understanding that combines token dropping and masking strategies. 
Our approach eliminates the need for a decoder and enables per-patch loss computation, overcoming limitations of previous video SSL methods. Moreover, we propose a simple yet effective video compression technique using k-means clustering in pixel space, significantly accelerating data loading and facilitating rapid experimentation. ViDROP demonstrates remarkable scalability across model sizes, from ViT-Small to ViT-Huge, when starting from pretrained models (VideoMAE or V-JEPA), achieving significant performance gains. Pushing the boundaries even further, we leverage network expansion techniques to successfully train ViT-Huge from scratch using modest computational resources, achieving comparable accuracy to VideoMAE 25$\times$ faster in training time. This marks a significant breakthrough in large-scale video SSL, enabling the training of state-of-the-art models with limited resources.
Extensive experiments show that ViDROP achieves state-of-the-art performance on various video understanding benchmarks, including Kinetics400, SSv2, UCF101, and HMDB51, as well as in temporal action detection (THUMOS14). These results highlight the effectiveness of our fine-grained token-level learning strategy in a domain traditionally dominated by fine-tuned SSL models, while enabling the training of large-scale models with limited computational resources.",ICLR.cc/2025/Conference,4.4,nan,0.8070,web scale pretraining bottlenecked data and compute yet not all samples contribute equally representation quality coresetssl streaming coreset selection for self supervised learning that retains small high value subset provable regret bounds coresetssl offers practical theoretically grounded route efficient pretraining enabling resource aware representation learning sacrificing quality,self supervised learning ssl has revolutionized image processing but extending its success video understanding presents unique challenges due increased data complexity and computational demands vidrop video dense representation spatio temporal sparsity ssl for video understanding that combines token dropping and masking strategies moreover simple yet effective video compression means clustering pixel space accelerating data loading and facilitating rapid experimentation pushing the boundaries even further leverage network expansion techniques train vit huge from scratch modest computational resources achieving comparable videomae times faster training time extensive experiments that vidrop achieves state the art various video understanding benchmarks including kinetics400 ssv2 ucf101 and hmdb51 well temporal action detection thumos14 these highlight the effectiveness our fine grained token level learning strategy domain traditionally dominated fine tuned ssl models while enabling the training large scale models limited computational resources,2025-08-26T01:35:51.567729
46,AV-SceneBind: Object-Centric Audio–Visual Self-Supervision via Cross-Modal Binding and Slot Dynamics,"Learning object-centric representations from videos remains challenging without labels. We present AV-SceneBind, a self-supervised framework that discovers objects by binding auditory and visual evidence and modeling their dynamics. A vision encoder and a learned spatial audio projector feed a slot-attention module that proposes object slots; cross-modal binding is enforced by maximizing mutual information between slot features and separated audio components obtained via differentiable beamforming. A dynamics head predicts slot trajectories with a contrastive cycle-consistency objective, while an occlusion-aware mask modeling reconstructs missing pixels and spectrogram regions. We provide identifiability results showing that cross-modal binding disambiguates objects under mild independence assumptions and derive bounds linking binding strength to segmentation error. Training scales via multi-view audio perturbations and permutation-invariant slot losses. On MUSIC-AV, AudioSet-Objects, and MOVi-A/B synthetic scenes, AV-SceneBind improves unsupervised object segmentation (mBO +4–9) and audio–visual source localization over recent slot-based and audiovisual baselines, and yields superior linear probes for downstream detection. It is robust to clutter and reverberation and supports zero-shot audiovisual retrieval at the object level. Ablations highlight the role of beamforming and cycle dynamics. AV-SceneBind demonstrates that cross-modal binding and object-centric dynamics unlock richer representations, advancing multimodal scene understanding.",ICLR,representation learning,gpt-5,True,2253,Efficient Object-Centric Learning for Videos,"This paper introduces a method for efficiently learning video-level object-centric representations by bootstrapping off a pre-trained image backbone, which we term Interpreter. It presents a novel hierarchical slot attention architecture with local learning and an optimal transport objective that yields fully unsupervised video segmentation. We first learn to compress images into image-level object-centric representations. Interpreter then learns to compress and reconstruct the object-centric representations for each frame across a video, allowing us to circumvent the costly process of reconstructing full frame feature maps. Unlike prior work, this allows us to scale to significantly longer videos without resorting to chunking videos into segments and matching between them. To deal with the unordered nature of object-centric representations, we employ Sinkhorn divergence, a relaxed optimal transport objective, to compute the distance between unordered sets of representations. We evaluate the resulting segmentation maps on video instance segmentation in both realistic and synthetic settings, using YTVIS-19 and MOVi-E, respectively. Interpreter achieves state-of-the-art results on the realistic YTVIS-19 dataset and presents a promising approach of scaling object-centric representation learning to longer videos.",ICLR.cc/2025/Conference,3.0,nan,0.8499,learning object centric representations from videos remains challenging labels vision encoder and learned spatial audio projector feed slot attention module that proposes object slots cross modal binding enforced maximizing mutual information between slot features and separated audio components obtained differentiable beamforming provide identifiability showing that cross modal binding disambiguates objects under mild independence assumptions and derive bounds linking binding strength segmentation error music audioset objects and movi synthetic scenes scenebind improves unsupervised object segmentation mbo and audio visual source localization over recent slot based and audiovisual baselines and yields superior linear probes for downstream detection robust clutter and reverberation and supports zero shot audiovisual retrieval the object level,this introduces for learning video level object centric representations bootstrapping off pre trained image backbone which term interpreter presents hierarchical slot attention local learning and optimal transport objective that yields fully unsupervised video segmentation interpreter then learns compress and reconstruct the object centric representations for each frame across video allowing circumvent the costly process reconstructing full frame feature maps the resulting segmentation maps video instance segmentation both realistic and synthetic settings ytvis and movi respectively interpreter achieves state the art the realistic ytvis and presents promising scaling object centric representation learning longer videos,2025-08-26T01:35:51.567732
47,IntervSSL: Synthetic Interventions for Causally Robust Self-Supervised Representations,"Self-supervised objectives often entangle causal content with spurious factors tied to environment, undermining transfer. We propose IntervSSL, a single-view framework that learns intervention-stable representations by generating and enforcing invariance to synthetic interventions on suspected nuisances. A causal editor—trained adversarially with disentangling priors—produces counterfactual views that perturb style, background, or context while preserving semantic content. The encoder is trained to be invariant across natural and edited views via an invariance loss and to remain sensitive to content via a discriminative content-contrast term; a causal cycle constraint penalizes violations of edit reversibility. We provide identifiability up to invertible content transforms under a structured mixing model and show that invariance to editor-induced perturbations bounds worst-environment risk in a distributional robustness framework. Efficient training uses classifier-free guidance for editing strength and an edit-detection critic to prevent trivial edits. On DomainBed (PACS, VLCS, OfficeHome), Waterbirds, and Stylized-ImageNet, IntervSSL improves worst-domain accuracy by 5–10% over SimCLR, IRM-Mixup, and augmentation-heavy baselines, while maintaining average accuracy. It reduces reliance on textures and backgrounds, as confirmed by concept activation and counterfactual tests. Ablations validate the necessity of causal editing and cycle constraints. IntervSSL offers a practical path to causally robust features without multi-environment supervision.",ICLR,representation learning,gpt-5,True,10023,Enhancing Graph Invariant Learning from a Negative Inference Perspective,"The out-of-distribution (OOD) generalization challenge is a longstanding problem  in graph learning. Through studying the fundamental cause of data distribution shift, i.e., the changes of environments, significant progress has been achieved in addressing this issue. However, we observe that existing works still fail to effectively address complex environment shifts. Previous practices place excessive attention on extracting causal subgraphs, inevitably treating spurious subgraphs as environment variables. While spurious subgraphs are controlled by environments, the space of environment changes encompass more than the scale of spurious subgraphs. Therefore, existing efforts have a limited inference space for environments,  leading to failure under severe environment changes. To tackle this issue, we propose a negative inference graph OOD framework (NeGo)  to broaden the inference space for environment factors. Inspired by the successful practice of prompt learning in capturing underlying semantics and causal associations in large language models, we design a negative prompt environment inference to extract underlying environment information. We further introduce the environment-enhanced invariant subgraph learning method to effectively exploit inferred environment embedding, ensuring the robust extraction of causal subgraph in the environment shifts. Lastly, we conduct a comprehensive evaluation of NeGo on real-world datasets and synthetic datasets across domains. NeGo outperforms baselines on nearly all datasets, which verify the effectiveness of our framework. Our source code is available at \url{https://anonymous.4open.science/r/NeGo-E4C1}.",ICLR.cc/2025/Conference,4.5,nan,0.8102,self supervised objectives often entangle causal content spurious factors tied environment undermining transfer causal editor trained adversarially disentangling priors produces counterfactual views that perturb style background context while preserving semantic content provide identifiability invertible content transforms under structured mixing and that invariance editor induced perturbations bounds worst environment risk distributional robustness,the out distribution ood generalization challenge longstanding problem graph learning previous practices place excessive attention extracting causal subgraphs inevitably treating spurious subgraphs environment variables inspired the successful practice prompt learning capturing underlying semantics and causal associations large language models negative prompt environment inference extract underlying environment information further the environment enhanced invariant subgraph learning exploit inferred environment embedding ensuring the robust extraction causal subgraph the environment shifts,2025-08-26T01:35:51.567740
48,AutoInv: Learning Invariance Profiles with Bilevel Risk for Robust Self-Supervised Transfer,"Self-supervised methods hard-wire invariances (e.g., color, geometry) that need not match downstream tasks, causing either under- or over-invariance and degraded transfer. We introduce AutoInv, a framework that learns an invariance profile—a continuous sensitivity map over a library of transformations—by optimizing a bilevel objective that proxies worst-case downstream risk. The inner level trains an encoder with a sensitivity-weighted masked/contrastive loss, where each transformation contributes proportionally to a learnable invariance weight. The outer level updates these weights to minimize a distributionally robust linear-probe risk estimated on a small held-out buffer using influence-function corrections and gradient alignment between classifier and encoder Jacobians. We show that AutoInv converges to a stationary point of a risk-regularized mutual information surrogate and derive generalization bounds that trade off invariance-induced bias and variance from data augmentation. Algorithmically, we introduce a low-variance estimator for invariance gradients via randomized transformation sketches and a simplex-constrained optimizer with monotone updates. On ImageNet-100, DomainBed (PACS, VLCS, OfficeHome), and Stylized-ImageNet, AutoInv improves worst-domain accuracy by 4–9% and average accuracy by 1.8–3.6% over strong baselines (SimCLR, BYOL, MAE) with matched compute. Learned profiles downweight harmful color invariance for fine-grained tasks and upweight viewpoint invariance for shape-centric transfer. Ablations confirm the necessity of bilevel risk and Jacobian alignment. AutoInv replaces manual augmentation choices with principled, task-robust invariance learning, yielding representations that generalize reliably under unknown deployment shifts.",ICLR,representation learning,gpt-5,True,9157,Unleash The Power of Color for Point Cloud Registration,"Point cloud registration (PCR) has been an important research subject for many years but remains an open problem, presenting numerous challenges. The stability of existing registration methods is often inadequate, particularly in scenarios with low overlap. This issue primarily arises from the insufficient distinctiveness of extracted point cloud features, leading to ambiguous matches and the proliferation of outliers. To address these bottlenecks in point cloud registration, it is crucial to fully leverage the color information of the point clouds to discern point correspondences effectively.
However, excessive control over color may disrupt the spatial structure of the point cloud, making it essential to find a balance between the aggressiveness and stability of color integration.
To tackle these challenges, we propose UPC-PCR, which unlocks the potential of color information while maintaining stability. Specifically, we design a Curvature-Color Fusion Module (CCF) to initialize distinctive features. Additionally, to balance color aggressiveness, we enhance the geometric structure by introducing a Centroid Angular (CA) embedding for superpoint structure encoding, which is particularly effective in low-overlap scenes.
While CCF and CA ensure the distinctiveness of point features, the aggressive use of color in the feature enhancement process may still introduce errors. Therefore, we develop a robust estimator equipped with Feature-based Compatibility Hypergraph Convolution (FCH) to learn higher-order compatibility of correspondences and effectively filter out outliers.
Evaluation across multiple datasets has demonstrated the state-of-the-art performance of UPC-PCR, achieving registration recalls of 98.4%/90.4% on Color3DMatch/Color3DLoMatch.",ICLR.cc/2025/Conference,4.0,nan,0.8045,color geometry that need not match downstream tasks causing either under over invariance and degraded transfer learned profiles downweight harmful color invariance for fine grained tasks and upweight viewpoint invariance for shape centric transfer,additionally balance color aggressiveness enhance the geometric structure introducing centroid angular embedding for superpoint structure encoding which effective low overlap scenes while ccf and ensure the distinctiveness point features the aggressive use color the feature enhancement process may still errors therefore robust estimator equipped feature based compatibility hypergraph convolution fch learn higher order compatibility correspondences and filter out outliers,2025-08-26T01:35:51.567742
49,MAP-Embed: Monge–Ampère Projection for Isotropic and Well-Conditioned Representations,"Learned features often exhibit spectral anisotropy and collapsed directions, undermining optimization and transfer. We propose MAP-Embed, a geometry-regularized self-supervised framework that projects latent distributions toward a uniform measure on the sphere by solving a Monge–Ampère optimal transport problem. Concretely, an encoder produces features that are passed through a convex potential whose gradient implements transport; the potential is parameterized by an input-convex network and trained to minimize a spherical 2-Wasserstein discrepancy with a uniform target while preserving semantics via a standard masked/contrastive objective. A spectral flatness penalty on the latent covariance and a Jacobian isotropy regularizer ensure well-conditioned local geometry. We prove that MAP-Embed bounds feature anisotropy, controls the condition number of linear probes, and tightens generalization guarantees through Rademacher complexity terms dependent on spherical discrepancy. Efficient training uses stochastic convex potentials, Sinkhorn-free score surrogates, and Hutchinson estimators for Jacobian norms. On ImageNet-100, CIFAR-100, and COCO transfer, MAP-Embed improves linear-probe accuracy by 1.9–4.4% over MAE/iBOT and reduces worst-case corruption error (ImageNet-C/A) by 3–6%. It accelerates fine-tuning by 15–25% fewer steps and reduces sensitivity to learning-rate schedules and batch size. Ablations show that Monge–Ampère projection, not whitening alone, drives isotropy and robustness. MAP-Embed turns measure projection into a practical inductive bias, yielding isotropic, stable representations that transfer broadly.",ICLR,representation learning,gpt-5,True,6921,Interpretable Dimensionality Reduction by Feature-preserving Manifold Approximation and Projection,"Nonlinear dimensionality reduction often lacks interpretability due to the absence of source features in low-dimensional embedding space. We propose FeatureMAP, an interpretable method that preserves source features by tangent space embedding. The core of FeatureMAP is to use local principal component analysis (PCA) to approximate tangent spaces. By leveraging these tangent spaces, FeatureMAP computes gradients to locally reveal feature directions and importance. Additionally, FeatureMAP embeds the tangent spaces into low-dimensional space while preserving alignment between them, providing local gauges for projecting the high-dimensional data points. Unlike UMAP, FeatureMAP employs anisotropic projection to preserve both the manifold structure and the original data density. We apply FeatureMAP to interpreting digit classification, object detection and MNIST adversarial examples, where it effectively distinguishes digits and objects using feature importance and provides explanations for misclassifications in adversarial attacks. We also compare FeatureMAP with other state-of-the-art methods using both local and global metrics.",ICLR.cc/2025/Conference,5.6,False,0.8210,learned features often exhibit spectral anisotropy and collapsed directions undermining optimization and transfer concretely encoder produces features that are passed convex potential whose gradient implements transport the potential parameterized input convex network and trained minimize spherical wasserstein discrepancy uniform target while preserving semantics standard masked contrastive objective that map embed bounds feature anisotropy controls the condition number linear probes and tightens generalization guarantees rademacher complexity terms dependent spherical discrepancy ablations that monge ampère projection not whitening alone drives isotropy and robustness map embed turns measure projection into practical inductive bias yielding isotropic stable representations that transfer broadly,nonlinear dimensionality reduction often lacks interpretability due the absence source features low dimensional embedding space featuremap interpretable that preserves source features tangent space embedding leveraging these tangent spaces featuremap computes gradients locally reveal feature directions and importance apply featuremap interpreting digit classification object detection and mnist adversarial examples where distinguishes digits and objects feature importance and provides explanations for misclassifications adversarial attacks,2025-08-26T01:35:51.567748
50,ActView: Embodied Active-View Self-Supervision for 3D-Aware Perceptual Representations,"Static augmentation pipelines ignore the agent’s ability to acquire informative views, limiting 3D understanding and sample efficiency. We present ActView, an embodied self-supervised framework that jointly trains a representation and a view-selection policy to maximize geometry-aware information. A lightweight policy controls camera poses; an encoder fuses multi-view observations using a differentiable epipolar attention module. The pretext objective couples masked multi-view reconstruction with a mutual-information lower bound between latent shape codes and future observations, estimated via contrastive predictive coding. A Fisher-information regularizer ties the policy to viewpoint choices that maximize observability of depth and surface normals, while a cycle-consistency constraint enforces multi-view coherence. We prove that ActView improves identifiability of 3D latents up to gauge under mild texture and parallax conditions and bound reconstruction error in terms of the policy’s information gain. Training uses on-policy rollouts with gradient-through-renderer approximations and a stable two-timescale update. On CO3D, Objaverse-LVIS subsets, and ScanNet, ActView boosts linear-probe performance on pose-robust recognition by 3–6% over video/MAE baselines and improves novel-view consistency and depth estimation by 12–20% relative errors. In simulated manipulation, pretraining with ActView reduces demonstrations needed by 30%. Ablations highlight the importance of Fisher-guided view selection. ActView elevates self-supervision from passive to embodied, producing 3D-aware features that generalize across viewpoints and tasks.",ICLR,representation learning,gpt-5,True,1234,Learning Fused State Representations for Control from Multi-View Observations,"In visual control tasks, leveraging observations from multiple views enables Reinforcement Learning (RL) agents to perceive the environment more effectively. However, while multi-view observations enrich decision-making information, they also increase the dimension of observation space and introduce more redundant information. Thus, how to learn compact and task-relevant representations from multi-view observations for downstream RL tasks remains a challenge. In this paper, we propose a Multi-view Fusion State for Control (MFSC), which integrates a self-attention mechanism with bisimulation metric learning to fuse task-relevant representations from multi-view observations. To foster more compact fused representations, we also incorporate a mask-based latent reconstruction auxiliary task to learn cross-view information. Additionly, this mechanism of mask and reconstruction can enpower the model with the ability to handle missing views by learning an additional mask tokens. We conducted extensive experiments on the Meta-World and Pybullet benchmarks, and the results demonstrate that our proposed method outperforms other multi-view RL algorithms and effectively aggregates task-relevant details from multi-view observations, coordinating attention across different views.",ICLR.cc/2025/Conference,4.5,nan,0.8482,actview embodied self supervised that jointly trains representation and view selection policy maximize geometry aware information lightweight policy controls camera poses encoder fuses multi view observations differentiable epipolar attention module co3d objaverse lvis subsets and scannet actview boosts linear probe pose robust recognition over video mae baselines and improves novel view consistency and depth estimation relative errors,visual control tasks leveraging observations from multiple views enables reinforcement learning agents perceive the environment more this multi view fusion state for control mfsc which integrates self attention mechanism bisimulation learning fuse task relevant representations from multi view observations additionly this mechanism mask and reconstruction can enpower the the ability handle missing views learning additional mask tokens conducted extensive experiments the meta world and pybullet benchmarks and the that our proposed outperforms other multi view algorithms and aggregates task relevant details from multi view observations coordinating attention across different views,2025-08-26T01:35:51.567751
51,BlueNoiseVQ: Token-Free Discrete Representations via Poisson Disk Packing in Latent Space,"Discrete latents enable compression and generative modeling but standard vector quantization suffers from code collapse and irregular spacing, hindering transfer. We propose BlueNoiseVQ, a token-free discrete representation learner that constructs a blue-noise codebook via Poisson disk packing on the hypersphere, ensuring uniform coverage and a controllable minimum separation. An encoder maps inputs to continuous features; a differentiable nearest-disk operator snaps them to codewords while preserving gradients via a straight-through estimator. Training minimizes masked reconstruction (or contrastive) loss augmented with (i) a repulsive potential enforcing packing radii, (ii) a usage-entropy regularizer to avoid dead codes, and (iii) a curvature penalty stabilizing local geometry. We derive bounds on quantization error and mutual information preserved under packing constraints, and show that blue-noise spacing improves k-NN stability and reduces adversarial drift compared to Voronoi-unstable VQ. Efficient sampling leverages dart throwing with fast rejection and periodic codebook annealing. On ImageNet-100, LibriSpeech, and VGGSound, BlueNoiseVQ yields 20–40% fewer active codes than VQ-VAEs at similar reconstruction quality, while improving linear probes by 1.5–3.2% and retrieval mAP by 3–6%. It enhances robustness to codebook pruning and supports edit-friendly discrete latents. BlueNoiseVQ offers a principled alternative to heuristic VQ, producing compact, evenly spaced codebooks that strengthen discrete representation learning across modalities.",ICLR,representation learning,gpt-5,True,10760,Addressing Representation Collapse in Vector Quantized Models with One Linear Layer,"Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose \textbf{SimVQ}, a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the \textit{entire linear space} spanned by the codebook, rather than merely updating \textit{the code vector} selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts.",ICLR.cc/2025/Conference,4.0,nan,0.8881,discrete latents enable compression and generative modeling but standard vector quantization suffers from code collapse and irregular spacing hindering transfer bluenoisevq token free discrete representation learner that constructs blue noise codebook poisson disk packing the hypersphere ensuring uniform coverage and controllable minimum separation enhances robustness codebook pruning and supports edit friendly discrete latents bluenoisevq offers principled alternative heuristic producing compact evenly spaced codebooks that strengthen discrete representation learning across modalities,vector quantization used for converting continuous representations into discrete codes which has become fundamental unsupervised representation learning and latent generative models however models are often hindered the problem representation collapse the latent space which leads low codebook utilization and limits the scalability the codebook for large scale training existing methods designed mitigate representation collapse reduce the dimensionality latent space the expense capacity which not fully resolve the core issue this conduct theoretical analysis representation collapse models and identify its primary cause the disjoint optimization the codebook where only small subset code vectors are updated gradient descent the that simvq not only addresses the problem representation collapse but also proves highly adaptable and easy suggesting its broad applicability diverse machine learning contexts,2025-08-26T01:35:51.567756
52,OmniStitch: Self-Supervised Manifold Stitching for Unified Single-Cell Multi-Omics Representations,"Single-cell assays capture heterogeneous views of cellular state (RNA, chromatin, proteins), but labeled co-assays are scarce and batch effects abound. We introduce OmniStitch, a self-supervised framework that “stitches” omics-specific manifolds into a unified latent space without requiring extensive pairing. Modality encoders map inputs to a shared latent governed by a latent ODE capturing pseudo-temporal dynamics; training combines masked modeling per modality with cross-modality cycle consistency and a manifold Laplacian alignment that preserves local neighborhoods while respecting batch covariates. A Cauchy–Schwarz divergence between co-assay distributions encourages global alignment, and a sparsity prior on ODE vector fields captures regulatory simplicity. We prove that under mild overlap and Lipschitz dynamics, OmniStitch identifies a shared embedding up to orthonormal transform and bounds alignment distortion. Efficient training uses neighbor-graph mini-batches, continuous normalizing flows for batch correction, and negative sampling across cell types. On PBMC co-assays, SHARE-seq, and CITE-seq, OmniStitch improves cell-type linear probes by 3–7% over scJoint, BABEL, and multimodal VAEs, reduces batch effects (kBET +10–18), and reconstructs RNA from ATAC more accurately. It recovers developmental trajectories consistent with known lineages and enables cross-omics retrieval and perturbation response prediction. Ablations confirm the roles of cycle consistency and latent dynamics. OmniStitch delivers unified, biology-consistent representations that facilitate downstream discovery in single-cell analysis.",ICLR,representation learning,gpt-5,False,,Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy,"Non-rigid alignment of point clouds is crucial for scene understanding, reconstruction, and various computer vision and robotics tasks. Recent advancements in implicit deformation networks for non-rigid registration have significantly reduced the reliance on large amounts of annotated training data. However, existing state-of-the-art methods still face challenges in handling occlusion scenarios. To address this issue, this paper introduces an innovative unsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly aligning point clouds. The key innovation of our method lies in the utilization of the adaptive correntropy function as a localized similarity measure, enabling us to treat individual points distinctly. In contrast to previous approaches that solely minimize overall deviations between two shapes, we combine unsupervised implicit neural representations with the maximum correntropy criterion to optimize the deformation of unoccluded regions. This effectively avoids collapsed, tearing, and other physically implausible results. Moreover, we present a theoretical analysis and establish the relationship between the maximum correntropy criterion and the commonly used Chamfer distance, highlighting that the correntropy-induced metric can be served as a more universal measure for point cloud analysis. Additionally, we introduce
locally linear reconstruction to ensure that regions lacking correspondences between shapes still undergo physically natural deformations. Our method achieves superior or competitive performance compared to existing approaches, particularly when dealing with occluded geometries. We also demonstrate the versatility of our method in challenging tasks such as large deformations, shape interpolation, and shape completion under occlusion disturbances.",ICLR.cc/2025/Conference,5.75,True,0.7805,that under mild overlap and lipschitz dynamics omnistitch identifies shared embedding orthonormal transform and bounds alignment distortion recovers developmental trajectories consistent known lineages and enables cross omics retrieval and perturbation response prediction,non rigid alignment point clouds crucial for scene understanding reconstruction and various computer vision and robotics tasks address this issue this introduces innovative unsupervised called occlusion aware registration oar for non rigidly aligning point clouds contrast previous approaches that solely minimize overall deviations between two shapes combine unsupervised implicit neural representations the maximum correntropy criterion optimize the deformation unoccluded regions,2025-08-26T01:35:51.567762
53,HawkGraph: Temporal Point-Process Self-Supervision for Dynamic Graph Representations,"Dynamic graphs evolve through discrete interaction events, yet most pretraining treats time coarsely or ignores self-excitation, limiting temporal fidelity. We present HawkGraph, a self-supervised method that marries graph encoding with neural Hawkes point processes to learn event-consistent representations. Nodes maintain latent states updated by a graph transformer; inter-event dynamics are modeled by intensity functions whose parameters depend on current latents and structural context. Pretraining optimizes (i) a masked event prediction objective using thinning-based simulation, (ii) a time-to-event likelihood that calibrates intensities, and (iii) a temporal contrastive loss aligning states across sampled counterfactual timelines. We establish that maximizing the joint objective learns sufficient statistics for event timing under mild mixing and derive stability bounds for state drift between events. Implementation uses piecewise-constant caches, numba-accelerated thinning, and neighbor sampling. On Reddit, MOOC, Wikipedia, and JODIE, HawkGraph improves next-event prediction and dynamic link forecasting by 3–8% AUC over TGN, TGAT, and masked dynamic GNNs, while yielding better temporal calibration (NLL −6–12%). Downstream node classification and churn prediction also benefit under time shift and subsampling. Ablations highlight the necessity of Hawkes likelihood and counterfactual timelines. HawkGraph grounds temporal graph representation learning in point-process theory, producing time-calibrated features for evolving networks.",ICLR,representation learning,gpt-5,True,10569,Inference of Sequential Patterns for Neural Message Passing in Temporal Graphs,"The modelling of temporal patterns in dynamic graphs is an important current research issue in the development of time-aware Graph Neural Networks (GNNs).
However, whether or not a specific sequence of events in a temporal graph constitutes a temporal pattern not only depends on the frequency of its occurrence.
We must also consider whether it deviates from what is expected in a temporal graph where timestamps are randomly shuffled.
While accounting for such a random baseline is important to model temporal patterns, it has mostly been ignored by current temporal graph neural networks.
To address this issue we propose HYPA-DBGNN, a novel two-step approach that combines (i) the inference of anomalous sequential patterns in time series data on graphs based on a statistically principled null model, with (ii) a neural message passing approach that utilizes a higher-order De Bruijn graph whose edges capture overrepresented sequential patterns.
Our method leverages hypergeometric graph ensembles to identify anomalous edges within both first- and higher-order De Bruijn graphs, which encode the temporal ordering of events. 
Consequently, the model introduces an inductive bias that enhances model interpretability.

We evaluate our approach for static node classification using established benchmark datasets and a synthetic dataset that showcases its ability to incorporate the observed inductive bias regarding over- and under-represented temporal edges. 
Furthermore, we demonstrate the framework's effectiveness in detecting similar patterns within empirical datasets, resulting in superior performance compared to baseline methods in node classification tasks. 
To the best of our knowledge, our work is the first to introduce statistically informed GNNs that leverage temporal and causal sequence anomalies. 
HYPA-DBGNN represents a promising path for bridging the gap between statistical graph inference and neural graph representation learning, with potential applications to static GNNs.",ICLR.cc/2025/Conference,4.5,False,0.8397,hawkgraph self supervised that marries graph encoding neural hawkes point processes learn event consistent representations pretraining optimizes masked event prediction objective thinning based simulation time event likelihood that calibrates intensities and iii temporal contrastive loss aligning states across sampled counterfactual timelines reddit mooc wikipedia and jodie hawkgraph improves next event prediction and dynamic link forecasting auc over tgn tgat and masked dynamic gnns while yielding better temporal calibration nll downstream node classification and churn prediction also benefit under time shift and subsampling hawkgraph grounds temporal graph representation learning point process theory producing time calibrated features for evolving networks,the modelling temporal patterns dynamic graphs important current issue the development time aware graph neural networks gnns while accounting for such random important temporal patterns has mostly been ignored current temporal graph neural networks address this issue hypa dbgnn two step that combines the inference anomalous sequential patterns time series data graphs statistically principled null neural message passing that utilizes higher order bruijn graph whose edges capture overrepresented sequential patterns consequently the introduces inductive bias that enhances interpretability our for static node classification established datasets and synthetic that showcases its ability incorporate the observed inductive bias regarding over and under represented temporal edges furthermore the framework effectiveness detecting similar patterns within empirical datasets resulting superior compared methods node classification tasks hypa dbgnn represents promising path for bridging the gap between statistical graph inference and neural graph representation learning potential applications static gnns,2025-08-26T01:35:51.567766
54,FixedPointRep: Anderson-Accelerated Self-Distillation with Convergence and Stability Guarantees,"Self-distillation methods (e.g., BYOL, DINO) can be viewed as fixed-point iterations between student and teacher, yet their convergence properties are opaque and training is sensitive to heuristics. We propose FixedPointRep, a principled self-distillation framework that explicitly solves the representation fixed point with Anderson acceleration and spectral stabilization. We cast the teacher update as T(·), a contractive operator on the space of encoders under a Jacobian-induced metric; the student applies m-step Anderson extrapolation to approximate the fixed point. A spectral penalty constrains the largest singular value of the representation Jacobian, ensuring contraction, while a projection step enforces alignment with predictor heads. We prove local linear convergence to a stable fixed point under mild smoothness and show improved robustness to predictor/EMA hyperparameters. Practically, we develop a memory-efficient Anderson buffer with orthogonalization and curvature correction. On ImageNet-100, CIFAR-100, and VTAB-1k, FixedPointRep improves linear-probe accuracy by 1.6–3.4% over BYOL/DINO with matched compute, reduces training variance across seeds by 30–50%, and converges in 20–30% fewer epochs. Under label-scarce fine-tuning, it yields stronger few-shot performance and better calibration. Ablations confirm the role of spectral stabilization and Anderson depth. FixedPointRep reframes self-distillation as a stable fixed-point problem, delivering faster, more reliable representation learning.",ICLR,representation learning,gpt-5,False,,A Unified Theory of Stochastic Proximal Point Methods without Smoothness,"This paper presents a comprehensive analysis of a broad range of variations of the stochastic proximal point method (SPPM). Proximal point methods have attracted considerable interest owing to their numerical stability and robustness against imperfect tuning, a trait not shared by the dominant stochastic gradient descent (SGD) algorithm. A framework of assumptions that we introduce encompasses methods employing techniques such as variance reduction and arbitrary sampling. A cornerstone of our general theoretical approach is a parametric assumption on the iterates, correction and control vectors. We establish a single theorem that ensures linear convergence under this assumption and $\mu$-strong convexity of the loss function, and without the need to invoke smoothness. This integral theorem reinstates best known complexity and convergence guarantees for several existing methods, which demonstrates the robustness of our approach. We expand our study by developing three new variants of SPPM, and through numerical experiments elucidate various properties inherent to them.",ICLR.cc/2025/Conference,4.5,False,0.7869,fixedpointrep principled self distillation that explicitly solves the representation fixed point anderson acceleration and spectral stabilization spectral penalty constrains the largest singular value the representation jacobian ensuring contraction while projection step enforces alignment predictor heads local linear convergence stable fixed point under mild smoothness and improved robustness predictor ema hyperparameters under label scarce fine tuning yields stronger few shot and better calibration fixedpointrep reframes self distillation stable fixed point problem delivering faster more reliable representation learning,proximal point methods have attracted considerable interest owing their numerical stability and robustness against imperfect tuning trait not shared the dominant stochastic gradient descent sgd this integral theorem reinstates best known complexity and convergence guarantees for several existing methods which demonstrates the robustness our,2025-08-26T01:35:51.567769
55,DocScene: Structure-Aware Self-Supervision for Layout and Semantics in Documents,"Document understanding requires representations that capture both layout structure and textual semantics, but most pretraining treats them separately or relies on heavy supervision. We present DocScene, a structure-aware self-supervised framework that learns unified document representations by coupling geometric parsing with language signals. A multimodal encoder processes text tokens with positional embeddings and visual patches; a layout graph module induces a tree of regions via differentiable parsing. Pretraining combines (i) masked text–patch modeling, (ii) reading-order prediction as a permutation task with Sinkhorn relaxation, and (iii) a layout–language consistency loss aligning region embeddings with sentence semantics via cross-attention. A tree-regularized contrast enforces stability across reflows and format changes. We prove that tree consistency reduces permutation error bounds for reading order and derive generalization guarantees for layout-sensitive k-NN under document reformatting. Efficient training uses sparse attention over detected regions and Gumbel-Sinkhorn parsing. On DocLayNet, PubLayNet, FUNSD, and DocVQA, DocScene improves zero-shot layout analysis and linear probes by 3–7% over LayoutLM-style and masked vision baselines, and maintains performance under drastic reformatting (two-column↔single, language swaps). It supports robust retrieval by logical sections and better OCR error tolerance. Ablations confirm the contributions of reading-order prediction and tree-regularized contrast. DocScene delivers layout- and semantics-aware features, advancing reliable document intelligence.",ICLR,representation learning,gpt-5,True,7324,Lay-Your-Scene: Open-Vocabulary Text to Layout Generation,"We present Lay-Your-Scene (shorthand LayouSyn), a novel diffusion-Transformer based architecture for open-vocabulary natural scene layout generation. Prior works have used close-sourced scene-unaware Large Language models for open-vocabulary layout generation, limiting their widespread use and scene-specific modeling capability. This work presents the first end-to-end text-to-natural-scene-layout generation pipeline that utilizes lightweight open-source language models to predict objects in the scene and a new conditional layout diffusion Transformer trained in a scene-aware manner. Extensive experiments demonstrate that LayouSyn outperforms existing methods on open-vocabulary and closed-vocabulary layout generation and achieves state-of-the-art performance on challenging spatial and numerical reasoning tasks. Additionally, we present two applications of LayouSyn: First, we demonstrate an interesting finding that we can seamlessly combine initialization from the Large Language model to reduce the diffusion sampling steps. Second, we present a new pipeline for adding objects to the image, demonstrating the potential of LayouSyn in image editing applications.",ICLR.cc/2025/Conference,4.0,nan,0.8207,docscene structure aware self supervised that learns unified document representations coupling geometric parsing language signals pretraining combines masked text patch modeling reading order prediction permutation task sinkhorn relaxation and iii layout language consistency loss aligning region embeddings sentence semantics cross attention efficient training uses sparse attention over detected regions and gumbel sinkhorn parsing doclaynet publaynet funsd and docvqa docscene improves zero shot layout analysis and linear probes over layoutlm style and masked vision baselines and maintains under drastic reformatting two column single language swaps ablations confirm the contributions reading order prediction and tree regularized contrast,lay your scene shorthand layousyn diffusion transformer for open vocabulary natural scene layout generation prior works have used close sourced scene unaware large language models for open vocabulary layout generation limiting their widespread use and scene specific modeling capability this presents the first end end text natural scene layout generation pipeline that utilizes lightweight open source language models predict objects the scene and conditional layout diffusion transformer trained scene aware manner extensive experiments that layousyn outperforms existing methods open vocabulary and closed vocabulary layout generation and achieves state the art challenging spatial and numerical reasoning tasks additionally two applications layousyn first interesting that can seamlessly combine initialization from the large language reduce the diffusion sampling steps,2025-08-26T01:35:51.567777
56,RevSSL: Reversible Self-Supervision with Measure-Preserving Encoders,"Self-supervised encoders discard information that may be predictive for downstream tasks, and ad hoc auxiliary losses cannot guarantee retention without collapse. We introduce RevSSL, a representation learning framework that constrains the feature map to be invertible and approximately measure-preserving, enabling lossless pretraining while still enforcing semantic structure. RevSSL employs a multi-layer bijection built from volume-preserving coupling flows and orthogonal transforms; a lightweight predictor head operates on features but the core encoder remains reversible. Training combines a masked modeling or contrastive objective with two structural regularizers: (i) log-Jacobian variance control to preserve local volume and avoid anisotropy, and (ii) an information bottleneck applied only in the predictor, not the encoder, to shape task-relevant compression post hoc. We prove that reversible encoders upper-bound excess risk of linear probes by the conditional codelength induced solely by the predictor, and derive stability guarantees linking Jacobian spectra to nearest-neighbor consistency. Efficient implementations use butterfly orthogonal layers, low-rank coupling, and Hutchinson estimators for Jacobian traces. On ImageNet-100, CIFAR-100, LibriSpeech, and ModelNet40, RevSSL matches or exceeds MAE/SimCLR in linear probes by 1.6–3.8%, while uniquely supporting exact feature inversion for attribution and data auditing. Robustness to distribution shift (ImageNet-C/A) improves by 2–4%, and seed variance drops markedly. RevSSL makes information preservation a first-class constraint, yielding reliable, auditable features without sacrificing downstream utility.",ICLR,representation learning,gpt-5,True,3556,Adversarial Robustness of  Self-Supervised Learning in Vision,"Self-supervised learning (SSL) has advanced significantly in visual representation learning, yet large-scale evaluations of its adversarial robustness remain limited. In this study, we evaluate the adversarial robustness of seven SSL models and one supervised model across a range of tasks, including ImageNet classification, transfer learning, segmentation, and detection. Our findings demonstrate that SSL models generally exhibit superior robustness to adversarial attacks compared to their supervised counterpart on ImageNet, with this advantage extending to transfer learning in classification tasks. However, this robustness is less pronounced in segmentation and detection tasks. We also explore the role of architectural choices in model robustness, observing that their impact varies depending on the SSL objective. Finally, we assess the effect of extended training durations on adversarial robustness, finding that longer training may offer slight improvements without compromising robustness. Our analysis highlights promising directions for enhancing the adversarial robustness of visual self-supervised representation systems in complex environments.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8208,revssl representation learning that constrains the feature map invertible and approximately measure preserving enabling lossless pretraining while still enforcing semantic structure imagenet cifar librispeech and modelnet40 revssl matches exceeds mae simclr linear probes while uniquely supporting exact feature inversion for attribution and data auditing robustness distribution shift imagenet improves and seed variance drops markedly,self supervised learning ssl has advanced visual representation learning yet large scale evaluations its adversarial robustness remain limited this the adversarial robustness seven ssl models and one supervised across range tasks including imagenet classification transfer learning segmentation and detection our that ssl models exhibit superior robustness adversarial attacks compared their supervised counterpart imagenet this advantage extending transfer learning classification tasks however this robustness less pronounced segmentation and detection tasks finally assess the effect extended training durations adversarial robustness that longer training may offer slight improvements compromising robustness our analysis highlights promising directions for enhancing the adversarial robustness visual self supervised representation systems complex environments,2025-08-26T01:35:51.567782
57,SetFlow: Optimal-Transport Self-Supervision for Permutation-Equivariant Set Representations,"Many domains provide data as unordered sets—point clouds, bags of features, multiple-instance samples—yet most pretraining assumes grid structure or imposes order, impairing transfer. We propose SetFlow, a self-supervised framework that learns permutation-equivariant representations by aligning instances via differentiable optimal transport and enforcing consistency across cardinalities. A backbone set encoder (DeepSets/SetTransformer) maps elements to latent tokens; a Sinkhorn layer computes transport plans between augmented sets, and training minimizes a transport-aligned contrastive loss plus masked-element reconstruction in latent space. A cardinality-robust regularizer penalizes sensitivity of pooled features to subsampling, and an equivariant Jacobian constraint stabilizes element-wise geometry. We prove that SetFlow approximates a canonical embedding of the empirical measure, with generalization bounds depending on Wasserstein complexity rather than set size, and show identifiability up to an orthogonal map under a mixture-of-components model. Efficient batching uses sliced OT with entropic warm-starts and adaptive element dropping. On ModelNet40 point clouds, WILDS multiple-instance pathology (Camelyon), and set-structured tabular benchmarks, SetFlow improves linear-probe and few-shot performance by 2–6% over contrastive baselines adapted to sets, while maintaining accuracy across varying set cardinalities and heavy subsampling. Under permutation and cardinality shifts, performance degrades gracefully. SetFlow establishes transport-aligned, permutation-equivariant self-supervision, enabling robust set representations for 3D perception, multi-instance learning, and scientific data.",ICLR,representation learning,gpt-5,True,8614,Not-So-Optimal Transport Flows for 3D Point Cloud Generation,"Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data and we show that these models scale poorly on large point clouds. Also, we observe learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. To remedy these, we propose not-so-optimal transport flow models that obtain an approximate OT by an offline OT precomputation, enabling an efficient construction of OT pairs for training. During training, we can additionally construct a hybrid coupling by combining our approximate OT and independent coupling to make the target flow models easier to learn. In an extensive empirical study, we show that our proposed model outperforms prior diffusion- and flow -based approaches on a wide range of unconditional generation and shape completion on the ShapeNet benchmark.",ICLR.cc/2025/Conference,6.8,True,0.8066,many domains provide data unordered sets point clouds bags features multiple instance samples yet most pretraining assumes grid structure imposes order impairing transfer that setflow approximates canonical embedding the empirical measure generalization bounds depending wasserstein complexity rather than set size and identifiability orthogonal map under mixture components modelnet40 point clouds wilds multiple instance pathology camelyon and set structured tabular benchmarks setflow improves linear probe and few shot over contrastive baselines adapted sets while maintaining across varying set cardinalities and heavy subsampling,learning generative models point clouds one the fundamental problems generative learning also observe learning equivariant flows challenging since straightening flow trajectories makes the learned flow complex the beginning the trajectory extensive empirical that our proposed outperforms prior diffusion and flow approaches wide range unconditional generation and shape completion the shapenet,2025-08-26T01:35:51.567785
58,ECC-Latents: Error-Correcting Discrete Codes for Robust Representation Learning,"Discrete latents enable compression and compositionality, but standard vector quantization yields codebooks with small minimum distances and unstable neighborhoods, harming robustness and retrieval. We present ECC-Latents, a discrete representation learning framework that endows latent codes with explicit error-correcting structure. ECC-Latents parameterizes a codebook as a learnable binary code with prescribed minimum Hamming distance via algebraic constraints (BCH/LDPC generators) and a differentiable hard-assignment using straight-through estimators. Training couples masked reconstruction or contrastive objectives with (i) a minimum-distance regularizer enforcing code separation, (ii) a syndrome sparsity penalty to prevent near-violations, and (iii) a local isometry loss aligning Euclidean encoder outputs with Hamming neighborhoods. We prove that large minimum distance bounds adversarial drift in latent space and derive retrieval stability guarantees under bounded input perturbations. An efficient encoder–decoder factorization minimizes overhead, and a pruning schedule maintains code utilization. On ImageNet-100, VGGSound, and text retrieval (MSMARCO passages with learned codes), ECC-Latents improves linear probes by 1.8–3.5% over VQ-VAEs and discrete masked models, increases recall@k by 4–8%, and yields 2× faster approximate search via Hamming indexing. Under corruptions (ImageNet-C/A) and quantization, performance degrades smoothly, outperforming baselines. Ablations validate the role of algebraic constraints and syndrome penalties. ECC-Latents brings principled coding theory to discrete representation learning, producing compact, robust, and search-friendly latents.",ICLR,representation learning,gpt-5,True,10760,Addressing Representation Collapse in Vector Quantized Models with One Linear Layer,"Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose \textbf{SimVQ}, a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the \textit{entire linear space} spanned by the codebook, rather than merely updating \textit{the code vector} selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts.",ICLR.cc/2025/Conference,4.0,nan,0.8529,discrete latents enable compression and compositionality but standard vector quantization yields codebooks small minimum distances and unstable neighborhoods harming robustness and retrieval ecc latents discrete representation learning that endows latent codes explicit error correcting structure ecc latents brings principled coding theory discrete representation learning producing compact robust and search friendly latents,vector quantization used for converting continuous representations into discrete codes which has become fundamental unsupervised representation learning and latent generative models however models are often hindered the problem representation collapse the latent space which leads low codebook utilization and limits the scalability the codebook for large scale training existing methods designed mitigate representation collapse reduce the dimensionality latent space the expense capacity which not fully resolve the core issue this conduct theoretical analysis representation collapse models and identify its primary cause the disjoint optimization the codebook where only small subset code vectors are updated gradient descent the that simvq not only addresses the problem representation collapse but also proves highly adaptable and easy suggesting its broad applicability diverse machine learning contexts,2025-08-26T01:35:51.567787
59,FourierCurriculum: Frequency-Targeted Masked Pretraining with Spectral Generalization Guarantees,"Masked modeling treats all patches or tokens uniformly, often overfitting high-frequency nuisances and underusing structure across scales. We introduce FourierCurriculum, a self-supervised pretraining strategy that explicitly schedules frequency content of masked targets and contexts to shape spectral bias. Using fast steerable transforms, we decompose inputs into band-limited components; a scheduler gradually reveals higher frequencies, while the model is tasked with reconstructing withheld bands conditioned on lower ones. A spectral consistency loss enforces phase alignment across bands, and a bandwise Jacobian penalty stabilizes local geometry. We provide generalization bounds for linear probes that scale with the Dirichlet energy of the retained spectrum and prove that the curriculum reduces sensitivity to spurious high-frequency cues under common corruption models. Training remains efficient with FFT-based masking, shared decoders, and curriculum-aware batch norm. On ImageNet-100, CIFAR-100, and AudioSet spectrograms, FourierCurriculum improves linear-probe accuracy by 2.2–4.6% over MAE/iBOT at matched compute and reduces corruption error (ImageNet-C/A) by 3–7%. On Sketch/Style domain shifts, it preserves shape-biased features and narrows worst-group gaps. Few-shot transfer benefits from early low-frequency emphasis. Ablations confirm the necessity of spectral scheduling and phase consistency. FourierCurriculum provides a principled route to sculpt spectral inductive bias in masked pretraining, yielding robust, scale-aware representations across modalities.",ICLR,representation learning,gpt-5,False,,Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering,"3D sketches are widely used for visually representing the 3D shape and structure of objects or scenes. However, the creation of 3D sketch often requires users to possess professional artistic skills. Existing research efforts primarily focus on enhancing the ability of interactive sketch generation in 3D virtual systems. In this work, we propose Diff3DS, a novel differentiable rendering framework for generating view-consistent 3D sketch by optimizing 3D parametric curves under various supervisions. Specifically, we perform perspective projection to render the 3D rational Bézier curves into 2D curves, which are subsequently converted to a 2D raster image via our customized differentiable rasterizer. Our framework bridges the domains of 3D sketch and raster image, achieving end-to-end optimization of 3D sketch through gradients computed in the 2D image domain. Our Diff3DS can enable a series of novel 3D sketch generation tasks, including text-to-3D sketch and image-to-3D sketch, supported by the popular distillation-based supervision, such as Score Distillation Sampling (SDS). Extensive experiments have yielded promising results and demonstrated the potential of our framework. Project: https://yiboz2001.github.io/Diff3DS/",ICLR.cc/2025/Conference,6.5,True,0.7900,sketch style domain shifts preserves shape biased features and narrows worst group gaps few shot transfer benefits from early low frequency emphasis,existing efforts primarily focus enhancing the ability interactive sketch generation virtual systems our bridges the domains sketch and raster image achieving end end optimization sketch gradients computed the image domain our diff3ds can enable series sketch generation tasks including text sketch and image sketch supported the popular distillation based supervision such distillation sampling sds,2025-08-26T01:35:51.567790
60,CircuitMAE: Tractable Probabilistic Circuits as Decoders for Masked Representation Learning,"Decoders in masked autoencoding are typically neural networks trained with heuristic losses, providing no likelihoods and limited uncertainty calibration. We propose CircuitMAE, which replaces the decoder with tractable probabilistic circuits—sum–product networks and structured decomposable architectures—yielding exact marginal likelihoods and calibrated imputations for masked tokens. An expressive encoder maps inputs to circuit parameters via hypernetworks while preserving a low-dimensional latent bottleneck; training maximizes exact masked log-likelihood alongside a representation loss (variance regularization or contrast) and a mutual-information proxy that discourages shortcut memorization. We prove that CircuitMAE’s exact marginals reduce variance in gradient estimators and derive sample-complexity gains for reconstruction under structured missingness. A sparsity prior on circuit structure and parameter sharing across positions ensures scalability. On ImageNet-100 patches, text (WikiText-103 subword tokens), and tabular UCI datasets, CircuitMAE improves linear probes by 1.7–3.9% over MAE and masked language modeling baselines, and achieves superior calibration (ECE −25–40%) and imputation NLL. Under structured occlusions and MNAR missingness, it maintains accuracy where neural decoders fail. Ablations show the benefits of decomposability and hypernetwork parameterization. CircuitMAE brings tractable probabilistic inference into masked pretraining, producing uncertainty-aware representations that transfer while supporting principled likelihood-based diagnostics.",ICLR,representation learning,gpt-5,True,11482,Path Selection Makes BERT-family Good Generators,"The Mask-Predict decoding algorithm has been widely used to enhance the generation capacity of traditional non-autoregressive (NAR) models and provide a good recipe for adapting the pre-trained BERT-like masked language models (MLMs) to NAR generation scenarios.
However, these models, which we denote as NAR-MLMs, are still regarded as inferior to competitive autoregressive (AR) models in terms of performance.
In this paper, we further explore the core problems leading to the performance gap of NAR-MLMs and delve into effective solutions for technological innovation.
Specifically, most related works neglect the impact of the training sequence decomposition format, i.e., 
Unlike the AR models which can naturally decompose the text sequence in a left-to-right manner for training and inference, NAR-MLMs are trained with a random decomposition but aim to find a determined optimal composition (denoted as decoding paths) during inference.
To alleviate this mismatching, we propose decoding path selection to increase the search space for finding a better 
composition, and path optimization methods to enable the model decoding path preference during the training process. 
Results on various zero-shot common sense reasoning and reading comprehension tasks and several task-specific generation tasks demonstrate that our NAR-MLM achieves significant performance improvements on common benchmarks with the methods mentioned above, reaching performance levels comparable to even outperforming AR pre-trained models. Our model and code will be available at Github.",ICLR.cc/2025/Conference,3.75,False,0.8519,decoders masked autoencoding are neural networks trained heuristic losses providing likelihoods and limited uncertainty calibration expressive encoder maps inputs circuit parameters hypernetworks while preserving low dimensional latent bottleneck training maximizes exact masked log likelihood alongside representation loss variance regularization contrast and mutual information proxy that discourages shortcut memorization imagenet patches text wikitext subword tokens and tabular uci datasets circuitmae improves linear probes over mae and masked language modeling baselines and achieves superior calibration ece and imputation nll under structured occlusions and mnar missingness maintains where neural decoders fail circuitmae brings tractable probabilistic inference into masked pretraining producing uncertainty aware representations that transfer while supporting principled likelihood based diagnostics,the mask predict decoding has been used enhance the generation capacity traditional non autoregressive nar models and provide good recipe for adapting the pre trained bert like masked language models mlms nar generation scenarios alleviate this mismatching decoding path selection increase the search space for better composition and path optimization methods enable the decoding path preference during the training process various zero shot common sense reasoning and reading comprehension tasks and several task specific generation tasks that our nar mlm achieves significant improvements common benchmarks the methods mentioned above reaching levels comparable even outperforming pre trained models,2025-08-26T01:35:51.567794
61,GroupNCE: Subquadratic Contrastive Learning via Combinatorial Negative Design,"Contrastive learning scales poorly with batch size due to quadratic pairwise similarities and suffers from false negatives. We introduce GroupNCE, a subquadratic contrastive objective that uses combinatorial group testing to construct informative, low-variance negative sets. Instead of scoring all pairs, GroupNCE encodes items into b pooled groups using a binary design; similarities are computed groupwise, and a sparse recovery step approximates item-level negatives. We show that appropriately designed matrices (disjunct/separable codes) yield unbiased gradient estimators with variance that decays as O(1/b), and derive bounds on the bias from collisions. A learned group assignment head adapts designs to the data, regularized by mutual incoherence and balanced usage, while a small memory bank supplies hard negatives selectively. The resulting complexity is O(n b) per batch rather than O(n^2). On ImageNet-100, CIFAR-100, and AudioSet, GroupNCE matches or outperforms SimCLR/MoCo-v3 in linear probes with 1.5–2.3× lower wall-clock at large batch sizes and reduces false-negative effects, improving transfer on fine-grained datasets. Scaling to 65k negatives is practical on a single node. Ablations validate design choices and variance predictions. GroupNCE provides a principled, scalable alternative to brute-force contrastive scoring, enabling efficient pretraining at industrial scales without sacrificing representation quality.",ICLR,representation learning,gpt-5,True,248,Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss,"Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.",ICLR.cc/2025/Conference,4.5,nan,0.8518,contrastive learning scales poorly batch size due quadratic pairwise similarities and suffers from false negatives imagenet cifar and audioset groupnce matches outperforms simclr moco linear probes lower wall clock large batch sizes and reduces false negative effects improving transfer fine grained datasets groupnce provides principled scalable alternative brute force contrastive scoring enabling efficient pretraining industrial scales sacrificing representation quality,contrastive loss powerful for representation learning where larger batch sizes enhance providing more negative samples better distinguish between similar and dissimilar data,2025-08-26T01:35:51.567798
62,JL-SSL: Random Projection Ensembles Stabilize Geometry in Self-Supervised Pretraining,"Self-supervised features are sensitive to initialization and training noise, causing geometric drift that hurts reproducibility and downstream agreement. We propose JL-SSL, which stabilizes geometry by enforcing agreement across an ensemble of random projections inspired by Johnson–Lindenstrauss embeddings. During pretraining, each batch is processed by K lightweight projection heads with i.i.d. Gaussian weights; the encoder is optimized to minimize a multi-view agreement loss that aligns projected similarities and preserves pairwise distances up to a learned tolerance, alongside a base masked/contrastive objective. A projection diversity regularizer prevents trivial alignment and a spectral penalty controls feature anisotropy. We prove that JL-SSL reduces estimator variance of pairwise distances and yields concentration guarantees on neighborhood preservation akin to JL bounds, translating into tighter risk bounds for k-NN and linear probes. Efficient training uses shared MLP stems and low-rank projections. On ImageNet-100, CIFAR-100, and DomainNet, JL-SSL improves linear-probe accuracy by 1.5–3.1% over baselines while reducing cross-seed variation in k-NN accuracy and silhouette scores by 35–60%. Under distributed training with asynchronous updates, it maintains representation consistency and improves downstream calibration. JL-SSL offers a simple, theory-grounded tool to tame geometric instability, delivering reliable, transferable representations without architectural changes.",ICLR,representation learning,gpt-5,False,,Gaussian Splatting Lucas-Kanade,"Gaussian Splatting and its dynamic extensions are effective for reconstructing 3D scenes from 2D images when there is significant camera movement to facilitate motion parallax and when scene objects remain relatively static. However, in many real-world scenarios, these conditions are not met. As a consequence, data-driven semantic and geometric priors have been favored as regularizers, despite their bias toward training data and their neglect of broader movement dynamics.

Departing from this practice, we propose a novel analytical approach that adapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By leveraging the intrinsic properties of the forward warp field network, we derive an analytical velocity field that, through time integration, facilitates accurate scene flow computation. This enables the precise enforcement of motion constraints on warp fields, thus constraining both 2D motion and 3D positions of the Gaussians. Our method excels in reconstructing highly dynamic scenes with minimal camera movement, as demonstrated through experiments on both synthetic and real-world scenes.",ICLR.cc/2025/Conference,6.0,True,0.7999,projection diversity regularizer prevents trivial alignment and spectral penalty controls feature anisotropy under distributed training asynchronous updates maintains representation consistency and improves downstream calibration,consequence data driven semantic and geometric priors have been favored regularizers despite their bias toward training data and their neglect broader movement dynamics,2025-08-26T01:35:51.567801
63,"AnyTimeRep: Budget-Aware Self-Supervision for Nested, Early-Exiting Representations","Many deployments require anytime predictions under variable compute and latency, but standard pretraining does not enforce nestedness or stable early exits. We present AnyTimeRep, a self-supervised framework that learns hierarchically nested representations supporting accurate early exits and smooth accuracy–compute trade-offs. A multi-stage encoder exposes intermediate features; each stage has a lightweight predictor trained with a shared masked/contrastive objective and a nesting loss that enforces monotone refinement—later layers must preserve and improve earlier semantics. A conditional computation gate learns to skip blocks using a differentiable budget controller, and a KL consistency term propagates corrections from deeper predictions to shallow ones. We prove that nesting constrains the Lipschitz constant of stage mappings and derive risk bounds for early exits under budgeted inference. Training employs curriculum budgets, stochastic depth calibrated by the controller, and distillation from deeper stages. On ImageNet-100, VTAB-1k, and Kinetics-200, AnyTimeRep achieves up to 85–95% of final linear-probe accuracy using 25–50% of FLOPs, outperforming strong baselines with shallow heads or post-hoc distillation by 3–7%. Under latency constraints and mobile hardware, it yields consistent gains and improved calibration for early exits. AnyTimeRep turns budget-awareness into a first-class pretraining goal, enabling flexible, efficient deployment without sacrificing representational quality.",ICLR,representation learning,gpt-5,False,,Neural Electrostatics: A 3D Physics-Informed Boundary Element Poisson Equation Solver,"Electrostatics solvers relate an imposed voltage to a
corresponding charge density. Current classical methods require fine
discretization and scale poorly due to the construction of a large linear system
of equations. We recast the problem using neural networks and introduce
neural electrostatics, a hybrid 3D boundary element method (BEM). By using the
boundary element form, we are able to overcome many shortcomings of previous
neural solvers, such as learning trivial solutions and balancing loss terms
between the domain and boundary, at the cost of introducing a large integral
containing a singular kernel. We handle this singularity by locally
transforming the integral into polar coordinates and applying a numerical
quadrature. We also show that previous neural solver sampling methods are unable
to minimize the PDE residual, and propose a variational adaptive sampling
method. This technique is able to reduce mean absolute error by 5 times, while
keeping training time constant. Extensive scaling and ablation studies are
performed to justify our method. Results show that our method learns a charge
distribution within 1.2 $pC/m^2$ of mean absolute error from a classical BEM
solver, while using 25 times fewer rectangular elements.",ICLR.cc/2025/Conference,4.0,False,0.0000,,recast the problem neural networks and neural electrostatics hybrid boundary element bem the boundary element form are able overcome many shortcomings previous neural solvers such learning trivial solutions and balancing loss terms between the domain and boundary the cost introducing large integral containing singular kernel also that previous neural solver sampling methods are unable minimize the pde residual and variational adaptive sampling,2025-08-26T01:35:51.567803
64,TranspoGame: Adversarial Transport for View-Consistent Self-Supervised Representations,"Self-supervised learning typically aligns augmented views by minimizing a fixed divergence between their feature distributions, implicitly assuming a single optimal coupling. We challenge this assumption and introduce TranspoGame, a min–max framework that casts view alignment as a two-player transport game between an encoder and an adversarial coupling. The encoder maps inputs to a latent space; an adversary selects a transport plan over latent batches to maximize alignment cost under entropy and marginal constraints, while the encoder minimizes the worst-case cost subject to a semantic preservation loss. We derive conditions under which the resulting saddle point recovers the Monge map between augmentation distributions and show that adversarial transport tightens a mutual-information lower bound by eliminating “easy” couplings. A strongly convex–concave regularizer yields uniqueness and stability, and a stochastic mirror–prox solver scales the inner game with linear-time Sinkhorn projections. On ImageNet-100, CIFAR-100, and DomainBed (PACS, VLCS), TranspoGame improves linear-probe accuracy by 2.0–4.5% over SimCLR/MoCo-v3 with matched compute and reduces worst-domain error by up to 7%. Under augmentation violations and heavy corruptions (ImageNet-C/A), it maintains 3–6% higher accuracy, with ablations confirming the necessity of adversarial coupling versus fixed transport. By adversarially hardening alignment, TranspoGame learns view-consistent, geometry-aware representations that transfer more reliably across domains and perturbations.",ICLR,representation learning,gpt-5,True,7877,IO-LVM: Inverse optimization latent variable models with applications to inferring and explaining paths,"Learning representations from solutions of constrained optimization problems (COPs) with unknown cost functions is challenging, as models like (Variational) Autoencoders struggle to capture constraints to decode structured outputs. We propose an inverse optimization latent variable model (IO-LVM) that constructs a latent space of COP costs based on observed decisions, enabling the inference of feasible and meaningful solutions by reconstructing them with a COP solver. To achieve this, we leverage estimated gradients of a Fenchel-Young loss through a non-differentiable deterministic solver while shaping the embedding space. In contrast to established Inverse Optimization or Inverse Reinforcement Learning methods, which typically identify a single or context-conditioned cost function, we exploit the learned representation to capture underlying COP cost structures and identify solutions likely originating from different agents, each using distinct or slightly different cost functions when making decisions. Using both synthetic and actual ship routing data, we validate our approach through experiments on path planning problems using the Dijkstra algorithm, demonstrating the interpretability of the latent space and its effectiveness in path inference and path distribution reconstruction.",ICLR.cc/2025/Conference,4.5,False,0.8146,self supervised learning aligns augmented views minimizing fixed divergence between their feature distributions implicitly assuming single optimal coupling the encoder maps inputs latent space adversary selects transport plan over latent batches maximize alignment cost under entropy and marginal constraints while the encoder minimizes the worst case cost subject semantic preservation loss adversarially hardening alignment transpogame learns view consistent geometry aware representations that transfer more reliably across domains and perturbations,learning representations from solutions constrained optimization problems cops unknown cost functions challenging models like variational autoencoders struggle capture constraints decode structured outputs inverse optimization latent variable lvm that constructs latent space cop costs observed decisions enabling the inference feasible and meaningful solutions reconstructing them cop solver achieve this leverage estimated gradients fenchel young loss non differentiable deterministic solver while shaping the embedding space contrast established inverse optimization inverse reinforcement learning methods which identify single context conditioned cost function exploit the learned representation capture underlying cop cost structures and identify solutions likely originating from different agents each distinct slightly different cost functions when making decisions both synthetic and actual ship routing data our experiments path planning problems the dijkstra demonstrating the interpretability the latent space and its effectiveness path inference and path distribution reconstruction,2025-08-26T01:35:51.567806
65,WaveMAE: Scattering-Augmented Masked Autoencoding with Stability to Deformations,"Masked autoencoders excel at reconstruction yet are sensitive to small diffeomorphic deformations and high-frequency noise. We propose WaveMAE, a self-supervised framework that integrates analytic wavelet–scattering features with learned encoders to yield deformation-stable, information-rich representations. Inputs are decomposed into multi-order scattering coefficients and residuals via steerable wavelets; a dual-path encoder processes scattering and residual branches with cross-attention bridges. Pretraining minimizes masked reconstruction on both branches with a spectral-phase consistency loss, complemented by a deformation-stability regularizer that bounds the encoder’s log-Lipschitz constant measured along random spline flows. We prove that the combined representation inherits stability from scattering while retaining discrimination through the residual path, and derive linear-probe risk bounds in terms of Dirichlet energy of the retained spectrum. Efficient training uses FFT kernels, low-rank cross-attention, and randomized deformation augmentations. On ImageNet-100, CIFAR-100, AudioSet spectrograms, and iNaturalist-LT, WaveMAE improves linear-probe accuracy by 2.1–4.3% over MAE/iBOT and reduces corruption error (ImageNet-C/A) by 3–6%. Under sketch, style, and small elastic shifts, performance degrades gracefully, and few-shot transfer benefits from early low-frequency emphasis. Ablations isolate the contributions of scattering inputs and phase consistency. WaveMAE unifies analytic stability with learned expressivity, producing deformation-robust features that generalize across visual and audio domains.",ICLR,representation learning,gpt-5,False,,XiEff Representation for Near-Field Optics,"Near-field optics, or near-field electrodynamics, is a field that studies the interaction between materials and light at spatial scales smaller than the wavelength. At these extremely small scales, below the diffraction limit, the interaction between materials and electromagnetic fields can exhibit unique behaviors and properties not observed in conventional optics. This area of research is crucial for understanding the optical characteristics of nanotechnical systems and nanoscale biological objects. One of the primary tools used in near-field optics research is scanning near-field optical microscopy (SNOM), which allows researchers to measure near-field optical images (NFI). However, these images often lack visual clarity and interpretability, hindering a comprehensive understanding of the properties of the probed particles.

The main goal of this paper is to introduce a novel approach that addresses these challenges. Inspired by the prominent progress in Neural Radiance Fields (NeRFs) from computer vision and ideas from physics-informed neural networks (PINNs). We propose an unsupervised method that introduces the XiEff representation – a neural field-based reparameterization of the effective susceptibility tensor. By integrating XiEff into the Lippmann-Schwinger integral equation framework for near-field optics we develop an optimization strategy to reconstruct the effective susceptibility distribution directly from NFI data.

The optimized XiEff representation provides an interpretable and explainable model of the particle's shape. Extensive evaluations on a synthetically generated NFI dataset demonstrate the effectiveness of the method, achieving high intersection-over-union scores between XiEff and ground truth shapes, even for complex geometries. Furthermore, the approach exhibits desirable robustness to measurement noise, a crucial property for practical applications. The XiEff representation, combined with the proposed optimization framework, potentially introduces a valuable tool for enabling explainable near-field optics imaging and enhancing the understanding of particle characteristics through interpretable representations",ICLR.cc/2025/Conference,3.5,nan,0.7609,that the combined representation inherits stability from scattering while retaining discrimination the residual path and derive linear probe risk bounds terms dirichlet energy the retained spectrum under sketch style and small elastic shifts degrades gracefully and few shot transfer benefits from early low frequency emphasis,inspired the prominent progress neural radiance fields nerfs from computer vision and ideas from physics informed neural networks pinns unsupervised that introduces the xieff representation neural field based reparameterization the effective susceptibility tensor integrating xieff into the lippmann schwinger integral equation for near field optics optimization strategy reconstruct the effective susceptibility distribution directly from nfi data the optimized xieff representation provides interpretable and explainable the particle shape furthermore the exhibits desirable robustness measurement noise crucial property for practical applications the xieff representation combined the proposed optimization potentially introduces valuable tool for enabling explainable near field optics imaging and enhancing the understanding particle characteristics interpretable representations,2025-08-26T01:35:51.567811
66,AutoFSM: Discovering Finite-State Structure for Control-Oriented Representation Learning,"Many interactive environments exhibit low-order temporal semantics—modes, subroutines, and event-driven logic—that are poorly captured by continuous latent dynamics. We introduce AutoFSM, a self-supervised framework that learns control-oriented representations by discovering finite-state structure from raw trajectories. An encoder maps observations to logits over latent symbols; a differentiable weighted finite automaton (WFA) models transitions conditioned on actions, trained with a masked trajectory prediction objective and a bisimulation regularizer that contracts behaviorally equivalent states. A sparsity-inducing prior on transition tensors and a determinization loss promote compact, interpretable automata. We prove that AutoFSM recovers a minimal deterministic automaton up to state relabeling for a class of partially observable Markov decision processes with separable emissions, and bound value-function approximation error using automaton features. Training scales via forward–backward recursions with Gumbel relaxations and short-horizon rollouts. On DMControl pixels, Atari-100k, and hierarchical gridworlds with latent modes, AutoFSM improves sample efficiency by 22–45% over DrQv2, CURL, and contrastive baselines, and enables zero-shot option discovery and goal-conditioned control. In offline RL (D4RL), WFA latents enhance policy improvement under dataset shift. Visualizations reveal interpretable state machines aligned with environment events. AutoFSM bridges symbolic structure and deep representation learning, delivering compact, decision-relevant features for efficient control.",ICLR,representation learning,gpt-5,True,7273,LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior,"We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). Project page:  https://hywang66.github.io/larp/",ICLR.cc/2025/Conference,7.5,True,0.8194,encoder maps observations logits over latent symbols differentiable weighted finite automaton wfa models transitions conditioned actions trained masked trajectory prediction objective and bisimulation regularizer that contracts behaviorally equivalent states dmcontrol pixels atari 100k and hierarchical gridworlds latent modes autofsm improves sample efficiency over drqv2 curl and contrastive baselines and enables zero shot option discovery and goal conditioned control autofsm bridges symbolic structure and deep representation learning delivering compact decision relevant features for efficient control,this allows larp capture more global and semantic representations rather than being limited local patch level information align the discrete token space downstream generation tasks larp integrates lightweight transformer training time prior that predicts the next token its discrete latent space incorporating the prior during training larp learns latent space that not only optimized for video reconstruction but also structured way that more conducive autoregressive generation moreover this process defines sequential order for the discrete tokens progressively pushing them toward optimal configuration during training ensuring smoother and more accurate generation inference time comprehensive experiments larps strong achieving state the art fvd the ucf101 class conditional video generation larp enhances the compatibility models videos and opens the potential build unified high fidelity multimodal large language models mllms,2025-08-26T01:35:51.567815
67,HoloSSL: Coherent Optics-Aware Self-Supervision for Holographic Microscopy Representations,"Coherent imaging systems encode specimens in phase and amplitude through Fresnel propagation, yet most representation learners operate on intensity-only reconstructions, discarding physics and hindering transfer across setups. We present HoloSSL, a self-supervised framework that learns optics-aware features directly from holograms by embedding forward propagation and phase retrieval into pretraining. A physics-informed encoder processes raw interferograms; differentiable Fresnel operators generate multi-distance views, and a phase-consistency objective enforces agreement of latent features across propagated planes. Masked complex reconstruction predicts missing amplitude–phase patches, while a speckle-invariance regularizer reduces sensitivity to coherence artifacts. We prove identifiability of a canonical latent up to a global phase under twin-image suppression and derive bounds linking propagation-consistency loss to generalization across sensor distances and wavelengths. Efficient training leverages FFT-based propagators, complex-valued layers, and stochastic distance sampling. On digital holographic microscopy (dhQD, phase-contrast cells) and coherent diffraction imaging, HoloSSL surpasses MAE/SimCLR trained on reconstructed images by 3–6% in linear probes for morphology classification and segmentation, and retains accuracy across optical parameter shifts (distance, wavelength) with 25–40% lower degradation. It improves zero-shot transfer between microscopes and enhances downstream phase unwrapping. HoloSSL grounds self-supervision in coherent optics, producing durable, physics-consistent representations for biomedical imaging.",ICLR,representation learning,gpt-5,False,,Spectral Operator Methods for Learning Coherent Temporal Representations in Cellular Signaling Dynamics,"We present a novel operator-based framework for learning coherent temporal representations of cellular dynamics from live-cell imaging data. Recognizing the inherent stochasticity and measurement limitations in biological systems, our approach shifts the focus from predicting exact trajectories to characterizing key dynamical properties that shape cellular behaviors at the population level. By leveraging spectral analysis of the Koopman operator and smoothing via Markov semigroups of kernel integral operators, we identify near-resonant patterns and transient coherent structures that persist across different experimental conditions. This methodology effectively captures fundamental dynamics, providing insights into mechanisms of heterogeneous cell responses without the need to model precise transformation laws. We demonstrate the efficacy of our framework on a dataset of retinal pigment epithelial cells with an inducible oncogene, revealing conserved dynamical patterns across varying levels of ERK inhibition. Our work offers interpretable learned representations, even with limited and noisy single-cell-resolved recordings, advancing machine learning for dynamical systems and opening new avenues for understanding and predicting cellular behavior in response to external stimuli.",ICLR.cc/2025/Conference,3.6666666666666665,False,0.7898,coherent imaging systems encode specimens phase and amplitude fresnel propagation yet most representation learners operate intensity only reconstructions discarding physics and hindering transfer across setups holossl self supervised that learns optics aware features directly from holograms embedding forward propagation and phase retrieval into pretraining digital holographic microscopy dhqd phase contrast cells and coherent diffraction imaging holossl surpasses mae simclr trained reconstructed images linear probes for morphology classification and segmentation and retains across optical parameter shifts distance wavelength lower degradation improves zero shot transfer between microscopes and enhances downstream phase unwrapping,operator based for learning coherent temporal representations cellular dynamics from live cell imaging data our offers interpretable learned representations even limited and noisy single cell resolved recordings advancing machine learning for dynamical systems and opening avenues for understanding and predicting cellular behavior response external stimuli,2025-08-26T01:35:51.567818
68,SchemaMorph: Schema-Equivariant Self-Supervision for Heterogeneous Tabular Data,"Tabular representation learning is hampered by heterogeneous schemas—columns differ in type, scale, and semantics across datasets—leading to brittle transfer. We propose SchemaMorph, a self-supervised framework that learns schema-equivariant representations: features that transform predictably under column permutations, merges/splits, and monotone reparameterizations. Column encoders implement monotone normalizing flows to a shared canonical marginal; a schema graph models column relations via typed edges. Pretraining uses masked cell modeling with a schema-consistency loss that predicts the effect of schema morphisms (e.g., merging two columns or retyping ordinal/continuous), and an invariance penalty to column-wise monotone transforms. We prove that SchemaMorph identifies a canonical embedding up to an orthogonal map under a latent additive noise model and derive bounds on transfer risk across schemas using stability of the morphism predictors. Efficient batching samples synthetic morphisms and employs low-rank flow updates. On finance and healthcare (FEMNIST-tabular, MIMIC-III), e-commerce click logs, and UCI mixes, SchemaMorph improves linear-probe AUC by 2–6% over SAINT, VIME, and TabTransformer, while enabling cross-schema zero-shot transfer and robust imputation under column drift. Under missing-not-at-random patterns, performance degrades gracefully due to monotone flows. SchemaMorph establishes schema-equivariance as a powerful inductive bias for tabular representation learning at scale.",ICLR,representation learning,gpt-5,True,1171,Matchmaker: Schema Matching with self-improving compositional LLM programs,"Schema matching -- the task of finding matches between attributes across disparate data sources with different tables and hierarchies -- is critical for creating interoperable machine learning (ML)-ready data. Addressing this fundamental data-centric problem has wide implications, especially in domains like healthcare, finance and e-commerce --- but also has the potential to benefit ML models more generally, by increasing the data available for ML model training. However, schema matching is a challenging ML task due to structural/hierarchical and semantic heterogeneity between different schemas. Previous ML approaches to automate schema matching have either required significant labeled data for model training, which is often unrealistic, or suffer from poor zero-shot performance. To this end, we propose Matchmaker -  a compositional language model program for schema matching, comprised of candidate generation, refinement and confidence scoring. Matchmaker also self-improves in a zero-shot manner without the need for labeled demonstrations via a novel optimization approach, which constructs synthetic in-context demonstrations to guide the language model's reasoning process.  Empirically, we demonstrate on real-world medical schema matching benchmarks that Matchmaker outperforms previous ML-based approaches, highlighting its potential to accelerate data integration and interoperability of ML-ready data.",ICLR.cc/2025/Conference,5.25,False,0.8079,tabular representation learning hampered heterogeneous schemas columns differ type scale and semantics across datasets leading brittle transfer that schemamorph identifies canonical embedding orthogonal map under latent additive noise and derive bounds transfer risk across schemas stability the morphism predictors finance and healthcare femnist tabular mimic iii commerce click logs and uci mixes schemamorph improves linear probe auc over saint vime and tabtransformer while enabling cross schema zero shot transfer and robust imputation under column drift schemamorph establishes schema equivariance powerful inductive bias for tabular representation learning scale,schema matching the task matches between attributes across disparate data sources different tables and hierarchies critical for creating interoperable machine learning ready data however schema matching challenging task due structural hierarchical and semantic heterogeneity between different schemas previous approaches automate schema matching have either required significant labeled data for training which often unrealistic suffer from poor zero shot this end matchmaker compositional language program for schema matching comprised candidate generation refinement and confidence scoring matchmaker also self improves zero shot manner the need for labeled demonstrations optimization which constructs synthetic context demonstrations guide the language model reasoning process,2025-08-26T01:35:51.567821
69,Sketch2Slot: Stroke-Consistent Self-Supervision for Vector Graphics Representations,"Vector sketches encode shape as sequences of strokes with variable order and timing, but most pretraining rasterizes inputs, losing structural cues crucial for reasoning and editing. We introduce Sketch2Slot, a self-supervised framework that learns stroke-consistent representations directly from vector graphics. A transformer with relative-time encodings consumes Bézier control points; a slot attention module aggregates strokes into object-level slots. Pretraining combines (i) masked stroke prediction with spline parameterization, (ii) permutation-invariant contrast across random stroke orderings, and (iii) a topology loss that preserves adjacency and junction consistency in the slot graph. We prove that stroke-permutation invariance and topology preservation bound representation drift under drawing style changes, and show identifiability of slot assignments up to permutation under a separable part model. Efficient training uses differentiable rasterization for reconstruction checks and dynamic programming for stroke–slot assignment. On QuickDraw, TU-Berlin, and vector logos, Sketch2Slot improves linear-probe recognition by 3–7% over raster-based MAE and sequence baselines, and excels at part-level retrieval and zero-shot vector editing (e.g., style transfer, stroke reordering). Robustness to stroke dropout and overtracing increases substantially. Ablations highlight the role of topology loss and slot aggregation. Sketch2Slot advances representation learning for vector graphics, enabling structure-aware understanding and controllable editing.",ICLR,representation learning,gpt-5,False,,Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision,"Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. 
However, despite the great strides made in generative modeling,
SVG has remained underexplored due to the discrete and complex nature of such
data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised
of two modules: A Visual Shape Quantizer (VSQ) learns to map raster
images onto a discrete codebook by reconstructing them as vector shapes, and
an Auto-Regressive Transformer (ART) models the joint probability distribution
over shape tokens, positions and textual descriptions, allowing us to generate vector 
graphics from natural language. Unlike existing models that require direct
supervision from SVG data, GRIMOIRE learns shape image patches using only
raster image supervision which opens up vector generative modeling to significantly more data. 
We demonstrate the effectiveness of our method by fitting
GRIMOIRE for closed filled shapes on the MNIST and for outline strokes on icon and font data,
surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility.",ICLR.cc/2025/Conference,5.25,False,0.7992,vector sketches encode shape sequences strokes variable order and timing but most pretraining rasterizes inputs losing structural cues crucial for reasoning and editing transformer relative time encodings consumes bézier control points slot attention module aggregates strokes into object level slots pretraining combines masked stroke prediction spline parameterization permutation invariant contrast across random stroke orderings and iii topology loss that preserves adjacency and junction consistency the slot graph that stroke permutation invariance and topology preservation bound representation drift under drawing style changes and identifiability slot assignments permutation under separable part quickdraw berlin and vector logos sketch2slot improves linear probe recognition over raster based mae and sequence baselines and excels part level retrieval and zero shot vector editing robustness stroke dropout and overtracing increases sketch2slot advances representation learning for vector graphics enabling structure aware understanding and controllable editing,grimoire text guided svg generative that comprised two modules visual shape quantizer vsq learns map raster images onto discrete codebook reconstructing them vector shapes and auto regressive transformer art models the joint probability distribution over shape tokens positions and textual descriptions allowing generate vector graphics from natural language,2025-08-26T01:35:51.567830
70,PhyloRep: Phylogeny-Regularized Multimodal Pretraining for Cross-Lingual Audio–Text Representations,"Cross-lingual audio–text pretraining often collapses phonological diversity, degrading transfer to low-resource languages. We propose PhyloRep, a multimodal framework that regularizes representations with linguistic phylogeny to preserve cross-language structure. A shared encoder processes speech (log-mel) and text (subword) with modality adapters; alignment is learned via contrastive matching on parallel corpora and pseudo-parallel mined pairs. A phylogeny regularizer constrains language centroids to follow a known or learned tree metric, implemented with a hyperbolic prototype layer and a triplet loss respecting tree distances; a phoneme-inventory predictor ties acoustics to articulatory space. We prove that tree-regularized alignment bounds cross-language distortion and improves zero-shot transfer under missing pairs. Training scales via curriculum mining and hyperbolic optimization with gyrovector updates. On MLS+CommonVoice (20 languages) and FLEURS, PhyloRep improves zero-shot speech–text retrieval and ASR k-NN decoding by 3–6% over XLSR/CLIP-style baselines, with larger gains on low-resource branches. On TIMIT-style phoneme tasks, it preserves articulatory distinctions and enhances phoneme recognition. Ablations confirm the importance of hyperbolic prototypes and inventory prediction. PhyloRep injects linguistic structure into multimodal pretraining, advancing equitable performance across languages and modalities.",ICLR,representation learning,gpt-5,True,6400,Topic-XICL: Demonstration Selection with Topic Inference for Cross-lingual In-context Learning,"Cross-lingual in-context learning (XICL) shows promise for adapting large language models (LLMs) to low-resource languages. Previous methods typically rely on off-the-shelf similarity-based approaches or task-specific retrievers trained with LLM feedback for demonstration selection. However, these methods often overlook important factors beyond a single criterion or can be resource-intensive. To address these challenges, we propose a novel approach called Topic-XICL, which leverages a latent topic model for demonstration selection. We assume that latent topic variables encapsulate information that more accurately characterizes demonstrations. By training this topic model on rich-resource language data with a compact LLM, we obtain more relevant demonstrations through topic inference and apply them for in-context learning across various LLMs. We evaluated our method on three multilingual tasks (XNLI, XCOPA, and TyDiQA-GoldP) using three models with 7 to 8 billion parameters (BLOOM, Qwen1.5, and Llama3.1). Our approach outperformed the baselines—random selection, semantic similarity, and clustering-based methods—on TyDiQA-GoldP, XCOPA, and XNLI by 3.32\%, 2.47\%, and 1.77\%, respectively, while requiring only moderate additional resources.",ICLR.cc/2025/Conference,4.25,False,0.8269,cross lingual audio text pretraining often collapses phonological diversity degrading transfer low resource languages phylogeny regularizer constrains language centroids follow known learned tree implemented hyperbolic prototype layer and triplet loss respecting tree distances phoneme inventory predictor ties acoustics articulatory space that tree regularized alignment bounds cross language distortion and improves zero shot transfer under missing pairs training scales curriculum mining and hyperbolic optimization gyrovector updates mls commonvoice languages and fleurs phylorep improves zero shot speech text retrieval and asr decoding over xlsr clip style baselines larger gains low resource branches timit style phoneme tasks preserves articulatory distinctions and enhances phoneme recognition ablations confirm the importance hyperbolic prototypes and inventory prediction,cross lingual context learning xicl shows promise for adapting large language models llms low resource languages training this topic rich resource language data compact llm obtain more relevant demonstrations topic inference and apply them for context learning across various llms our outperformed the baselines random selection semantic similarity and clustering based methods tydiqa goldp xcopa and xnli and respectively while requiring only moderate additional resources,2025-08-26T01:35:51.567832
71,PromptFields: Post-Hoc Promptable Self-Supervision for Task-Conditioned Representations,"Foundation encoders are typically task-agnostic, requiring fine-tuning or large adapters to specialize. We introduce PromptFields, a self-supervised pretraining paradigm that endows encoders with post-hoc promptability: small, composable prompts condition representations without updating backbone weights. PromptFields attaches a set of low-rank, input-conditioned fields—additive vector fields over feature layers—whose coefficients are produced by tiny prompt networks from text or structured task descriptors. During pretraining, we optimize standard masked/contrastive losses alongside a meta-conditioning objective that simulates task prompts via pseudo-tasks (e.g., attribute filters, domain tags) and enforces linear compositionality of fields. A curvature control ensures prompts do not distort local geometry, and a mutual-orthogonality penalty yields disentangled prompt directions. We prove that PromptFields implements a first-order approximation to task-specific fine-tuning and bound linear-probe regret relative to full adaptation under limited prompt rank. On ImageNet-100/Places, VTAB-1k, and DomainNet, prompt-conditioned linear probes close 80–95% of the gap to full fine-tuning using <1% extra parameters, improving few-shot performance by 3–7% over adapter and prefix baselines. Prompts composed at test time enable controllable retrieval (e.g., “sketch + indoor”), and ablations verify compositionality and geometry control. PromptFields turns conditioning into a lightweight, post-hoc capability, enabling flexible, task-aware representations without retraining the backbone.",ICLR,representation learning,gpt-5,True,5094,ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning,"Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language Models (PLMs) to downstream tasks by optimizing a small amount of soft virtual tokens, which are prepended to the input token embeddings. Recently, Decomposed Prompt Tuning (DePT) has demonstrated superior adaptation capabilities by decomposing the soft prompt into a shorter soft prompt and a pair of low-rank matrices. The product of the pair of low-rank matrices is added to the input token embeddings to offset them. Additionally, DePT achieves faster inference compared to PT due to the shorter soft prompt. However, in this paper, we find that the position-based token embedding offsets of DePT restricts its ability to generalize across diverse model inputs, and that the shared embedding offsets across many token embeddings result in sub-optimization. To tackle these issues, we introduce \textbf{A}daptive \textbf{De}composed \textbf{P}rompt \textbf{T}uning (ADePT), which is composed of a short soft prompt and a shallow token-shared feed-forward neural network. ADePT utilizes the token-shared feed-forward neural network to learn the embedding offsets for each token, enabling adaptive embedding offsets that vary according to the model input and better optimization of token embedding offsets. This enables ADePT to achieve superior adaptation performance without requiring more inference time or additional trainable parameters compared to vanilla PT and its variants. In comprehensive experiments across 23 natural language processing tasks and 4 typical PLMs of different scales, ADePT consistently surpasses the leading parameter-efficient fine-tuning methods, and even outperforms the full fine-tuning in certain scenarios. We also provide a theoretical analysis towards ADePT. Code is available at https://github.com/HungerPWAY/ADePT.",ICLR.cc/2025/Conference,7.0,True,0.8387,promptfields attaches set low rank input conditioned fields additive vector fields over feature layers whose coefficients are produced tiny prompt networks from text structured task descriptors attribute filters domain tags and enforces linear compositionality fields that promptfields implements first order approximation task specific fine tuning and bound linear probe regret relative full adaptation under limited prompt rank imagenet places vtab and domainnet prompt conditioned linear probes close the gap full fine tuning extra parameters improving few shot over adapter and prefix baselines,prompt tuning enables the adaptation pre trained large language models plms downstream tasks optimizing small amount soft virtual tokens which are prepended the input token embeddings recently decomposed prompt tuning dept has demonstrated superior adaptation capabilities decomposing the soft prompt into shorter soft prompt and pair low rank matrices however this find that the position based token embedding offsets dept restricts its ability generalize across diverse inputs and that the shared embedding offsets across many token embeddings sub optimization tackle these issues textbf daptive textbf composed textbf rompt textbf uning adept which composed short soft prompt and shallow token shared feed forward neural network adept utilizes the token shared feed forward neural network learn the embedding offsets for each token enabling adaptive embedding offsets that vary according the input and better optimization token embedding offsets this enables adept achieve superior adaptation requiring more inference time additional trainable parameters compared vanilla and its variants comprehensive experiments across natural language processing tasks and typical plms different scales adept consistently surpasses the leading parameter efficient fine tuning methods and even outperforms the full fine tuning certain scenarios,2025-08-26T01:35:51.567836
72,OrbitalFusion: Physics-Guided Cross-Sensor Self-Supervision for Geospatial Representations,"Satellite observations are heterogeneous across sensors (optical, SAR, thermal), viewing geometries, and atmospheric conditions, causing brittle features and poor transfer across missions. We introduce OrbitalFusion, a physics-guided self-supervised framework that learns sensor-invariant geospatial representations by embedding radiative transfer, bidirectional reflectance (BRDF), and speckle physics into pretraining. OrbitalFusion pairs multi-sensor views (e.g., Sentinel-1 SAR with Sentinel-2 optical, Landsat, MODIS) and trains an encoder with three coupled objectives: (i) a BRDF-consistent contrast that aligns features across view/illumination angles via a differentiable kernel derived from kernel-driven reflectance models, (ii) a SAR–optical fusion loss using an amplitude–coherence simulator to map speckle statistics into a common latent, and (iii) an atmospheric-invariance regularizer that projects latents through a learned linearized radiative transfer operator and enforces consistency across aerosol/water vapor perturbations. We derive identifiability of a canonical latent up to orthogonal transforms when view kernels span a Schoenberg family and prove robustness bounds to illumination and aerosol shifts. Efficient training uses tile-level caching, cloud-aware masking, and physics-informed augmentations. On land cover mapping (EuroSAT, BigEarthNet), crop type and biomass estimation (Sen12MS, Radiant Earth), and flood segmentation across unseen storms, OrbitalFusion improves linear-probe mAP by 3–7% over MAE/SimCLR and recent multi-sensor baselines, and halves performance degradation under view-time shifts. Zero-shot transfer across missions (Landsat→Sentinel) is markedly improved. By aligning representations with physically grounded invariances, OrbitalFusion enables durable, mission-agnostic features for environmental monitoring, agriculture, and disaster response.",ICLR,representation learning,gpt-5,True,9839,Towards a Knowledge guided Multimodal Foundation Model for Spatio-Temporal Remote Sensing Applications,"In recent years, there has been an increased interest in foundation models for geoscience due to the vast amount of Earth observing satellite imagery. Existing remote sensing foundation models make use of the various sources of spectral imagery to create large models pretrained on the task of masked reconstruction. In this paper, we present a foundation model framework, where the pretraining task captures the causal relationship between multiple modalities. Our framework leverages the knowledge guided principles that the spectral imagery captures the impact of the physical drivers on the environmental system, and that the relationship between them is governed by the characteristics of the system. Specifically, our method, called MultiModal Variable Step Forecasting (MM-VSF), uses forecasting of satellite imagery as a pretraining task and is able to capture the causal relationship between spectral imagery and weather. In our evaluation we show that the forecasting of satellite imagery using weather can be used as an effective pretraining task for foundation models. We further show the effectiveness of the embeddings produced by MM-VSF on the downstream tasks of pixel wise crop mapping and missing image prediction of spectral imagery, when compared with embeddings created by models trained in alternative pretraining settings including the traditional single modality input masked reconstruction.",ICLR.cc/2025/Conference,3.5,nan,0.8244,satellite observations are heterogeneous across sensors optical sar thermal viewing geometries and atmospheric conditions causing brittle features and poor transfer across missions orbitalfusion physics guided self supervised that learns sensor invariant geospatial representations embedding radiative transfer bidirectional reflectance brdf and speckle physics into pretraining sentinel sar sentinel optical landsat modis and trains encoder three coupled objectives brdf consistent contrast that aligns features across view illumination angles differentiable kernel derived from kernel driven reflectance models sar optical fusion loss amplitude coherence simulator map speckle statistics into common latent and iii atmospheric invariance regularizer that projects latents learned linearized radiative transfer operator and enforces consistency across aerosol water vapor perturbations derive identifiability canonical latent orthogonal transforms when view kernels span schoenberg family and robustness bounds illumination and aerosol shifts land cover mapping eurosat bigearthnet crop type and biomass estimation sen12ms radiant earth and flood segmentation across unseen storms orbitalfusion improves linear probe map over mae simclr and recent multi sensor baselines and halves degradation under view time shifts zero shot transfer across missions landsat sentinel markedly improved,our leverages the knowledge guided principles that the spectral imagery captures the impact the physical drivers the environmental and that the relationship between them governed the characteristics the further the effectiveness the embeddings produced vsf the downstream tasks pixel wise crop mapping and missing image prediction spectral imagery when compared embeddings created models trained alternative pretraining settings including the traditional single modality input masked reconstruction,2025-08-26T01:35:51.567839
73,AlignRKHS: Kernel-Aligned Self-Supervision with Nyströmized Spectral Consistency,"Learned embeddings implicitly define a kernel, yet their spectral properties are rarely controlled, leading to anisotropic features and unstable generalization. We present AlignRKHS, a self-supervised framework that directly aligns the encoder-induced kernel with a target reproducing kernel Hilbert space (RKHS) via Nyströmized spectral consistency. Given augmentations of inputs, we build batch-wise Nyström approximations to the latent Gram matrix and optimize a spectral alignment objective that matches its eigenvalues/vectors to those of a learned target kernel composed from a mixture of universal bases (Gaussian, Laplacian, arc-cosine). A complementary InfoNCE or masked loss preserves semantics, while a Jacobian trace penalty stabilizes local geometry. We prove that AlignRKHS minimizes an upper bound on linear-probe risk proportional to the discrepancy between induced and target kernels and show generalization rates dependent on effective rank rather than ambient dimension. Computational efficiency arises from randomized feature maps and low-rank updates of Gram factors. On ImageNet-100, CIFAR-100, and Pascal VOC transfer, AlignRKHS improves linear-probe accuracy by 2.0–4.1% over MAE/iBOT and reduces worst-group error under texture/contrast shifts by 4–7%. It accelerates fine-tuning and yields well-conditioned embeddings with flatter spectra. Ablations confirm the benefits of spectral alignment over whitening and the necessity of Nyström consistency. AlignRKHS reframes self-supervision as kernel alignment, providing spectral control that translates into robust, transferable representations.",ICLR,representation learning,gpt-5,False,,FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training,"With the increase in the number of parameters in large language models, the process of pre-training and fine-tuning increasingly demands larger volumes of GPU memory. A significant portion of this memory is typically consumed by the optimizer state. To overcome this challenge, recent approaches such as low-rank adaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao
et al., 2024)), and block-wise optimization (BAdam (Luo et al., 2024)) have been proposed. However, in all these algorithms, the effective rank of the weight updates remains low-rank, which can lead to a substantial loss of information from the gradient. This loss can be critically important, especially during the pre-training stage. In this paper, we introduce **FRUGAL**; (**F**ull-**R**ank **U**pdates with **G**r**A**dient sp**L**itting),, a new memory-efficient optimization framework. The framework leverages gradient splitting to perform low-rank updates using advanced optimization algorithms (such as Adam), while updates along the remaining directions are
executed via state-free methods like SGD or signSGD. Our framework can be integrated with various low-rank update selection techniques, including GaLore and BAdam. We provide theoretical convergence guarantees for our framework when
using SGDM for low-rank updates and SGD for state-free updates. Additionally, our method consistently outperforms concurrent approaches across various fixed memory budgets, achieving state-of-the-art results in pre-training and fine-tuning
tasks while balancing memory efficiency and perplexity targets.",ICLR.cc/2025/Conference,5.2,False,0.7896,computational efficiency arises from randomized feature maps and low rank updates gram factors,the increase the number parameters large language models the process pre training and fine tuning increasingly demands larger volumes gpu memory overcome this challenge recent approaches such low rank adaptation low rank gradient projection and block wise optimization have been proposed this frugal ull ank pdates dient itting memory efficient optimization the leverages gradient splitting perform low rank updates advanced optimization algorithms such adam while updates along the remaining directions are executed state free methods like sgd signsgd,2025-08-26T01:35:51.567842
74,HESSL: Homomorphic-Encryption-Friendly Self-Supervision for Collaborative Representation Learning,"Training foundation encoders across institutions is impeded by strict privacy constraints; differential privacy protects individuals but often degrades utility, and secure aggregation still leaks activations. We propose HESSL, a self-supervised framework tailored to homomorphic encryption (HE) that supports collaborative pretraining on encrypted data while preserving representation quality. HESSL designs an encoder with polynomial activations and low-degree approximations compatible with CKKS schemes, paired with quantization-aware normalization that controls ciphertext growth. Self-supervision is adapted via (i) a polynomial InfoNCE surrogate with Chebyshev-approximated cosine similarity, (ii) masked reconstruction with spline decoders evaluated post-decryption, and (iii) an HE-aware curvature regularizer that limits multiplicative depth. We show that HESSL optimizes a consistent lower bound on mutual information under polynomial kernels and derive error bounds linking approximation degree and quantization to downstream risk. A hybrid protocol executes compute-heavy polynomial parts under HE and offloads linear heads to trusted enclaves, with provable privacy under chosen plaintext attacks. On medical images (NIH ChestX-ray, UK Biobank MRI), pathology tiles (Camelyon), and tabular EHR (MIMIC-III), HESSL matches or exceeds non-HE baselines at similar compute by 1–3% in linear probes while operating within practical HE budgets (depth ≤12). Cross-institutional pretraining yields 4–6% gains over local training without sharing raw data. HESSL demonstrates that cryptography-aligned design enables privacy-preserving, utility-preserving representation learning at scale.",ICLR,representation learning,gpt-5,True,966,Encryption-Friendly LLM Architecture,"Large language models (LLMs) offer personalized responses based on user interactions, but this use case raises serious privacy concerns. Homomorphic encryption (HE) is a cryptographic protocol supporting arithmetic computations in encrypted states and provides a potential solution for privacy-preserving machine learning (PPML). However, the computational intensity of transformers poses challenges for applying HE to LLMs. In this work, we propose a modified HE-friendly transformer architecture with an emphasis on inference following personalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian kernels, we achieve significant computational speedups---6.94$\times$ for fine-tuning and 2.3$\times$ for inference---while maintaining performance comparable to plaintext models. Our findings provide a viable proof of concept for offering privacy-preserving LLM services in areas where data protection is crucial. Our code is available on GitHub.",ICLR.cc/2025/Conference,6.333333333333333,True,0.8164,hessl self supervised tailored homomorphic encryption that supports collaborative pretraining encrypted data while preserving representation quality hessl demonstrates that cryptography aligned enables privacy preserving utility preserving representation learning scale,large language models llms offer personalized responses user interactions but this use case raises serious privacy concerns homomorphic encryption cryptographic protocol supporting arithmetic computations encrypted states and provides potential solution for privacy preserving machine learning ppml this modified friendly transformer emphasis inference following personalized private fine tuning,2025-08-26T01:35:51.567848
75,HyperSim-SSL: Self-Supervised Learning on Simplicial Complexes for Higher-Order Relational Structure,"Complex systems exhibit higher-order interactions (triads, cliques, motifs) that graphs cannot capture, but current pretraining overlooks simplicial topology. We introduce HyperSim-SSL, a self-supervised framework for learning representations on simplicial complexes that respects higher-order structure. A simplicial encoder performs Hodge-aware message passing across 0-, 1-, and 2-simplices with separate Laplacians, while a boundary-consistency module enforces commutativity between incidence maps and latent updates. Pretraining combines masked simplex prediction (recovering missing nodes, edges, and faces), cohomology-preserving contrast that aligns persistent cohomology across augmented complexes, and a Hodge-regularizer that controls curl/divergence components. We prove stability of learned features to simplicial perturbations and show that preserving harmonic subspaces improves transfer to tasks depending on cycles and cavities. Efficient implementations use sparse tensor ops and spectral filters via Chebyshev polynomials. On coauthorship and email datasets extended with motifs, knowledge bases with qualified relations, and transaction hypergraphs, HyperSim-SSL improves node/edge/face classification and link prediction by 3–8% over graph SSL baselines and recent hypergraph methods, and retains performance under motif and cardinality shifts. Visualizations reveal interpretable harmonic components linked to communities and controversies. HyperSim-SSL establishes simplicial structure as a first-class citizen in representation learning, enabling higher-order relational reasoning in networks, knowledge graphs, and multi-agent systems.",ICLR,representation learning,gpt-5,True,62,PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse,"Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.",ICLR.cc/2025/Conference,3.0,nan,0.8336,hypersim ssl self supervised for learning representations simplicial complexes that respects higher order structure pretraining combines masked simplex prediction recovering missing nodes edges and faces cohomology preserving contrast that aligns persistent cohomology across augmented complexes and hodge regularizer that controls curl divergence components stability learned features simplicial perturbations and that preserving harmonic subspaces improves transfer tasks depending cycles and cavities coauthorship and email datasets extended motifs knowledge bases qualified relations and transaction hypergraphs hypersim ssl improves node edge face classification and link prediction over graph ssl baselines and recent hypergraph methods and retains under motif and cardinality shifts hypersim ssl establishes simplicial structure first class citizen representation learning enabling higher order relational reasoning networks knowledge graphs and multi agent systems,self supervised learning ssl data driven learning that utilizes the innate structure the data guide the learning process contrast supervised learning which depends external labels ssl utilizes the inherent characteristics the data produce its own supervisory signal however one frequent issue ssl methods representation collapse where the outputs constant input invariant feature representation this issue hinders the potential application ssl methods data modalities trying avoid representation collapse wastes researchers time and effort this introduces ssl for time series data called prediction functionals from masked latents pfml the designed avoid representation collapse rendering straightforwardly applicable different time series data domains such sensor modalities clinical data the effectiveness pfml complex real life classification tasks across three different data modalities infant posture and movement classification from multi sensor inertial measurement unit data emotion recognition from speech data and sleep stage classification from eeg data the that pfml superior conceptually similar pre existing ssl and competitive against the current state the art ssl while also being conceptually simpler and suffering from representation collapse,2025-08-26T01:35:51.567850
76,GrassRep: Subspace-Valued Self-Supervision on the Grassmann Manifold,"Vector embeddings can be brittle under noise and domain shift, whereas subspace representations capture variability and support robust matching. We present GrassRep, a self-supervised method that represents each instance by a k-dimensional subspace and learns directly on the Grassmann manifold. An encoder outputs basis matrices via Stiefel layers; a Procrustes-aligned InfoNCE compares subspaces using principal angles, and a nested-multiplicity regularizer enforces stability of leading directions. A subspace-masked reconstruction task predicts projections of missing features, while a geodesic curvature penalty smooths subspace trajectories for videos and audio streams. We prove that GrassRep minimizes a bound on retrieval and k-NN risk that depends on principal angle concentration rather than Euclidean distances, and show improved robustness to heavy-tailed noise. Training employs manifold retractions, QR-based orthogonalization, and efficient principal angle computations. On action recognition (UCF101), speech commands, and EEG motor imagery, GrassRep improves linear probes and nearest-neighbor retrieval by 2–5% over strong baselines, with 20–40% lower sensitivity to additive noise and corruptions. For long sequences, subspace trajectories yield superior temporal alignment and few-shot transfer. Ablations confirm the role of Procrustes alignment and subspace masking. GrassRep makes subspaces a practical target for self-supervision, producing robust, geometry-aware representations for signals and videos.",ICLR,representation learning,gpt-5,True,11616,Nonlinear Sequence Embedding by Monotone Variational Inequality,"In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representations for the dynamics of each individual sequence incorporating the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.",ICLR.cc/2025/Conference,7.5,True,0.8022,vector embeddings can brittle under noise and domain shift whereas subspace representations capture variability and support robust matching that grassrep minimizes bound retrieval and risk that depends principal angle concentration rather than euclidean distances and improved robustness heavy tailed noise action recognition ucf101 speech commands and eeg motor imagery grassrep improves linear probes and nearest neighbor retrieval over strong baselines lower sensitivity additive noise and corruptions for long sequences subspace trajectories yield superior temporal alignment and few shot transfer,the learned representation can used for downstream machine learning tasks such clustering and classification cast the problem convex matrix parameter recovery problem monotone variational inequalities vis and encode the common domain assumption low rank constraint across the learned representations which can learn subspace approximately spanning the entire domain well faithful representations for the dynamics each individual sequence incorporating the domain information totality the competitive our real world time series data baselines and its effectiveness for symbolic text modeling and rna sequence clustering,2025-08-26T01:35:51.567853
77,SeqSetSSL: Self-Supervised Learning for Sequences of Unordered Sets with Temporal Permutation Invariance,"Many domains (electronic health records, shopping sessions) are sequences of unordered sets, with variable event rates and within-set permutation symmetry. Standard pretraining assumes fixed order or i.i.d. events, harming transfer. We propose SeqSetSSL, a self-supervised framework that learns representations for sequence-of-sets data with temporal and set-level invariances. A two-level encoder combines a SetTransformer for within-set aggregation and a continuous-time transformer across sets with event-time encodings. Pretraining uses (i) masked set imputation under missing-not-at-random patterns, (ii) temporal order prediction that respects within-set permutation invariance via Sinkhorn-regularized set ordering, and (iii) a time-warp consistency loss aligning latents under monotone reparameterizations. We provide identifiability up to an orthogonal map for a broad class of Hawkes mixture processes and derive generalization bounds that scale with the effective number of distinct sets rather than total events. Efficient batching groups episodes by length and uses negative sampling across patients/sessions. On MIMIC-III/IV EHR, retail sessions (RecoGym), and industrial maintenance logs, SeqSetSSL improves linear-probe AUROC for next-event and risk prediction by 3–7% over sequence or set baselines (T-Loss, TS2Vec, SAINT), and is robust to variable event rates and missingness. Few-shot personalization benefits from set-level invariance. SeqSetSSL provides a principled foundation for sequence-of-sets representation learning in healthcare and recommendation.",ICLR,representation learning,gpt-5,True,5,Generalizable autoregressive modeling of time series through functional narratives,"Time series data are inherently functions of time, yet current transformers often learn time series by modeling them as mere concatenations of time periods, overlooking their functional properties. In this work, we propose a novel objective for transformers that learn time series by re-interpreting them as temporal functions. We build an alternative sequence of time series by constructing degradation operators of different intensity in the functional space, creating augmented variants of the original sample that are abstracted or simplified to different degrees. Based on the new set of generated sequence, we train an autoregressive transformer that progressively recovers the original sample from the most simplified variant. Analogous to the next word prediction task in languages that learns narratives by connecting different words, our autoregressive transformer aims to learn the Narratives of Time Series (NoTS) by connecting different functions in time. Theoretically, we justify the construction of the alternative sequence through its advantages in approximating functions. When learning time series data with transformers, constructing sequences of temporal functions allows for a broader class of approximable functions (e.g., differentiation) compared to sequences of time periods, leading to a 26$\%$ performance improvement in synthetic feature regression experiments. Experimentally, we validate NoTS in 3 different tasks across 22 real-world datasets, where we show that NoTS significantly outperforms other pre-training methods by up to 6\%. Additionally, combining NoTS on top of existing transformer architectures can consistently boost the performance. Our results demonstrate the potential of NoTS as a general-purpose dynamic learner, offering a viable alternative for developing foundation models for time series analysis.",ICLR.cc/2025/Conference,4.8,False,0.8058,events harming transfer two level encoder combines settransformer for within set aggregation and continuous time transformer across sets event time encodings pretraining uses masked set imputation under missing not random patterns temporal order prediction that respects within set permutation invariance sinkhorn regularized set ordering and iii time warp consistency loss aligning latents under monotone reparameterizations mimic iii ehr retail sessions recogym and industrial maintenance logs seqsetssl improves linear probe auroc for next event and risk prediction over sequence set baselines loss ts2vec saint and robust variable event rates and missingness few shot personalization benefits from set level invariance seqsetssl provides principled foundation for sequence sets representation learning healthcare and recommendation,the set generated sequence train autoregressive transformer that progressively recovers the original sample from the most simplified variant analogous the next word prediction task languages that learns narratives connecting different words our autoregressive transformer aims learn the narratives time series nots connecting different functions time when learning time series data transformers constructing sequences temporal functions allows for broader class approximable functions differentiation compared sequences time periods leading improvement synthetic feature regression experiments additionally combining nots top existing transformer architectures can consistently boost the,2025-08-26T01:35:51.567855
78,TactiVista-SSL: Contact-Consistent Visuo-Tactile Self-Supervision for Manipulation Representations,"Manipulation requires features that jointly encode sight and touch, yet most pretraining focuses on vision alone or loosely couples modalities. We introduce TactiVista-SSL, a self-supervised framework that learns contact-consistent visuo-tactile representations aligned with contact mechanics. A dual-branch encoder processes RGB frames and high-resolution gel tactile images/force traces; cross-modal binding is enforced by a contact-consistency loss derived from an elastic quasi-static model that relates local image deformation to tactile pressure. A contact-invariant contrast aligns pre/post-contact states, while a masked cross-modal reconstruction predicts missing tactile from vision and vice versa. We prove that contact-consistency reduces spurious visual correlations and bound prediction error in terms of the compliance matrix’s condition number. Efficient training uses differentiable depth-from-deformation, synchronized augmentation, and event-based batching around contact onsets. On real robot datasets (GelSight manipulation, Tacto-Real, ContactDB) and simulated peg insertion, TactiVista-SSL improves success prediction, grasp classification, and pose estimation by 4–9% over visual SSL and audio-visual baselines, and enhances sim-to-real transfer. With only frozen encoders and small heads, policies achieve 15–25% higher success in few-shot insertion and regrasping. Ablations verify the role of contact mechanics and cross-modal masking. TactiVista-SSL grounds multimodal pretraining in physical contact, yielding robust, control-relevant features for manipulation.",ICLR,representation learning,gpt-5,True,691,VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning,"Vision and touch are the most commonly used senses in human manipulation. While leveraging human manipulation videos for robotic task pretraining has shown promise in prior works, it is limited to image and language modalities and deployment to simple parallel grippers. In this paper, aiming to address the limitations, we collect a vision-tactile dataset by humans manipulating 10 daily tasks and 182 objects. In contrast with the existing datasets, our dataset is the first visual-tactile dataset for complex robotic manipulation skill learning. Also, we introduce a novel benchmark, featuring six complex dexterous manipulation tasks and a reinforcement learning-based vision-tactile skill learning framework. 18 non-pretraining and pretraining methods within the framework are designed and compared to investigate the effectiveness of different modalities and pertaining strategies. Key findings based on our benchmark results and analyses experiments include: 1) Despite the tactile modality used in our experiments being binary and sparse, including it directly in the policy training boosts the success rate by about 20\% and joint pretraining it with vision gains a further 20\%. 2) Joint pretraining visual-tactile modalities exhibits strong adaptability in unknown tasks and achieves robust performance among all tasks. 3) Using binary tactile signals with vision is robust to viewpoint setting, tactile noise, and the binarization threshold, which facilitates to the visual-tactile policy to be deployed in reality. The dataset and benchmark are available at \url{https://github.com/LQTS/VTDexManip}.",ICLR.cc/2025/Conference,5.5,True,0.8980,manipulation requires features that jointly encode sight and touch yet most pretraining focuses vision alone loosely couples modalities contact invariant contrast aligns pre post contact states while masked cross modal reconstruction predicts missing tactile from vision and vice versa that contact consistency reduces spurious visual correlations and bound prediction error terms the compliance matrix condition number real robot datasets gelsight manipulation tacto real contactdb and simulated peg insertion tactivista ssl improves success prediction grasp classification and pose estimation over visual ssl and audio visual baselines and enhances sim real transfer only frozen encoders and small heads policies achieve higher success few shot insertion and regrasping,vision and touch are the most used senses human manipulation while leveraging human manipulation videos for robotic task pretraining has shown promise prior works limited image and language modalities and deployment simple parallel grippers contrast the existing datasets our the first visual tactile for complex robotic manipulation skill learning also featuring six complex dexterous manipulation tasks and reinforcement learning based vision tactile skill learning key our and analyses experiments include despite the tactile modality used our experiments being binary and sparse including directly the policy training boosts the success rate about and joint pretraining vision gains further binary tactile signals vision robust viewpoint setting tactile noise and the binarization threshold which facilitates the visual tactile policy deployed reality,2025-08-26T01:35:51.567860
79,QuantileFlow-Rep: Heavy-Tail-Robust Self-Supervision via Transport of Quantile Functions,"Real-world data often exhibit heavy-tailed, skewed noise, where mean–variance assumptions miscalibrate similarity and degrade robustness. We propose QuantileFlow-Rep, a self-supervised framework that compares representations through their conditional quantile functions rather than point estimates. An encoder outputs latent distributions via flexible monotone normalizing flows; similarity is defined by a sliced 2-Wasserstein distance between predicted quantiles across projection directions, yielding a quantile-InfoNCE objective. A tail-adaptive temperature and an extreme quantile penalty control sensitivity to outliers, while a Jacobian monotonicity regularizer stabilizes flows. We prove that QuantileFlow-Rep maximizes a lower bound on mutual information under heavy-tailed elliptical families and derive robustness guarantees to α-stable perturbations, with risk bounds depending on trimmed transport costs. Efficient training uses randomized projections, quantile interpolation, and low-rank flow parameterizations. On ImageNet-100 with synthetic heavy-tailed corruptions, AudioSet with impulsive noise, and tabular datasets with outliers, QuantileFlow-Rep matches or exceeds SimCLR/VICReg in clean linear probes and yields 4–9% gains under heavy-tailed shifts, with improved OOD detection and calibration (tail-ECE −30%). Few-shot performance in corrupted regimes improves consistently. Ablations confirm the benefits of quantile transport and tail penalties. QuantileFlow-Rep brings robust statistics into self-supervision, producing heavy-tail-resilient representations that maintain accuracy and calibration under real-world noise.",ICLR,representation learning,gpt-5,True,7688,Flow-based imputation of small data,"Many challenges in the physical sciences can be framed as small data problems, where theoretical progress is hindered by the sparsity, low-dimensionality, and/or limited sample size of available empirical data compared to a physical system’s numerous dynamical degrees of freedom. Developing trustworthy imputation methods for these datasets holds immense scientific importance. Normalizing flows are a promising model choice for imputation due to their ability to explicitly estimate sample likelihoods. However, research has shown that normalizing flows are often unreliable for out-of-distribution (OOD) detection in high-dimensional settings, which undermines their trustworthiness for imputation tasks. In contrast, low-dimensional settings provide opportunities to tractably evaluate and mitigate likelihood estimation errors, revealing strategies to reduce or eliminate specific error modes. We focus on the most stringent assumption in normalizing flows: diffeomorphism between the target and base distributions. This assumption introduces two distinct error modes, which we identify and address through a simple and effective strategy. Our approach significantly enhances the trustworthiness of normalizing flows for imputation in small data problems.",ICLR.cc/2025/Conference,3.0,False,0.8247,real world data often exhibit heavy tailed skewed noise where mean variance assumptions miscalibrate similarity and degrade robustness that quantileflow rep maximizes lower bound mutual information under heavy tailed elliptical families and derive robustness guarantees stable perturbations risk bounds depending trimmed transport costs imagenet synthetic heavy tailed corruptions audioset impulsive noise and tabular datasets outliers quantileflow rep matches exceeds simclr vicreg clean linear probes and yields gains under heavy tailed shifts improved ood detection and calibration tail ece few shot corrupted regimes improves consistently,however has shown that normalizing flows are often unreliable for out distribution ood detection high dimensional settings which undermines their trustworthiness for imputation tasks,2025-08-26T01:35:51.567864
80,OpCL: Operator-Consistent Functional Contrast for PDE Field Representations,"Scientific fields are governed by operators (e.g., elliptic, parabolic PDEs), yet most self-supervision treats samples as independent images or volumes, neglecting operator structure and harming generalization across boundary conditions, forcings, and discretizations. We introduce OpCL, a self-supervised framework that learns operator-consistent representations by contrasting functions under shared operators rather than raw inputs. Given paired fields generated by the same PDE operator but with different sources or boundary data, an encoder maps fields to latent feature fields, and a functional contrast objective aligns features across pairs while penalizing alignment across different operators. A collocation masking task reconstructs withheld point evaluations and fluxes using a neural operator head, and a Green’s-function regularizer enforces linear response to localized sources. We derive generalization bounds linking downstream risk to the operator gap in the learned RKHS and show stability to discretization via mesh-invariant lifting. Efficient training uses randomized boundary/source draws, mesh-agnostic sampling, and spectral preconditioners for the neural operator head. On Darcy flow, Navier–Stokes vorticity, reaction–diffusion patterns, and ERA5 climate reanalysis, OpCL improves zero-shot transfer to unseen boundary conditions and meshes by 4–9% in linear probes and reduces error under grid coarsening and noisy forcings relative to masked modeling and contrastive baselines. Impact: By elevating operators to first-class supervision, OpCL yields functional, discretization-robust features that accelerate downstream surrogate modeling, control, and scientific discovery.",ICLR,representation learning,gpt-5,True,4515,OpenWaves: A Large-Scale Anatomically Realistic Ultrasound-CT Dataset for Benchmarking Neural Wave Equation Solvers,"Accurate and efficient simulation of wave equations is crucial in computational physics, especially for wave imaging applications like ultrasound computed tomography (USCT), which reconstructs tissue properties from scattered waves. Traditional numerical solvers for wave equations are computationally intensive and often unstable, limiting their practical applications for quasi-real-time imaging. Neural operators offer an innovative approach by accelerating PDE solving using neural networks; however, their effectiveness in realistic imaging is constrained by existing datasets that oversimplify real-world complexity. In this paper, we present OpenWaves, a large-scale wave equation dataset designed to bridge the gap between theoretical equations and practical imaging applications. OpenWaves provides over 16 million frequency-domain wave simulations using real USCT configurations, featuring anatomically realistic human breast phantoms across four categories. It enables comprehensive benchmarking of popular neural operators for both forward simulation and inverse imaging tasks, allowing analysis of their performance, scalability, and generalization capabilities. By offering a realistic and extensive dataset, OpenWaves not only serves as a platform for developing innovative neural PDE solvers but also facilitates their deployment in real-world medical imaging problems.",ICLR.cc/2025/Conference,5.0,False,0.8082,given paired fields generated the same pde operator but different sources boundary data encoder maps fields latent feature fields and functional contrast objective aligns features across pairs while penalizing alignment across different operators collocation masking task reconstructs withheld point evaluations and fluxes neural operator head and green function regularizer enforces linear response localized sources efficient training uses randomized boundary source draws mesh agnostic sampling and spectral preconditioners for the neural operator head darcy flow navier stokes vorticity reaction diffusion patterns and era5 climate reanalysis opcl improves zero shot transfer unseen boundary conditions and meshes linear probes and reduces error under grid coarsening and noisy forcings relative masked modeling and contrastive baselines,neural operators offer innovative accelerating pde solving neural networks however their effectiveness realistic imaging constrained existing datasets that oversimplify real world complexity enables comprehensive benchmarking popular neural operators for both forward simulation and inverse imaging tasks allowing analysis their scalability and generalization capabilities offering realistic and extensive openwaves not only serves platform for developing innovative neural pde solvers but also facilitates their deployment real world medical imaging problems,2025-08-26T01:35:51.567868
81,MetaSlots: In-Context Adaptable Representations via Support-Conditioned Slotting,"Foundation encoders are typically task-agnostic, requiring gradient-based adaptation for new tasks. We propose MetaSlots, a representation learning paradigm that achieves in-context adaptation without updating backbone weights by conditioning features on a support set at inference. A support encoder produces permutation-invariant descriptors from a few labeled or unlabeled examples; a slot-conditioning module generates low-rank, input-dependent modulation fields that adjust intermediate feature maps. Pretraining simulates tasks episodically and optimizes a meta-objective that: (i) aligns query features with class-conditional slot prototypes, (ii) enforces permutation invariance over support order, and (iii) regularizes slot capacity via a sparsity and mutual-orthogonality penalty. We prove that MetaSlots approximates the first Newton step of task-specific fine-tuning in function space, with excess risk bounded by slot rank and support diversity, and derive stability guarantees for unlabeled supports via clusterability. Implementation uses shared stems, cross-attention between support and query tokens, and a cache of slot bases. On Meta-Dataset, VTAB-1k, and cross-domain few-shot (Sketch, Aircraft, Flowers), MetaSlots closes 80–95% of the gap to full fine-tuning using <1% extra parameters and improves 1–5-shot accuracy by 4–8% over adapter/prompt baselines. Under open-set and domain shift, conditioning on unlabeled supports preserves calibration and boosts retrieval. Impact: MetaSlots turns in-context conditioning into a first-class capability for representation learning, enabling fast, gradient-free specialization at deployment.",ICLR,representation learning,gpt-5,True,8769,MetaAdapter: Leveraging Meta-Learning for Expandable Representation in Few-Shot Class Incremental Learning,"Few-shot class incremental learning (FSCIL) aims to enable  models to learn new tasks from few labeled samples while retaining knowledge of previously ones. This  scenario typically involves an offline base session with sufficient data for pre-training, followed by online incremental sessions where new classes are learned from limited samples. Existing methods either rely on a frozen feature extractor or meta-testing simulation to address overfitting issues in online sessions. However, they primarily learn feature representations using only the base session data, which significantly compromises the model's plasticity in feature representations. To enhance plasticity and reduce overfitting, we propose the MetaAdapter framework, which makes use of meta-learning for expandable representation. During the base session, we expand the network with pre-trained weights by inserting parallel adapters and employ meta-learning to encode generalizable knowledge into these modules. Then, the backbone is further trained on abundant data from the base classes to acquire fundamental classification ability.  In each online session, the adapters are first initialized with parameters from meta-training, and subsequently tuned to adapt to the new classes. Leveraging  meta-learning to produce initial adapters, MetaAdapter enables the feature extractor to effectively adapt to few-shot new classes, thus improving the generalization  of the model.  Experimental results on the mini-ImageNet, CUB200, and CIFAR100 datasets demonstrate that our proposed framework achieves the state-of-the-art performance.",ICLR.cc/2025/Conference,5.4,False,0.8386,foundation encoders are task agnostic requiring gradient based adaptation for tasks metaslots representation learning paradigm that achieves context adaptation updating backbone weights conditioning features support set inference support encoder produces permutation invariant descriptors from few labeled unlabeled examples slot conditioning module generates low rank input dependent modulation fields that adjust intermediate feature maps meta dataset vtab and cross domain few shot sketch aircraft flowers metaslots closes the gap full fine tuning extra parameters and improves shot over adapter prompt baselines under open set and domain shift conditioning unlabeled supports preserves calibration and boosts retrieval impact metaslots turns context conditioning into first class capability for representation learning enabling fast gradient free specialization deployment,few shot class incremental learning fscil aims enable models learn tasks from few labeled samples while retaining knowledge previously ones existing methods either rely frozen feature extractor meta testing simulation address overfitting issues online sessions however they primarily learn feature representations only the base session data which compromises the model plasticity feature representations enhance plasticity and reduce overfitting the metaadapter which makes use meta learning for expandable representation during the base session expand the network pre trained weights inserting parallel adapters and employ meta learning encode generalizable knowledge into these modules then the backbone further trained abundant data from the base classes acquire fundamental classification ability leveraging meta learning produce initial adapters metaadapter enables the feature extractor adapt few shot classes thus improving the generalization the,2025-08-26T01:35:51.567872
82,LeakGuard: Leakage-Constrained Self-Supervision with Adversarial Inference Certificates,"Self-supervised pretraining can inadvertently encode sensitive attributes, enabling property or membership inference, yet strong differential privacy often degrades utility. We introduce LeakGuard, a training framework that constrains information leakage while preserving representation quality. LeakGuard formulates pretraining as a constrained optimization: maximize a standard SSL objective subject to upper bounds on the performance of adversaries attempting membership inference, attribute prediction, and property inference from features. We solve the problem via a primal–dual scheme with adaptive Lagrange multipliers and variance-controlled adversary updates; a Jacobian sensitivity penalty and an invariance projector attenuate leakage directions identified by the adversaries. We provide distribution-dependent certificates: PAC-Bayesian bounds translate adversary losses into feature leakage guarantees, and a stability analysis links Lipschitz geometry to membership risk. Practically, we implement efficient adversaries (nearest-neighbor membership, linear/MLP property predictors) trained on a small privacy audit set and amortize leakage gradients. On CIFAR-100, CelebA (sensitive attributes), Waterbirds (spurious background), and CheXpert (demographics), LeakGuard matches or surpasses SimCLR/BYOL on linear probes while reducing calibrated membership advantage by 40–60% and attribute leakage by 25–45%. Under domain shift, worst-group performance improves due to removal of spurious correlations. Impact: LeakGuard offers a practical, certifiable route to privacy- and fairness-aware self-supervision, balancing utility with explicit leakage control without DP’s severe utility costs.",ICLR,representation learning,gpt-5,True,5598,Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses,"Vulnerability of Frontier language models to misuse has prompted the development of safety measures like filters and alignment training seeking to ensure safety through robustness to adversarially crafted prompts. We assert that robustness is fundamentally insufficient for ensuring safety goals due to inferential threats from dual-intent queries, with current defenses and evaluations failing to account for these risks. To quantify these risks, we introduce a new safety evaluation framework based on $\textit{impermissible information leakage}$ of model outputs and demonstrate how our proposed question-decomposition attack can extract dangerous knowledge from a censored LLM more effectively than traditional jailbreaking. Underlying our proposed evaluation method is a novel information-theoretic threat model of $\textit{inferential adversaries}$, distinguished from $\textit{security adversaries}$, such as jailbreaks, in that success involves inferring impermissible knowledge from victim outputs as opposed to forcing explicitly impermissible victim outputs. Through our information-theoretic framework, we show that ensuring safety against inferential adversaries requires defenses which bound impermissible information leakage, and, such defenses inevitably incur safety-utility trade-offs.",ICLR.cc/2025/Conference,5.8,True,0.8073,leakguard training that constrains information leakage while preserving representation quality provide distribution dependent certificates pac bayesian bounds translate adversary losses into feature leakage guarantees and stability analysis links lipschitz geometry membership risk under domain shift worst group improves due removal spurious correlations,vulnerability frontier language models misuse has prompted the development safety measures like filters and alignment training seeking ensure safety robustness adversarially crafted prompts assert that robustness fundamentally insufficient for ensuring safety goals due inferential threats from dual intent queries current defenses and evaluations failing account for these risks quantify these risks safety evaluation textit impermissible information leakage outputs and how our proposed question decomposition attack can extract dangerous knowledge from censored llm more than traditional jailbreaking underlying our proposed evaluation information theoretic threat textit inferential adversaries distinguished from textit security adversaries such jailbreaks that success involves inferring impermissible knowledge from victim outputs opposed forcing explicitly impermissible victim outputs,2025-08-26T01:35:51.567875
83,InvDec-MAE: Exact-Likelihood Masked Autoencoding with Invertible Decoders,"Masked autoencoders reconstruct missing content with arbitrary decoders and surrogate losses, yielding uncalibrated uncertainty and weak likelihood grounding. We propose InvDec-MAE, a masked modeling framework with an invertible decoder that enables exact log-likelihood for masked targets and principled uncertainty. An encoder produces compact latents; a conditional normalizing flow maps Gaussian noise and visible context to masked token distributions, trained by maximizing exact masked log-likelihood and a feature-space consistency loss that discourages shortcut copying. A context Jacobian regularizer stabilizes conditioning, and a shared coupling architecture amortizes parameters across locations. We prove that exact masked likelihood tightens bounds on reconstruction risk and establishes a mutual information lower bound between latents and inputs; we also derive calibration guarantees for token-wise uncertainty. Efficient training uses masked coupling splits, randomized channel factorization, and parallelizable spline transforms. On ImageNet-100 patches, LibriSpeech spectrogram tokens, and masked tabular benchmarks, InvDec-MAE improves linear probes by 2.0–3.8% over MAE/iBOT at matched compute, and achieves better calibration (ECE −25–40%) and lower NLL on held-out masks. Under structured occlusions and distribution shifts (ImageNet-C/A), performance degrades gracefully with well-calibrated uncertainties. Impact: InvDec-MAE marries masked modeling with exact likelihoods, producing uncertainty-aware, transferable representations suited for risk-sensitive downstream decision-making.",ICLR,representation learning,gpt-5,False,,Neural Electrostatics: A 3D Physics-Informed Boundary Element Poisson Equation Solver,"Electrostatics solvers relate an imposed voltage to a
corresponding charge density. Current classical methods require fine
discretization and scale poorly due to the construction of a large linear system
of equations. We recast the problem using neural networks and introduce
neural electrostatics, a hybrid 3D boundary element method (BEM). By using the
boundary element form, we are able to overcome many shortcomings of previous
neural solvers, such as learning trivial solutions and balancing loss terms
between the domain and boundary, at the cost of introducing a large integral
containing a singular kernel. We handle this singularity by locally
transforming the integral into polar coordinates and applying a numerical
quadrature. We also show that previous neural solver sampling methods are unable
to minimize the PDE residual, and propose a variational adaptive sampling
method. This technique is able to reduce mean absolute error by 5 times, while
keeping training time constant. Extensive scaling and ablation studies are
performed to justify our method. Results show that our method learns a charge
distribution within 1.2 $pC/m^2$ of mean absolute error from a classical BEM
solver, while using 25 times fewer rectangular elements.",ICLR.cc/2025/Conference,4.0,False,0.0000,,recast the problem neural networks and neural electrostatics hybrid boundary element bem the boundary element form are able overcome many shortcomings previous neural solvers such learning trivial solutions and balancing loss terms between the domain and boundary the cost introducing large integral containing singular kernel also that previous neural solver sampling methods are unable minimize the pde residual and variational adaptive sampling,2025-08-26T01:35:51.567877
84,MorphoGen: Morphology-Conditioned Self-Supervision for Cross-Embodiment Control Representations,"Policies and perception models rarely transfer across robot embodiments due to morphological differences that warp observation–action semantics. We present MorphoGen, a self-supervised framework that learns control-ready representations conditioned on robot morphology to enable cross-embodiment transfer. A morphology encoder ingests kinematic graphs and inertial parameters to produce modulation signals that gate a shared perception backbone and dynamics head. Pretraining couples (i) morphology-conditioned inverse/forward dynamics prediction, (ii) behavior bisimulation regularization across embodiments, and (iii) a morphology-invariance term that factors content from embodiment-specific nuisances. We prove that MorphoGen preserves a task-agnostic bisimulation metric while enabling linear value approximation across a class of morphologies; a domain adaptation bound shows improved transfer when morphologies lie on a smooth manifold. Implementation uses graph neural networks for morphology, FiLM-style feature modulation, and multi-task masked trajectory reconstruction from pixels and proprioception. On DMControl (body variants), Meta-World with tool changes, and real robot datasets (arm grippers, link swaps), MorphoGen improves sample efficiency by 20–40% over DrQv2/CURL and boosts cross-embodiment transfer success by 12–25% without fine-tuning the backbone. Impact: By conditioning representations on morphology, MorphoGen decouples control-relevant content from embodiment, offering a principled route to scalable, transferable robot learning across bodies and tools.",ICLR,representation learning,gpt-5,True,7358,BodyGen: Advancing Towards Efficient Embodiment Co-Design,"Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. 
While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.
We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.
To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io.",ICLR.cc/2025/Conference,7.5,True,0.8525,policies and perception models rarely transfer across robot embodiments due morphological differences that warp observation action semantics morphogen self supervised that learns control ready representations conditioned robot morphology enable cross embodiment transfer that morphogen preserves task agnostic bisimulation while enabling linear value approximation across class morphologies domain adaptation bound shows improved transfer when morphologies lie smooth manifold implementation uses graph neural networks for morphology film style feature modulation and multi task masked trajectory reconstruction from pixels and proprioception dmcontrol body variants meta world tool changes and real robot datasets arm grippers link swaps morphogen improves sample efficiency over drqv2 curl and boosts cross embodiment transfer success fine tuning the backbone impact conditioning representations morphology morphogen decouples control relevant content from embodiment offering principled route scalable transferable robot learning across bodies and tools,while prior has demonstrated its potential for generating environment adaptive robots this field still faces persistent challenges optimization efficiency due the combinatorial nature morphological search spaces and intricate dependencies between morphology and control that the ineffective morphology representation and unbalanced reward signals between the and control stages are key obstacles efficiency advance towards efficient embodiment design bodygen which utilizes topology aware self attention for both and control enabling efficient morphology representation lightweight sizes temporal credit assignment mechanism that ensures balanced reward signals for optimization,2025-08-26T01:35:51.567880
85,LatentLattice: Order-Theoretic Regularization for Concept-Structured Representations,"Many concepts obey partial orders (e.g., attributes, entailment, subset relations), but standard self-supervision ignores order structure, limiting compositional reasoning and retrieval. We introduce LatentLattice, a representation learning framework that embeds data in a latent space equipped with a learnable concept lattice. Instances map to upward-closed sets; meet and join operations are implemented via neural projections, and an isotonicity regularizer enforces that “more specific” inputs have features that dominate those of “more general” ones. Pretraining uses (i) order-consistent contrast that aligns positives across monotone augmentations while separating order-violating negatives, (ii) masked attribute reconstruction from meets/joins, and (iii) a Hasse-consistency loss that encourages sparse, acyclic covering relations. We prove that order-theoretic constraints bound error on monotone downstream tasks and derive generalization benefits from reduced hypothesis class complexity. Efficient training leverages antichain sampling, differentiable Galois connections, and lattice sparsification. On CLEVR-Hans (attributes/relations), WordNet hypernymy, and long-tailed ImageNet with hierarchical labels, LatentLattice improves zero-shot entailment and attribute retrieval by 6–12% and boosts linear-probe accuracy on order-sensitive tasks by 2–4% over contrastive and hyperbolic baselines. Impact: LatentLattice injects order structure into representations, enabling principled concept composition and robust reasoning in vision and language.",ICLR,representation learning,gpt-5,True,2866,Natural Language Inference Improves Compositionality in Vision-Language Models,"Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers. However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM. In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise. CECE produces lexically diverse sentences while maintaining their core meaning. Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features. By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of $+19.2\%$ (group score) and $+12.9\%$ on EqBen (group score) over the best prior work (finetuned with targeted data).",ICLR.cc/2025/Conference,7.0,True,0.8401,attributes entailment subset relations but standard self supervision ignores order structure limiting compositional reasoning and retrieval latentlattice representation learning that embeds data latent space equipped learnable concept lattice instances map upward closed sets meet and join operations are implemented neural projections and isotonicity regularizer enforces that more specific inputs have features that dominate those more general ones clevr hans attributes relations wordnet hypernymy and long tailed imagenet hierarchical labels latentlattice improves zero shot entailment and attribute retrieval and boosts linear probe order sensitive tasks over contrastive and hyperbolic baselines impact latentlattice injects order structure into representations enabling principled concept composition and robust reasoning vision and language,compositional reasoning vision language models vlms remains challenging these models often struggle relate objects attributes and spatial relationships recent methods aim address these limitations relying the semantics the textual description large language models llms break them down into subsets questions and answers response these issues caption expansion contradictions and entailments cece principled that leverages natural language inference nli generate entailments and contradictions from given premise extensive experiments that cece enhances interpretability and reduces overreliance biased superficial features,2025-08-26T01:35:51.567889
86,SpaCell-SSL: Histology-Aligned Self-Supervision for Spatial Transcriptomics Representations,"Spatial transcriptomics links gene expression to tissue morphology, but labeled annotations are scarce and batch effects are strong. We propose SpaCell-SSL, a self-supervised framework that aligns histology images and spatial gene counts to learn biologically meaningful representations. A dual-encoder processes H&E tiles and spot-level expression; cross-modal alignment uses an optimal transport loss between spot embeddings and local histology features, guided by tissue boundary priors. A masked gene prediction head reconstructs withheld genes using neighborhood-aware attention, and a graph Laplacian regularizer preserves spatial continuity while factoring out slide-level batch covariates using conditional normalization. We establish identifiability up to orthogonal maps under shared slide structure and derive generalization guarantees for cell-type classification with limited labels. Implementation uses multiscale tissue graphs, gene subsetting via variance-informed masking, and mini-batch OT with entropic regularization. On 10x Visium brain and tumor datasets, MERFISH mouse cortex, and STNet benchmarks, SpaCell-SSL improves cell-type linear probes by 3–7% and domain transfer across slides by 6–10% over image-only SSL and multimodal baselines; it enhances deconvolution and ligand–receptor interaction recovery. Impact: By coherently aligning histology and expression without labels, SpaCell-SSL yields robust, biology-consistent representations that facilitate downstream discovery in spatial genomics and pathology.",ICLR,representation learning,gpt-5,True,10257,PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration,"Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model (LMM) to generate captions for extracted images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathFoundation/PathGen-1.6M.",ICLR.cc/2025/Conference,7.5,True,0.8192,masked gene prediction head reconstructs withheld genes neighborhood aware attention and graph laplacian regularizer preserves spatial continuity while factoring out slide level batch covariates conditional normalization establish identifiability orthogonal maps under shared slide structure and derive generalization guarantees for cell type classification limited labels 10x visium brain and tumor datasets merfish mouse cortex and stnet benchmarks spacell ssl improves cell type linear probes and domain transfer across slides over image only ssl and multimodal baselines enhances deconvolution and ligand receptor interaction recovery,vision language models vlms like clip have attracted substantial attention pathology serving backbones for applications such zero shot image classification and whole slide image wsi analysis additionally they can function vision encoders when combined large language models llms support broader capabilities extensive experiments that integrating these generated pairs existing datasets train pathology specific clip pathgen clip enhances its ability pathological images substantial improvements across nine pathology related zero shot image classification tasks and three whole slide image tasks overall provide scalable pathway for high quality data generation pathology paving the way for next generation general pathology models,2025-08-26T01:35:51.567892
87,TensorSSL: Matrix Product State Representations for Long-Range Sequence Modeling,"Long sequences in text, bioinformatics, and audio exhibit long-range dependencies that strain attention and state-space models. We introduce TensorSSL, a self-supervised framework that represents sequences with matrix product states (MPS), a class of tensor networks with controllable capacity via bond dimension. An MPS encoder maps token embeddings to a chain of low-rank tensors; self-supervision combines (i) masked segment reconstruction via local tensor contractions, (ii) a mutual-information contrast across distant segments estimated with randomized contractions, and (iii) an entanglement regularizer that penalizes unnecessary bond growth to prevent overfitting. We prove that TensorSSL’s excess risk depends on effective entanglement (bond entropy) rather than sequence length, yielding sample complexity benefits for low-entanglement structures, and derive robustness to insertion/deletion noise via canonical form stability. Efficient training uses parallel prefix contractions, mixed-precision tensor cores, and segmental masking. On character-level language modeling (enwik8 subsets), DNA variant prediction (human genome windows), and long-context audio commands, TensorSSL matches or exceeds Transformers/SSMs of similar compute on linear probes by 2–5%, while using 2–4× less memory for 100k+ length inputs. It improves long-context retrieval and maintains performance under indels and repeats. Impact: TensorSSL brings tensor network inductive biases to self-supervision, delivering scalable, memory-efficient representations that capture long-range structure in real-world sequences.",ICLR,representation learning,gpt-5,True,7164,On Sequence Segmentation with overlapped Chunks in Machine Learning,"Operating on very long sequences can be problematic for many sequence modelling methods like Transformers or recurrent neural networks. To avoid this issue, long sequences are often split into smaller chunks instead.
For various reasons, these chunks typically are overlapped with each other which causes an increase in tensor size by however much the chunks are overlapping.

This paper attempts to find a better understanding on overlapped sequence chunks and what they accomplish. Specifically, the focus of this paper is on audio inputs in both the time and frequency domain. Previous models for speech separation and audio super resolution which use overlapped chunks are modified to allow for reduced or even removed overlaps which causes significant decreases in computational cost while maintaining accuracy.",ICLR.cc/2025/Conference,2.5,False,0.8417,long sequences text bioinformatics and audio exhibit long range dependencies that strain attention and state space models that tensorssl excess risk depends effective entanglement bond entropy rather than sequence length yielding sample complexity benefits for low entanglement structures and derive robustness insertion deletion noise canonical form stability character level language modeling enwik8 subsets dna variant prediction human genome windows and long context audio commands tensorssl matches exceeds transformers ssms similar compute linear probes while less memory for 100k length inputs impact tensorssl brings tensor network inductive biases self supervision delivering scalable memory efficient representations that capture long range structure real world sequences,operating very long sequences can problematic for many sequence modelling methods like transformers recurrent neural networks the focus this audio inputs both the time and frequency domain,2025-08-26T01:35:51.567895
88,ParetoRep: Hypernetwork Pretraining that Traces the Invariance–Sensitivity Pareto Front,"Hard-wiring a single invariance profile (e.g., color or viewpoint invariance) can hurt transfer when downstream tasks require different sensitivities. We introduce ParetoRep, a multi-objective pretraining framework that learns a continuum of representations spanning the Pareto front between competing invariance goals and semantic fidelity. A small hypernetwork conditions the encoder by emitting low-rank modulation weights given a trade-off vector λ, allowing a single backbone to realize many invariance–sensitivity configurations. Training optimizes a vector of objectives—(i) invariance to a library of transformations, (ii) sensitivity to content-preserving cues, and (iii) a stability regularizer on the representation Jacobian—using a differentiable scalarization that provably sweeps the Pareto set under mild convexity. We derive generalization bounds for linear probes that depend on a λ-weighted Rademacher complexity, and show that conditioning reduces worst-case transfer risk across task mixtures compared to single-point training. Efficient algorithms employ low-rank FiLM layers, meta-batch sampling of trade-offs, and an influence-corrected proxy for downstream risk. On ImageNet-100, VTAB-1k, and DomainBed (PACS, VLCS, OfficeHome), ParetoRep improves average linear-probe accuracy by 2.1–4.6% over SimCLR/MAE and reduces worst-domain error by up to 8%, while enabling post-hoc selection of λ without retraining. In few-shot settings, conditioning closes 80–90% of the gap to task-specific fine-tuning with <1% extra parameters. Impact: ParetoRep turns invariance design into a learnable interface, delivering a single pretrained model whose representations can be tuned on demand to match downstream requirements.",ICLR,representation learning,gpt-5,True,1135,Horizon Generalization in Reinforcement Learning,"We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Horizon generalization and invariance to planning are appealing because of their potential reach: they imply that a policy trained to reach nearby goals would succeed at reaching goals that are arbitrarily more distant.Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results, as well as recalling results from prior work, in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.",ICLR.cc/2025/Conference,4.25,True,0.8042,color viewpoint invariance can hurt transfer when downstream tasks require different sensitivities paretorep multi objective pretraining that learns continuum representations spanning the pareto front between competing invariance goals and semantic fidelity training optimizes vector objectives invariance library transformations sensitivity content preserving cues and iii stability regularizer the representation jacobian using differentiable scalarization that provably sweeps the pareto set under mild convexity derive generalization bounds for linear probes that depend weighted rademacher complexity and that conditioning reduces worst case transfer risk across task mixtures compared single point training few shot settings conditioning closes the gap task specific fine tuning extra parameters,goal conditioned the lens generalization but not the traditional sense random augmentations and domain randomization the same way that invariance closely linked generalization other areas machine learning normalization layers make network invariant scale and therefore generalize inputs varying scales that this notion horizon generalization closely linked invariance planning policy navigating towards goal will select the same actions were navigating waypoint route that goal taken together our open the door studying how techniques for invariance and generalization developed other areas machine learning might adapted achieve this alluring property,2025-08-26T01:35:51.567899
89,LieDisc-Rep: Unsupervised Discovery of Latent Lie Group Actions for Equivariance,"Many modalities exhibit continuous symmetries (rotations, scalings, color shifts), yet the relevant symmetry group is often unknown a priori and varies across domains. We propose LieDisc-Rep, a self-supervised framework that discovers latent Lie group actions and trains encoders to be equivariant to the uncovered symmetries. Two ingredients enable discovery: (i) a generator network that outputs local tangent vectors acting on latent features, forming a learnable Lie algebra; and (ii) a commutator consistency loss that enforces the Jacobi identity and matches finite flows to compositions of infinitesimal generators via BCH approximations. A paired-view objective aligns features across natural augmentations, while a contrastive discriminator ensures nontrivial action. We prove local identifiability of the Lie algebra up to linear isomorphism when views cover a neighborhood of the identity, and show that equivariant latents yield invariants for classification and steerable components for regression. Efficient training uses exponential map integration with adjoint backpropagation and spectral normalization of generators. On Rotated-ImageNet, smallNORB, and SO(3) point cloud benchmarks, LieDisc-Rep surpasses equivariant contrastive baselines by 3–7% on linear probes and improves OOD generalization to unseen angles/scales by 10–15% relative error. Visualization reveals recovered subalgebras (e.g., rotation, illumination) and steerable traversals. Impact: By lifting symmetry discovery from a manual design to a learnable objective, LieDisc-Rep enables symmetry-aware features that transfer across domains without requiring domain-specific group engineering.",ICLR,representation learning,gpt-5,True,8735,Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups,"The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models.",ICLR.cc/2025/Conference,6.5,True,0.8300,two ingredients enable discovery generator network that outputs local tangent vectors acting latent features forming learnable lie algebra and commutator consistency loss that enforces the jacobi identity and matches finite flows compositions infinitesimal generators bch approximations local identifiability the lie algebra linear isomorphism when views cover neighborhood the identity and that equivariant latents yield invariants for classification and steerable components for regression impact lifting symmetry discovery from manual learnable objective liedisc rep enables symmetry aware features that transfer across domains requiring domain specific group engineering,the quest for robust and generalizable machine learning models has driven recent interest exploiting symmetries equivariant neural networks the context pde solvers recent works have shown that lie point symmetries can useful inductive bias for physics informed neural networks pinns data and loss augmentation this lie algebra canonicalization lielac that exploits only the action infinitesimal generators the symmetry group circumventing the need for knowledge the full group structure finally showcase lielac efficacy tasks invariant image classification and lie point symmetry equivariant neural pde solvers pre trained models,2025-08-26T01:35:51.567903
90,Compress2Rep: Self-Supervised Learning Directly from Compressive Measurements,"Pretraining typically assumes access to fully sampled data, whereas real-world sensing often produces compressed measurements (random projections, coded snapshots, undersampled k-space). We introduce Compress2Rep, a modality-agnostic framework that learns robust representations directly from compressive measurements without explicit reconstruction. An encoder ingests measurements and measurement metadata; a contrastive projector enforces consistency between different measurement operators of the same latent scene by aligning features across randomized sensing matrices, while a masked consistency head predicts withheld measurements conditioned on observed ones via a learned physics adapter. A restricted isometry-aware regularizer penalizes operator-specific leakage in the latent space. We provide guarantees that, under sub-Gaussian measurement ensembles satisfying RIP, Compress2Rep recovers operator-invariant latents up to a unitary and bounds linear-probe error in terms of the measurement distortion. Training is efficient via operator-aware batching, fast transforms (FFT, Hadamard), and meta-data embeddings for operator parameters. On coded aperture imaging, MRI (fastMRI k-space), and random-projected images (ImageNet-100), Compress2Rep improves downstream detection/segmentation linear probes by 2–5% over reconstruction-then-SSL and operator-agnostic SSL, and halves degradation under operator shift (new masks, sampling ratios). Zero-shot transfer from compressed to fully sampled domains remains strong. Impact: Compress2Rep turns compressive sensing from a hindrance into a pretraining signal, enabling measurement-consistent features that obviate expensive reconstructions and generalize across sensing pipelines.",ICLR,representation learning,gpt-5,False,,Zero-Shot Image Compression with Diffusion-Based Posterior Sampling,"Diffusion models dominate the field of image generation, however they have yet to make major breakthroughs in the field of image compression. Indeed, while pre-trained diffusion models have been successfully adapted to a wide variety of downstream tasks, 
existing work in diffusion-based image compression require task specific model training, which can be both cumbersome and limiting. This work addresses this gap by harnessing the image prior learned by existing pre-trained diffusion models for solving the task of lossy image compression. This enables the use of the wide variety of publicly-available models, and avoids the need for training or fine-tuning. Our method, PSC (Posterior Sampling-based Compression), utilizes zero-shot diffusion-based posterior samplers. It does so through a novel sequential process inspired by the active acquisition technique ""Adasense"" to accumulate informative measurements of the image. This strategy minimizes uncertainty in the reconstructed image and allows for construction of an image-adaptive transform coordinated between both the encoder and decoder. PSC offers a progressive compression scheme that is both practical and simple to implement. Despite minimal tuning, and a simple quantization and entropy coding, PSC achieves competitive results compared to established methods, paving the way for further exploration of pre-trained diffusion models and posterior samplers for image compression.",ICLR.cc/2025/Conference,4.25,nan,0.7921,zero shot transfer from compressed fully sampled domains remains strong,our psc posterior sampling based compression utilizes zero shot diffusion based posterior samplers,2025-08-26T01:35:51.567906
91,PsychoSSL: Psychoacoustic-Aware Self-Supervision for Robust Audio Representations,"Conventional audio self-supervision treats spectrogram pixels uniformly, ignoring psychoacoustic principles that govern human perception and robustness (masking, loudness, critical bands). We propose PsychoSSL, a self-supervised audio pretraining method that encodes psychoacoustic structure into the objective and architecture. A cochlear front-end maps waveforms to gammatone-derived features; masking-aware augmentation perturbs sub-threshold regions more aggressively, while preserving perceptually salient components. Training combines (i) masked reconstruction with perceptual weighting (Zwicker loudness), (ii) a critical-band contrast that enforces invariance across within-band perturbations and sensitivity across bands, and (iii) a temporal modulation regularizer that preserves envelope cues. We prove that PsychoSSL bounds a perceptual distortion measure and derive generalization guarantees for linear probes under additive and convolutional noise constrained by perceptual thresholds. Efficient training employs fast auditory filterbanks and differentiable loudness approximations. On AudioSet, LibriSpeech, and ESC-50, PsychoSSL improves linear-probe and few-shot transfer by 2.4–5.3% over SimCLR/MaskSpec baselines, and enhances robustness under real-world distortions (background noise, compression, reverberation), with 4–8% higher accuracy at fixed perceptual SNR. In keyword spotting on-device, it achieves better accuracy–latency trade-offs. Impact: By grounding objectives in human hearing, PsychoSSL yields robust, semantically faithful audio features that better match perceptual relevance, benefiting speech, bioacoustics, and low-resource on-device applications.",ICLR,representation learning,gpt-5,True,3311,SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes,"Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the self-supervised pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio self-supervised learning (SSL) methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research,  designed to improve the model’s ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against state-of-the-art (SOTA) methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9% improvement on the AudioSet-2M(AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1%(mAP). These results demonstrate SSLAM's effectiveness in both polyphonic and monophonic soundscapes, significantly enhancing the performance of audio SSL models. Code and pre-trained models are available at https://github.com/ta012/SSLAM.",ICLR.cc/2025/Conference,7.0,True,0.8156,conventional audio self supervision treats spectrogram pixels uniformly ignoring psychoacoustic principles that govern human perception and robustness masking loudness critical bands audioset librispeech and esc psychossl improves linear probe and few shot transfer over simclr maskspec baselines and enhances robustness under real world distortions background noise compression reverberation higher fixed perceptual snr,self supervised pre trained audio networks have seen widespread adoption real world systems multi modal large language models current audio self supervised learning ssl methods are often benchmarked datasets predominantly featuring monophonic audio such environmental sounds and speech this limitation raises concerns about the practical robustness ssl models more realistic audio settings address this gap self supervised learning from audio mixtures sslam direction audio ssl designed improve the model ability learn from polyphonic data while maintaining strong monophonic data,2025-08-26T01:35:51.567910
92,LandmarkStream: Spectral Landmark Memory for Continual Graph Representation Learning,"Continual pretraining on evolving graphs (e.g., citation, e-commerce) suffers from catastrophic drift and an inability to retain global structure under limited memory. We introduce LandmarkStream, a self-supervised framework that maintains a spectral landmark memory to stabilize graph representations over time. Landmark nodes and subgraphs are selected online by maximizing coverage in the Laplacian spectral domain using leverage scores; their embeddings and local spectra are stored in a compact memory. Pretraining uses masked subgraph modeling and a diffusion consistency loss, augmented with a landmark alignment term that anchors current embeddings to historical spectral signatures. We establish dynamic regret bounds for node classification under piecewise-stationary graph shifts and show that landmark coverage controls drift in commute-time distances. Efficient updates use randomized SVD on graph sketches and subgraph sampling with incremental diffusion. On temporal OGB datasets (ogbn-arxiv, ogbn-papers100M slices), Reddit, and Epinions, LandmarkStream improves average and worst-time-slice accuracy by 3–7% over BGRL/GraphMAE and streaming baselines with the same memory budget, while reducing feature drift by 40–60%. It handles sparsity/degree shifts and supports fast adaptation to unseen communities. Impact: LandmarkStream provides a principled, memory-efficient mechanism for continual graph pretraining, preserving global spectral structure and enabling stable, transferable node and graph representations in dynamic environments.",ICLR,representation learning,gpt-5,False,,Degree-aware Spiking Graph Domain Adaptation for Classification,"Spiking Graph Networks (SGNs) have garnered significant interest from both researchers and industry due to their ability to address energy consumption challenges in graph classification. However, SGNs are typically inference under the same distribution of training dataset, which is difficult to satisfy in real applications. In this paper, we first propose the domain adaptation problem in SGNs, and introduce the novel framework named \textbf{De}gree-aware \textbf{S}piking \textbf{G}raph \textbf{D}omain \textbf{A}daptation for Classification (\method{}). To address this problem, we propose solutions in terms of three aspects: node distribution-aware personalized spiking representation, graph feature distribution alignment, and pseudo-label distillation. Firstly, we introduce the personalized spiking representation method that varies with node degrees. The difficulty of triggering a spike is determined by the node degree, allowing this personalized approach to capture more expressive information for classification. Then, we propose the graph feature distribution alignment module that is adversarially trained using membrane potential against a domain discriminator, efficiently maintaining high performance and low energy consumption in the case of inconsistent distribution. Additionally, we extract consistent predictions across two spaces to create reliable pseudo-labels, effectively leveraging unlabeled data to enhance graph classification performance. 
Extensive experiments on benchmark datasets validate the superiority of the proposed \method{} compared with baselines.",ICLR.cc/2025/Conference,5.75,False,0.7982,landmark nodes and subgraphs are selected online maximizing coverage the laplacian spectral domain leverage scores their embeddings and local spectra are stored compact memory establish dynamic regret bounds for node classification under piecewise stationary graph shifts and that landmark coverage controls drift commute time distances temporal ogb datasets ogbn arxiv ogbn papers100m slices reddit and epinions landmarkstream improves average and worst time slice over bgrl graphmae and streaming baselines the same memory budget while reducing feature drift handles sparsity degree shifts and supports fast adaptation unseen communities,spiking graph networks sgns have garnered significant interest from both researchers and industry due their ability address energy consumption challenges graph classification this first the domain adaptation problem sgns and the named textbf gree aware textbf piking textbf raph textbf omain textbf daptation for classification address this problem solutions terms three aspects node distribution aware personalized spiking representation graph feature distribution alignment and pseudo label distillation firstly the personalized spiking representation that varies node degrees the difficulty triggering spike determined the node degree allowing this personalized capture more expressive information for classification then the graph feature distribution alignment module that adversarially trained membrane potential against domain discriminator maintaining high and low energy consumption the case inconsistent distribution additionally extract consistent predictions across two spaces create reliable pseudo labels leveraging unlabeled data enhance graph classification,2025-08-26T01:35:51.567914
93,FenchelCL: Convex Dual Contrastive Learning with Global Convergence Guarantees,"Contrastive learning is powerful but relies on nonconvex objectives without convergence guarantees and is sensitive to negative sampling. We propose FenchelCL, a contrastive framework derived from Fenchel duality that yields a provably well-behaved training objective. We replace InfoNCE with a convex–concave saddle objective where the critic function lies in a convex RKHS ball and the encoder is updated via an upper bound on the dual gap. A variance-controlled negative sampler is incorporated analytically, obviating explicit pair enumeration. We prove existence and uniqueness of saddle points, global convergence of a primal–dual mirror-prox algorithm, and bounds connecting the dual gap to mutual information lower bounds and linear-probe risk. Anisotropy is controlled via a Jacobian trace penalty aligned with the RKHS norm. Practically, we parameterize the critic with random feature maps and use low-variance stochastic gradients. On ImageNet-100, CIFAR-100, and Places205 transfer, FenchelCL matches or exceeds SimCLR/MoCo-v3 by 1.7–3.2% on linear probes, with 1.5–2× faster and more stable convergence and reduced sensitivity to batch size and temperature. Under limited negatives and small batches, it remains robust. Impact: FenchelCL grounds contrastive learning in convex duality, providing global convergence guarantees and practical stability while delivering high-quality representations.",ICLR,representation learning,gpt-5,True,303,Stochastic Approximation to Contrastive Learning,"Contrastive learning is a powerful paradigm that has been crucial for self-supervised representation learning. While there is evidence for its effectiveness, these methods typically rely on arbitrary definitions of positive and negative pairs. Most existing contrastive learning methods require large batch sizes during training due to their rigid control over the tradeoff between the two contrastive terms. Consequences are that, substantial computational resources are wasted on negative pairs that provide minimal learning signals. To address this issue, this work present a novel method. We reformulate contrastive learning as a matrix approximation problem using I-divergence, a non-normalized form of Kullback-Leibler divergence. Our proposed objective function is decomposable across instance pairs, enabling the development of efficient stochastic approximation algorithms from neighbor embeddings which perform well with fewer negative samples. Additionally, we generalize the scaling factor beyond normalization, allowing it to adaptively emphasize positive pairs that carry more learning signals, thereby reducing the computational waste associated with negative pairs. Experimental results on visual representation learning benchmark datasets such as CIFAR and ImageNet demonstrate major improvements over other contrastive learning methods, particularly when using small batches and with only one negative pair.",ICLR.cc/2025/Conference,4.75,False,0.8789,contrastive learning powerful but relies nonconvex objectives convergence guarantees and sensitive negative sampling practically parameterize the critic random feature maps and use low variance stochastic gradients impact fenchelcl grounds contrastive learning convex duality providing global convergence guarantees and practical stability while delivering high quality representations,contrastive learning powerful paradigm that has been crucial for self supervised representation learning most existing contrastive learning methods require large batch sizes during training due their rigid control over the tradeoff between the two contrastive terms consequences are that substantial computational resources are wasted negative pairs that provide minimal learning signals reformulate contrastive learning matrix approximation problem divergence non normalized form kullback leibler divergence additionally generalize the scaling factor beyond normalization allowing adaptively emphasize positive pairs that carry more learning signals thereby reducing the computational waste associated negative pairs experimental visual representation learning datasets such cifar and imagenet major improvements over other contrastive learning methods when small batches and only one negative pair,2025-08-26T01:35:51.567917
94,SteinRep: Self-Supervision with Score-Matching Regularization via Stein Operators,"Self-supervised objectives often lack distributional control, leading to brittle features under shift. We introduce SteinRep, which augments standard pretraining with a score-matching regularizer computed through Stein operators on the latent distribution. An encoder induces a parametric energy on features; we minimize (i) a base masked/contrastive loss and (ii) a kernelized Stein discrepancy (KSD) between the empirical latent distribution and a target with desired geometry (e.g., isotropic or mixture-structured), estimated via witness functions with Nyström kernels. We prove that reducing KSD controls Wasserstein distances and tightens generalization bounds for linear probes through improved concentration of pairwise distances. A heteroscedastic variant predicts per-sample temperatures to match local scores, stabilizing anisotropy. Efficient training uses Hutchinson trace estimators and minibatch KSD approximations with fast kernel features. On ImageNet-100, STL-10, and DomainNet, SteinRep improves linear-probe accuracy by 2–4% over SimCLR/VICReg while decreasing sensitivity to corruption (ImageNet-C/A) and label noise; embeddings exhibit better calibration and isotropy. An ablation shows KSD outperforms whitening and spectral penalties in robustness. Impact: By importing Stein’s method into self-supervision, SteinRep couples representation quality with controlled latent geometry, yielding features that generalize and calibrate better under distribution shift.",ICLR,representation learning,gpt-5,False,,Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios,"Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations due to the lack of reference images. To address this issue, we propose an interpretable, zero-reference joint denoising and low-light enhancement framework tailored for real-world scenarios. Our method derives a training strategy based on paired sub-images with varying illumination and noise levels, grounded in physical imaging principles and retinex theory. Additionally, we leverage the Discrete Cosine Transform (DCT) to perform frequency domain decomposition in the sRGB space, and introduce an implicit-guided hybrid representation strategy that effectively separates intricate compounded degradations. In the backbone network design, we develop retinal decomposition network guided by implicit degradation representation mechanisms. Extensive experiments demonstrate the superiority of our method. Code will be available at https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025.",ICLR.cc/2025/Conference,6.4,True,0.7836,ablation shows ksd outperforms whitening and spectral penalties robustness impact importing stein into self supervision steinrep couples representation quality controlled latent geometry yielding features that generalize and calibrate better under distribution shift,supervised methods tend overfit specific scenarios while unsupervised methods though better generalization struggle these degradations due the lack reference images additionally leverage the discrete cosine transform dct perform frequency domain decomposition the srgb space and implicit guided hybrid representation strategy that separates intricate compounded degradations the backbone network retinal decomposition network guided implicit degradation representation mechanisms,2025-08-26T01:35:51.567920
95,AgentCausal-Rep: Counterfactual Segment Editing for Causally Robust Multi-Agent Representations,"Multi-agent videos co-mingle causal interactions with spurious co-occurrences (background, synchronized distractors), confounding representation learners. We present AgentCausal-Rep, a self-supervised framework that isolates interaction-relevant features by generating counterfactual segment edits and enforcing invariance to nuisances. A structured editor uses learned object tracks and motion primitives to resynthesize segments where non-interacting agents or backgrounds are replaced, while preserving focal interactions. Training couples (i) an invariance loss across natural and edited segments, (ii) a sensitivity loss for edited interaction outcomes, and (iii) a temporal counterfactual cycle constraint to avoid degenerate edits. We show that, under a factorizable generative process, invariance to editor-induced counterfactuals bounds worst-environment risk and preserves a bisimulation metric over interaction states. Efficient implementation leverages video slot discovery, optical flow-based compositing, and classifier-free guidance on edit strength. On SoccerNet, STAD (sports), and synthetic Multi-Agent Interactions, AgentCausal-Rep improves linear probes for event recognition and interaction forecasting by 3–7% over video-MAE and contrastive baselines, and reduces reliance on background by 30–45% in concept activation tests. In robot soccer simulation, pretraining accelerates policy learning and transfer under field/camera changes. Impact: AgentCausal-Rep brings counterfactual editing to multi-agent self-supervision, producing causally grounded features that are robust to spurious correlations and deployment shifts.",ICLR,representation learning,gpt-5,True,2023,Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos,"We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal agents non-invasively through video observations recorded over a long time-span (e.g. a month) in a single environment.
Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on animals given monocular RGBD videos captured by a smartphone. Project page: gengshan-y.github.io/agent2sim-www.",ICLR.cc/2025/Conference,6.75,True,0.8624,multi agent videos mingle causal interactions spurious occurrences background synchronized distractors confounding representation learners soccernet stad sports and synthetic multi agent interactions agentcausal rep improves linear probes for event recognition and interaction forecasting over video mae and contrastive baselines and reduces reliance background concept activation tests robot soccer simulation pretraining accelerates policy learning and transfer under field camera changes,agent sim ats for learning interactive behavior models agents from casual longitudinal video collections obtain such data coarse fine registration that tracks the agent and the camera over time canonical space resulting complete and persistent spacetime representation ats enables real sim transfer from video recordings agent interactive behavior simulator,2025-08-26T01:35:51.567923
96,NTK-Morph: Shaping the Neural Tangent Kernel for Transferable Self-Supervised Representations,"Self-supervised encoders are typically optimized via surrogate losses that do not control the induced optimization geometry, leading to brittle linear probes and unstable fine-tuning. We propose NTK-Morph, a pretraining framework that explicitly shapes the encoder’s empirical neural tangent kernel (NTK) toward a target spectrum and eigenbasis aligned with task-agnostic invariances. NTK-Morph augments masked/contrastive objectives with a spectral alignment term computed from stochastic Jacobian features: we match the encoder’s batch NTK to a learned mixture of kernels (arc-cosine, Gaussian) via Nyström approximations and penalize anisotropy through a trace-normalized eigenvalue regularizer. A Jacobian commutativity constraint stabilizes the NTK across standard augmentations, while a low-rank update head allows fast adaptation without corrupting the shaped spectrum. Theoretically, we prove that NTK-Morph minimizes an upper bound on linear-probe excess risk that depends on the discrepancy between the induced and target NTKs, with rates governed by effective rank rather than feature dimension. We further show that shaping the NTK reduces gradient interference during fine-tuning and improves conditioning of least-squares probes. Practically, we implement efficient JVP/VJP sketches, randomized SVD, and memory-aware eigenvalue smoothing. On ImageNet-100, CIFAR-100, VTAB-1k, and COCO transfer, NTK-Morph improves linear-probe accuracy by 2.1–4.3% over MAE/iBOT and accelerates fine-tuning by 20–30% fewer steps, with 3–6% lower error on ImageNet-C/A. Ablations confirm the necessity of spectral alignment and commutativity. NTK-Morph elevates optimization geometry to a first-class objective, yielding stable, transferable representations.",ICLR,representation learning,gpt-5,True,4682,"Characterizing trainability, expressivity, and generalization of neural architecture with metrics from neural tangent kernel","Zero-shot neural architecture search aims to predict multiple characteristics of neural architectures using proxy indicators without actual training, yet most methods focus on evaluating only a single characteristic of neural networks.
Since the Neural Tangent Kernel (NTK) offers a promising theoretical framework for understanding the characteristics of neural networks, we propose NTK-score,  including three metrics derived from NTK's eigenvalues and kernel regression, to assess three critical characteristics: trainability, expressivity, and generalization. 
Moreover, to exploit three metrics of our NTK-score, we employ the Borda Count approach on our NTK-score to rank architectures in neural architecture search.
Compared with state-of-the-art proxies, experimental results demonstrate that the NTK-score correlates well with both the accuracy and training time of architectures, and exhibits excellent performance across various search spaces and methods, including NAS-bench-201, DARTS, and ResNet, as well as pruning, reinforce, and evolutionary algorithm.",ICLR.cc/2025/Conference,3.5,nan,0.8022,self supervised encoders are optimized surrogate losses that not control the induced optimization geometry leading brittle linear probes and unstable fine tuning ntk morph pretraining that explicitly shapes the encoder empirical neural tangent kernel ntk toward target spectrum and eigenbasis aligned task agnostic invariances jacobian commutativity constraint stabilizes the ntk across standard augmentations while low rank update head allows fast adaptation corrupting the shaped spectrum theoretically that ntk morph minimizes upper bound linear probe excess risk that depends the discrepancy between the induced and target ntks rates governed effective rank rather than feature dimension ntk morph elevates optimization geometry first class objective yielding stable transferable representations,zero shot neural search aims predict multiple characteristics neural architectures proxy indicators actual training yet most methods focus evaluating only single characteristic neural networks since the neural tangent kernel ntk offers promising theoretical for understanding the characteristics neural networks ntk score including three metrics derived from ntk eigenvalues and kernel regression assess three critical characteristics trainability expressivity and generalization moreover exploit three metrics our ntk score employ the borda count our ntk score rank architectures neural search,2025-08-26T01:35:51.567929
97,GW-Fuse: Unpaired Multimodal Representation Learning via Gromov–Wasserstein Contrast,"Many multimodal corpora are weakly or entirely unpaired (e.g., images and texts from different sources), hindering cross-modal pretraining that depends on aligned pairs. We introduce GW-Fuse, a self-supervised framework that learns joint representations from unpaired modalities by aligning their internal geometries with Gromov–Wasserstein (GW) optimal transport. GW-Fuse trains modality-specific encoders whose mini-batch neighborhoods (constructed via k-NN graphs in latent space) are matched by minimizing the GW discrepancy between their pairwise distance structures; a cycle-consistency contrast encourages that cross-modal round trips preserve local neighborhoods. Within each modality, masked modeling stabilizes local features, and a curvature regularizer preserves metric structure. We prove that minimizing GW induces an isometry up to a global transform under a shared latent metric model and derive generalization bounds for cross-modal retrieval based on the GW distortion. Efficient training uses entropic GW with low-rank kernelization, neighborhood subsampling, and warm-started potentials. On unpaired subsets of COCO, Flickr30k, and Conceptual Captions, GW-Fuse approaches paired CLIP performance for zero-shot retrieval (within 5–8% recall@10) without any paired supervision, and surpasses concatenation or CCA-style baselines by 7–12%. In audio–video and genomics–histology pairings, it achieves robust cross-modal alignment and improves downstream classification. GW-Fuse enables scalable, pair-free multimodal pretraining, unlocking cross-modal transfer when curated alignments are scarce or unavailable.",ICLR,representation learning,gpt-5,True,4449,What If We Recaption Billions of Web Images with LLaMA-3?,"Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. 
Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM.
Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption \app1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe \cg{an average of 3.1\% enhanced zero-shot performance cross four cross-modal retrieval tasks using a mixed set of the original and our captions}. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries.",ICLR.cc/2025/Conference,5.25,False,0.8439,unpaired subsets coco flickr30k and conceptual captions fuse approaches paired clip for zero shot retrieval within any paired supervision and surpasses concatenation cca style baselines audio video and genomics histology pairings achieves robust cross modal alignment and improves downstream classification fuse enables scalable pair free multimodal pretraining unlocking cross modal transfer when curated alignments are scarce unavailable,prior studies that semantically aligning and enriching textual descriptions these pairs can enhance training across various vision language tasks text image generation for discriminative models like clip observe average enhanced zero shot cross four cross modal retrieval tasks mixed set the original and our captions,2025-08-26T01:35:51.567930
98,Sim2Real-ID: Simulator-Identifiable Self-Supervision for Robust Embodied Representations,"Domain randomization improves sim-to-real transfer by varying simulator parameters, but self-supervised encoders often entangle task-relevant content with simulation artifacts, leading to poor real-world performance. We present Sim2Real-ID, a self-supervised framework that learns embodied representations disentangled from simulator-specific factors by explicitly inferring and factoring out latent domain parameters. A simulator adaptor produces views with randomized physics, textures, and sensors; a domain-identifier head predicts latent parameters (e.g., friction, mass, illumination) from features, while an invariance projector removes identifiable components via orthogonal decomposition. Pretraining couples masked dynamics prediction with an invariance–sensitivity objective that enforces stability to domain changes yet preserves control-relevant signals via bisimulation regularization. We prove that under a structured generative process, factoring identifiable parameters bounds worst-case transfer risk on real domains and preserves a value-aware metric. Implementation uses differentiable rendering for parameter gradients, domain-conditioned FiLM layers, and contrastive rollouts across paired randomizations. On DMControl and Habitat–Gibson (pixels), Sim2Real-ID improves real-robot transfer for visuomotor tasks (peg insertion, drawer opening) by 12–22% success over contrastive and masked baselines pretrained with domain randomization alone, and reduces calibration sensitivity to camera/lighting changes. Linear probes on real datasets (RAILBench) also improve by 3–6%. Sim2Real-ID establishes simulator identifiability as a principled lever for robust, deployment-ready embodied representations.",ICLR,representation learning,gpt-5,True,4190,Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning,"Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the \emph{Simulation-Guided Fine-tuning} (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps.",ICLR.cc/2025/Conference,5.75,True,0.8316,domain randomization improves sim real transfer varying simulator parameters but self supervised encoders often entangle task relevant content simulation artifacts leading poor real world sim2real self supervised that learns embodied representations disentangled from simulator specific factors explicitly inferring and factoring out latent domain parameters pretraining couples masked dynamics prediction invariance sensitivity objective that enforces stability domain changes yet preserves control relevant signals bisimulation regularization that under structured generative process factoring identifiable parameters bounds worst case transfer risk real domains and preserves value aware dmcontrol and habitat gibson pixels sim2real improves real robot transfer for visuomotor tasks peg insertion drawer opening success over contrastive and masked baselines pretrained domain randomization alone and reduces calibration sensitivity camera lighting changes,robot learning requires considerable amount high quality data realize the promise generalization this makes direct zero shot transfer from simulation reality challenging tasks where precise and force sensitive manipulation necessary thus fine tuning these policies small real world data sets appealing pathway for scaling robot learning however current reinforcement learning fine tuning frameworks leverage general unstructured exploration strategies which are too inefficient make real world adaptation practical this introduces the emph simulation guided fine tuning sgft which demonstrates how extract structural priors from physics simulators accelerate real world adaptation this across five real world dexterous manipulation tasks where zero shot sim real transfer fails,2025-08-26T01:35:51.567934
99,BregmanCluster-SSL: Differentiable Exponential-Family Clustering for Geometry-Stable Representations,"Prototype-based self-labeling improves structure but relies on k-means in Euclidean space, which is brittle to heavy tails and mismatched geometry. We introduce BregmanCluster-SSL, a self-supervised framework that replaces heuristic clustering with a differentiable mixture of exponential-family components optimized via Bregman divergences matched to data geometry. An encoder outputs sufficient statistics; a variational EM layer learns component parameters with natural-gradient updates, and assignments serve as soft targets for a clustering contrastive loss. A curvature-aware penalty enforces stability of decision boundaries under augmentations, and a cluster-capacity prior prevents collapse. We derive convergence guarantees for the variational EM layer and prove generalization bounds for linear probes that depend on the margin under the appropriate Bregman divergence (e.g., Itakura–Saito for spectrograms, KL for counts). Efficient training uses amortized E-steps, temperature annealing, and mini-batch natural gradients. On ImageNet-100, STL-10, and AudioSet, BregmanCluster-SSL improves linear probes by 2.0–4.2% over SwAV/DeepCluster while maintaining calibrated soft assignments; robustness increases under heavy-tailed corruptions and class imbalance. In tabular domains with count/positive data (Click logs, RNA-seq), KL-based clustering significantly outperforms Euclidean prototypes. Ablations confirm the role of exponential-family matching and curvature regularization. BregmanCluster-SSL unifies clustering and self-supervision under information geometry, yielding geometry-stable, transferable representations across modalities.",ICLR,representation learning,gpt-5,True,2303,Failure-Proof Non-Contrastive Self-Supervised Learning,"We identify sufficient conditions to avoid known failure modes, including representation, dimensional, cluster and intracluster collapses, occurring in non-contrastive self-supervised learning. Based on these findings, we propose a principled design for the projector and loss function. We theoretically demonstrate that this design introduces an inductive bias that promotes learning representations that are both decorrelated and clustered without explicit enforcing these properties and leading to improved generalization. To the best of our knowledge, this is the first solution that achieves robust training with respect to these failure modes while guaranteeing enhanced generalization performance in downstream tasks. We validate our theoretical findings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100, and show that our solution, dubbed FALCON, outperforms existing feature decorrelation and cluster-based self-supervised learning methods in terms of generalization to clustering and linear classification tasks.",ICLR.cc/2025/Conference,5.666666666666667,False,0.8151,bregmancluster ssl self supervised that replaces heuristic clustering differentiable mixture exponential family components optimized bregman divergences matched data geometry encoder outputs sufficient statistics variational layer learns component parameters natural gradient updates and assignments serve soft targets for clustering contrastive loss imagenet stl and audioset bregmancluster ssl improves linear probes over swav deepcluster while maintaining calibrated soft assignments robustness increases under heavy tailed corruptions and class imbalance tabular domains count positive data click logs rna seq based clustering outperforms euclidean prototypes bregmancluster ssl unifies clustering and self supervision under information geometry yielding geometry stable transferable representations across modalities,identify sufficient conditions avoid known failure modes including representation dimensional cluster and intracluster collapses occurring non contrastive self supervised learning theoretically that this introduces inductive bias that promotes learning representations that are both decorrelated and clustered explicit enforcing these properties and leading improved generalization our theoretical image datasets including svhn cifar10 cifar100 and imagenet and that our solution dubbed falcon outperforms existing feature decorrelation and cluster based self supervised learning methods terms generalization clustering and linear classification tasks,2025-08-26T01:35:51.567937
