ai_index,ai_title,ai_abstract,ai_conference,ai_domain,ai_model,match_found,human_index,human_title,human_abstract,human_venue,human_overall_score,human_accepted,similarity_score,ai_processed_text,human_processed_text,generated_at
0,Causal-Invariant Representation Learning for Out-of-Distribution Generalization,"Deep learning models often struggle with out-of-distribution (OOD) generalization, as they tend to learn spurious correlations present in training data rather than true causal features. This limitation makes them brittle in real-world deployments where environmental shifts are common. We propose a novel framework for learning causal-invariant representations that are robust to OOD shifts. Our approach leverages a disentanglement objective that separates causal factors from environment-specific spurious ones, enforced by a meta-learning strategy that simulates distribution shifts during training. By explicitly encouraging the representation to remain invariant across diverse simulated environments, the model learns features that generalize better. We empirically demonstrate significant improvements on challenging OOD benchmarks across vision and healthcare datasets, outperforming state-of-the-art methods. This work offers a principled way to build more reliable and trustworthy AI systems by grounding representation learning in causal principles, paving the way for safer and more robust deployments.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,11217,Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks,"Deep neural networks can obtain impressive performance on various tasks under the assumption that their training domain is identical to their target domain. Performance can drop dramatically when this assumption does not hold. One explanation for this discrepancy is the presence of spurious domain-specific correlations in the training data that the network exploits. Causal mechanisms, in the other hand, can be made invariant under distribution changes as they allow disentangling the factors of distribution underlying the data generation. Yet, learning causal mechanisms to improve out-of-distribution generalisation remains an under-explored area. We propose a Bayesian neural architecture that disentangles the learning of the the data distribution from the inference process mechanisms. We show theoretically and experimentally that our model approximates reasoning under causal interventions. We demonstrate the performance of our method, outperforming point estimate-counterparts, on out-of-distribution image recognition tasks where the data distribution acts as strong adversarial confounders.",ICLR.cc/2025/Conference,4.0,False,0.8671,deep learning models often struggle out distribution ood generalization they tend learn spurious correlations training data rather than true causal features for learning causal invariant representations that are robust ood shifts explicitly encouraging the representation remain invariant across diverse simulated environments the learns features that generalize better empirically significant improvements challenging ood benchmarks across vision and healthcare datasets outperforming state the art methods this offers principled way build more reliable and trustworthy systems grounding representation learning causal principles paving the way for safer and more robust deployments,deep neural networks can obtain impressive various tasks under the assumption that their training domain identical their target domain one explanation for this discrepancy the presence spurious domain specific correlations the training data that the network exploits causal mechanisms the other hand can made invariant under distribution changes they allow disentangling the factors distribution underlying the data generation yet learning causal mechanisms improve out distribution generalisation remains under explored area bayesian neural that disentangles the learning the the data distribution from the inference process mechanisms theoretically and experimentally that our approximates reasoning under causal interventions the our outperforming point estimate counterparts out distribution image recognition tasks where the data distribution acts strong adversarial confounders,2025-08-26T02:23:21.430536
1,Structural Gradient Flow: A Unified Framework for Neural Network Sparsification and Pruning,"The increasing size of deep neural networks poses significant challenges for deployment on resource-constrained devices, motivating research into efficient model sparsification. Current pruning methods often rely on post-training heuristic criteria or complex training pipelines. We introduce Structural Gradient Flow (SGF), a novel, unified framework that integrates structural sparsification directly into the optimization process via a continuous gradient-based approach. SGF defines a ""pruning potential"" landscape that guides network weights towards structural sparsity patterns (e.g., filter, channel, or block pruning) during standard training, without requiring iterative fine-tuning or specialized hardware. Our method derives from a theoretical understanding of how structural importance evolves during learning. Experiments across various architectures (ResNets, Transformers) and datasets (ImageNet, CIFAR-100) demonstrate that SGF achieves state-of-the-art sparsity-accuracy trade-offs, yielding highly compressed models with minimal performance degradation, often surpassing dedicated pruning techniques. This work provides a flexible and theoretically grounded approach for building efficient deep learning models.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4690,The Case for Gradual Structured Pruning in Image-based Deep Reinforcement Learning,"Scaling neural networks in image-based deep reinforcement learning often fails to improve performance. While it was shown that unstructured pruning of scaled networks can unlock performance gains, we find that refining the architecture of the scaled network yields even greater improvements. However, scaled networks in deep reinforcement learning present a practical challenge: the increased computational demands can hinder deployment on embedded devices, as commonly encountered in robotics applications. To address this, we propose a novel gradual group-structured pruning framework that allows performance gains through scaling while maintaining computational efficiency. Our method preserves the network's functional integrity of inter-layer dependencies in groups, such as residual connections, while seamlessly integrating with standard deep reinforcement learning algorithms. Experiments with PPO and DQN show that our approach sustains performance while significantly reducing inference time, making it the preferred approach for resource-limited deployment.",ICLR.cc/2025/Conference,4.666666666666667,False,0.8383,the increasing size deep neural networks poses significant challenges for deployment resource constrained devices motivating into efficient sparsification structural gradient flow sgf unified that integrates structural sparsification directly into the optimization process continuous gradient based sgf defines pruning potential landscape that guides network weights towards structural sparsity patterns our derives from theoretical understanding how structural importance evolves during learning this provides flexible and theoretically grounded for building efficient deep learning models,scaling neural networks image based deep reinforcement learning often fails improve while was shown that unstructured pruning scaled networks can unlock gains find that refining the the scaled network yields even greater improvements however scaled networks deep reinforcement learning practical challenge the increased computational demands can hinder deployment embedded devices encountered robotics applications our preserves the network functional integrity inter layer dependencies groups such residual connections while seamlessly integrating standard deep reinforcement learning algorithms,2025-08-26T02:23:21.430570
2,Coherent Multimodal Fusion via Cross-Generative Self-Supervision,"Learning effective representations from multiple modalities (e.g., vision and language) is crucial for many real-world AI applications, but often requires large amounts of paired data and suffers from representation misalignment. Existing self-supervised methods often rely on contrastive learning, which can struggle with high-dimensional cross-modal relationships. We propose Cross-Generative Self-Supervision (CGSS), a novel approach that learns coherent multimodal representations by enforcing cross-modal generative consistency without explicit contrastive pairs. CGSS trains a shared latent space by encouraging each modality's encoder to generate the *other* modality's input, conditioned on the shared latent representation. This generative objective implicitly aligns features and captures richer inter-modal dependencies. Our method outperforms traditional contrastive learning and other generative baselines on challenging multimodal tasks such as retrieval, captioning, and cross-modal generation, especially in low-resource settings. CGSS offers a powerful and flexible paradigm for learning rich, aligned multimodal representations, opening new avenues for understanding and generating multimodal data.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10471,What to align in multimodal contrastive learning?,"Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior.
Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce CoMM, a Contrastive Multimodal learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, CoMM learns complex multimodal interactions and achieves state-of-the-art results on seven multimodal tasks.",ICLR.cc/2025/Conference,6.25,True,0.8566,learning effective representations from multiple modalities vision and language crucial for many real world applications but often requires large amounts paired data and suffers from representation misalignment cgss trains shared latent space encouraging each modality encoder generate the other modality input conditioned the shared latent representation our outperforms traditional contrastive learning and other generative baselines challenging multimodal tasks such retrieval captioning and cross modal generation low resource settings cgss offers powerful and flexible paradigm for learning rich aligned multimodal representations opening avenues for understanding and generating multimodal data,contrastive learning offers appealing solution for multimodal self supervised learning indeed considering each modality different view the same entity learns align features different modalities shared representation space this comm contrastive multimodal learning strategy that enables the communication between modalities single multimodal space,2025-08-26T02:23:21.430595
3,Disentangled Latent Diffusion for Controllable and Compositional Image Generation,"Diffusion models have achieved remarkable success in high-fidelity image synthesis, yet fine-grained, compositional control over generated content remains a significant challenge. Existing control mechanisms often rely on conditional inputs or require extensive text prompting, limiting intuitive manipulation. We propose Disentangled Latent Diffusion (DLD), a novel framework that learns a disentangled latent space *within* the diffusion process, enabling precise and independent control over distinct image attributes. Our approach incorporates a VAE-like structure into the diffusion latent space, coupled with a supervision mechanism that encourages specific latent dimensions to correlate with predefined semantic concepts (e.g., object identity, background, style). This allows users to manipulate individual semantic attributes by editing specific latent dimensions during the reverse diffusion process, leading to compositional generation. DLD demonstrates superior disentanglement and precise control on diverse datasets, enabling intuitive scene editing, attribute manipulation, and object recombination, pushing the boundaries of controllable image synthesis.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1929,Semantic-Aware Diffusion Model for Sequential Recommendation,"Sequential recommendation aims to predict the next click for a particular user based on their historical interacted item sequences. Recently, diffusion-based methods have achieved the state-of-the-art performance in sequential recommendation. However, they fail to effectively utilize the rich semantic information embedded in items during the diffusion process to accurately guide the generation, leading to sub-optimal results. To address this limitation, we designed SDRec, a **S**emantic-aware **D**iffusion model for sequential **Rec**ommendation. Our model introduces a novel architecture, the Semantic Fusion Layer, which leverages the embedding table from the encoder to incorporate item semantics into the diffusion process through an attention mechanism. Together with the well-designed contrastive and generative losses, SDRec effectively utilizes the item semantics in diffusion model, unleashing the potential of sequential recommendation. Our experiments show that SDRec has over 10% relative gain with superior efficiency compared with existing methods.",ICLR.cc/2025/Conference,3.6666666666666665,nan,0.8114,our incorporates vae like structure into the diffusion latent space coupled supervision mechanism that encourages specific latent dimensions correlate predefined semantic concepts this allows users manipulate individual semantic attributes editing specific latent dimensions during the reverse diffusion process leading compositional generation,however they fail utilize the rich semantic information embedded items during the diffusion process accurately guide the generation leading sub optimal our introduces the semantic fusion layer which leverages the embedding table from the encoder incorporate item semantics into the diffusion process attention mechanism,2025-08-26T02:23:21.430612
4,Adaptive Message Passing for Dynamic Graph Neural Networks,"Graph Neural Networks (GNNs) have excelled in static graph analysis, but their application to dynamic graphs, where nodes and edges evolve over time, remains challenging. Existing dynamic GNNs often rely on recurrent architectures that struggle with long-term dependencies, or process snapshots independently, ignoring temporal coherence. We introduce Adaptive Message Passing (AMP), a novel GNN framework designed for dynamic graphs. AMP employs an attention-driven mechanism that adaptively weights messages from historical graph states and current neighbors based on their temporal relevance and structural proximity. This allows the model to capture both short-term dynamics and long-range temporal dependencies without explicit recurrence. Our approach incorporates a trainable memory module that selectively stores and retrieves relevant past information. Experiments on real-world dynamic graph datasets (e.g., social networks, traffic prediction) show AMP significantly outperforms state-of-the-art dynamic GNNs in tasks like link prediction and node classification, demonstrating its ability to learn robust representations from evolving graph structures.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10,Monophilic Neighbourhood Transformers,"Graph neural networks (GNNs) have seen widespread application across diverse fields, including social network analysis, chemical research, and computer vision.
Nevertheless, their efficacy is compromised by an inherent reliance on the homophily assumption, which posits that adjacent nodes should exhibit relevance or similarity.
This assumption becomes a limitation when dealing with heterophilic graphs, where it is more common for dissimilar nodes to be connected.
Addressing this challenge, recent research indicates that real-world graphs generally exhibit monophily, a characteristic where a node tends to be related to the neighbours of its neighbours.
Inspired by this insight, we introduce Neighbourhood Transformers (NT), a novel approach that employs self-attention within every neighbourhood of the graph to generate informative messages for the nodes within, as opposed to the central node in conventional GNN frameworks.
We develop a neighbourhood partitioning strategy equipped with switchable attentions, significantly reducing space consumption by over 95\% and time consumption by up to 92.67\% in NT.
Experimental results on node classification tasks across 5 heterophilic and 5 homophilic graphs demonstrate that NT outperforms current state-of-the-art methods, showcasing their expressiveness and adaptability to different graph types.
The code for this study is available at https://anonymous.4open.science/r/MoNT-BD3C .",ICLR.cc/2025/Conference,6.0,False,0.9196,graph neural networks gnns have excelled static graph analysis but their application dynamic graphs where nodes and edges evolve over time remains challenging social networks traffic prediction amp outperforms state the art dynamic gnns tasks like link prediction and node classification demonstrating its ability learn robust representations from evolving graph structures,graph neural networks gnns have seen widespread application across diverse fields including social network analysis chemical and computer vision experimental node classification tasks across heterophilic and homophilic graphs that outperforms current state the art methods showcasing their expressiveness and adaptability different graph types,2025-08-26T02:23:21.430619
5,Counterfactual Generative Data Augmentation for Offline Reinforcement Learning,"Offline Reinforcement Learning (RL) promises to learn effective policies from pre-collected datasets without costly online interaction, but its performance is severely limited by distribution shift and coverage issues in the static data. We propose Counterfactual Generative Data Augmentation (CGDA), a novel method to enrich offline datasets by synthesizing high-quality, relevant transitions. CGDA employs a conditional generative model trained to predict counterfactual outcomes: what would have happened if a different action had been taken in a given state. This model is carefully regularized to stay close to the observed data distribution while enabling diverse exploration of plausible alternative trajectories. By augmenting the original dataset with these counterfactual transitions, CGDA significantly expands coverage and mitigates conservatism. Our empirical results on various D4RL benchmarks demonstrate that CGDA consistently improves the performance of leading offline RL algorithms (e.g., CQL, IQL) across diverse tasks, enabling more robust policy learning from limited offline data.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10154,Model-Free Offline Reinforcement Learning with Enhanced Robustness,"Offline reinforcement learning (RL) has gained considerable attention for its ability to learn policies from pre-collected data without real-time interaction, which makes it particularly useful for high-risk applications. However, due to its reliance on offline datasets, existing works inevitably introduce assumptions to ensure effective learning, which, however, often lead to a trade-off between robustness to model mismatch and scalability to large environments. In this paper, we enhance both aspects with a novel double-pessimism principle, which conservatively estimates performance and accounts for both limited data and potential model mismatches, two major reasons for the previous trade-off. We then propose a universal, model-free algorithm to learn a policy that is robust to potential environment mismatches, which enhances robustness in a scalable manner. Furthermore, we provide a sample complexity analysis of our algorithm when the mismatch is modeled by the $l_\alpha$-norm, which also theoretically demonstrates the efficiency of our method. Extensive experiments further demonstrate that our approach significantly improves robustness in a more scalable manner than existing methods.",ICLR.cc/2025/Conference,6.4,True,0.9008,offline reinforcement learning promises learn effective policies from pre collected datasets costly online interaction but its severely limited distribution shift and coverage issues the static data cql iql across diverse tasks enabling more robust policy learning from limited offline data,offline reinforcement learning has gained considerable attention for its ability learn policies from pre collected data real time interaction which makes useful for high risk applications however due its reliance offline datasets existing works inevitably assumptions ensure effective learning which however often lead trade off between robustness mismatch and scalability large environments then universal model free learn policy that robust potential environment mismatches which enhances robustness scalable manner extensive experiments further that our improves robustness more scalable manner than existing methods,2025-08-26T02:23:21.430630
6,Bayesian Meta-Learning for Robust Continual Adaptation with Catastrophic Forgetting Mitigation,"Continual learning, where models sequentially learn new tasks without forgetting old ones, is a critical challenge for real-world AI, often hampered by catastrophic forgetting and poor adaptation to new tasks. While meta-learning offers promise for rapid adaptation, integrating it effectively with continual learning to maintain performance across tasks remains an open problem. We propose Bayesian Meta-Learning for Continual Adaptation (BMCA), a novel framework that combines the strengths of meta-learning with the robustness of Bayesian uncertainty quantification. BMCA learns a meta-prior distribution over model parameters that captures commonalities across tasks. When adapting to a new task, a Bayesian inference step efficiently updates the model's posterior, allowing for rapid learning while retaining knowledge of previous tasks through a carefully designed regularization term derived from the meta-prior. Our method significantly reduces catastrophic forgetting and improves adaptation speed on sequential task learning benchmarks (e.g., Omniglot, MiniImageNet, Split CIFAR-100), demonstrating a more robust and flexible approach to lifelong learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1240,LUNCH: Adaptive Balancing of Continual Learning via Hyperparameter Uncertainty,"Continual learning (CL) is characterized by learning sequentially arriving tasks and behaving as if they were observed simultaneously. In order to prevent catastrophic forgetting of old tasks when learning new tasks, representative CL methods usually employ additional loss terms to balance their contributions (e.g., regularization and replay), modulated by deterministic hyperparameters. However, this strategy struggles to accommodate real-time changes in data distributions and is also lack of robustness to subsequent unseen tasks, especially in online scenarios where CL is performed with a one-pass data stream. Inspired by adaptive weighting in multi-task learning, we propose an innovative approach named Learning UNCertain Hyperparameters (LUNCH) for adaptive balancing of task contributions in CL. Specifically, we formulate each CL-relevant hyperparameter as a function of optimizable uncertainty under homoscedastic assumption and ensure its training stability through the exponential moving average of network parameters. We further devise an evaluation protocol that moderately adjusts the hyperparameter values and reports their impact on performance, so as to analyze the sensitivity of these sub-optimal values in realistic applications. We perform extensive experiments to demonstrate the effectiveness and robustness of our approach, which significantly improves online CL in a plug-in manner (e.g., up to 11.26% and 5.64% on Split CIFAR-100 and Split Mini-ImageNet, respectively) as well as offline CL.",ICLR.cc/2025/Conference,3.5,nan,0.8861,continual learning where models sequentially learn tasks forgetting old ones critical challenge for real world often hampered catastrophic forgetting and poor adaptation tasks while meta learning offers promise for rapid adaptation integrating continual learning maintain across tasks remains open problem bayesian meta learning for continual adaptation bmca that combines the strengths meta learning the robustness bayesian uncertainty quantification when adapting task bayesian inference step updates the model posterior allowing for rapid learning while retaining knowledge previous tasks carefully designed regularization term derived from the meta prior our reduces catastrophic forgetting and improves adaptation speed sequential task learning benchmarks omniglot miniimagenet split cifar demonstrating more robust and flexible lifelong learning,continual learning characterized learning sequentially arriving tasks and behaving they were observed simultaneously order prevent catastrophic forgetting old tasks when learning tasks representative methods usually employ additional loss terms balance their contributions however this strategy struggles accommodate real time changes data distributions and also lack robustness subsequent unseen tasks online scenarios where performed one pass data stream inspired adaptive weighting multi task learning innovative named learning uncertain hyperparameters lunch for adaptive balancing task contributions formulate each relevant hyperparameter function optimizable uncertainty under homoscedastic assumption and ensure its training stability the exponential moving average network parameters perform extensive experiments the effectiveness and robustness our which improves online plug manner,2025-08-26T02:23:21.430639
7,Concept-Centric Explanations: Hierarchical Disentanglement for Post-Hoc Model Interpretability,"Understanding the reasoning behind complex deep learning models is essential for trust and deployment, but current interpretability methods often provide low-level pixel or feature attributions rather than human-understandable concepts. We propose Concept-Centric Explanations (CCE), a novel post-hoc interpretability framework that extracts and organizes high-level, hierarchical concepts learned by a black-box model. CCE employs a weakly supervised disentanglement approach applied to intermediate model representations, identifying semantically meaningful concept latent vectors without requiring explicit concept labels during training. These concepts are then structured into a hierarchy based on their activation patterns and co-occurrence. We demonstrate that CCE can generate intuitive, compositional explanations for model predictions, e.g., ""The model classified as 'zebra' because it detected 'striped pattern' and 'horse-like body' without detecting 'mane'."" Experiments on image classification tasks show CCE provides more human-aligned and faithful explanations compared to traditional saliency maps and concept-bottleneck models, facilitating deeper model understanding and debugging.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,992,CBM-zero: Concept Bottleneck Model With Zero Performance Loss,"Interpreting machine learning models with high-level, human-understandable \emph{concepts} has gained increasing interest. The concept bottleneck model (CBM) is a popular approach to providing interpretable models, relying on first predicting the presence of concepts in a given input, and then using these concept scores to predict a label of interest. Yet, CBMs suffer from lower accuracy compared with standard black-box models, as they use a surrogate (and thus, interpretable) predictor in lieu of the original model. In this work, we propose an approach that allows us to find a CBM in any standard black-box model via an invertible mapping from its latent space to an interpretable concept space. This method preserves the original black-box model's prediction and thus has zero performance drop while providing human-understandable explanations. We evaluate the accuracy and interpretability of our method across various benchmarks, demonstrating state-of-the-art explainability metrics while enjoying superior accuracy.",ICLR.cc/2025/Conference,3.0,nan,0.8512,understanding the reasoning behind complex deep learning models essential for trust and deployment but current interpretability methods often provide low level pixel feature attributions rather than human understandable concepts concept centric explanations cce post hoc interpretability that extracts and organizes high level hierarchical concepts learned black box cce employs weakly supervised disentanglement applied intermediate representations identifying semantically meaningful concept latent vectors requiring explicit concept labels during training experiments image classification tasks cce provides more human aligned and faithful explanations compared traditional saliency maps and concept bottleneck models facilitating deeper understanding and debugging,interpreting machine learning models high level human understandable emph concepts has gained increasing interest this preserves the original black box model prediction and thus has zero drop while providing human understandable explanations the and interpretability our across various benchmarks demonstrating state the art explainability metrics while enjoying superior,2025-08-26T02:23:21.430654
8,PAC-Bayesian Bounds for Sparse Neural Networks with L1/L0 Regularization,"The generalization capabilities of over-parameterized deep neural networks, particularly sparse ones, are not fully understood. While sparsity is crucial for efficiency, its theoretical implications for generalization often remain unexplored within tight bounds. We derive novel PAC-Bayesian generalization bounds specifically tailored for neural networks trained with L1 and L0 regularization, which induce sparsity. Our bounds explicitly account for the sparsity level of the network by incorporating a prior distribution over sparse architectures. Unlike traditional bounds that scale with the total number of parameters, our bounds tighten significantly with increasing sparsity, providing theoretical justification for why sparse models can generalize well even with very few active parameters. We demonstrate that these bounds are empirically tighter than existing PAC-Bayesian bounds for dense networks when applied to sparse models on various datasets (e.g., CIFAR-10, MNIST). This work provides a crucial theoretical step towards understanding and guaranteeing the performance of efficient, sparse deep learning models.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,9679,Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Power,"The primary objective of learning methods is generalization. Classic generalization bounds, based on VC-dimension or Rademacher complexity, are uniformly applicable to all networks in the hypothesis space. On the other hand, algorithm-dependent generalization bounds, like stability bounds, address more practical scenarios and provide generalization conditions for neural networks trained using SGD. However, these bounds often rely on strict assumptions, such as the NTK hypothesis or convexity of the empirical loss, which are typically not met by neural networks. In order to establish generalizability under less stringent assumptions, this paper investigates generalizability of neural networks that minimize the empirical risk. A lower bound for population accuracy is established based on the expressiveness of these networks, which indicates that with adequately large training sample and network sizes, these networks can generalize effectively. Additionally, we provide a lower bound necessary for generalization, demonstrating that, for certain data distributions, the quantity of data required to ensure generalization exceeds the network size needed to represent that distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust overfitting, importance of over-parameterization networks, and effects of loss functions.",ICLR.cc/2025/Conference,6.6,True,0.8360,the generalization capabilities over parameterized deep neural networks sparse ones are not fully understood derive pac bayesian generalization bounds tailored for neural networks trained and regularization which induce sparsity our bounds explicitly account for the sparsity level the network incorporating prior distribution over sparse architectures this provides crucial theoretical step towards understanding and guaranteeing the efficient sparse deep learning models,the primary objective learning methods generalization the other hand algorithm dependent generalization bounds like stability bounds address more practical scenarios and provide generalization conditions for neural networks trained sgd however these bounds often rely strict assumptions such the ntk hypothesis convexity the empirical loss which are not met neural networks order establish generalizability under less stringent assumptions this investigates generalizability neural networks that minimize the empirical risk lower bound for population established the expressiveness these networks which indicates that adequately large training sample and network sizes these networks can generalize additionally provide lower bound necessary for generalization demonstrating that for certain data distributions the quantity data required ensure generalization exceeds the network size needed represent that distribution finally provide theoretical insights into several phenomena deep learning including robust overfitting importance over parameterization networks and effects loss functions,2025-08-26T02:23:21.430663
9,Adaptive Client-Specific Model Fusion in Heterogeneous Federated Learning,"Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, performance often degrades significantly in heterogeneous (non-IID) data settings, where a single global model struggles to generalize across diverse client data distributions. We propose Adaptive Client-Specific Model Fusion (ACSMF), a novel FL paradigm that personalizes model aggregation while maintaining communication efficiency. Instead of a single global model, ACSMF learns a set of latent client prototypes and a client-specific weighted average over these prototypes. During aggregation, each client contributes to updating these prototypes and learns an adaptive fusion weight vector based on its local data distribution. This allows for personalized models that are better tailored to individual client needs while benefiting from collaborative learning. Experiments on various non-IID datasets (e.g., Federated EMNIST, CIFAR-100) demonstrate that ACSMF significantly outperforms traditional FedAvg and recent personalization techniques, achieving higher accuracy on diverse client test sets while maintaining privacy.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7080,FedMAP: Unlocking Potential in Personalized Federated Learning through Bi-Level MAP Optimization,"Federated Learning (FL) enables collaborative training of machine learning (ML) models on decentralized data while preserving data privacy. However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena.
Using information from these not identically distributed (non-IID) datasets causes challenges in training. Existing FL methods based on a single global model cannot effectively capture client data variations, resulting in suboptimal performance. Personalized FL (PFL) techniques were introduced to adapt to the local data distribution of each client and utilize the data from other clients. 
They have shown promising results in addressing these challenges. 
We propose FedMAP, a novel Bayesian PFL framework which applies Maximum A Posteriori (MAP) estimation to effectively mitigate various non-IID data issues, by means of a parametric prior distribution, which is updated during aggregation. We provide a theoretical foundation illustrating FedMAP's convergence properties. In particular, we prove that the prior updates in FedMAP correspond to gradient descent iterations for a linear combination of envelope functions associated with the local losses. This differs from previous FL approaches, that aim at minimizing a weighted average of local loss functions and often face challenges with heterogeneous data distributions, resulting in reduced client performance and slower convergence in non-IID settings. 
Finally, we show, through evaluations of synthetic and real-world datasets, that FedMAP achieves better performance than the existing methods. Moreover, we offer a robust, ready-to-use framework to facilitate practical deployment and further research.",ICLR.cc/2025/Conference,5.0,False,0.9466,federated learning enables collaborative training across decentralized devices while preserving data privacy this allows for personalized models that are better tailored individual client needs while benefiting from collaborative learning,federated learning enables collaborative training machine learning models decentralized data while preserving data privacy however data across clients often differs due class imbalance feature distribution skew sample size imbalance and other phenomena,2025-08-26T02:23:21.430674
10,Differentiable Logic Induction: Integrating Symbolic Reasoning with Neural Networks,"Deep learning excels at pattern recognition but struggles with symbolic reasoning, while symbolic AI offers interpretability and logical consistency but lacks perception. Bridging this gap is crucial for robust AI. We propose Differentiable Logic Induction (DLI), a novel neuro-symbolic framework that enables end-to-end learning of both deep neural features and explicit, differentiable logic rules. DLI represents logical rules as a continuous tensor, allowing standard gradient-based optimization to learn them from data. Our approach combines a neural perception module with a differentiable logical reasoning engine that can infer complex relations and derive new rules, all within a single differentiable pipeline. This allows the system to learn from both structured logical predicates and raw perceptual data. Experiments on relational reasoning tasks (e.g., visual question answering, predicate learning) show DLI significantly outperforms purely neural or purely symbolic methods, demonstrating superior generalization to unseen logical structures and improved interpretability through learned logical programs.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1657,Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning,"Neurosymbolic learning has emerged as a promising paradigm to incorporate
symbolic reasoning into deep learning models.
However, existing frameworks are limited in scalability with respect to both
the training data and the complexity of symbolic programs.
We propose Dolphin, a framework to scale neurosymbolic learning at a fundamental level by mapping both forward chaining and backward gradient propagation in symbolic programs 
to vectorized computations.
For this purpose, Dolphin introduces a set of abstractions and primitives 
built directly on top of a high-performance deep learning framework like 
PyTorch, effectively enabling symbolic programs to be written as PyTorch modules.
It thereby enables neurosymbolic programs to be written in a language like Python that is familiar to developers and compile them to computation graphs that are amenable to end-to-end differentiation on GPUs.
We evaluate Dolphin on a suite of 13 benchmarks across 5 neurosymbolic tasks that combine deep learning models for
text, image, or video processing with symbolic programs that involve multi-hop 
reasoning, recursion, and even black-box functions like Python `eval()`.
Dolphin achieves comparable or better accuracy on all benchmarks while taking 0.33% -- 61.73% of the time (and 23.23% on average) to train these models on the largest input per task compared to baselines Scallop, ISED, and IndeCateR+, which time out on most of these inputs.",ICLR.cc/2025/Conference,6.0,False,0.8588,deep learning excels pattern recognition but struggles symbolic reasoning while symbolic offers interpretability and logical consistency but lacks perception differentiable logic induction dli neuro symbolic that enables end end learning both deep neural features and explicit differentiable logic rules dli represents logical rules continuous tensor allowing standard gradient based optimization learn them from data our combines neural perception module differentiable logical reasoning engine that can infer complex relations and derive rules all within single differentiable pipeline experiments relational reasoning tasks visual question answering predicate learning dli outperforms purely neural purely symbolic methods demonstrating superior generalization unseen logical structures and improved interpretability learned logical programs,neurosymbolic learning has emerged promising paradigm incorporate symbolic reasoning into deep learning models dolphin scale neurosymbolic learning fundamental level mapping both forward chaining and backward gradient propagation symbolic programs vectorized computations for this purpose dolphin introduces set ions and primitives built directly top high performance deep learning like pytorch enabling symbolic programs written pytorch modules thereby enables neurosymbolic programs written language like python that familiar developers and compile them computation graphs that are amenable end end differentiation gpus dolphin suite benchmarks across neurosymbolic tasks that combine deep learning models for text image video processing symbolic programs that involve multi hop reasoning recursion and even black box functions like python eval,2025-08-26T02:23:21.430682
11,Self-Calibrating Debiasing: Addressing Long-Tail Recognition via Adaptive Representation Learning,"Real-world datasets often exhibit a long-tail distribution, where a few classes are over-represented while most are rare. Deep learning models trained on such data typically show poor performance on minority classes due to imbalanced representation learning, often resulting in biased predictions. We propose Self-Calibrating Debiasing (SCD), a novel method that adaptively corrects representation bias towards head classes without requiring explicit re-weighting or re-sampling strategies. SCD introduces a self-calibrating regularization term that encourages the model's representations to be equally discriminative for both head and tail classes. This is achieved by dynamically adjusting the influence of samples based on their estimated class uncertainty and proximity to decision boundaries. Our method implicitly balances the learning signal across classes. Extensive experiments on large-scale long-tail benchmarks (e.g., ImageNet-LT, Places-LT) demonstrate SCD consistently outperforms state-of-the-art long-tail learning techniques, achieving superior generalization and fairness across all classes while maintaining efficiency.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10604,Granularity Matters in Long-Tail Learning,"Balancing training on long-tail data distributions remains a long-standing challenge in deep learning. While methods such as re-weighting and re-sampling help alleviate the imbalance issue, limited sample diversity continues to hinder models from learning robust and generalizable feature representations, particularly for tail classes. In contrast to existing methods, we offer a novel perspective on long-tail learning, inspired by an observation: datasets with finer granularity tend to be less affected by data imbalance. In this paper, we investigate this phenomenon through both quantitative and qualitative studies, showing that increased granularity enhances the generalization of learned features in tail categories. Motivated by these findings, we propose a method to increase dataset granularity through category extrapolation. Specifically, we introduce open-set auxiliary classes that are visually similar to existing ones, aiming to enhance representation learning for both head and tail classes. This forms the core contribution and insight of our approach. To automate the curation of auxiliary data, we leverage large language models (LLMs) as knowledge bases to search for auxiliary categories and retrieve relevant images through web crawling. To prevent the overwhelming presence of auxiliary classes from disrupting training, we introduce a neighbor-silencing loss that encourages the model to focus on class discrimination within the target dataset. During inference, the classifier weights for auxiliary categories are masked out, leaving only the target class weights for use.  Extensive experiments and ablation studies on three standard long-tail benchmarks demonstrate the effectiveness of our approach, notably outperforming strong baseline methods that use the same amount of data. The code will be made publicly available.",ICLR.cc/2025/Conference,4.25,nan,0.8574,deep learning models trained such data poor minority classes due imbalanced representation learning often resulting biased predictions self calibrating debiasing scd that adaptively corrects representation bias towards head classes requiring explicit weighting sampling strategies our implicitly balances the learning signal across classes imagenet places scd consistently outperforms state the art long tail learning techniques achieving superior generalization and fairness across all classes while maintaining efficiency,balancing training long tail data distributions remains long standing challenge deep learning while methods such weighting and sampling help alleviate the imbalance issue limited sample diversity continues hinder models from learning robust and generalizable feature representations for tail classes open set auxiliary classes that are visually similar existing ones aiming enhance representation learning for both head and tail classes automate the curation auxiliary data leverage large language models llms knowledge bases search for auxiliary categories and retrieve relevant images web crawling,2025-08-26T02:23:21.430688
12,Hyperbolic Implicit Neural Representations for Multi-Scale Scene Reconstruction,"Implicit Neural Representations (INRs) have revolutionized 3D scene reconstruction, offering continuous representations and high fidelity. However, their ability to compactly represent scenes with large scale variations, intricate details, and hierarchical structures remains limited by Euclidean space assumptions, often leading to oversmoothing or memory inefficiencies. We propose Hyperbolic Implicit Neural Representations (HINR), a novel framework that leverages hyperbolic geometry to model scenes with inherent hierarchical and multi-scale properties. By embedding scene coordinates and features into hyperbolic space, HINR can more effectively represent objects at vastly different scales and capture fine details with fewer parameters, benefiting from hyperbolic space's exponential expansion. Our approach designs a hyperbolic MLP architecture that learns a continuous mapping from 3D coordinates in hyperbolic space to scene properties (e.g., occupancy, color). HINR demonstrates superior reconstruction quality for complex, multi-scale scenes and more compact representations compared to Euclidean INR baselines on various 3D datasets, showcasing the power of non-Euclidean geometry for spatial encoding.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3925,PIN: Prolate Spheroidal Wave Function-based Implicit Neural Representations,"Implicit Neural Representations (INRs) provide a continuous mapping between the coordinates of a signal and the corresponding values. As the performance of INRs heavily depends on the choice of nonlinear-activation functions, there has been a significant focus on encoding explicit signals within INRs using diverse activation functions. Despite recent advancements, existing INRs often encounter significant challenges, particularly at fine scales where they often introduce noise-like artifacts over smoother areas compromising the quality of the output. Moreover, they frequently struggle to generalize to unseen coordinates. These drawbacks highlight a critical area for further research and development to enhance the robustness and applicability of INRs across diverse scenarios. To address this challenge, we introduce the Prolate Spheroidal Wave Function-based Implicit Neural Representations (PIN), which exploits the optimal space-frequency domain concentration of Prolate Spheroidal Wave Functions (PSWFs) as the nonlinear mechanism in INRs. Our experimental results reveal that PIN excels not only in representing images and 3D shapes but also significantly outperforms existing methods in various vision tasks that require INR generalization, including image inpainting, novel view synthesis, edge detection, and image denoising.",ICLR.cc/2025/Conference,6.0,True,0.8939,implicit neural representations inrs have revolutionized scene reconstruction offering continuous representations and high fidelity hyperbolic implicit neural representations hinr that leverages hyperbolic geometry scenes inherent hierarchical and multi scale properties embedding scene coordinates and features into hyperbolic space hinr can more represent objects vastly different scales and capture fine details fewer parameters benefiting from hyperbolic space exponential expansion,implicit neural representations inrs provide continuous mapping between the coordinates signal and the corresponding values these drawbacks highlight critical area for further and development enhance the robustness and applicability inrs across diverse scenarios address this challenge the prolate spheroidal wave function based implicit neural representations pin which exploits the optimal space frequency domain concentration prolate spheroidal wave functions pswfs the nonlinear mechanism inrs our experimental reveal that pin excels not only representing images and shapes but also outperforms existing methods various vision tasks that require inr generalization including image inpainting view synthesis edge detection and image denoising,2025-08-26T02:23:21.430696
13,Conformal Risk Control: Quantifying Predictive Uncertainty for Robust Deep Learning,"Deep learning models often provide point predictions without reliable uncertainty estimates, leading to overconfident failures, especially under distribution shift. While Bayesian and ensemble methods offer uncertainty, they are computationally intensive and often lack rigorous statistical guarantees. We propose Conformal Risk Control (CRC), a novel framework that integrates conformal prediction with deep learning to provide valid, distribution-free uncertainty quantification and control over miscoverage rates. CRC adapts the notion of conformal p-values to deep neural networks, enabling the construction of prediction sets with guaranteed coverage probability for any underlying data distribution, without retraining. Furthermore, we extend CRC to allow for dynamic control over prediction risk at inference time, enabling adaptive decision-making based on the desired level of robustness. Experiments across various image and tabular datasets demonstrate that CRC produces tighter prediction sets with guaranteed coverage, even under significant distribution shifts, offering a powerful and statistically rigorous tool for robust and reliable deep learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7940,Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence,"Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty estimates (e.g. pretrained models predicting only point estimates), popular approaches that operate post-hoc include Laplace’s method and split conformal prediction (split-CP). However, Laplace’s method can be miscalibrated when the model is misspecified and split-CP requires sample splitting, and thus comes at the expense of statistical efficiency. In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. Coupled with linearization of the network, we express the absolute residual nonconformity score as a piecewise linear function of the candidate label allowing for an efficient procedure that avoids the exhaustive search over the output space. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.",ICLR.cc/2025/Conference,6.25,True,0.8379,deep learning models often provide point predictions reliable uncertainty estimates leading overconfident failures under distribution shift conformal risk control crc that integrates conformal prediction deep learning provide valid distribution free uncertainty quantification and control over miscoverage rates crc adapts the notion conformal values deep neural networks enabling the construction prediction sets guaranteed coverage probability for any underlying data distribution retraining furthermore extend crc allow for dynamic control over prediction risk inference time enabling adaptive decision making the desired level robustness experiments across various image and tabular datasets that crc produces tighter prediction sets guaranteed coverage even under significant distribution shifts offering powerful and statistically rigorous tool for robust and reliable deep learning,uncertainty quantification important prerequisite for the deployment deep learning models safety critical areas yet this hinges the uncertainty estimates being useful the extent the prediction intervals are well calibrated and sharp pretrained models predicting only point estimates popular approaches that operate post hoc include laplace and split conformal prediction split this construct prediction intervals for neural network regressors post hoc held out data this achieved approximating the full conformal prediction full standard regression benchmarks and bounding box localization the resulting prediction intervals are locally adaptive and often tighter than those split,2025-08-26T02:23:21.430706
14,Learning Intervention Targets from Observational Data for Causal Policy Discovery,"Discovering optimal policies from observational data is a key challenge in AI, particularly when direct interventions are costly or impossible. Traditional methods often assume known causal graphs or require extensive counterfactual modeling. We propose a novel framework, Learning Intervention Targets (LIT), which identifies optimal intervention variables and their effects directly from purely observational data, facilitating causal policy discovery without explicit causal graph inference. LIT leverages recent advances in causal representation learning and disentanglement to infer a latent causal structure, then employs a gradient-based optimization strategy to identify which latent causal factors, when intervened upon, lead to desired outcomes. The method minimizes the expected regret of an intervention policy by estimating the average treatment effect of latent interventions. Experiments on simulated and real-world datasets (e.g., observational health data, recommender systems) demonstrate LIT's ability to discover effective intervention targets and policies, significantly outperforming non-causal and naive causal baselines.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8,Adaptive Causal Experimental Design: Amortizing Sequential Bayesian Experimental Design for Causal Models,"Interventions are essential for causal discovery and causal reasoning. Acquiring interventional data, however, is often costly, especially in real-world systems.
A careful experimental design can therefore bring substantial savings. 
In the sequential experimental design setting, 
most existing approaches seek the best 
interventions in a greedy (myopic) manner that does not account for the synergy from the yet-to-come future experiments. We propose Adaptive Causal Experimental Design (ACED),
a novel Bayesian sequential design framework for learning a design policy capable of generating non-myopic interventions that incorporate the effect on future experiments.
In particular, ACED maximizes the Expected Information Gain (EIG) on flexible choices of causal quantities of interest (e.g., causal structure, specific causal effects) directly, bypassing the need for computing intermediate posteriors in the experimental sequence.
Leveraging a variational lower bound estimator for the EIG, ACED trains an amortized policy network that can be executed rapidly during deployment. 
We present numerical results demonstrating ACED's effectiveness on synthetic datasets with both linear and nonlinear structural causal models, as well as on in-silico single-cell gene expression datasets.",ICLR.cc/2025/Conference,5.0,False,0.8368,learning intervention targets lit which identifies optimal intervention variables and their effects directly from purely observational data facilitating causal policy discovery explicit causal graph inference lit leverages recent advances causal representation learning and disentanglement infer latent causal structure then employs gradient based optimization strategy identify which latent causal factors when intervened upon lead desired outcomes,interventions are essential for causal discovery and causal reasoning adaptive causal experimental aced bayesian sequential for learning policy capable generating non myopic interventions that incorporate the effect future experiments leveraging variational lower bound estimator for the eig aced trains amortized policy network that can executed rapidly during deployment,2025-08-26T02:23:21.430709
15,Compiler-Aware Neural Architecture Search for On-Device Edge Acceleration,"Deploying deep neural networks on edge devices requires models that are not only accurate but also highly efficient in terms of latency, energy, and memory. Traditional Neural Architecture Search (NAS) often optimizes for MACs or parameters, which may not directly translate to actual on-device performance due to complex interactions with compilers and hardware. We propose Compiler-Aware NAS (CANAS), a novel search framework that incorporates compiler-level feedback directly into the architecture search loop. CANAS uses a lightweight, differentiable proxy for compiler performance estimation, which considers factors like memory access patterns, kernel fusion, and instruction-level parallelism. This allows the NAS algorithm to explore architectures that are truly optimized for a specific compiler and target hardware (e.g., ARM Cortex-M, NVIDIA Jetson). Experiments on various edge AI benchmarks demonstrate that CANAS discovers architectures that achieve significantly higher inference speeds and lower energy consumption on target hardware, compared to models optimized by FLOPs or general-purpose NAS, without compromising accuracy.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3818,BAP: BRANCH-AWARE PARALLEL EXECUTION FOR FASTER DNN INFERENCE ON MOBILE CPUS,"The growing demand for real-time applications on edge devices underscores the need for faster inference of complex deep neural network (DNN) models. Although mobile devices increasingly incorporate specialized processors like GPUs and TPUs, modern DNN models such as Whisper and Vision Transformers often involve dynamic control flows and tensor operations that are incompatible and unsupported on current frameworks with these mobile accelerators. CPU presents the most viable option to improve inference latency on mobile devices due to their widespread availability, substantial memory caches, and ability to support all types of tensor operations. However, existing CPU optimization techniques focus on sequential execution, overlooking potential parallelization within Automatic Speech Recognition (ASR) and transformer-based models, leading to inefficiencies. This work introduces a novel runtime model analysis pipeline that extracts layer and branch structures from DNN model graphs to identify parallelizable branches. We propose BAP, a branch-aware memory allocation strategy that isolates memory arenas for parallel branches, reducing contention and optimizing memory reuse within each branch. Additionally, we leverage CPU multithreading to execute these branches concurrently, optimizing thread management and memory access to minimize overhead. Evaluated on ASR models and transformer-based models, our approach reduces inference latency by up to 38.5%, decreases memory allocation requirements by up to 15.6x and saves up to 20.2% energy cost compared to the TFLite naive memory allocation.",ICLR.cc/2025/Conference,5.25,nan,0.8489,deploying deep neural networks edge devices requires models that are not only accurate but also highly efficient terms latency energy and memory traditional neural search nas often optimizes for macs parameters which may not directly translate actual device due complex interactions compilers and hardware,the growing demand for real time applications edge devices underscores the need for faster inference complex deep neural network dnn models although mobile devices increasingly incorporate specialized processors like gpus and tpus modern dnn models such whisper and vision transformers often involve dynamic control flows and tensor operations that are incompatible and unsupported current frameworks these mobile accelerators however existing cpu optimization techniques focus sequential execution overlooking potential parallelization within automatic speech recognition asr and transformer based models leading inefficiencies,2025-08-26T02:23:21.430716
16,Gradient-Free Global Optimization for Robust Adversarial Training,"Adversarial training is a primary defense against adversarial attacks, but it often involves complex, non-convex optimization problems due to the min-max nature of the objective. Gradient-based adversarial training can get stuck in local optima, leading to sub-optimal robust accuracy. We propose a novel Gradient-Free Global Optimization (GFGO) approach for robust adversarial training. GFGO leverages a population-based evolutionary strategy (e.g., Covariance Matrix Adaptation Evolution Strategy, CMA-ES) to explore the parameter space of the inner maximization problem (finding adversarial examples) and the outer minimization problem (training the model). This gradient-free approach is less susceptible to local optima and can discover stronger adversarial examples and more robust model parameters. Our method consistently achieves state-of-the-art robust accuracy on CIFAR-10 and ImageNet against various attack types (e.g., PGD, AutoAttack), demonstrating that global optimization can significantly enhance the robustness of deep learning models where gradient-based methods falter.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3952,Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off,"Adversarial training often suffers from a robustness-accuracy trade-off, where achieving high robustness comes at the cost of accuracy.
One approach to mitigate this trade-off is leveraging invariance regularization, which encourages model invariance under adversarial perturbations; however, it still leads to accuracy loss.
In this work, we closely analyze the challenges of using invariance regularization in adversarial training and understand how to address them.
Our analysis identifies two key issues: (1) a ""gradient conflict"" between invariance and classification objectives, leading to suboptimal convergence, and (2) the mixture distribution problem arising from diverged distributions between clean and adversarial inputs.
To address these issues, we propose Asymmetric Representation-regularized Adversarial Training (ARAT), which incorporates asymmetric invariance loss with stop-gradient operation and a predictor to avoid gradient conflict, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem.
Our detailed analysis demonstrates that each component effectively addresses the identified issues, offering novel insights into adversarial defense.
ARAT shows superiority over existing methods across various settings. Finally, we discuss the implications of our findings to knowledge distillation-based defenses, providing a new perspective on their relative successes.",ICLR.cc/2025/Conference,7.0,True,0.8413,adversarial training primary defense against adversarial attacks but often involves complex non convex optimization problems due the min max nature the objective gradient free global optimization gfgo for robust adversarial training covariance matrix adaptation evolution strategy cma the parameter space the inner maximization problem adversarial examples and the outer minimization problem training the pgd autoattack demonstrating that global optimization can enhance the robustness deep learning models where gradient based methods falter,adversarial training often suffers from robustness accuracy trade off where achieving high robustness comes the cost our analysis identifies two key issues gradient conflict between invariance and classification objectives leading suboptimal convergence and the mixture distribution problem arising from diverged distributions between clean and adversarial inputs finally discuss the implications our knowledge distillation based defenses providing perspective their relative successes,2025-08-26T02:23:21.430724
17,Uncertainty-Aware Active Learning for Few-Shot Image Classification,"Few-shot image classification is challenging due to the scarcity of labeled data, and existing methods often struggle to generalize robustly. Active learning (AL) offers a promising avenue to mitigate this by strategically selecting informative samples for labeling, but its effectiveness in extreme few-shot scenarios is limited by unreliable uncertainty estimates and the cold-start problem. We propose Uncertainty-Aware Active Learning (UAAL), a novel framework specifically designed for few-shot settings. UAAL combines a meta-learning approach to rapidly adapt to new tasks with a sophisticated uncertainty quantification module based on a hybrid Bayesian-nonparametric approach, even with minimal initial labels. Our method then employs a diversity-promoting sampling strategy that selects samples based on both their predictive uncertainty and their representativeness within the feature space, ensuring both informativeness and coverage. Experiments on few-shot benchmarks (e.g., MiniImageNet, TieredImageNet) show UAAL significantly outperforms state-of-the-art active learning and few-shot learning methods, demonstrating superior classification performance with fewer labeled examples.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7376,Automatic Combination of Sample Selection Strategies for Few-Shot Learning,"In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the selection of samples has a significant impact on the performance of the trained model. Although many sample selection strategies are employed and evaluated in typical supervised settings, their impact on the performance of few-shot learning is largely unknown. In this paper, we investigate the impact of 20 sample selection strategies on the performance of 5 representative few-shot learning approaches over 8 image and 6 text datasets. We propose a new method for Automatic Combination of SamplE Selection Strategies (ACSESS), to leverage the strengths and complementarity of the individual strategies in order to select more impactful samples. The experimental results show that our method consistently outperforms all individual selection strategies. We also show that the majority of existing strategies strongly depend on modality, dataset characteristics and few-shot learning approach, while improving performance especially on imbalanced and noisy datasets. Lastly, we show that sample selection strategies work well even on smaller datasets and provide larger benefit when selecting a lower number of shots, while frequently regressing to random selection with higher numbers of shots.",ICLR.cc/2025/Conference,5.4,False,0.8540,few shot image classification challenging due the scarcity labeled data and existing methods often struggle generalize robustly active learning offers promising avenue mitigate this strategically selecting informative samples for labeling but its effectiveness extreme few shot scenarios limited unreliable uncertainty estimates and the cold start problem uncertainty aware active learning uaal designed for few shot settings our then employs diversity promoting sampling strategy that selects samples both their predictive uncertainty and their representativeness within the feature space ensuring both informativeness and coverage experiments few shot benchmarks miniimagenet tieredimagenet uaal outperforms state the art active learning and few shot learning methods demonstrating superior classification fewer labeled examples,few shot learning such meta learning few shot fine tuning context learning the selection samples has significant impact the the trained although many sample selection strategies are employed and evaluated typical supervised settings their impact the few shot learning largely unknown this the impact sample selection strategies the representative few shot learning approaches over image and text datasets also that the majority existing strategies strongly depend modality characteristics and few shot learning while improving imbalanced and noisy datasets,2025-08-26T02:23:21.430731
18,Stable and Efficient Training of Deep Neural SDEs via Variance Regularization,"Deep Neural Stochastic Differential Equations (SDEs) offer a powerful framework for modeling complex temporal dynamics and capturing inherent stochasticity, but their training is notoriously challenging due to numerical instability and high computational cost. Existing methods often rely on specialized solvers or approximations that compromise accuracy or stability. We propose a novel approach for Stable and Efficient Training of Deep Neural SDEs (SETSDE) that leverages a variance regularization technique. SETSDE introduces a differentiable regularization term derived from the Fenchel duality, which directly constrains the variance of the SDE's trajectories during training, promoting numerical stability without sacrificing expressivity. This allows for the use of more efficient SDE solvers and significantly reduces training time. Experiments on various sequential data modeling tasks (e.g., time-series forecasting, latent dynamics learning) demonstrate that SETSDE achieves state-of-the-art performance with improved stability and substantially faster convergence compared to existing neural SDE training techniques, opening up new possibilities for modeling stochastic processes.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3102,Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions,"Neural Stochastic Differential Equations (Neural SDEs) have emerged as powerful mesh-free generative models for continuous stochastic processes, with critical applications in fields such as finance, physics, and biology. Previous state-of-the-art methods have relied on adversarial training, such as GANs, or on minimizing distance measures between processes using signature kernels. However, GANs suffer from issues like instability, mode collapse, and the need for specialized training techniques, while signature kernel-based methods require solving linear PDEs and backpropagating gradients through the solver, whose computational complexity scales quadratically with the discretization steps. In this paper, we identify a novel class of strictly proper scoring rules for comparing continuous Markov processes. This theoretical finding naturally leads to a novel approach called Finite Dimensional Matching (FDM) for training Neural SDEs. Our method leverages the Markov property of SDEs to provide a computationally efficient training objective. This scoring rule allows us to bypass the computational overhead associated with signature kernels and reduces the training complexity from $O(D^2)$ to $O(D)$ per epoch, where $D$ represents the number of discretization steps of the process. We demonstrate that FDM achieves superior performance, consistently outperforming existing methods in terms of both computational efficiency and generative quality.",ICLR.cc/2025/Conference,6.0,True,0.8768,deep neural stochastic differential equations sdes offer powerful for modeling complex temporal dynamics and capturing inherent stochasticity but their training notoriously challenging due numerical instability and high computational cost for stable and efficient training deep neural sdes setsde that leverages variance regularization time series forecasting latent dynamics learning that setsde achieves state the art improved stability and faster convergence compared existing neural sde training techniques opening possibilities for modeling stochastic processes,neural stochastic differential equations neural sdes have emerged powerful mesh free generative models for continuous stochastic processes critical applications fields such finance physics and biology this theoretical naturally leads called finite dimensional matching fdm for training neural sdes,2025-08-26T02:23:21.430738
19,Self-Paced Curriculum Generation for Robust Self-Supervised Pre-training,"Self-supervised learning (SSL) has revolutionized representation learning, but its performance can be highly sensitive to the design of pretext tasks and the quality of augmentations. Manually designing effective curricula for SSL is challenging and often leads to suboptimal results, especially for complex real-world datasets. We propose Self-Paced Curriculum Generation (SPCG), a novel framework that automatically generates an optimal learning curriculum for SSL pre-training. SPCG continuously monitors the model's learning progress and adaptively adjusts the difficulty of generated pretext tasks and data augmentations. It dynamically identifies ""easy"" and ""hard"" samples and tasks, gradually exposing the model to more complex patterns as its confidence grows. This self-paced approach ensures efficient learning and avoids local optima. Experiments on various vision benchmarks (e.g., ImageNet, CIFAR) demonstrate that SPCG consistently improves the robustness and transferability of representations learned via leading SSL methods (e.g., SimCLR, BYOL), reducing the need for manual hyperparameter tuning and leading to more effective pre-training.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3556,Adversarial Robustness of  Self-Supervised Learning in Vision,"Self-supervised learning (SSL) has advanced significantly in visual representation learning, yet large-scale evaluations of its adversarial robustness remain limited. In this study, we evaluate the adversarial robustness of seven SSL models and one supervised model across a range of tasks, including ImageNet classification, transfer learning, segmentation, and detection. Our findings demonstrate that SSL models generally exhibit superior robustness to adversarial attacks compared to their supervised counterpart on ImageNet, with this advantage extending to transfer learning in classification tasks. However, this robustness is less pronounced in segmentation and detection tasks. We also explore the role of architectural choices in model robustness, observing that their impact varies depending on the SSL objective. Finally, we assess the effect of extended training durations on adversarial robustness, finding that longer training may offer slight improvements without compromising robustness. Our analysis highlights promising directions for enhancing the adversarial robustness of visual self-supervised representation systems in complex environments.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8826,self supervised learning ssl has revolutionized representation learning but its can highly sensitive the pretext tasks and the quality augmentations self paced curriculum generation spcg that automatically generates optimal learning curriculum for ssl pre training spcg continuously monitors the model learning progress and adaptively adjusts the difficulty generated pretext tasks and data augmentations this self paced ensures efficient learning and avoids local optima experiments various vision benchmarks imagenet cifar that spcg consistently improves the robustness and transferability representations learned leading ssl methods,self supervised learning ssl has advanced visual representation learning yet large scale evaluations its adversarial robustness remain limited this the adversarial robustness seven ssl models and one supervised across range tasks including imagenet classification transfer learning segmentation and detection our that ssl models exhibit superior robustness adversarial attacks compared their supervised counterpart imagenet this advantage extending transfer learning classification tasks however this robustness less pronounced segmentation and detection tasks finally assess the effect extended training durations adversarial robustness that longer training may offer slight improvements compromising robustness our analysis highlights promising directions for enhancing the adversarial robustness visual self supervised representation systems complex environments,2025-08-26T02:23:21.430746
20,Cross-Modal Disentanglement for Robust Multimodal Representation Fusion,"Multimodal learning aims to integrate information from diverse sources (e.g., vision, audio, text) for holistic understanding. However, fusing these modalities effectively is challenging due to inherent heterogeneity, noise, and the presence of shared and private information. Existing methods often struggle with robust fusion in the presence of missing modalities or noise. We propose Cross-Modal Disentanglement (CMD), a novel framework that learns robust multimodal representations by explicitly disentangling shared and modality-specific latent factors. CMD employs a variational autoencoder-like architecture coupled with adversarial training to ensure that shared latent factors are truly invariant across modalities, while private factors capture unique modality characteristics. This disentanglement enables more robust fusion, allowing the model to effectively combine information even when one modality is noisy or absent. Experiments on multimodal tasks (e.g., sentiment analysis, emotion recognition) demonstrate CMD achieves superior performance and robustness under partial modality input compared to state-of-the-art fusion techniques, offering a robust approach to multimodal understanding.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8993,Unimodal-driven Distillation in Multimodal Emotion Recognition with Dynamic Fusion,"Multimodal Emotion Recognition in Conversations (MERC) seeks to identify emotional states across multiple modalities, including text, audio, and video. This field of study is pivotal for advancing machine intelligence, with significant implications for applications such as intelligent dialogue systems and public opinion analysis. Most existing approaches primarily employ full-sequence interaction and distillation techniques, aiming to construct a comprehensive global contextual understanding while simultaneously enhancing the interaction among heterogeneous modalities. However, the presence of repetitive and redundant information, coupled with gradient conflicts arising from modal heterogeneity, can significantly impede the effectiveness of multimodal learning and long-range relationship modeling. In this work, we propose an innovative heterogeneous multimodal integration method called SUMMER, grounded in attention mechanism and knowledge distillation techniques, which facilitates dynamic interactive fusion of multimodal representations. Specifically, the Sparse Dynamic Mixture of Experts strategy is proposed to dynamically adjust the relevance of the temporal information to construct local to global token-wise interactions. Then a Global Mixture of Experts is employed to enhance the model's overall contextual understanding across modalities. Notably, we introduce retrograde distillation that utilizes a pre-trained unimodal teacher model to guide the learning of multimodal student model, intervening and supervising multimodal fusion within both the latent and logit spaces. Experiments on the IEMOCAP and MELD datasets demonstrate that our SUMMER framework consistently outperforms existing state-of-the-art methods, with particularly significant improvements in recognizing minority and semantically similar emotions in MERC tasks.",ICLR.cc/2025/Conference,3.8,nan,0.9009,multimodal learning aims integrate information from diverse sources sentiment analysis emotion recognition cmd achieves superior and robustness under partial modality input compared state the art fusion techniques offering robust multimodal understanding,multimodal emotion recognition conversations merc seeks identify emotional states across multiple modalities including text audio and video this field pivotal for advancing machine intelligence significant implications for applications such intelligent dialogue systems and public opinion analysis however the presence repetitive and redundant information coupled gradient conflicts arising from modal heterogeneity can impede the effectiveness multimodal learning and long range relationship modeling this innovative heterogeneous multimodal integration called summer grounded attention mechanism and knowledge distillation techniques which facilitates dynamic interactive fusion multimodal representations notably retrograde distillation that utilizes pre trained unimodal teacher guide the learning multimodal student intervening and supervising multimodal fusion within both the latent and logit spaces,2025-08-26T02:23:21.430753
21,Parameter-Efficient Fine-Tuning with Adaptive Latent Projections for Foundation Models,"Fine-tuning large pre-trained foundation models for downstream tasks is computationally expensive and memory-intensive, as it typically requires updating billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) methods reduce this cost, but often rely on fixed low-rank adaptations that may not capture the full expressivity needed for diverse tasks. We propose Adaptive Latent Projections (ALP), a novel PEFT method that learns task-specific adaptations more flexibly and efficiently. ALP introduces a small set of trainable, adaptive projection matrices that transform the model's latent activations, rather than directly modifying weights. These projections are learned in a low-dimensional ""latent space"" and dynamically adjusted based on the input, allowing for more nuanced and context-aware adaptations. Our method dramatically reduces the number of trainable parameters while maintaining competitive performance compared to full fine-tuning. Experiments across various NLP (e.g., GLUE, SuperGLUE) and vision tasks (e.g., Few-Shot ImageNet) show ALP outperforms existing PEFT methods in terms of efficiency and often matches full fine-tuning performance, enabling broader application of foundation models.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4233,PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models,"Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames. Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models. Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist training-free model by 16.3$\%$, 14.9$\%$, and 15$\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various foundation models and even surpasses the specialist training-based methods by 5.6$\%$-8$\%$ mAP across various datasets, serving as an effective generalist model.",ICLR.cc/2025/Conference,5.25,nan,0.8163,glue superglue and vision tasks few shot imagenet alp outperforms existing peft methods terms efficiency and often matches full fine tuning enabling broader application foundation models,recent success vision foundation models have shown promising for the perception tasks however difficult train foundation network directly due the limited and remains under explored whether existing foundation models can lifted space seamlessly this pointseg training free paradigm that leverages off the shelf vision foundation models address scene perception tasks concretely two branch prompts learning structure construct the point box prompts pairs combining the bidirectional matching strategy for accurate point and proposal prompts generation then perform the iterative post refinement adaptively when cooperated different vision foundation models pointseg demonstrates impressive segmentation across various datasets all training,2025-08-26T02:23:21.430760
22,Scalable Natural Gradient Approximation for Large-Scale Deep Learning Optimization,"Second-order optimization methods, particularly natural gradient descent, promise faster convergence and better generalization in deep learning by accounting for the geometry of the parameter space. However, their application to large-scale neural networks is typically infeasible due to the prohibitive cost of computing and inverting the Fisher information matrix. We propose Scalable Natural Gradient Approximation (SNGA), a novel method that combines low-rank approximations with a block-diagonal structure to efficiently estimate the inverse Fisher matrix. SNGA leverages the architectural properties of deep networks (e.g., layer-wise independence, parameter grouping) to break down the computation into manageable blocks, and employs an iterative, memory-efficient procedure for its inverse. Our approach provides a significantly more accurate approximation than first-order methods and scales to models with millions of parameters. Experiments on large-scale image classification (ImageNet) and language modeling tasks demonstrate SNGA achieves faster convergence and better generalization compared to Adam and other adaptive optimizers, bringing the power of natural gradients to modern deep learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8080,AdaFisher: Adaptive Second Order Optimization via Fisher Information,"First-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD. However, their practicality in training DNNs is still limited due to increased per-iteration computations compared to the first-order methods. We present *AdaFisher*--an adaptive second-order optimizer that leverages a *diagonal block-Kronecker* approximation of the Fisher information matrix for adaptive gradient preconditioning. AdaFisher aims to bridge the gap between enhanced *convergence/generalization* capabilities and computational efficiency in second-order optimization framework for training DNNs. Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modeling and stands out for its stability and robustness in hyper-parameter tuning. We demonstrate that AdaFisher **outperforms the SOTA optimizers** in terms of both accuracy and convergence speed. Code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.",ICLR.cc/2025/Conference,6.25,True,0.8559,second order optimization methods natural gradient descent promise faster convergence and better generalization deep learning accounting for the geometry the parameter space however their application large scale neural networks infeasible due the prohibitive cost computing and inverting the fisher information matrix snga leverages the architectural properties deep networks experiments large scale image classification imagenet and language modeling tasks snga achieves faster convergence and better generalization compared adam and other adaptive optimizers bringing the power natural gradients modern deep learning,first order optimization methods are currently the mainstream training deep neural networks dnns despite their widespread second order optimization algorithms exhibit superior convergence properties compared their first order counterparts adafisher aims bridge the gap between enhanced convergence generalization capabilities and computational efficiency second order optimization for training dnns despite the slow pace second order optimizers showcase that adafisher can reliably adopted for image classification language modeling and stands out for its stability and robustness hyper parameter tuning,2025-08-26T02:23:21.430768
23,Policy-Guided Synthetic Data Generation for Robust Domain Generalization,"Domain generalization (DG) aims to train models that perform well on unseen target domains, a critical challenge for real-world robustness. Data augmentation and synthetic data generation are promising strategies, but often struggle to capture the complex variations necessary for true domain transfer. We propose Policy-Guided Synthetic Data Generation (PGSG), a novel framework that learns an optimal data generation policy to synthesize diverse, domain-agnostic training samples. PGSG employs a reinforcement learning agent that interacts with a generative model (e.g., GAN, diffusion model) to iteratively refine the synthesis process. The agent receives rewards based on the generalization performance of a downstream classifier trained on the synthetic data, effectively learning to generate samples that bridge domain gaps. Experiments on standard DG benchmarks (e.g., PACS, Office-Home) demonstrate that PGSG significantly outperforms existing data augmentation and synthetic generation techniques, achieving superior generalization to unseen target domains, showcasing a new paradigm for enhancing model robustness through intelligent data synthesis.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3898,Mousterian: exploring the equivalence of generative and real data augmentation in classification,"In this paper, we address a key question in machine learning: **How effectively can generative data augmentation enhance image classification?** We begin by examining the differences and similarities between real and synthetic data generated by advanced text-to-image models. Through comprehensive experiments, we provide systematic insights into leveraging synthetic data for improved classification performance. Our findings show that: 1). Generative data augmentation by models trained solely on the internal (available training) set can effectively improve classification performance, validating the long-held hypothesis that synthesis enhances analysis by enriching modeling capability.
2). For generative data augmentation by models trained on both internal and external data (e.g. large-scale image-text pairs) separately, the size of equivalent synthetic dataset augmentation can be determined empirically. In addition to being aligned with a common intuition that real data augmentation is always preferred, our empirical formulation also provides a guideline for quantitatively estimating how much larger the size of generative dataset augmentation is, over the real data augmentation, to achieve comparable improvements. Our CIFAR-10 and ImageNet results also demonstrate its impact w.r.t. the size of the baseline training set and the quality of generative models.",ICLR.cc/2025/Conference,3.8333333333333335,nan,0.8520,domain generalization aims train models that perform well unseen target domains critical challenge for real world robustness data augmentation and synthetic data generation are promising strategies but often struggle capture the complex variations necessary for true domain transfer policy guided synthetic data generation pgsg that learns optimal data generation policy synthesize diverse domain agnostic training samples pgsg employs reinforcement learning agent that interacts generative the agent receives rewards the generalization downstream classifier trained the synthetic data learning generate samples that bridge domain gaps pacs office home that pgsg outperforms existing data augmentation and synthetic generation techniques achieving superior generalization unseen target domains showcasing paradigm for enhancing robustness intelligent data synthesis,this address key question machine learning how can generative data augmentation enhance image classification comprehensive experiments provide systematic insights into leveraging synthetic data for improved classification generative data augmentation models trained solely the internal available training set can improve classification validating the long held hypothesis that synthesis enhances analysis enriching modeling capability,2025-08-26T02:23:21.430771
24,Private Knowledge Distillation with Adaptive Noise Scheduling for Federated Learning,"Federated Learning (FL) often faces a privacy-utility trade-off, where differential privacy (DP) guarantees can significantly degrade model performance. Knowledge distillation (KD) offers a way to transfer knowledge from a larger, private model to a smaller, public one, but integrating DP-KD effectively into the FL paradigm while maintaining utility is challenging. We propose Private Knowledge Distillation with Adaptive Noise Scheduling (PKD-ANS), a novel approach that enhances privacy-preserving FL without sacrificing accuracy. PKD-ANS leverages a student-teacher framework where the teacher model's outputs (logits) are privatized using adaptive noise scheduling based on the historical training loss and the student's learning progress. This dynamic noise allocation ensures higher utility during critical learning phases and stronger privacy when gradients are stable. Our method supports private aggregation of gradients and private distillation. Experiments on various FL benchmarks (e.g., CIFAR-10, EMNIST) demonstrate that PKD-ANS achieves strong DP guarantees with significantly higher accuracy compared to traditional DP-FL and fixed-noise DP-KD methods, offering a practical solution for privacy-preserving collaborative AI.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,671,Differentially Private Federated Learning with Time-Adaptive Privacy Spending,"Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different amounts of noise to each client's model updates. One underlying assumption is that all clients spend their privacy budgets uniformly over time (learning rounds). However, it has been shown in the literature that learning in early rounds typically focuses on more coarse-grained features that can be learned at lower signal-to-noise ratios while later rounds learn fine-grained features that benefit from higher signal-to-noise ratios. Building on this intuition, we propose a time-adaptive DP-FL framework that expends the privacy budget non-uniformly across both time and clients. Our framework enables each client to save privacy budget in early rounds so as to be able to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. We theoretically prove utility improvements in the case that clients with stricter privacy budgets spend budgets unevenly across rounds, compared to clients with more relaxed budgets, who have sufficient budgets to distribute their spend more evenly. Our practical experiments on standard benchmark datasets support our theoretical results and show that, in practice, our algorithms improve the privacy-utility trade-offs compared to baseline schemes.",ICLR.cc/2025/Conference,7.0,True,0.8796,federated learning often faces privacy utility trade off where differential privacy guarantees can degrade knowledge distillation offers way transfer knowledge from larger private smaller public one but integrating into the paradigm while maintaining utility challenging private knowledge distillation adaptive noise scheduling pkd ans that enhances privacy preserving sacrificing pkd ans leverages student teacher where the teacher model outputs logits are privatized adaptive noise scheduling the historical training loss and the student learning progress this dynamic noise allocation ensures higher utility during critical learning phases and stronger privacy when gradients are stable,federated learning differential privacy provides for collaborative machine learning enabling clients train shared while adhering strict privacy constraints however has been shown the literature that learning early rounds focuses more coarse grained features that can learned lower signal noise ratios while later rounds learn fine grained features that benefit from higher signal noise ratios our enables each client save privacy budget early rounds able spend more later rounds when additional beneficial learning more fine grained features,2025-08-26T02:23:21.430783
25,Explainable Reinforcement Learning via Reward Decomposition and Counterfactual Traces,"Understanding the rationale behind an agent's decisions in Reinforcement Learning (RL) is crucial for trust, debugging, and deployment in safety-critical applications. Existing interpretability methods often provide local saliency maps or state visitations, failing to explain *why* an action was chosen in terms of underlying reward contributions or alternative outcomes. We propose a novel framework for Explainable Reinforcement Learning that decomposes the expected reward into contributing factors and generates counterfactual action traces. Our approach leverages a disentangled reward function to attribute parts of the cumulative reward to specific environmental features or sub-goals. Coupled with this, we introduce a method to generate ""what-if"" counterfactual trajectories by simulating alternative actions and contrasting their outcomes, providing insights into the agent's decision-making process. We demonstrate that this framework offers intuitive and actionable explanations on various standard RL environments (e.g., Atari, Mujoco), revealing the agent's learned priorities and potential biases. This work provides a significant step towards more transparent and accountable autonomous systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6352,Incorporating Human Preferences into Interpretable Reinforcement Learning with Tree Policies,"Interpretable reinforcement learning (RL) seeks to create agents that are efficient, transparent, and understandable to the populations that they impact. A significant gap in current approaches is the underutilization of human feedback, which is typically employed only for post-hoc evaluation. We propose to center the needs of end users by incorporating the feedback that would be obtained in a user study directly into the training of interpretable RL algorithms.  Our approach involves preference learning, where we learn preferences over high-level features that are not directly optimizable during the RL training process. We introduce an evolutionary algorithm that leverages user feedback to guide training toward interpretable decision-tree policies that are better-aligned with human preferences. We demonstrate the effectiveness of our method through experiments using synthetic preference data. Our results show an improvement in preference alignment compared to baselines, yielding policies that are more aligned with underlying user preferences but does so with sample efficiency in the number of user queries, thereby decreasing the burden on the user in providing such data.",ICLR.cc/2025/Conference,4.0,False,0.8407,understanding the rationale behind agent decisions reinforcement learning crucial for trust debugging and deployment safety critical applications existing interpretability methods often provide local saliency maps state visitations failing explain why action was chosen terms underlying reward contributions alternative outcomes for explainable reinforcement learning that decomposes the expected reward into contributing factors and generates counterfactual action traces,interpretable reinforcement learning seeks create agents that are efficient transparent and understandable the populations that they impact,2025-08-26T02:23:21.430792
26,Certified Robustness for Vision Transformers Against Lp Adversarial Perturbations,"Vision Transformers (ViTs) have achieved state-of-the-art performance in computer vision, yet their robustness to adversarial attacks remains a critical concern. While empirical adversarial training can improve robustness, it lacks formal guarantees against unseen or stronger attacks. Certified robustness, which provides provable lower bounds on accuracy within a perturbation radius, has largely focused on CNNs and is challenging for the complex architecture of ViTs. We introduce a novel certified robustness framework specifically designed for Vision Transformers against $L_p$ adversarial perturbations. Our approach leverages a combination of randomized smoothing and carefully engineered attention mechanisms to propagate certification guarantees through the transformer layers. We develop a robust attention module and propose a new certification procedure that accounts for the global dependencies induced by self-attention. Experiments on CIFAR-10 and ImageNet show our method achieves the first significant certified robust accuracy for ViTs against $L_\infty$ and $L_2$ attacks, surpassing previous certified CNN methods and providing formal guarantees for this powerful architecture.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3258,An Adaptive Defense Against Adversarial Patch Attacks For Vision Transformers,"Vision Transformers (ViTs) have become the prominent architecture for various computer vision tasks due to their superior ability to capture long-range dependencies through the self-attention mechanism. However, recent research indicates that ViTs are highly susceptible to carefully crafted adversarial patch attacks, presenting a significant challenge for practical deployment, particularly in security-critical applications. Existing approaches towards robust ViT frameworks often sacrifice clean accuracy and/or achieve suboptimal robustness, likely due to their uniform handling of diverse input samples. In this paper, we present NeighborViT, a novel adaptive defense framework specifically designed to counter adversarial patch attacks for ViTs. NeighborViT stands out by detecting and categorizing different types of attacks on inputs and applying adaptive, tailored defense mechanisms for each type of attack. To realize effective attack detection, categorization, and mitigation, NeighborViT explores the information in neighbor patches of the target patch and strategically employs them for defense. Our experimental results on the ImageNet dataset using various state-of-the-art ViT models demonstrate that NeighborViT significantly enhances robust accuracy without compromising clean accuracy. Our code is available at https://anonymous.4open.science/r/NeighborViT-8255.",ICLR.cc/2025/Conference,4.0,nan,0.8674,vision transformers vits have achieved state the art computer vision yet their robustness adversarial attacks remains critical concern certified robustness designed for vision transformers against l_p adversarial perturbations our leverages combination randomized smoothing and carefully engineered attention mechanisms propagate certification guarantees the transformer layers robust attention module and certification procedure that accounts for the global dependencies induced self attention,vision transformers vits have become the prominent for various computer vision tasks due their superior ability capture long range dependencies the self attention mechanism,2025-08-26T02:23:21.430798
27,Learning Invariant Causal Mechanisms for Robust Time Series Forecasting,"Time series forecasting models often struggle with robustness when deployed in environments exhibiting distribution shifts, as they tend to learn spurious correlations rather than underlying causal mechanisms. This limitation severely hampers their reliability in critical applications like financial prediction or medical diagnosis. We propose a novel framework for learning invariant causal mechanisms in time series data, enabling robust forecasting under various interventional or environmental changes. Our approach introduces a module that actively searches for feature transformations that render the predictive mechanism invariant across observed training environments. This is achieved by regularizing the model to predict consistently under simulated interventions on potential confounders and by maximizing the invariance of the learned representation. We demonstrate significant improvements in out-of-distribution generalization on synthetic and real-world time series datasets (e.g., weather, energy consumption), outperforming state-of-the-art methods. This work offers a principled way to build more reliable and adaptable time series models by grounding prediction in causal invariance.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6456,InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems,"Time-series modeling is broadly adopted to capture underlying patterns present in historical data, allowing prediction of future values. However, one crucial aspect of such modeling is often overlooked: in highly dynamic environments, data distributions can shift drastically within a second or less. Under these circumstances, traditional predictive models, and even online learning methods, struggle to adapt to the ultra-fast and complex distribution shifts present in highly dynamic scenarios. To address this, we propose InstaTrain, a novel learning approach that enables ultra-fast model updates for real-world prediction tasks, thereby keeping pace with rapidly evolving data distributions. In this work, (1) we transform the slow and expensive training process into an ultra-fast natural annealing process within a dynamical system. (2) Leveraging a recently proposed electronic dynamical system, we augment the system with parameter update modules, extending its capabilities to encompass both rapid training and inference. Experimental results on highly dynamic datasets demonstrate that our method achieves orders-of-magnitude improvements in training speed and energy efficiency while delivering superior accuracy compared to baselines running on GPUs.",ICLR.cc/2025/Conference,6.25,True,0.8355,time series forecasting models often struggle robustness when deployed environments exhibiting distribution shifts they tend learn spurious correlations rather than underlying causal mechanisms this limitation severely hampers their reliability critical applications like financial prediction medical diagnosis for learning invariant causal mechanisms time series data enabling robust forecasting under various interventional environmental changes our introduces module that actively searches for feature transformations that render the predictive mechanism invariant across observed training environments this achieved regularizing the predict consistently under simulated interventions potential confounders and maximizing the invariance the learned representation this offers principled way build more reliable and adaptable time series models grounding prediction causal invariance,time series modeling broadly adopted capture underlying patterns historical data allowing prediction future values under these circumstances traditional predictive models and even online learning methods struggle adapt the ultra fast and complex distribution shifts highly dynamic scenarios address this instatrain learning that enables ultra fast updates for real world prediction tasks thereby keeping pace rapidly evolving data distributions,2025-08-26T02:23:21.430805
28,Memory-Efficient Training of Extremely Deep Neural Networks via Hierarchical Checkpointing,"Training extremely deep neural networks, such as large Transformers or very deep ResNets, often hits memory bottlenecks due to storing intermediate activations for backpropagation. Gradient checkpointing mitigates this by recomputing activations, but naive implementations still incur significant computational overhead or require manual specification. We propose Hierarchical Checkpointing (HC), a novel, automatic, and memory-efficient strategy for training ultra-deep models. HC introduces a dynamic, multi-level checkpointing scheme that adapts to available memory and computational budget. It automatically determines optimal checkpointing granularity and placement across nested computational graphs, minimizing recomputation while ensuring memory constraints are met. Our method reformulates checkpointing as an optimization problem, solved efficiently via dynamic programming. We demonstrate that HC enables training models with significantly more layers (e.g., up to 1000 layers in ResNet-style architectures) and larger batch sizes than conventional checkpointing, with minimal overhead. HC provides a practical and scalable solution for pushing the boundaries of deep model training on limited hardware resources.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7557,Inverted Activations: Reducing Memory Footprint in Neural Network Training,"The scaling of neural networks with increasing data and model sizes necessitates the development of more efficient deep learning algorithms. 
    A significant challenge in neural network training is the memory footprint associated with activation tensors, particularly in pointwise nonlinearity layers that traditionally save the entire input tensor for the backward pass, leading to substantial memory consumption.
    
    In this paper, we propose a modification to the handling of activation tensors in pointwise nonlinearity layers. 
    Our method involves saving the output tensor instead of the input tensor during the forward pass. Since the subsequent layer typically also saves its input tensor, this approach reduces the total memory required by storing only one tensor between layers instead of two. This optimization is especially beneficial for transformer-based architectures like GPT, BERT, Mistral, and Llama.

    To enable this approach, we utilize the inverse function of the nonlinearity during the backward pass. As the inverse cannot be computed analytically for most nonlinearities, we construct accurate approximations using simpler functions. 
    Experimental results demonstrate that our method significantly reduces memory usage without affecting training accuracy or computational performance.

    Our implementation is provided as a drop-in replacement for standard nonlinearity layers in the PyTorch framework, facilitating easy adoption without requiring architectural modifications. The code is available at \url{https://github.com/removed/for/anonimity}.",ICLR.cc/2025/Conference,6.333333333333333,False,0.8461,training extremely deep neural networks such large transformers very deep resnets often hits memory bottlenecks due storing intermediate activations for backpropagation our reformulates checkpointing optimization problem solved dynamic programming provides practical and scalable solution for pushing the boundaries deep training limited hardware resources,the scaling neural networks increasing data and sizes necessitates the development more efficient deep learning algorithms significant challenge neural network training the memory footprint associated activation tensors pointwise nonlinearity layers that traditionally save the entire input tensor for the backward pass leading substantial memory consumption this optimization beneficial for transformer based architectures like gpt bert mistral and llama,2025-08-26T02:23:21.430810
29,Multi-Agent Reinforcement Learning with Differentiable Communication and Self-Correction,"Effective coordination in multi-agent reinforcement learning (MARL) under partial observability and dynamic environments remains a grand challenge. Centralized approaches struggle with scalability, while decentralized methods often suffer from poor communication and coordination failures. We propose a novel MARL framework that integrates Differentiable Communication with a Self-Correction mechanism. Our approach employs a communication protocol where agents learn to exchange continuous messages that are end-to-end differentiable, allowing gradient-based optimization of communication strategies. To address potential misunderstandings or noise in communication, we introduce a self-correction module that enables agents to refine their understanding of received messages and adapt their policies based on local observations and predictions of other agents' intentions. Experiments on complex multi-agent tasks (e.g., StarCraft Multi-Agent Challenge, Hanabi) demonstrate superior performance, faster convergence, and enhanced robustness to communication noise compared to state-of-the-art MARL algorithms, showcasing a promising direction for intelligent multi-agent systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10367,Human-like Communication Strategies for Improved Multi-Agent Reinforcement Learning,"Multi-Agent Reinforcement Learning (MARL) has seen significant progress in recent years, enabling multiple agents to coordinate and optimize their actions in complex environments. However, integrating effective communication protocols into MARL frameworks remains a challenge, as it introduces issues such as increased state space dimensionality, lack of stationarity, and the need for interpretability. Inspired by human communication, which relies on prior knowledge, contextual awareness, and efficient information exchange, we propose a novel framework for incorporating human-like communication strategies to enhance the learning process. Motivated by recent advancements in natural language processing (NLP), multi-modal AI and object detection, we use text-to-mask models and human feedback to learn compact and informative communication strategies that facilitate coordination among agents to improve the overall performance. We demonstrate the efficiency of our approach on various multi-agent tasks and provide insights into emergent communication behaviors observed during training.",ICLR.cc/2025/Conference,3.0,nan,0.9278,effective coordination multi agent reinforcement learning marl under partial observability and dynamic environments remains grand challenge our employs communication protocol where agents learn exchange continuous messages that are end end differentiable allowing gradient based optimization communication strategies starcraft multi agent challenge hanabi superior faster convergence and enhanced robustness communication noise compared state the art marl algorithms showcasing promising direction for intelligent multi agent systems,multi agent reinforcement learning marl has seen significant progress recent years enabling multiple agents coordinate and optimize their actions complex environments however integrating effective communication protocols into marl frameworks remains challenge introduces issues such increased state space dimensionality lack stationarity and the need for interpretability inspired human communication which relies prior knowledge contextual awareness and efficient information exchange for incorporating human like communication strategies enhance the learning process motivated recent advancements natural language processing nlp multi modal and object detection use text mask models and human feedback learn compact and informative communication strategies that facilitate coordination among agents improve the overall,2025-08-26T02:23:21.430818
30,Neural Inverse Rendering with Differentiable Point Cloud Priors,"High-fidelity inverse rendering, the task of reconstructing 3D scene geometry, materials, and lighting from 2D images, is fundamental for computer graphics and vision but highly ill-posed. Recent Neural Radiance Fields (NeRFs) show promise but lack explicit control over geometry and struggle with sharp features or sparse views. We propose Neural Inverse Rendering with Differentiable Point Cloud Priors (NIR-DPC), a novel framework that integrates a differentiable point cloud representation as an explicit geometric prior into the inverse rendering pipeline. Our approach uses a point cloud to directly model the scene's surface, while a neural implicit function learns surface details and appearance properties relative to these points. The point cloud is continuously optimized alongside the neural implicit function via a differentiable renderer. This combination enables the reconstruction of geometrically faithful and highly detailed 3D assets from sparse multi-view images, outperforming NeRF-based methods in geometry extraction and sharpness. NIR-DPC offers a powerful paradigm for robust and controllable 3D scene reconstruction.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5873,OMG: Opacity Matters in Material Modeling with Gaussian Splatting,"Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting has boosted it to the next level by showing real-time rendering potentials.  An intuitive finding is that the models used for inverse rendering do not take into account the dependency of opacity w.r.t. material properties, namely cross section, as suggested by optics. Therefore, we develop a novel approach that adds this dependency to the modeling itself. Inspired by radiative transfer, we augment the opacity term by introducing a neural network that takes as input material properties to provide modeling of cross section and a physically correct activation function. The gradients for material properties are therefore not only from color but also from opacity, facilitating a constraint for their optimization. Therefore, the proposed method incorporates more accurate physical properties compared to previous works. We implement our method into 3 different baselines that use Gaussian Splatting for inverse rendering and achieve significant improvements universally in terms of novel view synthesis and material modeling.",ICLR.cc/2025/Conference,7.0,True,0.8769,high fidelity inverse rendering the task reconstructing scene geometry materials and lighting from images fundamental for computer graphics and vision but highly ill posed recent neural radiance fields nerfs promise but lack explicit control over geometry and struggle sharp features sparse views neural inverse rendering differentiable point cloud priors nir dpc that integrates differentiable point cloud representation explicit geometric prior into the inverse rendering pipeline our uses point cloud directly the scene surface while neural implicit function learns surface details and appearance properties relative these points the point cloud continuously optimized alongside the neural implicit function differentiable renderer,decomposing geometry materials and lighting from set images namely inverse rendering has been long standing problem computer vision and graphics recent advances neural rendering enable photo realistic and plausible inverse rendering inspired radiative transfer augment the opacity term introducing neural network that takes input material properties provide modeling cross section and physically correct activation function the gradients for material properties are therefore not only from color but also from opacity facilitating constraint for their optimization,2025-08-26T02:23:21.430826
31,Adaptive Knowledge Transfer across Architectures via Latent Space Alignment,"Knowledge transfer, especially distillation, typically assumes a teacher and student model with similar architectures or focuses on compressing a large model. Transferring knowledge effectively between fundamentally different network architectures (e.g., CNN to Transformer, or different backbone sizes) remains challenging, often leading to performance degradation. We propose Adaptive Knowledge Transfer (AKT), a novel method for robust knowledge transfer across diverse deep learning architectures. AKT introduces a learnable latent alignment module that projects the teacher's and student's intermediate representations into a common latent space, minimizing discrepancies irrespective of their architectural differences. This module dynamically adapts the latent space to capture essential knowledge for the target task, allowing for more flexible and effective knowledge flow. Experiments demonstrate that AKT significantly improves knowledge transfer performance between disparate architectures on classification and object detection tasks (e.g., ResNet to EfficientNet, MobileNet to Vision Transformer), achieving state-of-the-art results and bridging the gap in heterogeneous model deployment.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8031,TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant,"Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of  11.47% on CIFAR-100 and 3.67% on ImageNet-1K for distilled models. Our code and models will be released.",ICLR.cc/2025/Conference,4.0,nan,0.8803,knowledge transfer distillation assumes teacher and student similar architectures focuses compressing large transferring knowledge between fundamentally different network architectures adaptive knowledge transfer akt for robust knowledge transfer across diverse deep learning architectures this module dynamically adapts the latent space capture essential knowledge for the target task allowing for more flexible and effective knowledge flow experiments that akt improves knowledge transfer between disparate architectures classification and object detection tasks resnet efficientnet mobilenet vision transformer achieving state the art and bridging the gap heterogeneous deployment,most knowledge distillation methodologies predominantly focus teacher student pairs similar architectures such both being convolutional neural networks cnns however the potential and flexibility can greatly improved expanding cross architecture cakd where the knowledge homogeneous and heterogeneous teachers can transferred flexibly given student the primary challenge cakd lies the substantial feature gaps between heterogeneous models originating from the distinction their inherent inductive biases and module functions this end assistant bridge facilitate smooth feature knowledge transfer between heterogeneous teachers and students more importantly within our proposed principle the assistant combines the advantages cross architecture inductive biases and module functions merging convolution and attention modules derived from both student and teacher module functions therefore leverage spatial agnostic infonce loss align features after spatial smoothing thereby improving the feature alignments cakd,2025-08-26T02:23:21.430830
32,Event-Stream Transformers for Irregular and High-Frequency Time-Series Modeling,"Traditional time-series models often assume regularly sampled data, performing poorly on event streams or highly irregular, sparse, and high-frequency data (e.g., electronic health records, financial transactions). Recurrent models struggle with long-term dependencies and irregular timestamps. We propose Event-Stream Transformers (EST), a novel architecture designed to robustly process event-based and irregular time-series data. EST adapts the Transformer's self-attention mechanism to incorporate continuous-time representations, explicitly modeling the temporal distance and event types within the attention weights. A novel ""event embedding"" combines timestamp information, feature values, and event type into a single representation, allowing the model to capture fine-grained temporal dynamics. Experiments on diverse irregular time-series datasets (e.g., MIMIC-III for medical prediction, a financial trading dataset) demonstrate EST significantly outperforms state-of-the-art recurrent and neural ODE-based models in tasks like forecasting, classification, and imputation, showcasing its ability to handle complex temporal patterns and irregularities.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7334,Transformer Meets Twicing: Harnessing Unattended Residual Information,"Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means  (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses *kernel twicing procedure* in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",ICLR.cc/2025/Conference,6.25,True,0.8547,est adapts the transformer self attention mechanism incorporate continuous time representations explicitly modeling the temporal distance and event types within the attention weights event embedding combines timestamp information feature values and event type into single representation allowing the capture fine grained temporal dynamics mimic iii for medical prediction financial trading est outperforms state the art recurrent and neural ode based models tasks like forecasting classification and imputation showcasing its ability handle complex temporal patterns and irregularities,transformer based deep learning models have achieved state the art across numerous language and vision tasks while the self attention mechanism core component transformers has proven capable handling complex data patterns has been observed that the representational capacity the attention matrix degrades across transformer layers thereby hurting its overall this leverage the connection between self attention computations and low pass non local means nlm smoothing filters and the twicing attention attention mechanism that uses kernel twicing procedure nonparametric regression alleviate the low pass behavior associated nlm smoothing compelling theoretical guarantees empirically the gains our over transformers multiple tasks and benchmarks including image classification and language modeling both clean and corrupted data,2025-08-26T02:23:21.430839
33,Continual Learning of Skill Hierarchies in Embodied Agents,"Embodied agents operating in dynamic environments must continually learn new skills and compose them, often facing catastrophic forgetting of previously acquired knowledge. Existing continual learning methods primarily focus on classification tasks, and their application to complex hierarchical skill acquisition in robotics remains largely unexplored. We propose a novel framework for Continual Learning of Skill Hierarchies (CLSH) that enables embodied agents to sequentially acquire and compose new skills while mitigating forgetting. CLSH combines a meta-learning approach for rapid adaptation to new skill tasks with a modular, hierarchical policy architecture. It employs a ""skill-centric"" replay buffer and a knowledge distillation mechanism that explicitly preserves the learned skill latent space, preventing catastrophic forgetting of foundational behaviors. Experiments in simulated robotic environments (e.g., manipulation, navigation with sequential tasks) demonstrate that CLSH effectively learns a diverse range of skills, efficiently transfers knowledge, and composes them into more complex behaviors, significantly outperforming baseline continual learning and RL methods.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6610,Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning,"In dynamic domains such as autonomous robotics and video game simulations, agents must continuously adapt to new tasks while retaining previously acquired skills. This ongoing process, known as Continual Reinforcement Learning, presents significant challenges, including the risk of forgetting past knowledge and the need for scalable solutions as the number of tasks increases. To address these issues, we introduce HIerarchical LOW-rank Subspaces of Policies (HILOW), a novel framework designed for continual learning in offline navigation settings. HILOW leverages hierarchical policy subspaces to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study,  the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like simulations, showcasing competitive performance and satisfying adaptability according to classical continual learning metrics, in particular regarding memory usage. Our work provides a promising framework for real-world applications where continuous learning from pre-collected data is essential.",ICLR.cc/2025/Conference,5.75,False,0.8579,embodied agents operating dynamic environments must continually learn skills and compose them often facing catastrophic forgetting previously acquired knowledge existing continual learning methods primarily focus classification tasks and their application complex hierarchical skill acquisition robotics remains largely unexplored for continual learning skill hierarchies clsh that enables embodied agents sequentially acquire and compose skills while mitigating forgetting clsh combines meta learning for rapid adaptation skill tasks modular hierarchical policy employs skill centric replay buffer and knowledge distillation mechanism that explicitly preserves the learned skill latent space preventing catastrophic forgetting foundational behaviors manipulation navigation sequential tasks that clsh learns diverse range skills transfers knowledge and composes them into more complex behaviors outperforming continual learning and methods,this ongoing process known continual reinforcement learning presents significant challenges including the risk forgetting past knowledge and the need for scalable solutions the number tasks increases address these issues hierarchical low rank subspaces policies hilow designed for continual learning offline navigation settings hilow leverages hierarchical policy subspaces enable flexible and efficient adaptation tasks while preserving existing knowledge careful experimental the effectiveness our both classical mujoco maze environments and complex video game like simulations showcasing competitive and satisfying adaptability according classical continual learning metrics particular regarding memory usage our provides promising for real world applications where continuous learning from pre collected data essential,2025-08-26T02:23:21.430845
34,Provably Fair Representation Learning via Minimax Game Theory,"Ensuring fairness in deep learning models, particularly in critical applications, is paramount. Many fairness interventions are heuristic or lack strong theoretical guarantees for complex, high-dimensional data. We propose a novel framework for Provably Fair Representation Learning (PFRL) based on a minimax game-theoretic formulation. PFRL trains a feature extractor to learn representations that are simultaneously discriminative for the main task and indistinguishable to an adversary attempting to predict sensitive attributes (e.g., race, gender). Unlike prior adversarial debiasing, our method uses a specific game-theoretic objective that provides a certified guarantee on the independence of the learned representations from sensitive attributes, under certain assumptions. We provide theoretical bounds on the achieved fairness and empirical evidence demonstrating its effectiveness. Experiments on benchmark fair classification datasets (e.g., Adult, COMPAS) show PFRL achieves state-of-the-art fairness-accuracy trade-offs, offering a robust and theoretically grounded approach to mitigate algorithmic bias and promote equitable AI systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6265,A Causal Lens for Learning Long-term Fair Policies,"Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic decision-making systems while simultaneously considering instantaneous fairness requirements. In the context of reinforcement learning, we propose a general framework where long-term fairness is measured by the difference in the average expected qualification gain that individuals from different groups could obtain. Then, through a causal lens, we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect the policy has on the qualification gain. We analyze the intrinsic connection between these components and an emerging fairness notion called benefit fairness that aims to control the equity of outcomes in decision-making. Finally, we develop a simple yet effective approach for balancing various fairness notions.",ICLR.cc/2025/Conference,5.5,True,0.8681,ensuring fairness deep learning models critical applications paramount many fairness interventions are heuristic lack strong theoretical guarantees for complex high dimensional data for provably fair representation learning pfrl minimax game theoretic formulation pfrl trains feature extractor learn representations that are simultaneously discriminative for the main task and indistinguishable adversary attempting predict sensitive attributes provide theoretical bounds the achieved fairness and empirical evidence demonstrating its effectiveness experiments fair classification datasets,fairness aware learning studies the development algorithms that avoid discriminatory decision outcomes despite biased training data while most studies have concentrated immediate bias static contexts this highlights the importance investigating long term fairness dynamic decision making systems while simultaneously considering instantaneous fairness requirements the context reinforcement learning general where long term fairness measured the difference the average expected qualification gain that individuals from different groups could obtain the intrinsic connection between these components and emerging fairness notion called benefit fairness that aims control the equity outcomes decision making finally simple yet effective for balancing various fairness notions,2025-08-26T02:23:21.430851
35,Graph-Based Program Synthesis for Domain-Specific Languages,"Automating program synthesis from natural language or input-output examples holds immense potential but remains challenging for complex programs, especially in domain-specific languages (DSLs). Traditional neural program synthesis often struggles with structural generalization and logical constraints. We propose a novel Graph-Based Program Synthesis (GPS) framework that leverages graph neural networks (GNNs) to represent and manipulate programs in a DSL, enabling better structural reasoning and generalization. GPS translates DSL programs into abstract syntax tree (AST) graphs, on which a GNN operates to iteratively refine and generate programs. A differentiable rule-based executor provides feedback, allowing the GNN to learn to satisfy symbolic constraints. This tight integration of neural graph processing with symbolic execution ensures both expressivity and correctness. Experiments on a variety of DSLs (e.g., spreadsheet manipulation, data wrangling) demonstrate that GPS significantly outperforms existing sequence-to-sequence and symbolic synthesis methods, achieving higher accuracy and stronger generalization to unseen program structures.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6149,COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural Networks Feedback Control for Program Synthesis,"Program synthesis methods, whether formal or neural-based, lack fine-grained control and flexible modularity, which limits their adaptation to complex software development. These limitations stem from rigid Domain-Specific Language (DSL) frameworks and neural network incorrect predictions. To this end, we propose the \textbf{Chain of Logic (CoL)}, which organizes synthesis stages into a chain and provides precise heuristic control to guide the synthesis process. Furthermore, by integrating neural networks with libraries and introducing a \textbf{Neural Network Feedback Control (NNFC)} mechanism, our approach modularizes synthesis and mitigates the impact of neural network mispredictions. Experiments on relational and symbolic synthesis tasks show that CoL significantly enhances the efficiency and reliability of DSL program synthesis across multiple metrics. Specifically, CoL improves accuracy by 70\% while reducing tree operations by 91\% and time by 95\%. Additionally, NNFC further boosts accuracy by 6\%, with a 64\% reduction in tree operations under challenging conditions such as insufficient training
data, increased difficulty, and multidomain synthesis. These improvements confirm COOL as a highly efficient and reliable program synthesis framework.",ICLR.cc/2025/Conference,2.5,nan,0.8408,automating program synthesis from natural language input output examples holds immense potential but remains challenging for complex programs domain specific languages dsls traditional neural program synthesis often struggles structural generalization and logical constraints graph based program synthesis gps that leverages graph neural networks gnns represent and manipulate programs dsl enabling better structural reasoning and generalization this tight integration neural graph processing symbolic execution ensures both expressivity and correctness,program synthesis methods whether formal neural based lack fine grained control and flexible modularity which limits their adaptation complex software development these limitations stem from rigid domain specific language dsl frameworks and neural network incorrect predictions furthermore integrating neural networks libraries and introducing textbf neural network feedback control nnfc mechanism our modularizes synthesis and mitigates the impact neural network mispredictions,2025-08-26T02:23:21.430859
36,Federated Unsupervised Domain Adaptation with Global-Local Consistency,"Federated Learning (FL) enables collaborative model training across clients without sharing raw data, but performance severely degrades when data distributions differ between the source domain (central server) and diverse target domains (clients). Unsupervised Domain Adaptation (UDA) in FL is critical for real-world deployments. We propose Federated Unsupervised Domain Adaptation with Global-Local Consistency (FL-GOLC), a novel framework that simultaneously aligns representations globally across all clients and locally within each client's target domain. FL-GOLC employs a feature extractor trained with a global consistency regularizer to align features from diverse clients to a shared, domain-invariant space. Concurrently, a local consistency module within each client optimizes for domain adaptation using unlabeled target data, minimizing distribution discrepancies for its specific domain. Experiments on several federated UDA benchmarks (e.g., Digit-Five, Office-Caltech) show FL-GOLC significantly outperforms state-of-the-art FL and UDA methods, achieving robust generalization across diverse target domains while preserving privacy.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6273,Personalized Prompt Tuning for Unsupervised Federated Learning,"Federated learning facilitates collaborative model training across multiple distributed clients without requiring data sharing.
However, conventional federated methods struggle with classification tasks in an unsupervised paradigm due to the absence of category knowledge.
Recently, CLIP, a prominent visual language model, has demonstrated impressive results, particularly its remarkable zero-shot classification ability, which alleviates the dependence on labeled data.
In this paper, we first explore a new realistic problem, unsupervised federated learning using CLIP, where clients with unlabeled heterogeneous data collaborate to enhance global performance.
To address this problem, we propose FedPP, a method that incorporates a cooperative pseudo-label selection strategy and a partial prompt aggregation protocol.
Our selection strategy ensures that all classes are trained in a balanced manner through global pseudo-label allocation.
Concurrently, the aggregation protocol divides parameters into aggregated and retained components to optimize global performance while supporting local personalization.
Extensive experiments across six datasets with various types of heterogeneity demonstrate the effectiveness of FedPP.
Our code is available in the supplementary materials.",ICLR.cc/2025/Conference,4.0,nan,0.8791,federated learning enables collaborative training across clients sharing raw data but severely degrades when data distributions differ between the source domain central server and diverse target domains clients unsupervised domain adaptation uda critical for real world deployments federated unsupervised domain adaptation global local consistency golc that simultaneously aligns representations globally across all clients and locally within each client target domain golc employs feature extractor trained global consistency regularizer align features from diverse clients shared domain invariant space concurrently local consistency module within each client optimizes for domain adaptation unlabeled target data minimizing distribution discrepancies for its specific domain,federated learning facilitates collaborative training across multiple distributed clients requiring data sharing however conventional federated methods struggle classification tasks unsupervised paradigm due the absence category knowledge recently clip prominent visual language has demonstrated impressive its remarkable zero shot classification ability which alleviates the dependence labeled data this first realistic problem unsupervised federated learning clip where clients unlabeled heterogeneous data collaborate enhance global,2025-08-26T02:23:21.430865
37,Quantization-Aware Neural Architecture Search for Extreme Edge Devices,"Deploying deep neural networks on extreme edge devices with limited computational and memory resources requires aggressive quantization (e.g., 4-bit or less) alongside efficient architectures. Standard Neural Architecture Search (NAS) often optimizes for floating-point performance, leading to sub-optimal results after quantization. We propose Quantization-Aware NAS (QANAS), a novel framework that jointly searches for neural architectures and their optimal low-bit quantization schemes, specifically targeting extreme edge deployments. QANAS integrates a differentiable quantization simulator directly into the NAS search space and reward function, allowing the search algorithm to evaluate architectures based on their quantized performance, latency, and memory footprint on a specific hardware platform (e.g., microcontrollers). Our method employs a multi-objective optimization approach to balance accuracy with resource constraints. Experiments on embedded vision benchmarks (e.g., TinyML, low-power IoT applications) demonstrate that QANAS discovers highly efficient and accurate low-bit quantized models, significantly outperforming models designed without quantization awareness.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10647,SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning,"How can we accurately quantize a pre-trained model without any data?
Quantization algorithms are widely used for deploying neural networks on resource-constrained edge devices.
Zero-shot Quantization (ZSQ) addresses the crucial and practical scenario where training data are inaccessible for privacy or security reasons.
However, three significant challenges hinder the performance of existing ZSQ methods: 1) noise in the synthetic dataset, 2) predictions based on off-target patterns, and the 3) misguidance by erroneous hard labels.
In this paper, we propose SynQ (Synthesis-aware Fine-tuning for Zero-shot Quantization),
a carefully designed ZSQ framework to overcome the limitations of existing methods.
SynQ minimizes the noise from the generated samples by exploiting a low-pass filter.
Then, SynQ trains the quantized model to improve accuracy by aligning its class activation map with the pre-trained model.
Furthermore, SynQ mitigates misguidance from the pre-trained model's error by leveraging only soft labels for difficult samples.
Extensive experiments show that SynQ provides the state-of-the-art accuracy, over existing ZSQ methods.",ICLR.cc/2025/Conference,6.5,True,0.8562,deploying deep neural networks extreme edge devices limited computational and memory resources requires aggressive quantization standard neural search nas often optimizes for floating point leading sub optimal after quantization quantization aware nas qanas that jointly searches for neural architectures and their optimal low bit quantization schemes targeting extreme edge deployments our employs multi objective optimization balance resource constraints experiments embedded vision benchmarks,quantization algorithms are used for deploying neural networks resource constrained edge devices zero shot quantization zsq addresses the crucial and practical scenario where training data are inaccessible for privacy security reasons this synq synthesis aware fine tuning for zero shot quantization carefully designed zsq overcome the limitations existing methods,2025-08-26T02:23:21.430870
38,Generative Model for Protein Structure Prediction with Equivariant Graph Networks,"Predicting the 3D structure of proteins from their amino acid sequence is a fundamental problem in biology with broad applications in drug discovery and disease understanding. Traditional methods are computationally expensive or rely on homology. Recent deep learning approaches have shown promise, but often lack an explicit generative model for diverse structure prediction. We propose a novel Generative Model for Protein Structure Prediction (GM-PSP) leveraging equivariant graph neural networks. GM-PSP learns a probability distribution over valid protein structures by iteratively refining atom coordinates and backbone angles. It incorporates SE(3)-equivariant GNNs to ensure predictions are invariant to rotations and translations, crucial for physical realism. The model learns to generate diverse structural conformations (e.g., for flexible regions) from a sequence, offering a sampling-based approach to explore the protein conformational landscape. Experiments on established protein structure benchmarks demonstrate GM-PSP generates physically plausible and accurate protein structures, often on par with or surpassing state-of-the-art predictive methods.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1847,Dual Flows with Contrastive Guidance for Generating Highly Designable Proteins,"Deep generative models have achieved substantial success in protein design. A prevalent approach for de novo protein design involves initially designing a protein backbone structure using deep generative models, such as diffusion and flow models, followed by using a separate inverse folding model to design the correponding sequence. Recently, co-design methods, which aim to jointly generate the structure and sequence of a protein, have attracted considerable attention. Despite this, co-designing sequences and structures of long proteins remains challenging. The complexity of this high-dimensional multimodal generative modeling makes sampling of diffusion and flow models prone to accumulated errors, often leading to non-designable regions. To tackle this challenge, we introduce a contrastive guided sampling algorithm with dual multimodal flows to sample both sequences and structures of highly designable proteins. The contrastive guidance uses the lower-quality flow to help the higher-quality flow avoid non-designable regions by gently steering it during sampling. Our method achieves designability of 80% for length-400 proteins and 37% for length-500 proteins, significantly outperforming previous approaches.",ICLR.cc/2025/Conference,4.75,False,0.8375,recent deep learning approaches have shown promise but often lack explicit generative for diverse structure prediction generative for protein structure prediction psp leveraging equivariant graph neural networks,deep generative models have achieved substantial success protein prevalent for novo protein involves initially designing protein backbone structure deep generative models such diffusion and flow models followed separate inverse folding the correponding sequence recently design methods which aim jointly generate the structure and sequence protein have attracted considerable attention,2025-08-26T02:23:21.430875
39,Online Continual Learning with Concept Drift Detection and Replay Management,"Online continual learning, where models learn from a continuous stream of data one sample at a time, is critical for real-world adaptability but extremely challenging due to severe catastrophic forgetting and concept drift. Existing methods often rely on fixed memory budgets or struggle to adapt to evolving data distributions. We propose a novel framework for Online Continual Learning with Concept Drift Detection and Replay Management (OCL-CDDRM). OCL-CDDRM integrates a lightweight, non-parametric drift detection mechanism that monitors changes in the incoming data stream and triggers adaptive learning strategies. Upon detecting drift, the framework intelligently updates its replay buffer by prioritizing ""hard"" or ""representative"" samples from past tasks and dynamically adjusts the learning rate. This adaptive approach enables robust knowledge retention while quickly adapting to new concepts. Experiments on online class-incremental and stream-based learning benchmarks demonstrate superior performance and stability compared to state-of-the-art online continual learning methods.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,586,Online Sequential Learning from Physiological Data with Weighted Prototypes: Tackling Cross-Subject Variability,"Online Continual Learning (OCL) enables machine learning models to adapt to sequential data streams in real-time, especially when only a small amount of data is available. However, applying OCL to physiological data such as electroencephalography (EEG) and electrocardiography (ECG) is often complicated by inter-subject variability, which can lead to catastrophic forgetting and performance degradation. Existing OCL methods are currently unable to effectively address this challenge, leading to difficulties in retaining previously learned knowledge while adapting to new data. This paper presents Online Prototypes Weighted Aggregation (OPWA), a novel method specifically designed to address the problem of catastrophic forgetting in the presence of inter-subject variability through the use of prototypical networks. OPWA facilitates the retention of knowledge from past subjects while adapting to new data streams.
The OPWA method uses an innovative prototype aggregation mechanism that fuses intra-class prototypes into generalized representations by accounting for both within-class and inter-class variation between subjects. Extensive experiments show that OPWA consistently outperforms existing OCL methods in terms of fast adaptation and mitigation of catastrophic forgetting on different physiological datasets with different modalities, and provides a robust solution for learning on sequential data streams.",ICLR.cc/2025/Conference,4.75,False,0.8794,for online continual learning concept drift detection and replay management ocl cddrm ocl cddrm integrates lightweight non parametric drift detection mechanism that monitors changes the incoming data stream and triggers adaptive learning strategies upon detecting drift the intelligently updates its replay buffer prioritizing hard representative samples from past tasks and dynamically adjusts the learning rate this adaptive enables robust knowledge retention while quickly adapting concepts experiments online class incremental and stream based learning benchmarks superior and stability compared state the art online continual learning methods,online continual learning ocl enables machine learning models adapt sequential data streams real time when only small amount data available existing ocl methods are currently unable address this challenge leading difficulties retaining previously learned knowledge while adapting data opwa facilitates the retention knowledge from past subjects while adapting data streams extensive experiments that opwa consistently outperforms existing ocl methods terms fast adaptation and mitigation catastrophic forgetting different physiological datasets different modalities and provides robust solution for learning sequential data streams,2025-08-26T02:23:21.430881
40,Self-Supervised Learning for Anomaly Detection in High-Dimensional Data Streams,"Anomaly detection in high-dimensional data streams (e.g., network traffic, sensor readings, industrial logs) is critical for system health monitoring and security, but often suffers from scarce labeled anomalies and the dynamic nature of data. Traditional methods struggle with complex, evolving patterns. We propose a novel Self-Supervised Learning (SSL) framework for Anomaly Detection (SSAD) that leverages the inherent structure of normal data to learn robust representations for anomaly scoring. SSAD trains a deep autoencoder or generative model using carefully designed pretext tasks (e.g., masked feature prediction, contrastive learning on temporal coherence) that force the model to capture the normal data distribution. Anomalies, by definition, deviate significantly from this learned distribution and are detected by high reconstruction errors or low similarity scores in the learned latent space. Experiments on diverse real-world anomaly detection benchmarks (e.g., cybersecurity, industrial IoT) demonstrate SSAD significantly outperforms unsupervised and semi-supervised baselines, proving robust and effective without explicit anomaly labels.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1829,Anomaly Detection by Context Contrasting,"Anomaly detection focuses on identifying samples that deviate from the norm.
When working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture concepts of normality. 
Recent advances in self-supervised learning have shown great promise in this regard. 
However, many successful self-supervised anomaly detection methods assume prior knowledge about anomalies to create synthetic outliers during training. 
Yet, in real-world applications, we often do not know what to expect from unseen data, and we can solely leverage knowledge about normal data. 
In this work, we propose Con$_2$, which learns representations through context augmentations that allow us to observe samples from
two distinct perspectives while keeping the invariances of normal data. 
Con$_2$ learns rich representations of context-augmented samples by clustering them according to their context while simultaneously 
aligning their positions across clusters. 
At test time, representations of anomalies that do not adhere to the invariances of normal data then deviate from their respective context cluster. 
Learning representations in such a way thus allows us to detect anomalies without making
assumptions about anomalous data.",ICLR.cc/2025/Conference,4.5,False,0.8715,anomaly detection high dimensional data streams network traffic sensor readings industrial logs critical for health monitoring and security but often suffers from scarce labeled anomalies and the dynamic nature data self supervised learning ssl for anomaly detection ssad that leverages the inherent structure normal data learn robust representations for anomaly scoring ssad trains deep autoencoder generative carefully designed pretext tasks masked feature prediction contrastive learning temporal coherence that force the capture the normal data distribution experiments diverse real world anomaly detection benchmarks cybersecurity industrial iot ssad outperforms unsupervised and semi supervised baselines proving robust and effective explicit anomaly labels,anomaly detection focuses identifying samples that deviate from the norm when working high dimensional data such images crucial requirement for detecting anomalous patterns learning lower dimensional representations that capture concepts normality recent advances self supervised learning have shown great promise this regard however many successful self supervised anomaly detection methods assume prior knowledge about anomalies create synthetic outliers during training yet real world applications often not know what expect from unseen data and can solely leverage knowledge about normal data con learns rich representations context augmented samples clustering them according their context while simultaneously aligning their positions across clusters learning representations such way thus allows detect anomalies making assumptions about anomalous data,2025-08-26T02:23:21.430887
41,Disentangled Causal Representation Learning for Robust Medical Image Analysis,"Medical image analysis faces unique challenges: models must be robust to variations in imaging protocols, patient populations, and scanner artifacts (spurious correlations), while focusing on genuine disease features (causal factors). Current deep learning models often conflate these, leading to poor generalization. We propose Disentangled Causal Representation Learning (DCRL) for robust medical image analysis. DCRL leverages an unsupervised disentanglement objective to separate causal factors (e.g., disease presence, severity) from confounding factors (e.g., scanner type, image noise). Our approach uses a modified variational autoencoder augmented with an adversarial component to enforce independence between latent dimensions corresponding to causal and confounding variables, without requiring explicit labels for confounders. This results in representations robust to domain shifts. Experiments on diverse medical imaging tasks (e.g., disease classification across different hospitals) demonstrate DCRL achieves significantly higher accuracy and improved out-of-distribution generalization, enabling more reliable and trustworthy AI for healthcare.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6796,Unifying Causal Representation Learning with the Invariance Principle,"Causal representation learning (CRL) aims at recovering latent causal variables from high-dimensional observations to solve causal downstream tasks, such as predicting the effect of new interventions or more robust classification. 
  A plethora of methods have been developed, each tackling carefully crafted problem settings that lead to different types of identifiability. 
  These different settings are widely assumed to be important because they are often linked to different rungs of Pearl's causal hierarchy, even though this correspondence is not always exact.
    This work shows that instead of strictly conforming to this hierarchical mapping, *many causal representation learning approaches methodologically align their representations with inherent data symmetries.*
  Identification of causal variables is guided by invariance principles that are not necessarily causal. 
  This result allows us to unify many existing approaches in a single method that can mix and match different assumptions, including non-causal ones, based on the invariance relevant to the problem at hand. 
  It also significantly benefits applicability, which we demonstrate by improving treatment effect estimation on real-world high-dimensional ecological data. Overall, this paper clarifies the role of causal assumptions in the discovery of causal variables and shifts the focus to preserving data symmetries.",ICLR.cc/2025/Conference,7.0,True,0.8269,current deep learning models often conflate these leading poor generalization disentangled causal representation learning dcrl for robust medical image analysis dcrl leverages unsupervised disentanglement objective separate causal factors this representations robust domain shifts disease classification across different hospitals dcrl achieves higher and improved out distribution generalization enabling more reliable and trustworthy for healthcare,causal representation learning crl aims recovering latent causal variables from high dimensional observations solve causal downstream tasks such predicting the effect interventions more robust classification this shows that instead strictly conforming this hierarchical mapping many causal representation learning approaches methodologically align their representations inherent data symmetries,2025-08-26T02:23:21.430895
42,Implicit Neural Policies for Continuous Control with Sparse Rewards,"Reinforcement Learning for continuous control tasks with sparse rewards is notoriously challenging, as agents struggle to discover reward signals in high-dimensional action spaces. Traditional policy gradient methods often rely on dense rewards or extensive exploration. We propose Implicit Neural Policies (INP), a novel framework that addresses sparse reward continuous control by learning implicit representations of optimal policies. INP formulates the policy as the implicit solution to an energy-based model or an optimization problem, allowing for a more flexible and robust policy representation than direct parameterization. Our approach leverages a self-supervised objective to guide exploration towards high-reward regions, implicitly shaping the policy even with sparse signals. This energy-based formulation enables efficient sampling of diverse actions and robust learning. Experiments on challenging sparse-reward continuous control benchmarks (e.g., robotic manipulation, locomotion with sparse goals) demonstrate INP achieves significantly higher success rates and faster learning compared to state-of-the-art on-policy and off-policy RL algorithms.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2890,Deep Exploration with PAC-Bayes,"Reinforcement learning for continuous control under sparse rewards is an under-explored problem despite its significance in real life. Many complex skills build on intermediate ones as prerequisites. For instance, a humanoid locomotor has to learn how to stand before it can learn to walk. To cope with reward sparsity, a reinforcement learning agent has to perform deep exploration. However, existing deep exploration methods are designed for small discrete action spaces, and their successful generalization to state-of-the-art continuous control remains unproven.  We address the deep exploration problem for the first time from a PAC-Bayesian perspective in the context of actor-critic learning.  To do this, we quantify the error of the Bellman operator through a PAC-Bayes bound, where a bootstrapped ensemble of critic networks represents the posterior distribution, and their targets serve as a data-informed function-space prior. 
We derive an objective function from this bound and use it to train the critic ensemble. Each critic trains an individual actor network, implemented as a shared trunk and critic-specific heads. The agent performs deep exploration by acting deterministically on a randomly chosen actor head. Our proposed algorithm, named PAC-Bayesian Actor-Critic (PBAC), is the only algorithm to successfully discover sparse rewards on a diverse set of continuous control tasks with varying difficulty.",ICLR.cc/2025/Conference,4.75,False,0.9078,reinforcement learning for continuous control tasks sparse rewards notoriously challenging agents struggle discover reward signals high dimensional action spaces implicit neural policies inp that addresses sparse reward continuous control learning implicit representations optimal policies inp formulates the policy the implicit solution energy based optimization problem allowing for more flexible and robust policy representation than direct parameterization this energy based formulation enables efficient sampling diverse actions and robust learning robotic manipulation locomotion sparse goals inp achieves higher success rates and faster learning compared state the art policy and off policy algorithms,reinforcement learning for continuous control under sparse rewards under explored problem despite its significance real life cope reward sparsity reinforcement learning agent has perform deep exploration however existing deep exploration methods are designed for small discrete action spaces and their successful generalization state the art continuous control remains unproven address the deep exploration problem for the first time from pac bayesian perspective the context actor critic learning the agent performs deep exploration acting deterministically randomly chosen actor head,2025-08-26T02:23:21.430902
43,Towards Certified Robustness for Graph Neural Networks,"Graph Neural Networks (GNNs) are increasingly used in security-sensitive domains, yet their vulnerability to adversarial attacks on graph structures or features is a critical concern. Providing certified robustness for GNNs, which offers provable guarantees against such attacks, remains largely unexplored compared to CNNs. We propose a novel framework for Certified Robustness for Graph Neural Networks (CR-GNN). Our approach leverages randomized smoothing, adapted to the unique challenges of graph data, specifically against discrete graph structure perturbations (e.g., edge additions/deletions) and continuous feature perturbations. We develop a graph-specific smoothing distribution and a corresponding certification procedure that accounts for the non-Euclidean nature of graph attacks. Experiments on benchmark graph classification and node classification datasets (e.g., Cora, PubMed, Reddit) demonstrate that CR-GNN provides the first meaningful certified robust accuracy for GNNs against graph structure and feature attacks, paving the way for trustworthy GNN deployments in critical applications.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2786,GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning,"Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a novel adversarial attack framework that exploits the topological structure of graphs to undermine the integrity of GNN predictions systematically. 

By defining a topology-aware method to manipulate graph orbits, our approach generates adversarial modifications that are both subtle and effective, posing a severe test to the robustness of GNNs. We evaluate the efficacy of GOttack across multiple prominent GNN architectures using standard benchmark datasets. Our results show that GOttack outperforms existing state-of-the-art adversarial techniques and completes training in approximately 55% of the time required by the fastest competing model, achieving the highest average misclassification rate in 155 tasks. 
This work not only sheds light on the susceptibility of GNNs to structured adversarial attacks 
but also shows that certain topological patterns may play a significant role in the underlying robustness of the GNNs. Our Python implementation is shared at https://github.com/cakcora/GOttack.",ICLR.cc/2025/Conference,6.5,True,0.9160,graph neural networks gnns are increasingly used security sensitive domains yet their vulnerability adversarial attacks graph structures features critical concern providing certified robustness for gnns which offers provable guarantees against such attacks remains largely unexplored compared cnns for certified robustness for graph neural networks gnn edge additions deletions and continuous feature perturbations experiments graph classification and node classification datasets cora pubmed reddit that gnn provides the first meaningful certified robust for gnns against graph structure and feature attacks paving the way for trustworthy gnn deployments critical applications,graph neural networks gnns have demonstrated superior node classification tasks across diverse applications defining topology aware manipulate graph orbits our generates adversarial modifications that are both subtle and effective posing severe the robustness gnns this not only sheds light the susceptibility gnns structured adversarial attacks but also shows that certain topological patterns may play significant role the underlying robustness the gnns,2025-08-26T02:23:21.430909
44,Multimodal Transformers with Gating Mechanisms for Noisy Data Fusion,"Fusing information from multiple modalities (e.g., vision, audio, text) often enhances deep learning model performance, but real-world multimodal data is frequently noisy, incomplete, or contains conflicting signals. Naive fusion strategies can degrade performance when one modality is compromised. We propose Multimodal Transformers with Gating Mechanisms (MT-GM), a novel architecture that adaptively fuses information from multiple modalities while explicitly handling noise. MT-GM extends the Transformer architecture with learnable, modality-specific gating mechanisms that dynamically weigh the contribution of each modality's features to the cross-attention layers, based on its perceived reliability or informativeness. This allows the model to selectively attend to trustworthy modalities and suppress noisy ones. Experiments on challenging multimodal datasets (e.g., multimodal sentiment analysis, emotion recognition with injected noise) demonstrate MT-GM significantly outperforms conventional multimodal fusion methods, exhibiting superior robustness to noise and missing modalities, leading to more resilient and adaptable multimodal AI systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,11643,Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction,"Multimodal learning enables various machine learning tasks to benefit from diverse data sources, effectively mimicking the interplay of different factors in real life events. While the heterogeneous nature of these modalities may necessitate the design of complex architectures, their interpretability is often overlooked. In this study, we leverage the intrinsic explainability of Transformer-based models to explain multimodal learning frameworks. We utilize the self-attention mechanism alongside model-specific feature attribution techniques, comparing these against post-hoc methods. Our detailed analysis focuses on the challenging task of crop yield prediction, exploiting the characteristics of the modalities and the data to aggregate local explanations at multiple levels. Our findings indicate that Transformers significantly outperform other architectures in yield prediction, making them well-suited for further intrinsic interpretability analysis. Among the modalities, satellite data emerged as the most influential but requires deeper layers for effective feature extraction due to its complex structure. Additionally, we observed that the Attention Rollout method is more robust than Generic Attention, aligns more closely with Shapley-based attributions and shows reduced sensitivity to minor input variations.",ICLR.cc/2025/Conference,3.8333333333333335,nan,0.8682,vision audio text often enhances deep learning but real world multimodal data frequently noisy incomplete contains conflicting signals extends the transformer learnable modality specific gating mechanisms that dynamically weigh the each modality features the cross attention layers its perceived reliability informativeness multimodal sentiment analysis emotion recognition injected noise outperforms conventional multimodal fusion methods exhibiting superior robustness noise and missing modalities leading more resilient and adaptable multimodal systems,multimodal learning enables various machine learning tasks benefit from diverse data sources mimicking the interplay different factors real life events while the heterogeneous nature these modalities may necessitate the complex architectures their interpretability often overlooked this leverage the intrinsic explainability transformer based models explain multimodal learning frameworks utilize the self attention mechanism alongside model specific feature attribution techniques comparing these against post hoc methods our indicate that transformers outperform other architectures yield prediction making them well suited for further intrinsic interpretability analysis among the modalities satellite data emerged the most influential but requires deeper layers for effective feature extraction due its complex structure additionally observed that the attention rollout more robust than generic attention aligns more closely shapley based attributions and shows reduced sensitivity minor input variations,2025-08-26T02:23:21.430915
45,Learning Differentiable Energy-Based Models for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) samples is crucial for reliable deep learning systems, preventing confident incorrect predictions. While generative models and uncertainty estimation methods show promise, many suffer from computational cost, sensitivity to hyper-parameters, or lack theoretical guarantees for OOD detection. We propose learning Differentiable Energy-Based Models (EBMs) for robust OOD detection. Our approach trains an EBM to directly model the probability density of in-distribution data, such that OOD samples naturally correspond to regions of low energy (or low probability). We leverage recent advances in contrastive divergence and adversarial learning to efficiently train deep EBMs on high-dimensional data, making them scalable and stable. The EBM's energy score provides a direct, unnormalized probability for OOD detection. Experiments on various OOD benchmarks across vision and tabular data demonstrate our EBM-based method achieves state-of-the-art OOD detection performance, offering a theoretically grounded and highly effective approach without relying on auxiliary classifiers or specific model architectures.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4428,Splitting & Integrating: Out-of-Distribution Detection via Adversarial Gradient Attribution,"Out-of-distribution (OOD) detection is essential for enhancing the robustness and security of deep learning models in unknown and dynamic data environments. Gradient-based OOD detection methods, such as GAIA, analyse the explanation pattern representations of in-distribution (ID) and OOD samples by examining the sensitivity of model outputs w.r.t. model inputs, resulting in superior performance compared to traditional OOD detection methods. However, we argue that the non-zero gradient behaviors of OOD samples do not exhibit significant distinguishability, especially when ID samples are perturbed by random noise in high-dimensional spaces, which negatively impacts the accuracy of OOD detection. In this paper, we propose a novel OOD detection method called **S \& I** based on layer **S**plitting and gradient **I**ntegration via Adversarial Gradient Attribution. Specifically, our approach involves splitting the model's intermediate layers and iteratively updating adversarial examples layer-by-layer. We then integrate the attribution gradients from each intermediate layer along the attribution path from adversarial examples to the actual input, yielding true explanation pattern representations for both ID and OOD samples. Experiments demonstrate that our S \& I algorithm achieves state-of-the-art results, with the average FPR95 of 29.05\% (38.61\%) and 37.31\% on the CIFAR100 and ImageNet benchmarks, respectively. Our code is available at: https://anonymous.4open.science/r/S-I-F6F7/.",ICLR.cc/2025/Conference,5.0,False,0.8580,detecting out distribution ood samples crucial for reliable deep learning systems preventing confident incorrect predictions while generative models and uncertainty estimation methods promise many suffer from computational cost sensitivity hyper parameters lack theoretical guarantees for ood detection learning differentiable energy based models ebms for robust ood detection leverage recent advances contrastive divergence and adversarial learning train deep ebms high dimensional data making them scalable and stable the ebm energy provides direct unnormalized probability for ood detection experiments various ood benchmarks across vision and tabular data our ebm based achieves state the art ood detection offering theoretically grounded and highly effective relying auxiliary classifiers specific architectures,out distribution ood detection essential for enhancing the robustness and security deep learning models unknown and dynamic data environments gradient based ood detection methods such gaia analyse the explanation pattern representations distribution and ood samples examining the sensitivity outputs inputs resulting superior compared traditional ood detection methods however argue that the non zero gradient behaviors ood samples not exhibit significant distinguishability when samples are perturbed random noise high dimensional spaces which negatively impacts the ood detection this ood detection called layer plitting and gradient ntegration adversarial gradient attribution,2025-08-26T02:23:21.430923
46,Explainable Graph Neural Networks via Subgraph Importance and Counterfactuals,"Understanding predictions from Graph Neural Networks (GNNs) is critical for trust and adoption in domains like drug discovery, social network analysis, and fraud detection. Existing GNN explainability methods often rely on feature attribution or single-node importance, which fail to capture the complex, relational reasoning inherent in graph data. We propose a novel framework for Explainable GNNs that identifies Subgraph Importance and generates Counterfactual Explanations. Our approach first identifies a minimal, highly influential subgraph whose presence is crucial for the GNN's prediction. This is achieved through a differentiable masking mechanism optimized to preserve the original prediction. Furthermore, we generate counterfactual subgraphs by proposing minimal structural changes that flip the GNN's prediction, providing insights into *what needed to change* for a different outcome. Experiments on graph classification and link prediction tasks demonstrate our method provides more intuitive, faithful, and actionable explanations compared to existing GNN explainability techniques, enhancing the transparency of graph-based deep learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1202,Toward Human-Interpretable Explanations in a Unified Framework for GNNs,"As Graph Neural Networks (GNNs) are increasingly applied across various domains, explainability has become a critical factor for real-world applications. Existing post-hoc explainability methods primarily focus on estimating the importance of edges, nodes, or subgraphs in the input graph to identify substructures crucial for predictions. However, these methods often lack human interpretability and do not provide a unified framework that incorporates both model-level and instance-level explanations. In this context, we propose leveraging a set of graphlets---small, connected, non-isomorphic induced subgraphs widely used in various scientific fields---and their associated orbits as human-interpretable units to decompose GNN predictions. Domain experts can select the most relevant graphlets as interpretable units and request unified explanations based on these units. To address this problem, we introduce UO-Explainer, the Unified and Orbit-based Explainer for GNNs, which utilizes predefined orbits that are generalizable and universal across graph domains as interpretable units. Our model decomposes GNN weights into orbit units to extract class-specific graph patterns (model-level) and to identify important subgraphs within individual data instances for prediction (instance-level). Extensive experimental results demonstrate that UO-Explainer outperforms existing baselines in providing meaningful and interpretable explanations across both synthetic and real-world datasets.",ICLR.cc/2025/Conference,4.0,False,0.8991,understanding predictions from graph neural networks gnns critical for trust and adoption domains like drug discovery social network analysis and fraud detection existing gnn explainability methods often rely feature attribution single node importance which fail capture the complex relational reasoning inherent graph data our first identifies minimal highly influential subgraph whose presence crucial for the gnn prediction this achieved differentiable masking mechanism optimized preserve the original prediction experiments graph classification and link prediction tasks our provides more intuitive faithful and actionable explanations compared existing gnn explainability techniques enhancing the transparency graph based deep learning,graph neural networks gnns are increasingly applied across various domains explainability has become critical factor for real world applications however these methods often lack human interpretability and not provide unified that incorporates both model level and instance level explanations domain experts can select the most relevant graphlets interpretable units and request unified explanations these units our decomposes gnn weights into orbit units extract class specific graph patterns model level and identify important subgraphs within individual data instances for prediction instance level,2025-08-26T02:23:21.430927
47,Task-Agnostic Pre-training of Vision Models with Masked Autoencoders on Video Data,"Large-scale self-supervised pre-training, particularly with Masked Autoencoders (MAEs), has revolutionized static image understanding. Extending this paradigm to video data promises even richer representations due to the added temporal dimension, but presents unique challenges like higher dimensionality and temporal consistency. We propose a novel Task-Agnostic Video MAE (TAV-MAE) for self-supervised pre-training of general-purpose video backbones. TAV-MAE randomly masks a high percentage of both spatial and temporal tokens in video clips and trains the model to reconstruct the missing patches. Our key innovations include an efficient video-specific masking strategy and a spatio-temporal decoder that effectively leverages the redundancy in video data. We demonstrate that TAV-MAE pre-trained on large unlabeled video datasets learns highly transferable representations. Fine-tuning on downstream tasks like action recognition, video object detection, and temporal localization yields state-of-the-art performance, often surpassing supervised pre-training and other video SSL methods, establishing a new foundation for video understanding.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2052,ViDROP: Video Dense Representation through Spatio-Temporal Sparsity,"Self-supervised learning (SSL) has revolutionized image processing, but extending its success to video understanding presents unique challenges due to increased data complexity and computational demands. We introduce ViDROP (Video Dense Representation thrOugh spatio-temporal sParsity), a novel SSL architecture for video understanding that combines token dropping and masking strategies. 
Our approach eliminates the need for a decoder and enables per-patch loss computation, overcoming limitations of previous video SSL methods. Moreover, we propose a simple yet effective video compression technique using k-means clustering in pixel space, significantly accelerating data loading and facilitating rapid experimentation. ViDROP demonstrates remarkable scalability across model sizes, from ViT-Small to ViT-Huge, when starting from pretrained models (VideoMAE or V-JEPA), achieving significant performance gains. Pushing the boundaries even further, we leverage network expansion techniques to successfully train ViT-Huge from scratch using modest computational resources, achieving comparable accuracy to VideoMAE 25$\times$ faster in training time. This marks a significant breakthrough in large-scale video SSL, enabling the training of state-of-the-art models with limited resources.
Extensive experiments show that ViDROP achieves state-of-the-art performance on various video understanding benchmarks, including Kinetics400, SSv2, UCF101, and HMDB51, as well as in temporal action detection (THUMOS14). These results highlight the effectiveness of our fine-grained token-level learning strategy in a domain traditionally dominated by fine-tuned SSL models, while enabling the training of large-scale models with limited computational resources.",ICLR.cc/2025/Conference,4.4,nan,0.8703,fine tuning downstream tasks like action recognition video object detection and temporal localization yields state the art often surpassing supervised pre training and other video ssl methods establishing foundation for video understanding,self supervised learning ssl has revolutionized image processing but extending its success video understanding presents unique challenges due increased data complexity and computational demands vidrop video dense representation spatio temporal sparsity ssl for video understanding that combines token dropping and masking strategies moreover simple yet effective video compression means clustering pixel space accelerating data loading and facilitating rapid experimentation pushing the boundaries even further leverage network expansion techniques train vit huge from scratch modest computational resources achieving comparable videomae times faster training time extensive experiments that vidrop achieves state the art various video understanding benchmarks including kinetics400 ssv2 ucf101 and hmdb51 well temporal action detection thumos14 these highlight the effectiveness our fine grained token level learning strategy domain traditionally dominated fine tuned ssl models while enabling the training large scale models limited computational resources,2025-08-26T02:23:21.430936
48,Probabilistic Programming for Deep Learning with Latent Variable Models,"Integrating structured probabilistic reasoning with the representational power of deep learning is a key step towards more robust and interpretable AI. While deep generative models (e.g., VAEs) use latent variables, they often lack the explicit compositional control and logical structure offered by probabilistic programming languages (PPLs). We propose Probabilistic Programming for Deep Latent Variable Models (PPDL), a novel framework that bridges deep learning and PPLs. PPDL allows users to define complex generative processes with structured latent variables using a PPL, while neural networks are embedded within the program to handle high-dimensional observations (e.g., images, text). Our approach combines automatic differentiation for neural parameters with amortized variational inference for latent variables, enabling efficient end-to-end learning of both. Experiments on tasks involving complex data generation, causal inference, and scene understanding demonstrate PPDL's ability to learn rich, interpretable latent structures and perform robust inference, offering a powerful paradigm for building deep models with symbolic priors.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2389,Variational Neuro-Symbolic Generative Temporal Point Process,"Temporal point processes (TPPs) are a powerful framework for modeling event sequences with irregular timestamps, such as those commonly found in electronic health records (EHR), which often involve high-dimensional and diverse event types. However, building generative models for such complex datasets comes with several challenges, including addressing sample inefficiency, accurately capturing intricate event patterns, and producing outputs that are both trustworthy and interpretable. In this paper, we present a neuro-symbolic generative model for TPPs based on the Variational Autoencoder (VAE) framework. Our model incorporates a neural-symbolic reasoning layer into the latent space, allowing it to integrate interpretable, logic-based constraints and perform logical reasoning over learned representations. This integration enhances the interpretability of the latent space by embedding logic rules directly into the generative process, enabling structured reasoning and improved decision-making based on underlying data patterns. We validate our model on an ICU EHR dataset, demonstrating its effectiveness in capturing complex event dynamics with irregular timestamps. In addition to improving sample efficiency and accuracy, our model supports the secure and interpretable generation of synthetic event data, making it a valuable tool for healthcare applications where reliability and trustworthiness are critical.",ICLR.cc/2025/Conference,4.25,nan,0.8446,integrating structured probabilistic reasoning the representational power deep learning key step towards more robust and interpretable while deep generative models probabilistic programming for deep latent variable models ppdl that bridges deep learning and ppls ppdl allows users define complex generative processes structured latent variables ppl while neural networks are embedded within the program handle high dimensional observations our combines automatic differentiation for neural parameters amortized variational inference for latent variables enabling efficient end end learning both experiments tasks involving complex data generation causal inference and scene understanding ppdl ability learn rich interpretable latent structures and perform robust inference offering powerful paradigm for building deep models symbolic priors,our incorporates neural symbolic reasoning layer into the latent space allowing integrate interpretable logic based constraints and perform logical reasoning over learned representations this integration enhances the interpretability the latent space embedding logic rules directly into the generative process enabling structured reasoning and improved decision making underlying data patterns addition improving sample efficiency and our supports the secure and interpretable generation synthetic event data making valuable tool for healthcare applications where reliability and trustworthiness are critical,2025-08-26T02:23:21.430945
49,Resource-Constrained Active Learning for Drug Discovery,"Active learning (AL) is vital in high-cost domains like drug discovery, where experimental evaluation of candidates is expensive and time-consuming. However, standard AL strategies often assume unlimited access to labels or fail to account for the heterogeneous costs and constraints (e.g., synthesis feasibility, assay availability) inherent in real-world discovery pipelines. We propose Resource-Constrained Active Learning (RCAL), a novel framework specifically tailored for drug discovery. RCAL integrates a multi-objective acquisition function that balances predictive uncertainty, diversity, and practical resource constraints (e.g., budget, time, chemical synthesis difficulty) when selecting candidates for experimental testing. Our approach employs a deep generative model to explore chemical space and a robust uncertainty estimation method. Experiments on virtual screening and de novo drug design tasks demonstrate RCAL significantly accelerates the discovery of potent drug candidates with fewer experimental queries, making the active learning paradigm more practical and effective for real-world scientific applications.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1092,Self-Informed Generative Active Learning,"Active learning has been a cost-efficient approach to obtaining high-performance AI models with fewer selective annotations. In scenarios where the acquisition of original unlabeled data poses significant challenges, active learning harnessing synthesized data instances is more promising than traditional pool-based methods. In this paper, we propose the Self-Informed Generative Active Learning (SIGnAL) framework as an effective solution to actively generate and select data instances for annotation and downstream model training. In SIGnAL, we propose to guide the data generation based on a reinforcement learning policy, where the generator is self-informed by the reward to generate more informative instances. In addition, we introduce an acquisition function that measures both the informativeness and relevance of instances. Such acquisition function can be transformed to the reward seamlessly for generator optimization. Our experiments on the text classification task validate the effectiveness of our framework, especially when the original data scale is limited.",ICLR.cc/2025/Conference,3.0,False,0.8476,active learning vital high cost domains like drug discovery where experimental evaluation candidates expensive and time consuming resource constrained active learning rcal tailored for drug discovery our employs deep generative chemical space and robust uncertainty estimation experiments virtual screening and novo drug tasks rcal accelerates the discovery potent drug candidates fewer experimental queries making the active learning paradigm more practical and effective for real world scientific applications,active learning has been cost efficient obtaining high performance models fewer selective annotations scenarios where the acquisition original unlabeled data poses significant challenges active learning harnessing synthesized data instances more promising than traditional pool based methods this the self informed generative active learning signal effective solution actively generate and select data instances for annotation and downstream training signal guide the data generation reinforcement learning policy where the generator self informed the reward generate more informative instances such acquisition function can transformed the reward seamlessly for generator optimization our experiments the text classification task the effectiveness our when the original data scale limited,2025-08-26T02:23:21.430949
50,Equivariant Multi-Agent Reinforcement Learning with Topological Graph Neural Networks,"Multi-agent reinforcement learning (MARL) in physical environments often involves agents with inherent symmetries (e.g., rotational, translational invariance for robots) and complex, dynamic relationships. Traditional MARL approaches struggle to exploit these symmetries, leading to inefficient learning and poor generalization. We propose Equivariant Multi-Agent Reinforcement Learning (EMARL), a novel framework that integrates topological graph neural networks (T-GNNs) into MARL policies. T-GNNs are designed to maintain equivariance to geometric transformations, ensuring that the agents' learned policies are robust to orientation or position changes. Our approach constructs a dynamic graph representing agent-agent and agent-environment interactions, where messages are passed using equivariant operations. This allows the agents to learn inherently symmetric and thus more efficient and generalizable coordination strategies. Experiments on challenging multi-robot control tasks (e.g., decentralized navigation, formation control) demonstrate EMARL significantly outperforms non-equivariant MARL baselines in terms of sample efficiency, convergence, and generalization to unseen environmental configurations, showcasing the power of symmetry-aware learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5680,POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding,"Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.",ICLR.cc/2025/Conference,5.666666666666667,True,0.9058,multi agent reinforcement learning marl physical environments often involves agents inherent symmetries traditional marl approaches struggle exploit these symmetries leading inefficient learning and poor generalization equivariant multi agent reinforcement learning emarl that integrates topological graph neural networks gnns into marl policies decentralized navigation formation control emarl outperforms non equivariant marl baselines terms sample efficiency convergence and generalization unseen environmental configurations showcasing the power symmetry aware learning,multi agent reinforcement learning marl has recently excelled solving challenging cooperative and competitive multi agent problems various environments involving small number agents and full observability however this domain remains difficult not impossible conduct fair comparison between classical learning based and hybrid approaches due the lack unified that supports both learning and evaluation,2025-08-26T02:23:21.430957
51,Neural Lyapunov Functions for Certified Stability of Deep Reinforcement Learning Policies,"Ensuring the stability and safety of deep reinforcement learning (DRL) policies in real-world control systems is critical, especially in safety-sensitive domains like autonomous driving or robotics. Current DRL methods primarily focus on optimizing performance, often lacking formal guarantees on system stability, leading to unpredictable behavior. We propose learning Neural Lyapunov Functions (NLFs) to provide certified stability for DRL policies. Our approach jointly trains a DRL policy and a neural network representing a Lyapunov function candidate. We incorporate stability constraints, derived from Lyapunov theory, directly into the DRL objective function using a differentiable formulation. This ensures that the learned policy drives the system towards a stable equilibrium while satisfying performance objectives. We provide theoretical analysis on the certified region of attraction. Experiments on continuous control tasks (e.g., inverted pendulum, quadrotor control) demonstrate that our method produces policies with provable local stability guarantees, significantly improving the trustworthiness and safety of DRL systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1740,On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning,"Deep Reinforcement Learning (DRL) policies are highly susceptible to adversarial noise in observations, which poses significant risks in safety-critical scenarios. The challenge inherent to adversarial perturbations is that by altering the information observed by the agent, the state becomes only partially observable. Existing approaches address this by either enforcing consistent actions across nearby states or maximizing the worst-case value within adversarially perturbed observations. However, the former suffers from performance degradation when attacks succeed, while the latter tends to be overly conservative, leading to suboptimal performance in benign settings. We hypothesize that these limitations stem from their failing to account for partial observability directly. To this end, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), defined on the beliefs about the true state and balancing value optimization with robustness. To make ACoE scalable in model-free settings, we propose the theoretically-grounded surrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on standard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method significantly outperforms current state-of-the-art approaches for addressing adversarial RL challenges, offering a promising direction for improving robustness in DRL under adversarial conditions. Our code is available at https://github.com/romanbelaire/acoe-robust-rl.",ICLR.cc/2025/Conference,6.0,True,0.8556,ensuring the stability and safety deep reinforcement learning drl policies real world control systems critical safety sensitive domains like autonomous driving robotics learning neural lyapunov functions nlfs provide certified stability for drl policies our jointly trains drl policy and neural network representing lyapunov function candidate,deep reinforcement learning drl policies are highly susceptible adversarial noise observations which poses significant risks safety critical scenarios this end objective called adversarial counterfactual error acoe defined the beliefs about the true state and balancing value optimization robustness our empirical evaluations standard benchmarks mujoco atari and highway that our outperforms current state the art approaches for addressing adversarial challenges offering promising direction for improving robustness drl under adversarial conditions,2025-08-26T02:23:21.430963
52,Coordinated Fine-Tuning of Multi-Head Attention for Interpretable Feature Aggregation,"Multi-head attention (MHA), a cornerstone of Transformer models, is crucial for capturing long-range dependencies but its internal workings often remain opaque. Understanding how different attention heads aggregate information is vital for model interpretability and debugging, particularly in complex tasks like natural language understanding or vision. We propose Coordinated Fine-Tuning (CFT) of MHA, a novel method that promotes interpretable feature aggregation by encouraging specialized roles for individual attention heads. CFT introduces a regularization objective during fine-tuning that encourages diversity and distinctness among attention heads' learned representations, while also promoting redundancy within groups of heads focusing on similar semantic features. This leads to a more structured and understandable MHA mechanism. We demonstrate that CFT yields models with improved interpretability (e.g., specific heads consistently focusing on syntactic roles, coreference, or object parts) without sacrificing performance on downstream tasks across NLP and vision, facilitating a deeper understanding of Transformer's internal representations.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8092,Towards Better Multi-head Attention via Channel-wise Sample Permutation,"Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing, whose effectiveness is mainly attributed to its multi-head attention (MHA) mechanism. 
In this study, we propose a simple and novel channel-wise sample permutation (CSP) operator, achieving a new structured MHA with fewer parameters and lower complexity. 
Given an input matrix, CSP circularly shifts the samples of different channels with various steps and then sorts grouped samples of each channel. 
This operator is equivalent to implicitly implementing cross-channel attention maps as permutation matrices, which achieves linear complexity and suppresses the risk of rank collapse when representing data. 
We replace the MHA of some representative models with CSP and test the CSP-based models in several discriminative tasks, including image classification and long sequence analysis. 
Experiments show that the CSP-based models achieve comparable or better performance with fewer parameters and lower computational costs than the classic Transformer and its state-of-the-art variants. 
The code is available at https://anonymous.4open.science/r/CSP-BA52.",ICLR.cc/2025/Conference,4.25,nan,0.8758,multi head attention mha cornerstone transformer models crucial for capturing long range dependencies but its internal workings often remain opaque understanding how different attention heads aggregate information vital for interpretability and debugging complex tasks like natural language understanding vision coordinated fine tuning cft mha that promotes interpretable feature aggregation encouraging specialized roles for individual attention heads cft introduces regularization objective during fine tuning that encourages diversity and distinctness among attention heads learned representations while also promoting redundancy within groups heads focusing similar semantic features that cft yields models improved interpretability specific heads consistently focusing syntactic roles coreference object parts sacrificing downstream tasks across nlp and vision facilitating deeper understanding transformer internal representations,transformer plays central role many fundamental deep learning models the vit computer vision and the bert and gpt natural language processing whose effectiveness mainly attributed its multi head attention mha mechanism this operator equivalent implicitly implementing cross channel attention maps permutation matrices which achieves linear complexity and suppresses the risk rank collapse when representing data replace the mha some representative models csp and the csp based models several discriminative tasks including image classification and long sequence analysis experiments that the csp based models achieve comparable better fewer parameters and lower computational costs than the classic transformer and its state the art variants,2025-08-26T02:23:21.430966
53,Implicit Neural Control Fields for Robotic Manipulation from Demonstrations,"Learning robotic manipulation policies from human demonstrations is intuitive but challenging, especially for complex, contact-rich tasks. Traditional methods often require extensive expert labeling or struggle with generalization to novel objects and environments. We propose Implicit Neural Control Fields (INCF), a novel approach that learns a continuous control policy as an implicit function, enabling robust and generalizable manipulation from sparse demonstrations. INCF represents the control policy as a mapping from task-relevant state variables to a distribution over actions, conditioned on a learned implicit representation of the environment and task goals. This implicit formulation allows for efficient querying of optimal actions at any continuous state, including unobserved ones. Our method learns a compact representation of the control flow directly from raw observation-action pairs. Experiments on diverse robotic manipulation benchmarks demonstrate that INCF achieves high success rates, superior generalization to novel objects and task configurations, and improved adaptability compared to explicit policy learning methods.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3662,Latent Diffusion Planning for Imitation Learning,"Recent progress in robotic imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods rely on supervised learning of actions from expert demonstrations, which can be challenging to scale. We propose Latent Diffusion Planning, which forecasts future states as well as actions via diffusion. This objective can scalably leverage heterogeneous data sources and provides a denser supervision signal for learning. To plan over images, we learn a compact latent space through a variational autoencoder. We then train a planner to forecast future latent states, and an inverse dynamics model to extract actions from the plans. As planning is separated from action prediction, LDP can leverage suboptimal or action-free data to improve performance in low demonstration regimes. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches as they cannot leverage such additional data.",ICLR.cc/2025/Conference,3.4,False,0.8460,learning robotic manipulation policies from human demonstrations intuitive but challenging for complex contact rich tasks implicit neural control fields incf that learns continuous control policy implicit function enabling robust and generalizable manipulation from sparse demonstrations incf represents the control policy mapping from task relevant state variables distribution over actions conditioned learned implicit representation the environment and task goals our learns compact representation the control flow directly from raw observation action pairs experiments diverse robotic manipulation benchmarks that incf achieves high success rates superior generalization objects and task configurations and improved adaptability compared explicit policy learning methods,recent progress robotic imitation learning has been enabled policy architectures that scale complex visuomotor tasks multimodal distributions and large datasets however these methods rely supervised learning actions from expert demonstrations which can challenging scale this objective can scalably leverage heterogeneous data sources and provides denser supervision signal for learning simulated visual robotic manipulation tasks ldp outperforms state the art imitation learning approaches they cannot leverage such additional data,2025-08-26T02:23:21.430973
54,Adversarial Robustness via Differentiable Energy-Based Priors on Latent Space,"Deep learning models are vulnerable to adversarial attacks, yet training robust models remains a major challenge. Many adversarial training methods focus on modifying the classifier, often leading to reduced clean accuracy or limited generalization. We propose a novel approach to Adversarial Robustness via Differentiable Energy-Based Priors (AREBP) on the latent representation space. AREBP trains a deep generative model to learn an energy-based prior over the latent embeddings of natural, in-distribution images. During adversarial training, this prior is used to regularize the latent space, pushing adversarial examples (even those crafted in pixel space) towards regions of high energy (i.e., natural image manifold) in the latent space. This implicit regularization guides the classifier to make more robust decisions. Experiments on CIFAR-10, SVHN, and ImageNet demonstrate AREBP achieves state-of-the-art robust accuracy against various $L_p$ attacks while preserving high clean accuracy, offering a powerful and complementary defense strategy.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4328,Randomized Feature Squeezing against  Unseen Attacks without Adversarial Training,"Deep learning has made tremendous progress in the last decades; however, it is not robust to adversarial attacks.  
Perhaps the most effective approach for this is adversarial training, although it is impractical as it needs prior knowledge about the attackers and incurs high computational costs.
In this paper, we propose a novel approach that can train a robust network only through standard training
with clean images without awareness of the attacker's strategy. We add a specially designed network input layer,
which accomplishes a randomized feature squeezing to reduce the malicious perturbation. 
It achieves the state of the art of robustness against unseen ${l_1,l_2}$ and $ {l_\infty} $ attacks at one time in terms of the computational cost of the attacker versus the defender through just 100/50 epochs of standard training with clean images in CIFAR-10/ImageNet. Both experiments and Rademacher complexity analysis validate the high performance. Moreover, it can also defend against the ``attacks"" on training data, i.e., unlearnable examples, seemingly being the only solution for the One-Pixel Shortcut without any data augmentation.",ICLR.cc/2025/Conference,4.75,nan,0.8394,deep learning models are vulnerable adversarial attacks yet training robust models remains major challenge adversarial robustness differentiable energy based priors arebp the latent representation space arebp trains deep generative learn energy based prior over the latent embeddings natural distribution images,deep learning has made tremendous progress the last decades however not robust adversarial attacks perhaps the most effective for this adversarial training although impractical needs prior knowledge about the attackers and incurs high computational costs this that can train robust network only standard training clean images awareness the attacker strategy add specially designed network input layer which accomplishes randomized feature squeezing reduce the malicious perturbation achieves the state the art robustness against unseen l_1 l_2 and infty attacks one time terms the computational cost the attacker versus the defender just epochs standard training clean images cifar imagenet,2025-08-26T02:23:21.430978
55,Spectral Graph Norm Regularization for Enhancing GNN Generalization,"Graph Neural Networks (GNNs) have shown remarkable performance, but their generalization capabilities, especially on graphs with varying topological structures, are not fully understood. Over-smoothing and limitations in capturing long-range dependencies can hinder performance. We propose Spectral Graph Norm Regularization (SGNR), a novel technique to improve GNN generalization by explicitly regularizing the spectral properties of learned graph representations. SGNR introduces a differentiable penalty on the graph spectral norm (e.g., a form of graph Laplacian or adjacency matrix norm) of the GNN's intermediate embeddings, effectively controlling the smoothness and connectivity characteristics of the learned features. This regularization encourages the GNN to learn more stable and discriminative representations that are robust to graph perturbations and capture a broader range of topological information. Experiments on diverse graph datasets (e.g., node classification, graph classification) demonstrate that SGNR consistently improves generalization performance and robustness compared to standard GNN training.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2786,GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning,"Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a novel adversarial attack framework that exploits the topological structure of graphs to undermine the integrity of GNN predictions systematically. 

By defining a topology-aware method to manipulate graph orbits, our approach generates adversarial modifications that are both subtle and effective, posing a severe test to the robustness of GNNs. We evaluate the efficacy of GOttack across multiple prominent GNN architectures using standard benchmark datasets. Our results show that GOttack outperforms existing state-of-the-art adversarial techniques and completes training in approximately 55% of the time required by the fastest competing model, achieving the highest average misclassification rate in 155 tasks. 
This work not only sheds light on the susceptibility of GNNs to structured adversarial attacks 
but also shows that certain topological patterns may play a significant role in the underlying robustness of the GNNs. Our Python implementation is shared at https://github.com/cakcora/GOttack.",ICLR.cc/2025/Conference,6.5,True,0.9240,graph neural networks gnns have shown remarkable but their generalization capabilities graphs varying topological structures are not fully understood node classification graph classification that sgnr consistently improves generalization and robustness compared standard gnn training,graph neural networks gnns have demonstrated superior node classification tasks across diverse applications defining topology aware manipulate graph orbits our generates adversarial modifications that are both subtle and effective posing severe the robustness gnns this not only sheds light the susceptibility gnns structured adversarial attacks but also shows that certain topological patterns may play significant role the underlying robustness the gnns,2025-08-26T02:23:21.430979
56,Large Language Models as Zero-Shot Hierarchical Planners for Complex Robotic Tasks,"Enabling robots to execute long-horizon, multi-step tasks in unstructured environments remains a significant challenge, often requiring extensive task-specific engineering or tedious human programming. We explore the emergent planning capabilities of Large Language Models (LLMs) in a zero-shot setting to synthesize hierarchical plans for complex robotic tasks. Our approach leverages the LLM's vast world knowledge and reasoning abilities to decompose high-level natural language instructions into a sequence of executable sub-goals, which are then translated into low-level robot actions using a pre-defined skill library. Critically, the LLM is used to dynamically adapt and refine the plan based on real-time feedback from the robot's environment, demonstrating closed-loop control. We show that LLMs can generate coherent, executable, and robust hierarchical plans for tasks involving object manipulation, navigation, and interaction, significantly reducing the need for task-specific engineering. This work bridges the gap between high-level human intent and low-level robot control, paving the way for more intuitive and versatile robotic systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6784,R2C: Mapping Room to Chessboard to Unlock LLM As Low-Level Action Planner,"This paper explores the potential of leveraging large language models (LLMs) as low-level action planners capable of executing long-horizon tasks based on natural language instructions. Although LLMs can act as the ""brain"" of robots by excelling in high-level task planning, they are not yet capable of directly guiding the ""body"" to execute low-level motion plans. This limitation stems from a communication gap between the ""brain"" and the ""body"". Specifically, LLMs lack access to rich spatial semantic information from the robot's real-time observations, hindering their ability to generate precise and actionable low-level plans.To address this, we propose a unified framework that bridges high-level and low-level planning by establishing an efficient communication interface between LLMs and robots. Our insight is to formulate the task as playing chess with LLMs. We map the room into a semantic chessboard, which we call Room to Chessboard (R2C). Each grid represents the position and size of objects inside the room. We find that chessboard is \textbf{succinct} enough for LLMs to conduct semantic searches with global view of the room. Also, the chessboard is \textbf{informative} enough to convey detailed environmental state for LLMs to predict executable low-level actions. Additionally, we enhance decision-making through a Chain-of-Thought (CoT) paradigm, improving LLMs' interpretability and action reasoning. We implement R2C using both fine-tuned open-source LLMs and closed-source models like GPT-4, and demonstrate its efficacy on the challenging ALFRED benchmark. Our results show that with communication based on chessboard, LLMs can serve as effective low-level action planners, and can generalizes well to open-vocabulary robotic planning tasks. View the demos on our project page: https://anonymous4cv.github.io/Room2Chessboard.",ICLR.cc/2025/Conference,4.25,nan,0.8889,the emergent planning capabilities large language models llms zero shot setting synthesize hierarchical plans for complex robotic tasks our leverages the llm vast world knowledge and reasoning abilities decompose high level natural language instructions into sequence executable sub goals which are then translated into low level robot actions pre defined skill library,this explores the potential leveraging large language models llms low level action planners capable executing long horizon tasks natural language instructions llms lack access rich spatial semantic information from the robot real time observations hindering their ability generate precise and actionable low level plans map the room into semantic chessboard which call room chessboard r2c find that chessboard textbf succinct enough for llms conduct semantic searches global view the room additionally enhance decision making chain thought cot paradigm improving llms interpretability and action reasoning,2025-08-26T02:23:21.430983
57,Differentiable Causal Discovery from Interventional Time-Series Data,"Inferring causal relationships from observational and interventional time-series data is crucial for understanding complex systems in science and engineering. Existing causal discovery methods often struggle with temporal dynamics, scalability to high dimensions, and integrating diverse interventional signals effectively. We propose Differentiable Causal Discovery (DCD), a novel framework that learns causal graphs directly from time-series data, incorporating interventional evidence in a principled, differentiable manner. DCD parameterizes the causal graph as an adjacency matrix and learns it end-to-end using a continuous optimization approach that minimizes prediction error under interventions, regularized by acyclicity constraints. Our method leverages a deep neural network to model the functional relationships between variables. Experiments on simulated and real-world time-series datasets (e.g., neuroscience, climate modeling) demonstrate DCD's ability to accurately recover ground-truth causal graphs and infer causal effects, outperforming state-of-the-art causal discovery methods in terms of accuracy and scalability.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7158,Gradient based Causal Discovery with Diffusion Model,"Causal discovery from observational data is an important problem in many applied sciences. Incorporating a recently proposed smooth characterization of acyclicity, gradient-based causal discovery approaches search for a Directed Acyclic Graph (DAG) by optimizing various neural models. Although they show some inspiring results given certain assumptions satisfied, their capability of modeling complex nonlinear causal generative functions is still unsatisfactory. Motivated by recent advances in deep generative models, we propose to use diffusion models for causal discovery,  and search for the DAG under continuous optimization frameworks. The underlying nonlinear causal generative process is modeled with diffusion process, and with flexible parameter configurations, it has the ability to represent various functions, and the proposed causal discovery approach are able to generate graphs with satisfactory accuracy on  observational data generated by either linear or nonlinear causal models. This is evidenced by empirical results on both synthetic and real data.",ICLR.cc/2025/Conference,5.0,False,0.8220,dcd parameterizes the causal graph adjacency matrix and learns end end continuous optimization that minimizes prediction error under interventions regularized acyclicity constraints our leverages deep neural network the functional relationships between variables,incorporating recently proposed smooth characterization acyclicity gradient based causal discovery approaches search for directed acyclic graph dag optimizing various neural models motivated recent advances deep generative models use diffusion models for causal discovery and search for the dag under continuous optimization frameworks,2025-08-26T02:23:21.430991
58,Continual Meta-Reinforcement Learning for Rapid Skill Acquisition in Open-Ended Environments,"Embodied agents operating in open-ended environments must continually learn new tasks and adapt to novel situations without forgetting previous knowledge – a challenge for both continual learning and reinforcement learning. We propose Continual Meta-Reinforcement Learning (CMRL), a novel framework that enables agents to rapidly acquire new skills and adapt to unseen tasks in a lifelong learning setting. CMRL combines meta-learning for fast task adaptation with a memory-efficient continual learning strategy based on task-specific parameter regularization. The agent learns a meta-prior over policy parameters that facilitates rapid learning of new tasks, while a carefully designed elastic weight consolidation mechanism prevents catastrophic forgetting by prioritizing parameter importance for previously learned skills. Experiments in diverse simulated robotic and gaming environments demonstrate that CMRL significantly outperforms existing meta-RL and continual RL methods, achieving superior performance on novel tasks and better knowledge retention across a sequence of distinct tasks, paving the way for truly adaptive AI agents.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,9375,ARC-RL: Self-Evolution Continual Reinforcement Learning via Action Representation Space,"Continual Reinforcement Learning (CRL) is a powerful tool that enables agents to learn a sequence of tasks, accumulating knowledge learned in the past and using it for problemsolving or future task learning. However, existing CRL methods all assume that the agent’s capabilities remain static within dynamic environments, which doesn’t reflect realworld scenarios where capabilities evolve. This paper introduces *Self-Evolution Continual Reinforcement Learning* (SE-CRL), a new and realistic problem where the agent’s action space continually changes. It presents a significant challenge for RL agents: How can policy generalization across different action spaces be achieved? Inspired by the cortical functions that lead to consistent human behavior, we propose an **A**ction **R**epresentation **C**ontinual **R**einforcement **L**earning framework (ARC-RL) to address this challenge. Our framework builds a representation space for actions by self-supervised learning on transitions, decoupling the agent’s policy from the specific action space. For a new action space, the decoder of the action representation is expanded or masked for adaptation and regularized fine-tuned to improve the stability of the policy. Furthermore, we release a benchmark based on MiniGrid to validate the effectiveness of methods for SE-CRL. Experimental results demonstrate that our framework significantly outperforms popular CRL methods by generalizing the policy across different action spaces.",ICLR.cc/2025/Conference,3.75,nan,0.8740,embodied agents operating open ended environments must continually learn tasks and adapt situations forgetting previous knowledge challenge for both continual learning and reinforcement learning continual meta reinforcement learning cmrl that enables agents rapidly acquire skills and adapt unseen tasks lifelong learning setting cmrl combines meta learning for fast task adaptation memory efficient continual learning strategy task specific parameter regularization the agent learns meta prior over policy parameters that facilitates rapid learning tasks while carefully designed elastic weight consolidation mechanism prevents catastrophic forgetting prioritizing parameter importance for previously learned skills experiments diverse simulated robotic and gaming environments that cmrl outperforms existing meta and continual methods achieving superior tasks and better knowledge retention across sequence distinct tasks paving the way for truly adaptive agents,continual reinforcement learning crl powerful tool that enables agents learn sequence tasks accumulating knowledge learned the past and for problemsolving future task learning this introduces self evolution continual reinforcement learning crl and realistic problem where the agent action space continually changes our builds representation space for actions self supervised learning transitions decoupling the agent policy from the specific action space for action space the decoder the action representation expanded masked for adaptation and regularized fine tuned improve the stability the policy,2025-08-26T02:23:21.430999
59,Feature-Wise Quantization for Fine-Grained On-Device Deep Learning,"Deploying deep neural networks on edge devices with stringent memory and computational constraints necessitates aggressive quantization. However, current quantization methods often apply uniform quantization across all features or layers, overlooking the diverse sensitivity of different features to precision reduction. We propose Feature-Wise Quantization (FWQ), a novel approach that adapts quantization levels and schemes (e.g., symmetric/asymmetric, clipping ranges) to individual features or groups of features within each layer. FWQ learns a differentiable quantization policy for each feature dimension during training, allowing the model to dynamically determine the optimal bit-width and quantization parameters. This fine-grained control minimizes information loss and maximizes accuracy under extreme low-bit constraints. Experiments on various vision and NLP models demonstrate that FWQ achieves significantly higher accuracy for a given bit-width budget compared to uniform or per-layer quantization methods, enabling more efficient deployment of high-performing models on resource-limited edge devices.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10647,SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning,"How can we accurately quantize a pre-trained model without any data?
Quantization algorithms are widely used for deploying neural networks on resource-constrained edge devices.
Zero-shot Quantization (ZSQ) addresses the crucial and practical scenario where training data are inaccessible for privacy or security reasons.
However, three significant challenges hinder the performance of existing ZSQ methods: 1) noise in the synthetic dataset, 2) predictions based on off-target patterns, and the 3) misguidance by erroneous hard labels.
In this paper, we propose SynQ (Synthesis-aware Fine-tuning for Zero-shot Quantization),
a carefully designed ZSQ framework to overcome the limitations of existing methods.
SynQ minimizes the noise from the generated samples by exploiting a low-pass filter.
Then, SynQ trains the quantized model to improve accuracy by aligning its class activation map with the pre-trained model.
Furthermore, SynQ mitigates misguidance from the pre-trained model's error by leveraging only soft labels for difficult samples.
Extensive experiments show that SynQ provides the state-of-the-art accuracy, over existing ZSQ methods.",ICLR.cc/2025/Conference,6.5,True,0.8539,deploying deep neural networks edge devices stringent memory and computational constraints necessitates aggressive quantization fwq learns differentiable quantization policy for each feature dimension during training allowing the dynamically determine the optimal bit width and quantization parameters experiments various vision and nlp models that fwq achieves higher for given bit width budget compared uniform per layer quantization methods enabling more efficient deployment high performing models resource limited edge devices,quantization algorithms are used for deploying neural networks resource constrained edge devices zero shot quantization zsq addresses the crucial and practical scenario where training data are inaccessible for privacy security reasons this synq synthesis aware fine tuning for zero shot quantization carefully designed zsq overcome the limitations existing methods,2025-08-26T02:23:21.431000
60,Multi-Fidelity Bayesian Optimization for Neural Architecture Search,"Neural Architecture Search (NAS) is highly effective but computationally expensive, often requiring thousands of GPU hours. Bayesian Optimization (BO) is a promising approach for NAS, but standard BO can still be slow due to the high cost of evaluating candidate architectures. We propose Multi-Fidelity Bayesian Optimization (MF-BO) for NAS, which significantly accelerates the search process by leveraging cheaper, low-fidelity evaluations. MF-BO uses a probabilistic model that learns the correlation between performance at different fidelities (e.g., training epochs, dataset subsets, proxy tasks). This allows the optimizer to intelligently allocate computational resources, prioritizing full-fidelity evaluations only for the most promising architectures identified through cheap, low-fidelity assessments. Experiments on standard NAS benchmarks (e.g., NAS-Bench-101/201, DARTS search spaces) demonstrate that MF-BO finds high-performing architectures with orders of magnitude fewer computational resources than single-fidelity BO or other NAS methods, making advanced architecture search more accessible.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4725,Evaluating Ranking Loss Functions in Performance Predictor for NAS,"Performance evaluation is a critical but compute-intensive procedure in neural architecture search (NAS). To alleviate evaluation costs, performance predictors have been widely adopted to predict architecture performance directly. Recent studies have introduced ranking loss functions into predictors to focus on the architecture rankings instead of absolute accuracy, thus enhancing the ranking ability of performance predictors. Despite the successful application of ranking loss functions, the lack of comprehensive measure metrics and different experimental configurations make a fair comparison among these loss functions a huge challenge. Additionally, some well-known ranking loss functions have not been thoroughly examined in the context of performance predictors. In this paper, we conduct the first study for 11 ranking loss functions containing the existing and the novel ones by comparing their effectiveness in performance predictors under various settings. We find that: (i) The choice of ranking loss function has a major influence on the performance of predictors; (ii) the quality of the architectures searched by the predictor-based NAS methods is closely correlated with the predictor's performance on top-centered rank metrics, rather than traditional metrics like Kendall Tau. We believe these results and insights can serve as recommendations for the optimal loss function to employ in predictors across various search spaces and experimental conditions.",ICLR.cc/2025/Conference,4.0,nan,0.8656,neural search nas highly effective but computationally expensive often requiring thousands gpu hours bayesian optimization promising for nas but standard can still slow due the high cost evaluating candidate architectures multi fidelity bayesian optimization for nas which accelerates the search process leveraging cheaper low fidelity evaluations,evaluation critical but compute intensive procedure neural search nas,2025-08-26T02:23:21.431006
61,Temporal Graph Transformers for Long-Term Spatio-Temporal Prediction,"Long-term spatio-temporal prediction (e.g., traffic forecasting, weather prediction) is challenging due to complex non-linear dependencies across both space and time, and the need to capture long-range interactions. Existing GNNs often struggle with temporal dynamics, while recurrent models fail to capture long-term dependencies. We propose Temporal Graph Transformers (TGT), a novel architecture for robust long-term spatio-temporal forecasting. TGT combines a spatial graph attention mechanism to model inter-node dependencies with a temporal Transformer encoder-decoder to capture long-range temporal patterns. Our key innovation is a spatio-temporal positional encoding and a cross-attention mechanism that adaptively combines information from historical time steps and neighbor nodes. Experiments on real-world spatio-temporal datasets (e.g., traffic speed prediction on metropolitan networks, air quality forecasting) demonstrate TGT significantly outperforms state-of-the-art graph-based and recurrent forecasting models, achieving higher accuracy and better robustness for long-term predictions.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6855,Geometric Spatiotemporal Transformer to Simulate Long-Term Physical Dynamics,"Physical dynamics simulation plays a crucial role in various real-world applications. In this paper, we explore the potential of leveraging Transformers by framing the task as autoregressive next-graph prediction based on spatiotemporal graph inputs. To achieve this, we propose Geometric Spatiotemporal Transformers (GSTs), which adopt the expressive encoder-decoder architecture of traditional Transformers. At the core of GSTs are equivariant spatiotemporal blocks that alternate between spatial and temporal modules while preserving E(3) symmetries. Additionally, we introduce the Temporal Difference Graph (TDG), derived from the difference between the last two frames of historical input, to capture global dynamic patterns and mitigate cumulative errors in long-term prediction tasks. Unlike existing Graph Neural Network (GNN) methods, GSTs can process full input sequences of arbitrary lengths to effectively capture long-term context, and address cumulative errors over long-term rollouts thanks to the TDG mechanism. Our method achieves state-of-the-art  performance across multiple challenging physical systems at various scales (molecular-, protein-, and macro-level), demonstrating the robust dynamics simulation capabilities.",ICLR.cc/2025/Conference,5.0,False,0.8693,long term spatio temporal prediction tgt combines spatial graph attention mechanism inter node dependencies temporal transformer encoder decoder capture long range temporal patterns traffic speed prediction metropolitan networks air quality forecasting tgt outperforms state the art graph based and recurrent forecasting models achieving higher and better robustness for long term predictions,this the potential leveraging transformers framing the task autoregressive next graph prediction spatiotemporal graph inputs additionally the temporal difference graph tdg derived from the difference between the last two frames historical input capture global dynamic patterns and mitigate cumulative errors long term prediction tasks unlike existing graph neural network gnn methods gsts can process full input sequences arbitrary lengths capture long term context and address cumulative errors over long term rollouts thanks the tdg mechanism,2025-08-26T02:23:21.431010
62,Compositional Generative Models for Multi-Object Scene Synthesis with Relational Priors,"Generating complex, multi-object scenes with specific spatial and relational properties remains a challenge for deep generative models. Existing methods often struggle with compositional control, explicitly defining relationships between objects, or generalizing to novel object configurations. We propose Compositional Generative Models (CGM) for multi-object scene synthesis, leveraging relational priors. CGM explicitly models individual objects as components and learns their appearance and pose using a set-based latent representation. Crucially, a relational network within the generative process enforces user-defined or learned spatial and semantic relationships between these objects (e.g., ""A is on top of B,"" ""C is next to D""). This allows for precise, compositional control over scene generation, enabling the creation of diverse scenes that adhere to specified constraints. Experiments demonstrate CGM's ability to synthesize high-fidelity images with controllable object layouts and relationships, outperforming scene generation baselines in terms of compositional accuracy and diversity.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,63,"Field-DiT: Diffusion Transformer on Unified Video, 3D, and Game Field Generation","The probabilistic field models the distribution of continuous functions defined over metric spaces. While these models hold great potential for unifying data generation across various modalities, including images, videos, and 3D geometry, they still struggle with long-context generation beyond simple examples. This limitation can be attributed to their MLP architecture, which lacks sufficient inductive bias to capture global structures through uniform sampling. To address this, we propose a new and simple model that incorporates a view-wise sampling algorithm to focus on local structure learning, along with autoregressive generation to preserve global geometry. It adapts cross-modality conditions, such as text prompts for text-to-video generation, camera poses for 3D view generation, and control actions for game generation. Experimental results across various modalities demonstrate the effectiveness of our model, with its 675M parameter size, and highlight its potential as a foundational framework for scalable, architecture-unified visual content generation for different modalities with different weights. Our project page can be found at https://kfmei.com/Field-DiT/.",ICLR.cc/2025/Conference,5.0,True,0.8389,generating complex multi object scenes specific spatial and relational properties remains challenge for deep generative models cgm explicitly models individual objects components and learns their appearance and pose set based latent representation crucially relational network within the generative process enforces user defined learned spatial and semantic relationships between these objects experiments cgm ability synthesize high fidelity images controllable object layouts and relationships outperforming scene generation baselines terms compositional and diversity,while these models hold great potential for unifying data generation across various modalities including images videos and geometry they still struggle long context generation beyond simple examples address this and simple that incorporates view wise sampling focus local structure learning along autoregressive generation preserve global geometry adapts cross modality conditions such text prompts for text video generation camera poses for view generation and control actions for game generation experimental across various modalities the effectiveness our its 675m parameter size and highlight its potential foundational for scalable architecture unified visual content generation for different modalities different weights,2025-08-26T02:23:21.431016
63,Diffusion Models as Implicit Regularizers for Few-Shot Classification,"Few-shot image classification is a challenging problem where models must learn to generalize from extremely limited labeled data. Existing methods often rely on meta-learning or strong inductive biases. We propose leveraging pre-trained Diffusion Models (DMs) as implicit regularizers for few-shot classification. Our approach fine-tunes a classifier on the few-shot dataset while simultaneously using the diffusion model to guide the learning process. The DM implicitly defines a manifold of natural images, and we regularize the classifier's decision boundaries to align with this manifold, preventing overfitting to scarce data and encouraging generalization. This is achieved by penalizing classification errors on synthetic samples generated by the DM that are close to the decision boundary or by using the DM's score function as a consistency regularization. Experiments on standard few-shot benchmarks (e.g., MiniImageNet, TieredImageNet) demonstrate that this diffusion-regularized approach achieves state-of-the-art performance, showcasing a novel way to leverage powerful generative priors for robust few-shot learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8573,EXPLORING FEW-SHOT IMAGE GENERATION WITH MINIMIZED RISK OF OVERFITTING,"Few-shot image generation (FSIG) using deep generative models (DGMs) presents a significant challenge in accurately estimating the distribution of the target domain with extremely limited samples. Recent work has addressed the problem using a transfer learning approach, i.e., fine-tuning, leveraging a DGM that pre-trained on a large-scale source domain dataset, and then adapting it to the target domain with very limited samples. However, despite various proposed regularization techniques, existing frameworks lack a systematic mechanism to analyze the degree of overfitting, relying primarily on empirical validation without rigorous theoretical grounding.
We present Few-Shot Diffusion-regularized Representation Learning (FS-DRL), an innovative approach designed to minimize the risk of over-fitting while preserving distribution consistency in target image adaptation. 
Our method is distinct from conventional methods in two aspects: First, instead of fine-tuning, FS-DRL employs a novel scalable Invariant Guidance Matrix (IGM) during the diffusion process, which acts as a regularizer in the feature space of the model. This IGM is designed to have the same dimensionality as the target images, effectively constraining its capacity and encouraging it to learn a low-dimensional manifold that captures the essential structure of the target domain. Second, our method introduces a controllable parameter called sharing degree, which determines how many target images correspond to each IGM, enabling a fine-grained balance between overfitting risk and model flexibility, thus providing a quantifiable mechanism to analyze and mitigate overfitting.
Extensive experiments demonstrate that our approach effectively mitigates overfitting, enabling efficient and robust few-shot learning across diverse domains.",ICLR.cc/2025/Conference,5.0,nan,0.9030,few shot image classification challenging problem where models must learn generalize from extremely limited labeled data leveraging pre trained diffusion models dms implicit regularizers for few shot classification our fine tunes classifier the few shot while simultaneously the diffusion guide the learning process this achieved penalizing classification errors synthetic samples generated the that are close the decision boundary the function consistency regularization experiments standard few shot benchmarks miniimagenet tieredimagenet that this diffusion regularized achieves state the art showcasing way leverage powerful generative priors for robust few shot learning,few shot image generation fsig deep generative models dgms presents significant challenge accurately estimating the distribution the target domain extremely limited samples recent has addressed the problem transfer learning fine tuning leveraging dgm that pre trained large scale source domain and then adapting the target domain very limited samples few shot diffusion regularized representation learning drl innovative designed minimize the risk over fitting while preserving distribution consistency target image adaptation our distinct from conventional methods two aspects first instead fine tuning drl employs scalable invariant guidance matrix igm during the diffusion process which acts regularizer the feature space the this igm designed have the same dimensionality the target images constraining its capacity and encouraging learn low dimensional manifold that captures the essential structure the target domain extensive experiments that our mitigates overfitting enabling efficient and robust few shot learning across diverse domains,2025-08-26T02:23:21.431027
64,Self-Supervised Learning of Disentangled Graph Representations for Molecular Property Prediction,"Predicting molecular properties is crucial for drug discovery and materials science, but often requires extensive labeled data. Graph Neural Networks (GNNs) are powerful for molecular representation, but their performance can be limited by the quality of supervised data and the interpretability of learned features. We propose a novel Self-Supervised Learning (SSL) framework for learning Disentangled Graph Representations (SDGR) in molecules. SDGR identifies and separates latent factors corresponding to distinct molecular properties (e.g., functional groups, structural motifs, electronic properties) without requiring explicit labels for these factors during pre-training. Our approach combines contrastive learning with an information-theoretic disentanglement objective. This results in highly interpretable and robust representations. Experiments demonstrate that models pre-trained with SDGR achieve superior performance on various downstream molecular property prediction tasks (e.g., toxicity, solubility, quantum properties) with fewer labeled examples, also providing better interpretability of the learned features.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6711,"A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules","Graph neural networks (GNNs) -- learn graph representations by exploiting the graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine the general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks. Code is available at https://github.com/shihhsinwang0214/SCHull.",ICLR.cc/2025/Conference,8.0,True,0.8899,graph neural networks gnns are powerful for molecular representation but their can limited the quality supervised data and the interpretability learned features self supervised learning ssl for learning disentangled graph representations sdgr molecules our combines contrastive learning information theoretic disentanglement objective experiments that models pre trained sdgr achieve superior various downstream molecular property prediction tasks toxicity solubility quantum properties fewer labeled examples also providing better interpretability the learned features,graph neural networks gnns learn graph representations exploiting the graph sparsity connectivity and symmetries have become indispensable for learning geometric data like molecules,2025-08-26T02:23:21.431033
65,Knowledge Editing for Large Language Models via Orthogonal Subspace Projection,"Large Language Models (LLMs) are powerful but prone to storing outdated or incorrect information, and correcting these ""knowledge errors"" without degrading overall performance or introducing new biases is a critical challenge. Fine-tuning is expensive and often leads to catastrophic forgetting. We propose Knowledge Editing via Orthogonal Subspace Projection (KE-OSP), a parameter-efficient method for updating the factual knowledge of LLMs. KE-OSP identifies the specific parameter subspace responsible for encoding a piece of knowledge and modifies it while projecting the update orthogonally to other task-relevant subspaces. This ensures that only the targeted knowledge is altered, preserving the model's performance on unrelated tasks and avoiding interference with existing knowledge. Our method is memory-efficient and requires only a small number of updated parameters. Experiments demonstrate that KE-OSP successfully edits factual knowledge in pre-trained LLMs (e.g., Llama, GPT-J) with minimal impact on general capabilities and robustness, providing a practical solution for knowledge management in LLMs.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6209,O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing,"Large language models (LLMs) acquire knowledge during pre-training, but over time, this knowledge may become incorrect or outdated, necessitating updates after training. Knowledge editing techniques address this issue without the need for costly re-training. However, most existing methods are designed for single edits, and as the number of edits increases, they often cause a decline in the model's overall performance, posing significant challenges for sequential editing. To overcome this, we propose Orthogonal Subspace Editing, O-Edit. This algorithm orthogonalizes the direction of each knowledge update, minimizing interference between successive updates and reducing the impact of new updates on unrelated knowledge. Our approach does not require replaying previously edited data and processes each edit knowledge on time. It can perform thousands of edits on mainstream LLMs, achieving an average performance improvement that is 4.2 times better than existing methods while effectively preserving the model's performance on downstream tasks, all with minimal additional parameter overhead.",ICLR.cc/2025/Conference,5.666666666666667,False,0.9191,large language models llms are powerful but prone storing outdated incorrect information and correcting these knowledge errors degrading overall introducing biases critical challenge knowledge editing orthogonal subspace projection osp parameter efficient for updating the factual knowledge llms osp identifies the specific parameter subspace responsible for encoding piece knowledge and modifies while projecting the update orthogonally other task relevant subspaces this ensures that only the targeted knowledge altered preserving the model unrelated tasks and avoiding interference existing knowledge experiments that osp edits factual knowledge pre trained llms llama gpt minimal impact general capabilities and robustness providing practical solution for knowledge management llms,large language models llms acquire knowledge during pre training but over time this knowledge may become incorrect outdated necessitating updates after training knowledge editing techniques address this issue the need for costly training this orthogonalizes the direction each knowledge update minimizing interference between successive updates and reducing the impact updates unrelated knowledge our does not require replaying previously edited data and processes each edit knowledge time,2025-08-26T02:23:21.431037
66,Online Sparse Training with Dynamic Pruning and Growth for Continual Learning,"Online continual learning, where models sequentially learn from a stream of data, faces the dilemma of catastrophic forgetting and limited model capacity. While sparse training offers efficient resource utilization, applying it effectively in an online continual learning setting with dynamic sparsity patterns is challenging. We propose Online Sparse Training with Dynamic Pruning and Growth (OST-DPG), a novel method for continually learning sparse networks. OST-DPG dynamically prunes less important connections and grows new ones for incoming tasks, adapting the network's capacity and structure without increasing total parameter count. Our approach uses a differentiable criterion to identify task-relevant connections for pruning and a learned re-initialization strategy for growth, ensuring task-specific sparsity. Experiments on various online class-incremental benchmarks demonstrate that OST-DPG significantly reduces catastrophic forgetting and improves accuracy compared to dense continual learning methods, achieving competitive performance with a fixed sparse budget, offering a scalable solution for lifelong learning on resource-constrained devices.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10307,Online Continual Graph Learning,"The aim of Continual Learning (CL) is to learn new tasks incrementally while avoiding catastrophic forgetting. Online Continual Learning (OCL) specifically focuses on learning efficiently from a continuous stream of data with shifting distribution. While recent studies explore Continual Learning on graphs exploiting Graph Neural Networks (GNNs), only few of them focus on a streaming setting.  Many real-world graphs evolve over time and timely (online) predictions could be required. However, current approaches are not well aligned with the standard OCL literature, partly due to the lack of a clear definition of online continual learning on graphs. In this work, we propose a general formulation for online continual learning on graphs, emphasizing the efficiency of batch processing while accounting for graph topology, providing a grounded setting to analyze different methods. We present a set of benchmark datasets for online continual graph learning, together with the results of several methods in CL literature, adapted to our setting. Additionally, we address the challenge of GNN memory usage, as considering multiple hops of neighborhood aggregation can require access to the entire growing graph, resulting in prohibitive costs for the setting. We thus propose solutions to maintain bounded complexity for efficient online learning.",ICLR.cc/2025/Conference,5.0,False,0.8336,while sparse training offers efficient resource utilization applying online continual learning setting dynamic sparsity patterns challenging online sparse training dynamic pruning and growth ost dpg for continually learning sparse networks experiments various online class incremental benchmarks that ost dpg reduces catastrophic forgetting and improves compared dense continual learning methods achieving competitive fixed sparse budget offering scalable solution for lifelong learning resource constrained devices,the aim continual learning learn tasks incrementally while avoiding catastrophic forgetting online continual learning ocl focuses learning from continuous stream data shifting distribution while recent studies continual learning graphs exploiting graph neural networks gnns only few them focus streaming setting however current approaches are not well aligned the standard ocl literature partly due the lack clear definition online continual learning graphs this general formulation for online continual learning graphs emphasizing the efficiency batch processing while accounting for graph topology providing grounded setting different methods thus solutions maintain bounded complexity for efficient online learning,2025-08-26T02:23:21.431043
67,Hierarchical Deep Equilibrium Models for Multi-Scale Vision Tasks,"Deep Equilibrium Models (DEQs) offer an intriguing alternative to traditional deep networks by implicitly defining infinite-depth computations as the fixed point of a single layer. However, extending DEQs to multi-scale vision tasks (e.g., high-resolution image processing, dense prediction) is challenging, as a single fixed point often struggles to capture information across vastly different scales. We propose Hierarchical Deep Equilibrium Models (HDEQ), a novel architecture that captures multi-scale information by introducing a cascade of interacting DEQ layers, each operating at a different resolution. Each DEQ in the hierarchy computes its own fixed point, and these fixed points are implicitly coupled through multi-scale feature propagation, allowing for efficient information flow across scales. HDEQ achieves superior performance on dense prediction tasks (e.g., semantic segmentation, depth estimation) and high-resolution image processing, demonstrating that hierarchical fixed-point computation can effectively handle multi-scale visual data while retaining the benefits of DEQs.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6549,Faster Gradient Descent in Deep Linear Networks: The Advantage of Depth,"Gradient descent dynamics in deep linear networks has been studied under a wide range of settings. These studies have reported some negative results on the role of depth, in that, gradient descent in  deep linear networks: (i) can take exponential number of iterations to converge, (ii) can exhibit sigmoidal learning, i.e., almost no learning in initial phase followed by rapid learning, (iii) can delay convergence with increase in depth. Some of these results are also under stronger assumptions such as whitened data and balanced initialisation. These messages from prior works suggest that depth hurts the speed of convergence.

In this paper, we argue that the negative role of depth in the prior works is due to certain pitfalls which can be carefully avoided. We give a positive message on the role of depth, i.e., seen as an additional resource, depth can always be used to speed up convergence. For this purpose, we consider scalar regression with quadratic loss. In this setting, we propose a novel aligned gradient descent (AGD) algorithm for which we show that (i) linear convergence is always possible (ii) depth accelerates the speed of convergence. In AGD, feature alignment happens in first layer and the deeper layers accelerate by learning the right scale. We show acceleration in AGD happens in finite time for unwhitened data. We provide insights into the {acceleration} mechanism and also show that acceleration happens in phases. We also demonstrate the acceleration due to AGD on synthetic and benchmark datasets. Our main message is not propose AGD as a new algorithm in itself, but to demonstrate that depth is an advantage in linear networks thereby dispelling some of the past negative results on the role of depth.",ICLR.cc/2025/Conference,2.3333333333333335,False,0.8125,deep equilibrium models deqs offer intriguing alternative traditional deep networks implicitly defining infinite depth computations the fixed point single layer however extending deqs multi scale vision tasks hierarchical deep equilibrium models hdeq that captures multi scale information introducing cascade interacting deq layers each operating different resolution each deq the hierarchy computes its own fixed point and these fixed points are implicitly coupled multi scale feature propagation allowing for efficient information flow across scales hdeq achieves superior dense prediction tasks semantic segmentation depth estimation and high resolution image processing demonstrating that hierarchical fixed point computation can handle multi scale visual data while retaining the benefits deqs,gradient descent dynamics deep linear networks has been studied under wide range settings these studies have reported some negative the role depth that gradient descent deep linear networks can take exponential number iterations converge can exhibit sigmoidal learning almost learning initial phase followed rapid learning iii can delay convergence increase depth agd feature alignment happens first layer and the deeper layers accelerate learning the right scale,2025-08-26T02:23:21.431050
68,Optimal Transport-Guided Self-Supervised Learning for Unsupervised Domain Adaptation,"Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, a crucial task for real-world deployments where data shifts are common. Many UDA methods rely on adversarial training or moment matching, which can be unstable or lack strong theoretical guarantees for distribution alignment. We propose Optimal Transport-Guided Self-Supervised Learning (OT-SSL) for robust UDA. OT-SSL leverages the principled framework of optimal transport (OT) to align feature distributions between the source and target domains, minimizing the Wasserstein distance. Critically, we integrate this OT alignment with self-supervised learning pretext tasks (e.g., contrastive learning, masked autoencoding) on the target domain. This ensures that the learned target features are not only aligned with the source but also semantically meaningful and discriminative. Experiments on challenging UDA benchmarks demonstrate OT-SSL significantly outperforms state-of-the-art methods, achieving superior accuracy and more stable training by combining strong theoretical grounding with effective representation learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3798,Balanced Learning for Domain Adaptive Semantic Segmentation,"Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain, improving model performance on the target dataset without additional annotations.
Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains.
To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift between domains.
First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits.
Subsequently, we introduce a post-hoc approach to align the positive and negative logits distributions across different classes using anchor distributions and cumulative density functions.
To further consider the network's need to generate unbiased pseudo-labels during self-training, we couple Gaussian mixture models to estimate logits distributions online and incorporate logits correction terms into the loss function.
Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains.
Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into existing methods.
Our work highlights the importance of balanced learning in UDA and effectively mitigates class bias in domain adaptive semantic segmentation.",ICLR.cc/2025/Conference,5.5,False,0.8880,unsupervised domain adaptation uda aims transfer knowledge from labeled source domain unlabeled target domain crucial task for real world deployments where data shifts are common optimal transport guided self supervised learning ssl for robust uda ssl leverages the principled optimal transport align feature distributions between the source and target domains minimizing the wasserstein distance critically integrate this alignment self supervised learning pretext tasks contrastive learning masked autoencoding the target domain experiments challenging uda benchmarks ssl outperforms state the art methods achieving superior and more stable training combining strong theoretical grounding effective representation learning,unsupervised domain adaptation uda for semantic segmentation aims transfer knowledge from labeled source domain unlabeled target domain improving the target additional annotations address this issue balanced learning for domain adaptation blda directly assess and alleviate class bias requiring prior knowledge about the distribution shift between domains moreover leverage the resulting cumulative density domain shared structural knowledge connect the source and target domains extensive experiments two standard uda semantic segmentation benchmarks that blda consistently improves for under predicted classes when integrated into existing methods our highlights the importance balanced learning uda and mitigates class bias domain adaptive semantic segmentation,2025-08-26T02:23:21.431053
69,Scalable Gaussian Process Priors for Training Deep Neural Networks,"Gaussian Processes (GPs) provide a principled framework for uncertainty quantification and hyperparameter optimization, but their scalability to large datasets and integration with deep neural networks (DNNs) remains a challenge. We propose Scalable Gaussian Process Priors (SGPP) for enhancing DNN training. SGPP replaces traditional initialization schemes with a learned GP prior over network weights or latent activations, allowing the DNN to leverage the non-parametric flexibility and uncertainty modeling of GPs. Our approach uses sparse GP approximations and variational inference to make the method scalable to modern deep learning architectures and datasets. By incorporating a GP prior, the network can start in a more favorable region of the loss landscape, accelerate convergence, and intrinsically estimate predictive uncertainty. Experiments on classification and regression tasks demonstrate SGPP leads to faster training, better generalization, and more reliable uncertainty estimates compared to standard DNNs, bridging the gap between deep learning and Bayesian non-parametrics.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,9058,Bayesian Optimization via Continual Variational Last Layer Training,"Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.",ICLR.cc/2025/Conference,nan,nan,0.8792,gaussian processes gps provide principled for uncertainty quantification and hyperparameter optimization but their scalability large datasets and integration deep neural networks dnns remains challenge sgpp replaces traditional initialization schemes learned prior over network weights latent activations allowing the dnn leverage the non parametric flexibility and uncertainty modeling gps our uses sparse approximations and variational inference make the scalable modern deep learning architectures and datasets incorporating prior the network can start more favorable region the loss landscape accelerate convergence and intrinsically estimate predictive uncertainty experiments classification and regression tasks sgpp leads faster training better generalization and more reliable uncertainty estimates compared standard dnns bridging the gap between deep learning and bayesian non parametrics,gaussian processes gps are seen the state the art surrogate models for bayesian optimization due their ability uncertainty and their tasks where correlations are easily captured such those defined euclidean metrics and their ability updated online while bayesian neural networks bnns are promising direction for higher capacity surrogate models they have far seen limited use due poor some problem types exploit this connection efficient online training that interleaves conditioning and optimization,2025-08-26T02:23:21.431059
70,Robust Reinforcement Learning under Adversarial Behavioral Perturbations,"Reinforcement Learning (RL) agents are often brittle when deployed in environments where other agents (e.g., humans, competitors) exhibit unexpected or adversarial behaviors, leading to sub-optimal policies or catastrophic failures. Protecting against these ""behavioral perturbations"" is crucial for multi-agent systems. We propose Robust Reinforcement Learning under Adversarial Behavioral Perturbations (RRAB), a novel framework for training policies that are resilient to adversaries acting in the environment. RRAB formulates a min-max game where the RL agent learns to optimize its reward while anticipating and defending against an adaptive adversary that attempts to perturb other agents' actions or observations to degrade the target agent's performance. Our approach uses an online learning mechanism for the adversary and a robust policy optimization method for the agent. Experiments on multi-agent control and social dilemma games demonstrate RRAB's ability to learn robust policies that perform well even when facing intelligent behavioral adversaries, significantly outperforming non-robust RL baselines.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4681,ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization,"Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.",ICLR.cc/2025/Conference,6.5,True,0.8518,reinforcement learning agents are often brittle when deployed environments where other agents robust reinforcement learning under adversarial behavioral perturbations rrab for training policies that are resilient adversaries acting the environment our uses online learning mechanism for the adversary and robust policy optimization for the agent,offline reinforcement learning has garnered significant attention for its ability learn effective policies from pre collected datasets the need for further environmental interactions while promising have been demonstrated single agent settings offline multi agent reinforcement learning marl presents additional challenges due the large joint state action space and the complexity multi agent behaviors,2025-08-26T02:23:21.431063
71,Deep Implicit Function Learning for Inverse Design of Materials,"Inverse design of materials, where one seeks a material composition or structure that exhibits desired properties, is a grand challenge in scientific discovery. Traditional search methods are combinatorial and slow. Deep learning approaches show promise but often generate discrete structures or struggle with multi-objective optimization. We propose Deep Implicit Function Learning (DIFL) for the inverse design of materials. DIFL learns a continuous, implicit mapping from desired material properties (e.g., strength, conductivity, bandgap) to a latent representation of the material's microstructure or atomic configuration. A differentiable decoder then reconstructs the 3D material structure from this latent space. By optimizing in the continuous latent space, DIFL efficiently explores the vast material design space. Our method simultaneously optimizes for multiple objectives and can handle complex geometric constraints. Experiments on designing metamaterials and alloys demonstrate DIFL's ability to generate novel materials with superior target properties, accelerating the discovery cycle in materials science.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10339,DEQuify your force field: Towards efficient simulations using deep equilibrium models,"Machine learning force fields show great promise in enabling more accurate force fields than manually derived ones for molecular dynamics simulations. 
State-of-the-art approaches for ML force fields stack many equivariant graph neural network layers, resulting in long inference times and high memory costs. This work aims to improve these two aspects while simultaneously reaching higher accuracy.
Our key observation is that successive states in molecular dynamics simulations are extremely similar, but typical architectures treat each step independently, disregarding this information.
We show how deep equilibrium models (DEQs) can exploit this temporal correlation by recycling neural network features from previous time steps. 
Specifically, we turn a state-of-the-art force field architecture into a DEQ, enabling us to improve both accuracy and speed by $10\%-20\%$ on the MD17, MD22, and OC20 200k datasets. 
Compared to conventional approaches, DEQs are also naturally more memory efficient, facilitating the training of more expressive models on larger systems given limited GPU memory resources.",ICLR.cc/2025/Conference,5.0,False,0.8016,deep learning approaches promise but often generate discrete structures struggle multi objective optimization deep implicit function learning difl for the inverse materials strength conductivity bandgap latent representation the material microstructure atomic configuration,machine learning force fields great promise enabling more accurate force fields than manually derived ones for molecular dynamics simulations state the art approaches for force fields stack many equivariant graph neural network layers resulting long inference times and high memory costs how deep equilibrium models deqs can exploit this temporal correlation recycling neural network features from previous time steps,2025-08-26T02:23:21.431067
72,Federated Learning with Byzantine Robustness via Self-Supervised Client Clustering,"Federated Learning (FL) is vulnerable to Byzantine attacks, where malicious clients send corrupted updates to sabotage the global model. Existing Byzantine-robust FL methods often assume homogeneous clients or rely on strong statistical assumptions. We propose Federated Learning with Byzantine Robustness via Self-Supervised Client Clustering (Fed-BysCC), a novel approach that identifies and mitigates Byzantine attacks by clustering clients based on their update behavior. Fed-BysCC uses a self-supervised learning strategy to learn embeddings for client model updates. These embeddings are then used to dynamically cluster clients, allowing the central server to detect outliers (potential Byzantines) and filter their contributions. The clustering process itself is robust to Byzantine influence. Experiments on various non-IID datasets with diverse Byzantine attack types (e.g., data poisoning, model poisoning) demonstrate that Fed-BysCC significantly outperforms state-of-the-art Byzantine-robust FL algorithms, achieving high accuracy even under strong attacks and heterogeneous data, providing a more reliable collaborative AI framework.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10972,Kick Bad Guys Out! Conditionally Activated Anomaly Detection in Federated Learning with Zero-Knowledge Proof Verification,"Federated Learning (FL) systems are susceptible to adversarial attacks, where malicious clients submit poisoned models to disrupt the convergence or plant backdoors that cause the global model to misclassify some samples. Current defense methods are often impractical for real-world FL systems, as they either rely on unrealistic prior knowledge or cause accuracy loss even in the absence of attacks. Furthermore, these methods lack a protocol for verifying execution, leaving participants uncertain about the correct execution of the mechanism. To address these challenges, we propose a novel anomaly detection strategy that is designed for real-world FL systems. Our approach activates the defense only when potential attacks are detected, and enables the removal of malicious models without affecting the benign ones. Additionally, we incorporate zero-knowledge proofs to ensure the integrity of the proposed defense mechanism. Experimental results demonstrate the effectiveness of our approach in enhancing FL system security against a comprehensive set of adversarial attacks in various ML tasks.",ICLR.cc/2025/Conference,3.75,nan,0.8403,federated learning vulnerable byzantine attacks where malicious clients send corrupted updates sabotage the global federated learning byzantine robustness self supervised client clustering fed byscc that identifies and mitigates byzantine attacks clustering clients their update behavior fed byscc uses self supervised learning strategy learn embeddings for client updates the clustering process itself robust byzantine influence,federated learning systems are susceptible adversarial attacks where malicious clients submit poisoned models disrupt the convergence plant backdoors that cause the global misclassify some samples current defense methods are often impractical for real world systems they either rely unrealistic prior knowledge cause loss even the absence attacks address these challenges anomaly detection strategy that designed for real world systems,2025-08-26T02:23:21.431076
73,Physics-Informed Neural Operator for Solving High-Dimensional PDEs,"Solving high-dimensional Partial Differential Equations (PDEs) is fundamental in scientific computing and engineering, but traditional numerical methods suffer from the curse of dimensionality and high computational cost. Neural Operators, which learn mappings between function spaces, show promise but often lack physical constraints. We propose Physics-Informed Neural Operator (PInO), a novel framework that integrates physical laws directly into the learning of neural operators for solving high-dimensional PDEs. PInO uses an unrolled architecture that approximates the solution operator, with a loss function incorporating both data-driven supervision and a residual term from the PDE itself. This physics-informed regularization ensures that the learned solution respects the underlying physical laws, even with sparse training data. Experiments on challenging PDEs (e.g., Navier-Stokes, Schrödinger equation in high dimensions) demonstrate PInO achieves superior accuracy and generalization compared to purely data-driven neural operators and traditional solvers, opening new avenues for efficient and physically consistent scientific machine learning.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,331,Disentangled Representation Learning for Parametric Partial Differential Equations,"Neural operators (NOs) have demonstrated  remarkable success in learning mappings between function spaces, serving as efficient approximators for the forward solutions of complex physical systems governed by partial differential equations (PDEs). However, while  effective as black-box solvers, they offer limited insight into the underlying physical mechanism, due to the lack of interpretable representations of the physical parameters that drive the system. To tackle this challenge, we propose a new paradigm for learning disentangled representations from neural operator parameters, thereby effectively solving an inverse problem. Specifically, we introduce DisentangO, a novel hyper-neural operator architecture designed to unveil and disentangle the latent physical factors of variation embedded within the black-box neural operator parameters. At the core of DisentangO is a multi-task neural operator architecture that distills the varying parameters of the governing PDE through a task-wise adaptive layer, coupled with a hierarchical variational autoencoder that disentangles these variations into identifiable latent factors. By learning these disentangled representations, our model not only enhances physical interpretability but also enables more robust generalization across diverse physical systems. Empirical evaluations across supervised, semi-supervised, and unsupervised learning contexts show that DisentangO effectively extracts meaningful and interpretable latent features, bridging the gap between predictive performance and physical understanding in neural operator frameworks.",ICLR.cc/2025/Conference,5.5,False,0.8620,neural operators which learn mappings between function spaces promise but often lack physical constraints physics informed neural operator pino that integrates physical laws directly into the learning neural operators for solving high dimensional pdes navier stokes schrödinger equation high dimensions pino achieves superior and generalization compared purely data driven neural operators and traditional solvers opening avenues for efficient and physically consistent scientific machine learning,neural operators nos have demonstrated remarkable success learning mappings between function spaces serving efficient approximators for the forward solutions complex physical systems governed partial differential equations pdes tackle this challenge paradigm for learning disentangled representations from neural operator parameters thereby solving inverse problem disentango hyper neural operator designed unveil and disentangle the latent physical factors variation embedded within the black box neural operator parameters the core disentango multi task neural operator that distills the varying parameters the governing pde task wise adaptive layer coupled hierarchical variational autoencoder that disentangles these variations into identifiable latent factors learning these disentangled representations our not only enhances physical interpretability but also enables more robust generalization across diverse physical systems empirical evaluations across supervised semi supervised and unsupervised learning contexts that disentango extracts meaningful and interpretable latent features bridging the gap between predictive and physical understanding neural operator frameworks,2025-08-26T02:23:21.431083
74,Adaptive Feature Reweighting for Domain Generalization in Remote Sensing,"Deep learning models often struggle with domain generalization (DG) in remote sensing, where datasets exhibit significant shifts due to variations in sensors, atmospheric conditions, viewing angles, and geographic locations. Current DG methods often use domain-invariant features, which can discard useful domain-specific information. We propose Adaptive Feature Reweighting (AFR), a novel framework for robust domain generalization in remote sensing imagery. AFR learns to dynamically reweight the importance of different features based on their perceived domain-specificity and utility for the target task. It uses a meta-learning approach to train a reweighting network that enhances domain-invariant features while adaptively incorporating domain-specific but generalizable features. This avoids discarding informative features. Experiments on diverse remote sensing benchmarks (e.g., land cover classification across different satellites, scene recognition in varied geographies) demonstrate AFR significantly outperforms state-of-the-art DG methods, achieving superior generalization to unseen target domains, enabling more robust AI for Earth observation.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5536,Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap,"Domain generalization (DG) is an important problem that learns a model which generalizes to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (MUDG) problem, which uses a large task-agnostic unlabeled source dataset during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be accurately and efficiently searched in a joint vision-language space. We make three contributions in the MUDG setting. Firstly, we show theoretically that cross-modal approximate nearest neighbor search suffers from low recall due to the large distance between text queries and the image centroids used for coarse quantization. Accordingly, we propose paired k-means, a simple clustering algorithm that improves nearest neighbor recall by storing centroids in query space instead of image space. Secondly, we propose an adaptive text augmentation scheme for target labels designed to improve zero-shot accuracy and diversify retrieved image data. Lastly, we present two simple but effective components to further improve downstream target accuracy. We compare against state-of-the-art name-only transfer, source-free DG and zero-shot (ZS) methods on their respective benchmarks and show consistent improvement in accuracy on 20 diverse datasets. Code is available: https://github.com/Chris210634/mudg",ICLR.cc/2025/Conference,7.0,True,0.8178,deep learning models often struggle domain generalization remote sensing where datasets exhibit significant shifts due variations sensors atmospheric conditions viewing angles and geographic locations adaptive feature reweighting afr for robust domain generalization remote sensing imagery uses meta learning train reweighting network that enhances domain invariant features while adaptively incorporating domain specific but generalizable features land cover classification across different satellites scene recognition varied geographies afr outperforms state the art methods achieving superior generalization unseen target domains enabling more robust for earth observation,domain generalization important problem that learns which generalizes unseen domains leveraging one more source domains under the assumption shared label spaces for this setting tackle the multimodal version the unsupervised domain generalization mudg problem which uses large task agnostic unlabeled source during finetuning accordingly paired means simple clustering that improves nearest neighbor storing centroids query space instead image space secondly adaptive text augmentation scheme for target labels designed improve zero shot and diversify retrieved image data against state the art name only transfer source free and zero shot methods their respective benchmarks and consistent improvement diverse datasets,2025-08-26T02:23:21.431090
75,Differentiable Recurrent Graph Solvers for Neural Network Verification,"Verifying properties of deep neural networks (DNNs), such as robustness or fairness, is critical for safety-critical applications. Existing verification methods often struggle with scalability for large networks or rely on conservative approximations. We propose Differentiable Recurrent Graph Solvers (DRGS), a novel framework that reformulates DNN verification as finding fixed points of a recurrent graph traversal. DRGS represents the DNN's computational graph and its input constraints as a message-passing system. Instead of explicitly traversing the computation or relying on SAT/SMT solvers, DRGS uses a differentiable recurrent neural network to implicitly approximate the bounds of intermediate activations, converging to a fixed point that guarantees satisfaction or violation of the property. This continuous relaxation allows for gradient-based optimization and end-to-end learning. Experiments on verifying robustness properties of CNNs and ReLU networks against $L_\infty$ attacks demonstrate DRGS achieves competitive and often tighter bounds than state-of-the-art exact and approximate verifiers, with significantly improved scalability for larger networks, pushing the frontier of provable AI.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1289,Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness,"The security and robustness of deep neural networks (DNNs) have become increasingly concerning. This paper aims to provide both a theoretical foundation and a practical solution to ensure the reliability of DNNs. We explore the concept of Lipschitz continuity to certify the robustness of DNNs against adversarial attacks, which aim to mislead the network with adding imperceptible perturbations into inputs. We propose a novel algorithm that remaps the input domain into a constrained range, reducing the Lipschitz constant and potentially enhancing robustness. Unlike existing adversarially trained models, where robustness is enhanced by introducing additional examples from other datasets or generative models, our method is almost cost-free as it can be integrated with existing models without requiring re-training. Experimental results demonstrate the generalizability of our method, as it can be combined with various models and achieve enhancements in robustness. Furthermore, our method achieves the best robust accuracy for CIFAR10 and CIFAR100 datasets on the RobustBench leaderboard.",ICLR.cc/2025/Conference,4.8,False,0.8555,verifying properties deep neural networks dnns such robustness fairness critical for safety critical applications instead explicitly traversing the computation relying sat smt solvers drgs uses differentiable recurrent neural network implicitly approximate the bounds intermediate activations converging fixed point that guarantees satisfaction violation the property this continuous relaxation allows for gradient based optimization and end end learning experiments verifying robustness properties cnns and relu networks against infty attacks drgs achieves competitive and often tighter bounds than state the art exact and approximate verifiers improved scalability for larger networks pushing the frontier provable,the security and robustness deep neural networks dnns have become increasingly concerning the concept lipschitz continuity certify the robustness dnns against adversarial attacks which aim mislead the network adding imperceptible perturbations into inputs that remaps the input domain into constrained range reducing the lipschitz constant and potentially enhancing robustness unlike existing adversarially trained models where robustness enhanced introducing additional examples from other datasets generative models our almost cost free can integrated existing models requiring training experimental the generalizability our can combined various models and achieve enhancements robustness,2025-08-26T02:23:21.431096
76,Learning Implicit 3D Gaussian Splatting for Robust Novel View Synthesis,"Novel view synthesis has seen remarkable progress, with Neural Radiance Fields (NeRFs) achieving high fidelity. However, NeRFs are slow to train, require dense input views, and often struggle with sparse inputs or non-Lambertian surfaces. Recent 3D Gaussian Splatting (3DGS) offers faster rendering but relies on explicit point clouds, which are difficult to optimize from sparse views. We propose Learning Implicit 3D Gaussian Splatting (LI-3DGS), a novel framework that combines the efficiency of 3DGS with the robustness of implicit scene representations. LI-3DGS uses a deep implicit function to predict the parameters (mean, covariance, color, opacity) of a 3D Gaussian distribution at arbitrary points in space, rather than optimizing discrete Gaussians directly. This implicit representation enables learning from sparse views and generates more robust and smoother scene geometry. Experiments on standard datasets (e.g., LLFF, Mip-NeRF 360) show LI-3DGS achieves high-fidelity novel view synthesis, superior robustness to sparse input views, and significantly faster training and rendering compared to NeRF-based methods, bridging the gap between implicit and explicit 3D scene representation.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4296,MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis,"Recent works in volume rendering, \textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. 
Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries.
To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions:
1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 
2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions.
3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 
4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically.
As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.
We conduct extensive experiments to demonstrate that our proposed method is capable of improving novel view synthesis of the Gaussian-based explicit representation methods about 1 dB PSNR for various tasks. \href{https://mvgs666.github.io/}{\textcolor{magenta}{Codes are available.}}",ICLR.cc/2025/Conference,4.75,nan,0.9139,view synthesis has seen remarkable progress neural radiance fields nerfs achieving high fidelity learning implicit gaussian splatting 3dgs that combines the efficiency 3dgs the robustness implicit scene representations 3dgs uses deep implicit function predict the parameters mean covariance color opacity gaussian distribution arbitrary points space rather than optimizing discrete gaussians directly this implicit representation enables learning from sparse views and generates more robust and smoother scene geometry llff mip nerf 3dgs achieves high fidelity view synthesis superior robustness sparse input views and faster training and rendering compared nerf based methods bridging the gap between implicit and explicit scene representation,nerf and gaussian splatting 3dgs advance the rendering quality and efficiency the help the learned implicit neural radiance field gaussians solve aforementioned problems 3dgs optimization embodying four key contributions transform the conventional single view training paradigm into multi view training strategy conduct extensive experiments that our proposed capable improving view synthesis the gaussian based explicit representation methods about psnr for various tasks,2025-08-26T02:23:21.431101
77,Multi-Objective Reinforcement Learning with Preference-Conditioned Policy Blending,"Real-world reinforcement learning (RL) problems often involve optimizing multiple, potentially conflicting objectives (e.g., reward, safety, energy efficiency). Existing Multi-Objective RL (MORL) methods struggle with eliciting user preferences, handling non-stationary preferences, or generating a diverse set of Pareto-optimal policies. We propose Multi-Objective RL with Preference-Conditioned Policy Blending (MORL-PCPB), a novel framework that learns a continuous spectrum of Pareto-optimal policies and adaptively blends them based on user preferences. MORL-PCPB trains a set of specialized policies, each optimized for a specific objective, and a separate ""blending network"" that learns to combine these policies based on a user-specified preference vector (e.g., a weight vector for objectives). This blending network is trained to ensure smooth transitions across the Pareto front and to be robust to noisy preference inputs. Experiments on diverse MORL benchmarks (e.g., deep-sea treasure, robotic navigation with safety constraints) demonstrate MORL-PCPB efficiently generates optimal trade-offs and adapts dynamically to changing preferences, offering a practical solution for real-world MORL.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,6385,Customizing Reinforcement Learning Agent with Multi-Objective Preference Control,"Practical reinforcement learning (RL) usually requires agents to be optimized for multiple potentially conflicting criteria, e.g. speed vs. safety. 
Although Multi-Objective RL (MORL) algorithms have been studied in previous works, their trained agents often lack precise controllability of the delicate trade-off among multiple objectives. Hence, the resulting agent is not versatile in aligning with customized requests from different users. 
To bridge the gap, we develop ``Preference control (PC) RL'', which aims to train a meta-policy that takes user preference as input controlling the generation of a trajectory on the Pareto frontier adhering to the preference. To this end, we train a preference-conditioned meta-policy by our proposed preference-regularized MORL algorithm. The achieved meta-policy performs as a multi-objective optimizer that can produce user-desired solutions on the Pareto frontier. The proposed algorithm is analyzed and its convergence and controllability are theoretically justified. 
Experiments from discrete toy examples to higher-dimension robotic control tasks and experiments with more than two objectives are conducted to show its performance.  In these experiments, PCRL-trained policies show significantly better controllability than existing approaches and can generate Pareto optimal solutions with better diversity and utilities.",ICLR.cc/2025/Conference,5.75,False,0.8805,real world reinforcement learning problems often involve optimizing multiple potentially conflicting objectives this blending network trained ensure smooth transitions across the pareto front and robust noisy preference inputs,practical reinforcement learning usually requires agents optimized for multiple potentially conflicting criteria bridge the gap preference control which aims train meta policy that takes user preference input controlling the generation trajectory the pareto frontier adhering the preference,2025-08-26T02:23:21.431105
78,Cross-Domain Few-Shot Learning with Adversarial Feature Regularization,"Few-shot learning (FSL) aims to classify new classes with minimal labeled examples. Cross-domain FSL (CD-FSL), where the source and target domains differ significantly, is substantially more challenging due to domain shift. Standard FSL methods often overfit to source domain characteristics and fail to generalize. We propose Cross-Domain Few-Shot Learning with Adversarial Feature Regularization (CD-AFR), a novel framework for robust CD-FSL. CD-AFR employs an adversarial training scheme to learn domain-invariant, yet discriminative, feature representations. A feature extractor is trained to confuse a domain discriminator while simultaneously optimizing for few-shot classification on the source meta-tasks. Crucially, we introduce a regularization term that encourages the learned features to be robust to small adversarial perturbations, enhancing their generalizability across domains. Experiments on diverse CD-FSL benchmarks (e.g., EuroSAT to ChestX, miniImageNet to CUB) demonstrate CD-AFR significantly outperforms existing methods, achieving superior accuracy and transferability in challenging cross-domain few-shot scenarios.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7800,Adapting Informative Structures for Cross-Domain Few-Shot Segmentation,"Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes under domain shifts, using only a few mask-annotated support samples. However, directly applying pretrained CD-FSS models to unseen domains is often suboptimal due to their limited coverage of domain diversity by fixed parameters trained on source domains. Moreover, simply adjusting hand-selected model parameters, such as test-time training, typically neglects the distinct domain gaps and characteristics of target domains. To address these issues, we propose adapting informative model structures for target domains by learning domain characteristics from few-shot labeled support samples during inference. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. Our method selectively and gradually adapts the model to target domains, optimizing model adaptation, minimizing overfitting risks, and maximizing the use of limited support data. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing few-shot segmentation models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.",ICLR.cc/2025/Conference,4.833333333333333,nan,0.8763,few shot learning fsl aims classify classes minimal labeled examples cross domain fsl fsl where the source and target domains differ more challenging due domain shift standard fsl methods often overfit source domain characteristics and fail generalize cross domain few shot learning adversarial feature regularization afr for robust fsl afr employs adversarial training scheme learn domain invariant yet discriminative feature representations feature extractor trained confuse domain discriminator while simultaneously optimizing for few shot classification the source meta tasks eurosat chestx miniimagenet cub afr outperforms existing methods achieving superior and transferability challenging cross domain few shot scenarios,cross domain few shot segmentation fss aims segment objects classes under domain shifts only few mask annotated support samples however directly applying pretrained fss models unseen domains often suboptimal due their limited coverage domain diversity fixed parameters trained source domains moreover simply adjusting hand selected parameters such test time training neglects the distinct domain gaps and characteristics target domains address these issues adapting informative structures for target domains learning domain characteristics from few shot labeled support samples during inference the resulting informative structure adaptation isa addresses domain shifts and equips existing few shot segmentation models flexible adaptation capabilities for domains eliminating the need redesign retrain fss models base data,2025-08-26T02:23:21.431111
79,Learning Differentiable Optimizers for Efficient Neural Network Training,"The choice of optimizer (e.g., Adam, SGD) significantly impacts the efficiency and performance of deep neural network training. Hand-crafting optimizers is an arduous process, and existing learned optimizers often struggle with generalization to new tasks or network architectures. We propose Learning Differentiable Optimizers (LDO), a novel framework that meta-learns an optimizer which is itself a differentiable neural network. LDO trains an LSTM-based optimizer network to output update rules for the parameters of a ""learner"" neural network. This optimizer learns to adapt its update rules based on the historical gradients and previous updates of the learner network, effectively learning meta-parameters that are optimized for faster convergence and better generalization. Crucially, our optimizer is fully differentiable, allowing for efficient end-to-end meta-training. Experiments on diverse learning tasks (e.g., image classification on CIFAR, simple meta-learning benchmarks) demonstrate LDO discovers optimizers that converge faster and achieve better generalization than hand-tuned optimizers, showcasing the potential for automated and adaptive optimization.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1248,Narrowing the Focus: Learned Optimizers for Pretrained Models,"In modern deep learning, the models are learned by applying gradient updates using an optimizer, which transforms the updates based on various statistics. Optimizers are often hand-designed and tuning their hyperparameters is a big part of the training process. Learned optimizers have shown some initial promise, but are generally unsuccessful as a general optimization mechanism applicable to every problem. In this work we explore a different direction: instead of learning general optimizers, we instead specialize them to a specific training environment. We propose a novel optimizer technique that learns a layer-specific linear combination of update directions provided by a set of base optimizers, effectively adapting its strategy to the specific model and dataset. When evaluated on image classification tasks, this specialized optimizer significantly outperforms both traditional off-the-shelf methods such as Adam, as well as existing general learned optimizers. Moreover, it demonstrates robust generalization with respect to model initialization, evaluating on unseen datasets, and training durations beyond its meta-training horizon.",ICLR.cc/2025/Conference,4.5,False,0.8431,adam sgd impacts the efficiency and deep neural network training hand crafting optimizers arduous process and existing learned optimizers often struggle generalization tasks network architectures learning differentiable optimizers ldo that meta learns optimizer which itself differentiable neural network ldo trains lstm based optimizer network output update rules for the parameters learner neural network this optimizer learns adapt its update rules the historical gradients and previous updates the learner network learning meta parameters that are optimized for faster convergence and better generalization experiments diverse learning tasks image classification cifar simple meta learning benchmarks ldo discovers optimizers that converge faster and achieve better generalization than hand tuned optimizers showcasing the potential for automated and adaptive optimization,modern deep learning the models are learned applying gradient updates optimizer which transforms the updates various statistics learned optimizers have shown some initial promise but are unsuccessful general optimization mechanism applicable every problem this different direction instead learning general optimizers instead specialize them specific training environment when evaluated image classification tasks this specialized optimizer outperforms both traditional off the shelf methods such adam well existing general learned optimizers,2025-08-26T02:23:21.431114
80,Certified Robustness against Stochastic Perturbations for Medical Image Diagnosis,"Medical AI systems require not only high accuracy but also strong guarantees of reliability, especially against uncertainties inherent in real-world clinical data, such as noise from different scanners or acquisition protocols (stochastic perturbations). Standard adversarial robustness focuses on worst-case deterministic attacks. We propose a framework for Certified Robustness against Stochastic Perturbations (CR-SP) specifically for medical image diagnosis. Our approach leverages a novel extension of randomized smoothing tailored to certify robustness against random noise distributions (e.g., Gaussian noise, speckle noise) rather than adversarial ones. We derive tight certified lower bounds on accuracy for a given noise variance, providing a probabilistic guarantee that the model's prediction remains consistent under stochastic shifts. Experiments on medical imaging datasets (e.g., chest X-rays for disease classification, retinal OCT scans for pathology detection) demonstrate CR-SP achieves meaningful certified accuracy under realistic noise models, offering a crucial step towards trustworthy and robust AI for healthcare.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8070,Robust Representation Consistency Model via Contrastive Denoising,"Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3\% on average, with up to 11.6\% at larger radii, while reducing inference costs by 85x on average.",ICLR.cc/2025/Conference,6.75,True,0.8709,standard adversarial robustness focuses worst case deterministic attacks for certified robustness against stochastic perturbations for medical image diagnosis our leverages extension randomized smoothing tailored certify robustness against random noise distributions derive tight certified lower bounds for given noise variance providing probabilistic guarantee that the model prediction remains consistent under stochastic shifts,robustness essential for deep neural networks security sensitive applications this end randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations,2025-08-26T02:23:21.431117
81,Disentangled Representation Learning for Counterfactual Explanations in Fairness,"Explaining algorithmic fairness is challenging; current methods often identify biased features but rarely suggest actionable recourse or counterfactual interventions. We propose Disentangled Representation Learning for Counterfactual Explanations in Fairness (DCRL-CFE), a novel framework that provides interpretable counterfactual explanations for unfair decisions. DCRL-CFE learns a disentangled latent space where sensitive attributes (e.g., gender, race) are separated from non-sensitive, actionable features (e.g., education, experience). We then leverage this disentanglement to generate minimal, actionable counterfactuals: ""What is the smallest change in non-sensitive features that would alter an unfair decision, while keeping sensitive attributes fixed?"" This allows stakeholders to understand how to change an outcome without reinforcing stereotypes. Experiments on fairness benchmarks (e.g., Adult income, COMPAS recidivism) demonstrate DCRL-CFE generates more intuitive, actionable, and fair counterfactual explanations than existing methods, offering a principled way to audit and mitigate algorithmic bias.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5254,Towards counterfactual fairness through auxiliary variables,"The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. 
However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness without sacrificing accuracy. Our code is available at https://github.com/CASE-Lab-UMD/counterfactual_fairness_2025.",ICLR.cc/2025/Conference,6.0,True,0.8622,explaining algorithmic fairness challenging current methods often identify biased features but rarely suggest actionable recourse counterfactual interventions disentangled representation learning for counterfactual explanations fairness dcrl cfe that provides interpretable counterfactual explanations for unfair decisions experiments fairness benchmarks,the challenge balancing fairness and predictive machine learning models when sensitive attributes such race gender age are considered has motivated substantial recent years counterfactual fairness ensures that predictions remain consistent across counterfactual variations sensitive attributes which crucial concept addressing societal biases however existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features limiting their ability achieve fairness while simultaneously maintaining tackle this challenge exogenous causal reasoning exoc causal reasoning motivated exogenous variables our explicitly defines auxiliary node and control node that contribute counterfactual fairness and control the information flow within the our evaluation conducted synthetic and real world datasets validates exoc superiority showing that outperforms state the art approaches achieving counterfactual fairness sacrificing,2025-08-26T02:23:21.431124
82,Memory-Augmented Graph Neural Networks for Long-Term Dependency in Graph Sequences,"Processing sequences of graphs (e.g., dynamic social networks, evolving molecular structures, video scene graphs) where long-term dependencies are critical poses a significant challenge for existing Graph Neural Networks (GNNs). Standard GNNs often lack explicit memory mechanisms, struggling to aggregate information over extended temporal horizons. We propose Memory-Augmented Graph Neural Networks (MA-GNN), a novel architecture for modeling long-term dependencies in graph sequences. MA-GNN integrates an external, differentiable memory module that stores and retrieves relevant information from past graph states. This memory is dynamically updated and queried using attention mechanisms based on the current graph's features and topology, allowing the model to selectively recall crucial historical context. Experiments on dynamic graph tasks (e.g., future link prediction in social networks, traffic flow forecasting) demonstrate MA-GNN significantly outperforms recurrent GNNs and other sequence models, capturing long-range temporal and structural dependencies more effectively and achieving superior predictive performance.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,7813,When Graph Neural Networks Meet Dynamic Mode Decomposition,"Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper into this perspective by connecting the dynamics in GNNs to modern Koopman theory and its numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can estimate a low-rank, finite-dimensional linear operator based on multiple states of the system, effectively approximating potential nonlinear interactions between nodes in the graph. This approach allows us to capture complex dynamics within the graph accurately and efficiently. We theoretically establish a connection between the DMD-estimated operator and the original dynamic operator between system states. Building upon this foundation, we introduce a family of DMD-GNN models that effectively leverage the low-rank eigenfunctions provided by the DMD algorithm. We further discuss the potential of enhancing our approach by incorporating domain-specific constraints such as symmetry into the DMD computation, allowing the corresponding GNN models to respect known physical properties of the underlying system. Our work paves the path for applying advanced dynamical system analysis tools via GNNs. We validate our approach through extensive experiments on various learning tasks, including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs. We also empirically verify that our proposed models can serve as powerful encoders for link prediction tasks. The results demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance, highlighting the effectiveness of integrating DMD into GNN frameworks.",ICLR.cc/2025/Conference,6.6,True,0.8668,processing sequences graphs dynamic social networks evolving molecular structures video scene graphs where long term dependencies are critical poses significant challenge for existing graph neural networks gnns memory augmented graph neural networks gnn for modeling long term dependencies graph sequences this memory dynamically updated and queried attention mechanisms the current graph features and topology allowing the selectively crucial historical context future link prediction social networks traffic flow forecasting gnn outperforms recurrent gnns and other sequence models capturing long range temporal and structural dependencies more and achieving superior predictive,graph neural networks gnns have emerged fundamental tools for wide range prediction tasks graph structured data recent studies have drawn analogies between gnn feature propagation and diffusion processes which can interpreted dynamical systems our extensive experiments various learning tasks including directed graphs large scale graphs long range interactions and spatial temporal graphs also empirically verify that our proposed models can serve powerful encoders for link prediction tasks,2025-08-26T02:23:21.431127
83,Self-Supervised Learning with Latent Causal Discovery for Generalizable Reinforcement Learning,"Reinforcement Learning (RL) agents often struggle with generalization to new environments or tasks, frequently learning spurious correlations instead of generalizable causal mechanisms. Learning intrinsic causal factors from observations could enhance robustness. We propose Self-Supervised Learning with Latent Causal Discovery (SS-LCD) for more generalizable RL. SS-LCD pre-trains an encoder on diverse observational data using self-supervised objectives (e.g., contrastive learning, masked prediction) to learn a disentangled latent space. Crucially, a differentiable causal discovery module operates in this latent space, inferring a graph of causal relationships between latent factors. This causal graph is then used to regularize the RL agent's policy learning, encouraging it to focus on interventions on causal factors rather than spurious correlations. Experiments on diverse simulated environments (e.g., variations of Atari games, robotic control with distractors) demonstrate SS-LCD significantly improves the generalization capability and robustness of RL agents to environmental changes.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,3328,Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning,"Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning—where known components are reconfigured to handle new situations—we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks.",ICLR.cc/2025/Conference,6.0,True,0.8463,reinforcement learning agents often struggle generalization environments tasks frequently learning spurious correlations instead generalizable causal mechanisms learning intrinsic causal factors from observations could enhance robustness self supervised learning latent causal discovery lcd for more generalizable variations atari games robotic control distractors lcd improves the generalization capability and robustness agents environmental changes,generalization reinforcement learning remains significant challenge when agents encounter environments unseen dynamics this enhances generalization learning and leveraging compositional causal components unlike previous approaches focusing invariant representation learning meta learning wm3c identifies and utilizes causal dynamics among composable elements facilitating robust adaptation tasks our integrates language compositional modality decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions our practical implementation uses masked autoencoder mutual information constraints and adaptive sparsity regularization capture high level semantic information and disentangle transition dynamics,2025-08-26T02:23:21.431132
84,Neural Operator Learning for Inverse Problems in Geophysics,"Solving inverse problems in geophysics, such as seismic imaging or subsurface parameter estimation, is computationally expensive and critical for resource exploration and hazard assessment. Traditional methods rely on iterative solvers that are slow and require extensive domain expertise. Deep learning, particularly Neural Operators, offers a promising alternative by directly learning the mapping between infinite-dimensional function spaces. We propose a Neural Operator Learning framework for Inverse Problems in Geophysics (NO-IPG). Our approach trains a physics-informed neural operator to directly map observational data (e.g., seismograms, gravity measurements) to underlying geophysical properties (e.g., velocity models, density distributions). The operator is regularized by the known forward physics model, ensuring consistency with physical laws. Experiments on seismic inversion and groundwater flow modeling demonstrate NO-IPG achieves superior reconstruction accuracy and orders-of-magnitude faster inference compared to conventional iterative inverse solvers, accelerating scientific discovery in geosciences.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,9299,Robust Latent Neural Operators for a Family of Systems with Sparse Observations,"Neural operator methods have achieved significant success in the efficient simulation and inverse problems of complex systems by learning a mapping between two infinite-dimensional Banach spaces. However, existing methods still exhibit room for optimization in terms of robustness and modeling accuracy. Specifically, existing methods are characterized by sensitivity to noise and a tendency to overlook the importance of sparse observations. Therefore, we propose a robust latent neural operator based on the variational autoencoder framework. In this method, an encoder based on recurrent neural networks effectively extracts sequential information and dynamical characteristics embedded in sparse observations. Subsequently, a neural operator in latent space and a decoder facilitate the modelling of the original system. Additionally, for certain higher-dimensional systems, opting for a lower-dimensional latent space can reduce task complexity while still maintaining satisfactory modeling performance. We conduct experiments across several representative systems, and the results validate that our method achieves superior modeling accuracy and enhanced robustness compared to the state of the art baseline approaches.",ICLR.cc/2025/Conference,5.0,nan,0.8423,traditional methods rely iterative solvers that are slow and require extensive domain expertise deep learning neural operators offers promising alternative directly learning the mapping between infinite dimensional function spaces neural operator learning for inverse problems geophysics ipg our trains physics informed neural operator directly map observational data,neural operator methods have achieved significant success the efficient simulation and inverse problems complex systems learning mapping between two infinite dimensional banach spaces however existing methods still exhibit room for optimization terms robustness and modeling therefore robust latent neural operator the variational autoencoder this encoder recurrent neural networks extracts sequential information and dynamical characteristics embedded sparse observations subsequently neural operator latent space and decoder facilitate the modelling the original conduct experiments across several representative systems and the that our achieves superior modeling and enhanced robustness compared the state the art approaches,2025-08-26T02:23:21.431137
85,Privacy-Preserving Continual Learning via Decentralized Federated Knowledge Distillation,"Continual learning (CL) in privacy-sensitive scenarios, such as healthcare or finance, presents unique challenges, as models must learn from new data streams without forgetting old knowledge, all while preserving data privacy. Federated learning (FL) is a promising approach, but direct parameter sharing can still leak information. We propose Privacy-Preserving Continual Learning via Decentralized Federated Knowledge Distillation (PCL-DFKD), a novel framework that enables lifelong learning across decentralized clients with strong privacy guarantees. PCL-DFKD uses a teacher-student knowledge distillation approach, where clients only share ""soft labels"" or logits derived from their local models, rather than raw data or gradients. A decentralized aggregation mechanism allows clients to distill knowledge from each other's soft labels, preserving privacy while enabling knowledge transfer and preventing catastrophic forgetting of old tasks. Experiments on various sequential task learning benchmarks (e.g., Split CIFAR-100, EMNIST) demonstrate PCL-DFKD achieves competitive CL performance with significantly enhanced privacy compared to existing FL and CL methods.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1668,pMixFed: Mixing up model coefficients for Efficient Personalized Federated Learning,"Federated Learning  enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy  and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses   data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges,  we propose ``pMixFed'', a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model,  gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate   its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of  heterogeneity when using pMixFed as compared with the state-of-the-art PFL models.",ICLR.cc/2025/Conference,2.0,nan,0.8771,continual learning privacy sensitive scenarios such healthcare finance presents unique challenges models must learn from data streams forgetting old knowledge all while preserving data privacy federated learning promising but direct parameter sharing can still leak information privacy preserving continual learning decentralized federated knowledge distillation pcl dfkd that enables lifelong learning across decentralized clients strong privacy guarantees pcl dfkd uses teacher student knowledge distillation where clients only share soft labels logits derived from their local models rather than raw data gradients decentralized aggregation mechanism allows clients distill knowledge from each other soft labels preserving privacy while enabling knowledge transfer and preventing catastrophic forgetting old tasks experiments various sequential task learning benchmarks,federated learning enables decentralized collaborative learning machine learning models which presents challenges such data privacy and client drift for heterogeneous data personalized federated learning pfl addresses data heterogeneity tackling these issues balancing generalization and personalization level adaptive partitioning between shared and personalized layers the gradual transition personalization allow seamless adaptation local clients improved generalization across clients and mitigation catastrophic forgetting,2025-08-26T02:23:21.431140
86,Causal Attention for Interpretable and Robust Prediction in Tabular Data,"Deep learning for tabular data often struggles with interpretability and robustness, frequently relying on spurious correlations rather than true causal links. Standard attention mechanisms can highlight important features but don't explain *why* they are important causally. We propose Causal Attention (CA), a novel mechanism for interpretable and robust prediction in tabular data. CA learns attention weights that correspond to the causal influence of features on the prediction. This is achieved by regularizing the attention mechanism using a differentiable proxy for causal effects, derived from techniques like back-door adjustment or instrumental variables. The model is encouraged to attend to features that are causally related to the outcome, rather than mere correlates. Experiments on diverse tabular datasets (e.g., credit risk, medical diagnosis) demonstrate CA leads to more robust predictions under interventions and provides more trustworthy explanations compared to standard attention models, highlighting genuine causal drivers of decisions.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,215,MaskTab: Masked Tabular Data Modeling for Learning with Missing Features,"Tabular machine learning has garnered increasing attention due to its practical value. Unlike the complete and standardized data often assumed in academia, tabular data primarily originates from industrial contexts and usually faces the issue of incomplete data samples, i.e., some features of a sample may be unpredictably missing. In this work, we introduce MaskTab, a masked tabular data modeling framework designed to facilitate model learning despite missing features. Instead of pursuing to accurately restore missing features like existing imputation methods, we jointly approach missing feature modeling and downstream tasks (e.g., classification) with a unified objective. Concretely, we propose to randomly drop out some solid features during training, equipped with a missing-related masked attention mechanism, to help the model rely more on trustworthy features when making decisions. Experiments on the very recent industry-grade benchmark, TabReD, suggest that our method surpasses the second DNN-based competitor by a clear margin, demonstrating its effectiveness and robustness in real-world scenarios. We will release the code and the model to facilitate reproduction.",ICLR.cc/2025/Conference,3.25,nan,0.8210,deep learning for tabular data often struggles interpretability and robustness frequently relying spurious correlations rather than true causal links standard attention mechanisms can highlight important features but don explain why they are important causally causal attention mechanism for interpretable and robust prediction tabular data learns attention weights that correspond the causal influence features the prediction this achieved regularizing the attention mechanism differentiable proxy for causal effects derived from techniques like back door adjustment instrumental variables credit risk medical diagnosis leads more robust predictions under interventions and provides more trustworthy explanations compared standard attention models highlighting genuine causal drivers decisions,tabular machine learning has garnered increasing attention due its practical value this masktab masked tabular data modeling designed facilitate learning despite missing features instead pursuing accurately restore missing features like existing imputation methods jointly missing feature modeling and downstream tasks concretely randomly drop out some solid features during training equipped missing related masked attention mechanism help the rely more trustworthy features when making decisions experiments the very recent industry grade tabred suggest that our surpasses the second dnn based competitor clear margin demonstrating its effectiveness and robustness real world scenarios,2025-08-26T02:23:21.431145
87,Multi-Instance Learning with Hierarchical Attention for Weakly Supervised Video Localization,"Weakly supervised video localization, the task of identifying temporal segments of activities in videos given only video-level labels, is challenging due to the temporal ambiguity and the presence of irrelevant segments. Existing methods often rely on handcrafted attention mechanisms or struggle with long, complex videos. We propose Multi-Instance Learning with Hierarchical Attention (MIL-HA) for robust weakly supervised video localization. MIL-HA frames video as a bag of temporal segments (instances) and employs a hierarchical attention network that first identifies salient moments within segments and then aggregates segment-level information to predict the video-level label. Crucially, our hierarchical attention mechanism is end-to-end differentiable and learns to pinpoint the most discriminative segments for each activity. Experiments on standard benchmarks (e.g., THUMOS14, ActivityNet) demonstrate MIL-HA achieves state-of-the-art performance in weakly supervised action localization, effectively handling background noise and precisely identifying activity boundaries with minimal supervision.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8843,Dense Video Object Captioning from Disjoint Supervision,"We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic.",ICLR.cc/2025/Conference,7.5,True,0.8464,weakly supervised video localization the task identifying temporal segments activities videos given only video level labels challenging due the temporal ambiguity and the presence irrelevant segments existing methods often rely handcrafted attention mechanisms struggle long complex videos multi instance learning hierarchical attention mil for robust weakly supervised video localization mil frames video bag temporal segments instances and employs hierarchical attention network that first identifies salient moments within segments and then aggregates segment level information predict the video level label crucially our hierarchical attention mechanism end end differentiable and learns pinpoint the most discriminative segments for each activity thumos14 activitynet mil achieves state the art weakly supervised action localization handling background noise and precisely identifying activity boundaries minimal supervision,this task unifies spatial and temporal localization video whilst also requiring fine grained visual understanding that best described natural language although each pretraining task only provides weak supervision they are complementary and when combined noteworthy zero shot ability and serve strong initialization for additional finetuning further improve,2025-08-26T02:23:21.431152
88,Efficient and Robust Neural Point Set Generation via Score-Based Diffusion Models,"Generating high-quality 3D point clouds is crucial for computer graphics, robotics, and CAD, but existing methods often struggle with generating diverse, geometrically accurate, and topologically consistent shapes. While GANs and VAEs have been adapted for point clouds, they can suffer from mode collapse or blurry results. We propose Efficient and Robust Neural Point Set Generation (ER-NPSG) via score-based diffusion models. ER-NPSG frames point cloud generation as an iterative denoising process in the point cloud space. Our method leverages a novel graph-convolutional network to estimate the score function (gradient of the log-probability density) of data-corrupted point clouds, allowing for the sampling of high-quality shapes by iteratively removing noise. The model is robust to variations in point density and local geometry. Experiments on ShapeNet and other 3D datasets demonstrate ER-NPSG generates diverse, high-fidelity, and geometrically accurate point clouds, outperforming existing generative models in terms of quality, diversity, and coverage metrics.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,8652,ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors,"3D point clouds directly collected from objects through sensors are often incomplete due to self-occlusion. Conventional methods for completing these partial point clouds rely on manually organized training sets and are usually limited to object categories seen during training. In this work, we propose a test-time framework for completing partial point clouds across unseen categories without any requirement for training. Leveraging point rendering via Gaussian Splatting, we develop techniques of Partial Gaussian Initialization, Zero-shot Fractal Completion, and Point Cloud Extraction that utilize priors from pre-trained 2D diffusion models to infer missing regions and extract uniform completed point clouds. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects. Our project page is at \url{https://tianxinhuang.github.io/projects/ComPC/}.",ICLR.cc/2025/Conference,7.0,True,0.8388,efficient and robust neural point set generation npsg score based diffusion models npsg frames point cloud generation iterative denoising process the point cloud space our leverages graph convolutional network estimate the function gradient the log probability density data corrupted point clouds allowing for the sampling high quality shapes iteratively removing noise,leveraging point rendering gaussian splatting techniques partial gaussian initialization zero shot fractal completion and point cloud extraction that utilize priors from pre trained diffusion models infer missing regions and extract uniform completed point clouds,2025-08-26T02:23:21.431157
89,Active Learning for Regression Tasks with Uncertainty-Aware Ensemble Diversity,"Active learning (AL) significantly reduces labeling costs by strategically selecting informative samples, but its application to regression tasks is less explored than classification. Regression AL often struggles with quantifying uncertainty in continuous output spaces and ensuring diversity in selected samples, especially for complex, multi-modal functions. We propose Active Learning for Regression with Uncertainty-Aware Ensemble Diversity (AL-UED), a novel framework that combines ensemble-based uncertainty estimation with a diversity-promoting acquisition function. AL-UED maintains an ensemble of deep neural networks to estimate epistemic uncertainty and identify regions of high disagreement. Our acquisition function then selects samples that not only maximize uncertainty but also ensure comprehensive coverage of the input space and representativeness across different uncertainty modes, preventing redundant queries. Experiments on diverse real-world regression datasets (e.g., material properties, environmental modeling) demonstrate AL-UED significantly outperforms standard AL strategies, achieving higher predictive accuracy with fewer labeled samples.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,1741,ABAS-RAL: Adaptive BAtch Size using Reinforced Active Learning,"Active learning reduces annotation costs by selecting the most informative samples, however fixed batch sizes used in traditional methods often lead to inefficient use of resources. We propose Adaptive BAtch Size using Reinforced Active Learning, a novel approach that dynamically adjusts batch sizes based on model uncertainty and performance. By framing the annotation process as a Markov Decision Process, the proposed method employs reinforcement learning to optimize batch size selection, using two distinct policies: one targeting precision and budget, and the other for adapting the batch size based on learning progress. The proposed method is evaluated on both CIFAR-10, CIFAR-100 and MNIST datasets. The performance is measured across multiple metrics, including precision, accuracy, recall, F1-score, and annotation budget. Experimental results demonstrate that the proposed method consistently reduces annotation costs while maintaining or improving performance compared to fixed-batch Active Learning methods, achieving higher sample selection efficiency without compromising model quality.",ICLR.cc/2025/Conference,3.75,False,0.8448,active learning reduces labeling costs strategically selecting informative samples but its application regression tasks less explored than classification active learning for regression uncertainty aware ensemble diversity ued that combines ensemble based uncertainty estimation diversity promoting acquisition function ued maintains ensemble deep neural networks estimate epistemic uncertainty and identify regions high disagreement,active learning reduces annotation costs selecting the most informative samples however fixed batch sizes used traditional methods often lead inefficient use resources framing the annotation process markov decision process the proposed employs reinforcement learning optimize batch size selection two distinct policies one targeting and budget and the other for adapting the batch size learning progress experimental that the proposed consistently reduces annotation costs while maintaining improving compared fixed batch active learning methods achieving higher sample selection efficiency compromising quality,2025-08-26T02:23:21.431159
90,Towards Generalizable Graph-to-Graph Translation with Latent Alignment Networks,"Graph-to-graph translation, mapping one graph structure to another (e.g., molecule to reaction product, scene graph to 3D mesh), is a challenging task requiring robust understanding of structural and semantic correspondences. Existing methods often rely on specific graph types or struggle to generalize to unseen graph topologies. We propose Generalizable Graph-to-Graph Translation (GGT) with Latent Alignment Networks. GGT learns a shared latent space where both input and output graphs are represented. A novel latent alignment network ensures that structurally similar input graphs map to similar latent codes, regardless of their specific features, and that these codes can be decoded into corresponding target graphs. This latent space acts as an intermediate, canonical representation. Experiments on diverse graph translation tasks (e.g., molecule optimization, program synthesis from logical forms) demonstrate GGT's ability to generalize to novel graph structures and types, outperforming traditional graph translation models in terms of both structural and semantic fidelity.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4494,Improving Molecule-Language Alignment with Hierarchical Graph Tokenization,"Recently there has been a surge of interest in extending the success of large language models (LLMs) to graph modality, such as molecules. As LLMs are predominantly trained with 1D text data, most existing approaches adopt a graph neural network to represent a molecule as a series of node tokens and feed these tokens to LLMs for molecule-language alignment. Despite achieving some successes, existing approaches have overlooked the hierarchical structures that are inherent in molecules. Specifically, in molecular graphs, the high-order structural information contains rich semantics of molecular functional groups, which encode crucial biochemical functionalities of the molecules. We establish a simple benchmark showing that neglecting the hierarchical information in graph tokenization will lead to subpar molecule-language alignment and severe hallucination in generated outputs. To address this problem, we propose a novel strategy called HIerarchical GrapH Tokenization (HIGHT). HIGHT employs a hierarchical graph tokenizer that extracts and encodes the hierarchy of node, motif, and graph levels of informative tokens to improve the graph perception of LLMs. HIGHT also adopts an augmented molecule-language supervised fine-tuning dataset, enriched with the hierarchical graph information, to further enhance the molecule-language alignment. Extensive experiments on **14** molecule-centric benchmarks confirm the effectiveness of HIGHT in reducing hallucination by **40%**, as well as significant improvements in various molecule-language downstream tasks.",ICLR.cc/2025/Conference,4.5,False,0.8320,molecule reaction product scene graph mesh challenging task requiring robust understanding structural and semantic correspondences latent alignment network ensures that structurally similar input graphs map similar latent codes regardless their specific features and that these codes can decoded into corresponding target graphs this latent space acts intermediate canonical representation molecule optimization program synthesis from logical forms ggt ability generalize graph structures and types outperforming traditional graph translation models terms both structural and semantic fidelity,recently there has been surge interest extending the success large language models llms graph modality such molecules llms are predominantly trained text data most existing approaches adopt graph neural network represent molecule series node tokens and feed these tokens llms for molecule language alignment hight also adopts augmented molecule language supervised fine tuning enriched the hierarchical graph information further enhance the molecule language alignment,2025-08-26T02:23:21.431163
91,Energy-Based Policy Optimization for Safe Reinforcement Learning,"Guaranteeing safety during exploration and deployment is a paramount concern in Reinforcement Learning (RL), especially in real-world applications where violating constraints can lead to catastrophic consequences. Existing safe RL methods often rely on constrained optimization or risk-averse objectives, which can be conservative or computationally expensive. We propose Energy-Based Policy Optimization (EBPO) for Safe Reinforcement Learning, a novel framework that intrinsically embeds safety constraints into the policy learning process. EBPO defines a learnable energy function over state-action pairs, where low energy corresponds to safe and high-reward actions, and high energy corresponds to unsafe actions. The policy is then optimized to sample actions with low energy, implicitly avoiding unsafe regions of the state-action space. This energy-based formulation provides a natural way to integrate safety without explicit constrained optimization. Experiments on diverse safe RL benchmarks (e.g., robotic navigation with obstacle avoidance) demonstrate EBPO efficiently learns safe and high-performing policies, significantly reducing constraint violations compared to state-of-the-art safe RL algorithms.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,2507,Safety Representations for Safer Policy Learning,"Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.",ICLR.cc/2025/Conference,6.25,True,0.8860,guaranteeing safety during exploration and deployment paramount concern reinforcement learning real world applications where violating constraints can lead catastrophic consequences existing safe methods often rely constrained optimization risk averse objectives which can conservative computationally expensive energy based policy optimization ebpo for safe reinforcement learning that intrinsically embeds safety constraints into the policy learning process this energy based formulation provides natural way integrate safety explicit constrained optimization,reinforcement learning algorithms necessitate extensive exploration the state space find optimal policies existing safe exploration methods attempt mitigate this imposing constraints which often overly conservative behaviours and inefficient learning augmenting the state features these safety representations our naturally encourages safer exploration being excessively cautious resulting more efficient and safer policy learning safety critical scenarios,2025-08-26T02:23:21.431167
92,Neuro-Symbolic Model Checking for Neural Network Verification with Temporal Logic,"Verifying complex temporal properties of recurrent neural networks (RNNs) or sequential deep learning models, crucial for safety-critical systems, remains largely an open problem. Existing verification tools primarily focus on static properties of feedforward networks. We propose Neuro-Symbolic Model Checking (NSMC), a novel framework that combines deep learning with formal methods to verify temporal logic properties of neural networks. NSMC translates the dynamics of a neural network into a set of symbolic constraints and then uses a differentiable, approximate model checker to determine if these constraints satisfy a given temporal logic specification (e.g., LTL, CTL). This allows for verification of properties like ""the system will eventually reach a safe state"" or ""an unsafe state is never reached."" Experiments on verifying properties of RNN-based controllers and state estimators demonstrate NSMC can effectively identify and analyze violations of temporal safety properties, offering a powerful tool for formally reasoning about dynamic neural systems.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5928,VeriFlow: Modeling Distributions for Neural Network Verification,"Formal verification has emerged as a promising method to ensure the safety and reliability of neural networks.
Naively verifying a safety property amounts to ensuring the safety of a neural network for the whole input space irrespective of any training or test set.
However, this also implies that the safety of the neural network is checked even for inputs that do not occur in the real-world and have no meaning at all, often resulting in spurious errors.
To tackle this shortcoming, we propose the VeriFlow architecture as a flow based density model tailored to allow any verification approach to restrict its search to the some data distribution of interest.
We argue that our architecture is particularly well suited for this purpose because of two major properties. 
First, we show that the transformation and log-density function that are defined by our model are piece-wise affine. Therefore, the model allows the usage of verifiers based on SMT with linear arithmetic.
Second, upper density level sets (UDL) of the data distribution take the shape of an $L^p$-ball in the latent space. As a consequence, representations of UDLs specified by a given probability are effectively computable in latent space. This allows the use of SMT and abstract interpretation approaches with fine-grained, probabilistically interpretable, control regarding on how (a)typical the inputs subject to verification are.",ICLR.cc/2025/Conference,6.0,False,0.8150,verifying complex temporal properties recurrent neural networks rnns sequential deep learning models crucial for safety critical systems remains largely open problem neuro symbolic checking nsmc that combines deep learning formal methods verify temporal logic properties neural networks nsmc translates the dynamics neural network into set symbolic constraints and then uses differentiable approximate checker determine these constraints satisfy given temporal logic specification experiments verifying properties rnn based controllers and state estimators nsmc can identify and violations temporal safety properties offering powerful tool for formally reasoning about dynamic neural systems,formal verification has emerged promising ensure the safety and reliability neural networks naively verifying safety property amounts ensuring the safety neural network for the whole input space irrespective any training set however this also implies that the safety the neural network checked even for inputs that not occur the real world and have meaning all often resulting spurious errors,2025-08-26T02:23:21.431172
93,Parameter-Efficient Fine-Tuning with Latent Adapter Pooling for Cross-Modal Transfer,"Transferring knowledge from large pre-trained models to new, typically smaller, cross-modal tasks (e.g., visual question answering from image/text models) often requires extensive fine-tuning. Parameter-efficient fine-tuning (PEFT) methods reduce this cost but often treat modalities independently or rely on simple concatenation, limiting their effectiveness for complex cross-modal interactions. We propose Parameter-Efficient Fine-Tuning with Latent Adapter Pooling (PEFT-LAP), a novel approach for efficient cross-modal transfer. PEFT-LAP introduces a small, trainable adapter module for each modality that projects its features into a shared latent space. These latent features are then combined using a novel ""latent adapter pooling"" mechanism that learns to dynamically weight and fuse information based on cross-modal relevance. This allows for fine-grained interaction with minimal trainable parameters. Experiments on diverse cross-modal tasks (e.g., VQA, image-text retrieval) demonstrate PEFT-LAP achieves performance competitive with full fine-tuning, with orders of magnitude fewer trainable parameters, significantly reducing the cost of deploying multi-modal foundation models.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4647,Towards Optimal Adapter Placement for Efficient Transfer Learning,"Parameter-efficient transfer learning (PETL) aims to adapt pre-trained models to new downstream tasks while minimizing the number of fine-tuned parameters. Adapters, a popular approach in PETL, inject additional capacity into existing networks by incorporating low-rank projections, achieving performance comparable to full fine-tuning with significantly fewer parameters. This paper investigates the relationship between the placement of an adapter and its performance. We observe that adapter location within a network significantly impacts its effectiveness, and that the optimal placement is task-dependent. To exploit this observation, we introduce an extended search space of adapter connections, including long-range and recurrent adapters. We demonstrate that even randomly selected adapter placements from this expanded space yield improved results, and that high-performing placements often correlate with high gradient rank. Our findings reveal that a small number of strategically placed adapters can match or exceed the performance of the common baseline of adding adapters in every block, opening a new avenue for research into optimal adapter placement strategies.",ICLR.cc/2025/Conference,5.0,False,0.8597,transferring knowledge from large pre trained models smaller cross modal tasks parameter efficient fine tuning latent adapter pooling peft lap for efficient cross modal transfer,parameter efficient transfer learning petl aims adapt pre trained models downstream tasks while minimizing the number fine tuned parameters observe that adapter location within network impacts its effectiveness and that the optimal placement task dependent,2025-08-26T02:23:21.431175
94,Self-Supervised Disentanglement of Latent Skill Dynamics for Generalizable Robotic Control,"Learning generalizable and reusable skills in robotic control is crucial for open-ended tasks, but often requires extensive demonstrations or careful reward engineering. Disentangling underlying skill dynamics from mere observations can improve transferability. We propose Self-Supervised Disentanglement of Latent Skill Dynamics (SD-LSD), a novel framework that learns a disentangled latent space of primitive skill dynamics directly from unstructured, unlabeled robot interaction data. SD-LSD trains a deep generative model to encode high-dimensional observations into a latent space where distinct dimensions correspond to independent skill parameters (e.g., trajectory shape, speed, target location). This disentanglement is enforced through information-theoretic objectives combined with temporal consistency constraints. The learned disentangled skills can then be composed and re-used for novel tasks with minimal fine-tuning. Experiments on simulated robotic manipulation tasks demonstrate SD-LSD learns robust and semantically meaningful skill primitives, leading to superior generalization and faster learning of complex behaviors.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,4890,Constrained Skill Discovery: Quadruped Locomotion with Unsupervised Reinforcement Learning,"Representation learning and unsupervised skill discovery can allow robots to acquire diverse and reusable behaviors without the need for task-specific rewards. In this work, we learn a latent representation by maximizing the mutual information between skills and states subject to a distance constraint, using unsupervised reinforcement learning. Our method improves upon prior constrained skill discovery methods by replacing the latent transition maximization with a norm-matching objective. This not only results in a much a richer state space coverage, but allows the robot to learn more stable and easily controllable locomotive behaviors. In robotics this is particularly important, because state transition-maximizing behaviors can result in highly dangerous motions. We successfully deployed the learned policy on a real ANYmal quadruped robot and demonstrated that the robot can accurately reach arbitrary points of the Cartesian state space in a zero-shot manner, using only an intrinsic skill discovery and standard regularization rewards.",ICLR.cc/2025/Conference,5.25,False,0.8635,learning generalizable and reusable skills robotic control crucial for open ended tasks but often requires extensive demonstrations careful reward engineering lsd trains deep generative encode high dimensional observations into latent space where distinct dimensions correspond independent skill parameters experiments simulated robotic manipulation tasks lsd learns robust and semantically meaningful skill primitives leading superior generalization and faster learning complex behaviors,representation learning and unsupervised skill discovery can allow robots acquire diverse and reusable behaviors the need for task specific rewards this learn latent representation maximizing the mutual information between skills and states subject distance constraint unsupervised reinforcement learning deployed the learned policy real anymal quadruped robot and demonstrated that the robot can accurately reach arbitrary points the cartesian state space zero shot manner only intrinsic skill discovery and standard regularization rewards,2025-08-26T02:23:21.431182
95,Quantifying and Mitigating Data Leakage in Vision Language Models,"Vision-Language Models (VLMs) like CLIP and DALL-E have demonstrated impressive capabilities, but their training on massive, unfiltered web-scale datasets raises significant concerns about privacy and data leakage. Quantifying *what* information about their training data these models memorize and potentially reveal is crucial. We propose a novel framework for Quantifying and Mitigating Data Leakage (QMDL) in VLMs. QMDL adapts membership inference attacks and uses reconstruction-based metrics to identify specific training samples (e.g., unique image-text pairs) that VLMs can nearly perfectly reproduce or identify. We then propose a mitigation strategy based on differential privacy-inspired noise injection during the most sensitive stages of pre-training, or selective forgetting mechanisms. Experiments on public VLM checkpoints and synthetic datasets demonstrate significant data leakage, and our QMDL framework effectively quantifies this. Our mitigation strategies are shown to reduce leakage with a controlled trade-off in utility, paving the way for more privacy-aware large model training.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,False,,Neural Electrostatics: A 3D Physics-Informed Boundary Element Poisson Equation Solver,"Electrostatics solvers relate an imposed voltage to a
corresponding charge density. Current classical methods require fine
discretization and scale poorly due to the construction of a large linear system
of equations. We recast the problem using neural networks and introduce
neural electrostatics, a hybrid 3D boundary element method (BEM). By using the
boundary element form, we are able to overcome many shortcomings of previous
neural solvers, such as learning trivial solutions and balancing loss terms
between the domain and boundary, at the cost of introducing a large integral
containing a singular kernel. We handle this singularity by locally
transforming the integral into polar coordinates and applying a numerical
quadrature. We also show that previous neural solver sampling methods are unable
to minimize the PDE residual, and propose a variational adaptive sampling
method. This technique is able to reduce mean absolute error by 5 times, while
keeping training time constant. Extensive scaling and ablation studies are
performed to justify our method. Results show that our method learns a charge
distribution within 1.2 $pC/m^2$ of mean absolute error from a classical BEM
solver, while using 25 times fewer rectangular elements.",ICLR.cc/2025/Conference,4.0,False,0.0000,,recast the problem neural networks and neural electrostatics hybrid boundary element bem the boundary element form are able overcome many shortcomings previous neural solvers such learning trivial solutions and balancing loss terms between the domain and boundary the cost introducing large integral containing singular kernel also that previous neural solver sampling methods are unable minimize the pde residual and variational adaptive sampling,2025-08-26T02:23:21.431184
96,Adaptive Channel Pruning for Spiking Neural Networks with Event-Driven Saliency,"Spiking Neural Networks (SNNs) are gaining interest for their energy efficiency on neuromorphic hardware, but their deep versions often suffer from high latency and memory footprint. Standard pruning techniques for ANNs are not directly applicable due to SNNs' event-driven and sparse communication. We propose Adaptive Channel Pruning for Spiking Neural Networks (ACP-SNN), a novel method that prunes SNN channels based on event-driven saliency. ACP-SNN introduces a differentiable saliency score that quantifies the contribution of each channel to the overall spiking activity and downstream task performance, taking into account the temporal dynamics of spikes. This saliency score is then used to adaptively prune less important channels during training, leading to a compact and efficient SNN. Experiments on neuromorphic datasets (e.g., N-MNIST, DVS-CIFAR10) demonstrate ACP-SNN achieves significant sparsity (up to 80% channel reduction) with minimal accuracy loss, while simultaneously reducing inference latency and energy consumption on simulated neuromorphic hardware.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,10106,Discretized Quadratic Integrate-and-Fire Neuron Model for Direct Training of Spiking Neural Networks,"Spiking Neural Networks (SNNs) are a promising alternative to traditional artificial neural networks, offering significant energy-saving potential. Conventional SNN approaches typically utilize the Leaky Integrate-and-Fire (LIF) neuron model, where voltage decays linearly, decreasing proportionally to its current value. However, this linear decay can inadvertently increase energy consumption and reduce model performance due to extraneous spiking activity. To address these limitations, we introduce the discretized Quadratic Integrate-and-Fire (QIF) neuron model, which applies a non-linear transformation to the voltage proportional to its magnitude. The QIF neuron model achieves substantial energy reductions, ranging from $1.43 - 4.21\times$ compared to the LIF neuron model. On static datasets (CIFAR-10, CIFAR-100) and neuromorphic datasets (CIFAR-10 DVS, N-Caltech-101, N-Cars, DVS128-Gesture), the QIF neuron model demonstrates competitive performance and improved accuracy over state-of-the-art results. Furthermore, the QIF neuron model produces smoother loss landscapes and larger local minima, leading to faster training convergence. Our findings suggest that the QIF neuron model offers a promising alternative to the widely adopted LIF neuron model.",ICLR.cc/2025/Conference,4.6,nan,0.8996,spiking neural networks snns are gaining interest for their energy efficiency neuromorphic hardware but their deep versions often suffer from high latency and memory footprint adaptive channel pruning for spiking neural networks acp snn that prunes snn channels event driven saliency,spiking neural networks snns are promising alternative traditional artificial neural networks offering significant energy saving potential,2025-08-26T02:23:21.431188
97,Knowledge Graph Completion with Implicit Relational Paths and Meta-Path Embeddings,"Knowledge Graph (KG) completion, predicting missing links, is vital for question answering and recommender systems. Existing methods often rely on local neighborhood information or pre-defined meta-paths, struggling with sparse KGs or complex, multi-hop reasoning. We propose Knowledge Graph Completion with Implicit Relational Paths (KG-IRP), a novel framework that discovers and embeds implicit relational paths. KG-IRP uses a deep reinforcement learning agent to dynamically explore and infer relevant multi-hop paths between entities, even when they are not explicitly present. Concurrently, a ""meta-path embedding"" module learns a latent representation for these inferred paths, capturing their semantic meaning. This allows the model to leverage complex relational patterns beyond immediate neighbors. Experiments on large-scale KGs (e.g., Freebase, WN18RR) demonstrate KG-IRP significantly outperforms state-of-the-art KG completion methods, especially for low-resource relations and distant entities, showcasing superior reasoning capabilities over implicit graph structures.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5087,AutoRegressive Knowledge Base Completion,"Despite their large sizes, many Knowledge Graphs (KGs) remain highly incomplete. This problem has motivated numerous approaches to $\textit{complete}$ the KGs by embedding them in a latent space to find the missing links. Although these methods show promising performance, a general limitation is that the scores given to possible links are uncalibrated and cannot be interpreted across different queries. Hence, we say they are $\textit{local}$ as they relate to a specific context. This limitation makes it non-trivial to deduce the truth value of the links and to answer complex queries. Another limitation is that their learning depends on negative sampling, which is challenging due to the Open World Assumption (OWA). 

To solve this problem, we propose a novel auto-regressive generative model that learns a joint distribution of the entities and relations of the KG without resorting to negative sampling. This distribution can be used to infer the probability that a link is sampled from the KG, which allows us to return a $\textit{global}$ score that is interpretable in different contexts. Moreover, our method has the additional advantage that it offers probabilistic semantics for complex reasoning and knowledge base completion, achieving state-of-the-art performance on link prediction with consistent scores across the entire KG.",ICLR.cc/2025/Conference,3.0,False,0.8716,knowledge graph completion predicting missing links vital for question answering and recommender systems existing methods often rely local neighborhood information pre defined meta paths struggling sparse kgs complex multi hop reasoning knowledge graph completion implicit relational paths irp that discovers and embeds implicit relational paths irp uses deep reinforcement learning agent dynamically and infer relevant multi hop paths between entities even when they are not explicitly concurrently meta path embedding module learns latent representation for these inferred paths capturing their semantic meaning freebase wn18rr irp outperforms state the art completion methods for low resource relations and distant entities showcasing superior reasoning capabilities over implicit graph structures,despite their large sizes many knowledge graphs kgs remain highly incomplete this problem has motivated numerous approaches textit complete the kgs embedding them latent space find the missing links another limitation that their learning depends negative sampling which challenging due the open world assumption owa moreover our has the additional advantage that offers probabilistic semantics for complex reasoning and knowledge base completion achieving state the art link prediction consistent scores across the entire,2025-08-26T02:23:21.431193
98,Explainable AI for Tabular Time-Series via Dynamic Causal Graph Inference,"Interpreting predictions from deep learning models on tabular time-series data (e.g., financial forecasting, industrial prognostics) is critical for decision-making. Existing XAI methods often provide static feature attributions, failing to capture dynamic, time-varying causal influences. We propose Explainable AI for Tabular Time-Series (XAI-TTS) via Dynamic Causal Graph Inference. XAI-TTS learns a time-varying causal graph among the features of the time series, representing how features influence each other and the target variable over time. This causal graph is inferred dynamically using a differentiable, attention-based mechanism that captures temporal dependencies. This allows for explanations like ""Feature X causally influenced Feature Y, which then led to the prediction at time T, with magnitude Z."" Experiments on diverse real-world time-series datasets (e.g., energy demand forecasting, patient monitoring) demonstrate XAI-TTS provides more granular, causal, and intuitive explanations than static XAI methods, offering a powerful tool for understanding and debugging complex temporal models.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,9006,F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI,"Recent research has developed a number of eXplainable AI (XAI) techniques, such as gradient-based approaches, input perturbation-base methods, and black-box explanation methods. While these XAI techniques can extract meaningful insights from deep learning models, how to properly evaluate them remains an open problem.  The most widely used approach is to perturb or even remove what the XAI method considers to be the most important features in an input and observe the changes in the output prediction. This approach, although straightforward, suffers the Out-of-Distribution (OOD) problem as the perturbed samples may no longer follow the original data distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by retraining the model with perturbed samples guided by explanations. However, using the model retrained based on XAI methods to evaluate these explainers may cause information leakage and thus lead to unfair comparisons.  We propose Fine-tuned Fidelity (F-Fidelity), a robust evaluation framework for XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus mitigating the information leakage issue, and ii) a random masking operation that ensures that the removal step does not generate an OOD input. We also design controlled experiments with state-of-the-art (SOTA) explainers and their degraded version to verify the correctness of our framework. We conduct experiments on multiple data modalities, such as images, time series, and natural language. The results demonstrate that F-Fidelity significantly improves upon prior evaluation metrics in recovering the ground-truth ranking of the explainers. Furthermore, we show both theoretically and empirically that, given a faithful explainer,  F-Fidelity metric can be used to compute the sparsity of influential input components, i.e., to extract the true explanation size.",ICLR.cc/2025/Conference,6.666666666666667,True,0.8264,interpreting predictions from deep learning models tabular time series data existing xai methods often provide static feature attributions failing capture dynamic time varying causal influences this allows for explanations like feature causally influenced feature which then led the prediction time magnitude,while these xai techniques can extract meaningful insights from deep learning models how properly them remains open problem the most used perturb even remove what the xai considers the most important features input and observe the changes the output prediction conduct experiments multiple data modalities such images time series and natural language,2025-08-26T02:23:21.431196
99,Continual Learning with Class-Incremental Skill Transfer for Embodied Agents,"Embodied agents operating in complex, evolving environments must continually acquire new skills and adapt to new tasks. A key challenge is class-incremental learning: learning new object categories or interaction types without forgetting previous ones, and transferring these new ""skills"" effectively. We propose Continual Learning with Class-Incremental Skill Transfer (CL-CIST), a novel framework for embodied agents. CL-CIST learns a library of elementary interaction skills (e.g., grasp, push, open) in a class-agnostic manner. When a new object class is encountered, the agent uses a few-shot learning approach to quickly adapt these existing skills or learn novel, class-specific affordances for the new object, without catastrophic forgetting of the general skill library. A hierarchical policy then composes these adapted skills. Experiments in simulated robotic manipulation environments demonstrate CL-CIST enables rapid, class-incremental learning of new objects and their manipulation possibilities, significantly outperforming standard continual learning and transfer learning approaches by leveraging a flexible skill abstraction.",ICLR,deep learning,gemini-2.5-flash-preview-05-20,True,5911,Singular Value Fine-tuning for Few-Shot Class-Incremental Learning,"Class-Incremental Learning (CIL) aims to learn knowledge from new classes sequentially, while rataining the knowledge obtained from previously encountered classes, thereby mitigating the challenge of Catastrophic Forgetting. In a more realistic scenario, future unseen classes may contain only a few samples, leading to a new challenge of over-fitting, which is referred to as Few-Shot Class-Incremental Learning (FSCIL). Existing works explore FSCIL from various perspectives, such as classifier calibration and backbone extension. Most of them treat the many-shot base session and incremental few-shot sessions separately, as the model tends to overfit on few-shot classes. In this paper, we propose Singular Value Fine-tuning for few-shot Class-incremental Learning (SVFCL) to constantly learn base and incremental sessions based on the pre-trained ViT encoder. SVFCL incorporates incremental adapters, each of which is attached to a corresponding pre-trained module and contains only a small number of learnable parameters, effectively reducing the risk of overfitting. Furthermore, since each adapter is task-specific, information from previous tasks is well-preserved, mitigating catastrophic forgetting.
Our experimental results demonstrate that SVFCL achieves substantial improvements over state-of-the-art methods while requiring significantly less computational overhead and epochs.",ICLR.cc/2025/Conference,4.333333333333333,nan,0.8575,key challenge class incremental learning learning object categories interaction types forgetting previous ones and transferring these skills continual learning class incremental skill transfer cist for embodied agents when object class encountered the agent uses few shot learning quickly adapt these existing skills learn class specific affordances for the object catastrophic forgetting the general skill library experiments simulated robotic manipulation environments cist enables rapid class incremental learning objects and their manipulation possibilities outperforming standard continual learning and transfer learning approaches leveraging flexible skill ion,class incremental learning cil aims learn knowledge from classes sequentially while rataining the knowledge obtained from previously encountered classes thereby mitigating the challenge catastrophic forgetting more realistic scenario future unseen classes may contain only few samples leading challenge over fitting which referred few shot class incremental learning fscil most them treat the many shot base session and incremental few shot sessions separately the tends overfit few shot classes this singular value fine tuning for few shot class incremental learning svfcl constantly learn base and incremental sessions the pre trained vit encoder,2025-08-26T02:23:21.431199
